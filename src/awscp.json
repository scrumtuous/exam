[ {
  "id" : 1,
  "query" : "awscpYou are looking to migrate your Development and Test environments to AWS.\nYou have decided to use separate AWS accounts to host each environment.\nYou plan to link each account bill to a Management AWS account using Consolidated Billing.\nTo make sure that you keep within the budget, you would like to implement a way for administrators in the Management account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts.\nIdentify which of the options will allow you to achieve this goal.",
  "answer" : "Answer - C.\nThe scenario here is asking you to give permissions to administrators in the Management account such that they can have access to stop, delete, and terminate the resources in two accounts: Dev and Test.\nTip: Remember that you always create roles in the account whose resources are to be accessed.\nIn this example, that would be Dev and Test.\nThen you create the users in the account who will be accessing the resources and give them that particular role.\nIn this example, the Management account should create the users.\nOption A is incorrect because the Management account IAM user needs to assume roles from the Dev and Test accounts.\nThe roles should have suitable permissions so that the Management account IAM user can access resources.\nOption B is incorrect because the cross-account role should be created in Dev and Test accounts, not in the Management account.\nOption C is CORRECT because (a) the cross-account role is created in Dev and Test accounts, and the users are created in the Management account given that role.\nOption D is incorrect because consolidated billing does not give access to resources in this fashion.\nFor more information on cross-account access, please visit the below URL-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nThe correct answer is A. Create IAM users in the Management account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant Management account access to the resources in the account by inheriting permissions from the Management account.\nThis option uses IAM roles to grant access to resources in the Dev and Test accounts. By creating IAM users in the Management account with full Admin permissions and cross-account roles in the Dev and Test accounts, you can grant the Management account administrators access to the resources in both accounts.\nThe cross-account roles in the Dev and Test accounts can inherit permissions from the Management account, which allows for centralized control and management of permissions. This approach is secure as it ensures that access is only granted to the specific resources required, and not all resources in the account.\nOption B is incorrect as it gives full Admin permissions to the Dev and Test accounts, which can create security risks and violate the principle of least privilege.\nOption C is incorrect because the IAM users in the Management account with \"AssumeRole\" permissions can only assume the role in the Dev and Test accounts but won't have admin permissions to stop, delete, or terminate resources.\nOption D is also incorrect because Consolidated Billing only consolidates billing information, and it doesn't grant access to resources in the Dev and Test accounts.\nIn conclusion, option A is the most appropriate option as it provides granular control over permissions, and it is secure and straightforward to manage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create IAM users in the Management account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant Management account access to the resources in the account by inheriting permissions from the Management account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create IAM users and a cross-account role in the Management account that grants full Admin permissions to the Dev and Test accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create IAM users in the Management account with the \"AssumeRole\" permissions. Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant Management account access.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Link the accounts using Consolidated Billing. This will give IAM users in the Management account access to the resources in Dev and Test accounts.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 2,
  "query" : "awscpAn administrator in your company has created a VPC with an IPv4 CIDR block 10.0.0.0/24\nNow they want to add additional address space outside of the current VPC CIDR.\nBecause there is a requirement to host more resources in that VPC.\nWhich of the below requirement can be used to accomplish this? Choose an answer from the below options.",
  "answer" : "Answer - B.\nAn existing CIDR for a VPC is not modifiable.\nHowever, you can add additional CIDR blocks, i.e., up to four secondary IPv4 CIDR blocks to an already existing VPC.Option A is incorrect because you can change the CIDR of VPC by adding up to 4 secondary IPv4 IP CIDRs to your VPC.Option B is CORRECT because you can expand your existing VPC by adding up to four secondary IPv4 IP ranges (CIDRs) to your VPC.Option C is incorrect because deleting the subnets is unnecessary.\nOption D is incorrect because this configuration would peer the VPC.\nIt will not alter the existing VPC's CIDR.\nFor more information on VPC and its FAQs, please refer to the following links-\nhttps://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-now-allows-customers-to-expand-their-existing-vpcs/ https://aws.amazon.com/vpc/faqs/\nOption B is the correct answer:\nTo add additional address space outside of the current VPC CIDR, you can expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VPC. Amazon VPC allows you to expand the IP address range of your VPC by adding secondary IPv4 CIDR blocks to your VPC. This will enable you to use additional IP address space for your resources within the same VPC.\nTo add a secondary CIDR block to your VPC, you can follow these steps:\n1. Open the Amazon VPC console.\n2. In the navigation pane, choose Your VPCs.\n3. Select the VPC that you want to add the secondary CIDR block to.\n4. Choose Actions, and then choose Edit CIDRs.\n5. In the Edit CIDRs dialog box, choose Add IPv4 CIDR.\n6. In the Add IPv4 CIDR dialog box, enter the IPv4 CIDR block that you want to add to the VPC.\n7. Choose Save.\nAfter you add a secondary CIDR block to your VPC, you can create subnets in the new CIDR block. These subnets will be associated with the VPC and can be used for your resources.\nOption A is incorrect because it is possible to expand the IP address range of a VPC by adding secondary CIDR blocks, and it is not necessary to terminate and recreate the VPC.\nOption C is incorrect because deleting all the subnets in a VPC does not expand the VPC's IP address range.\nOption D is incorrect because creating a new VPC is not necessary when you can expand the IP address range of an existing VPC by adding secondary CIDR blocks. Additionally, connecting two VPCs would require setting up VPC peering, which can add complexity to your network configuration.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You cannot change a VPC`s size. Currently, to change the size of a VPC, you must terminate your existing VPC and create a new one.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VP.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Delete all the subnets in the VPC and expand the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new VPC with a greater range and then connect the older VPC to the newer one.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 3,
  "query" : "awscpA middle-sized company is planning to migrate its on-premises servers to AWS.\nAt the moment, they have used various licenses, including windows operating system server, SQL Server, IBM Db2, SAP ERP, etc.\nAfter migration, the existing licenses should continue to work in EC2\nThe IT administrators prefer to use a centralized place to control and manage the licenses to prevent potential non-compliant license usages.\nFor example, SQL Server Standard's license only allows 50 vCPUs, which means a rule is needed to limit the number of SQL Servers in EC2\nWhich option is correct for the IT administrators to use?",
  "answer" : "Correct Answer - B.\nAWS License Manager is a central place to manage licenses in AWS EC2 and on-premises instances.\nIt contains 3 parts to use:\nDefine licensing rules.\nEnforce licensing rules.\nTrack usage.\nAWS License Manager currently integrates with Amazon EC2, allowing you to track licenses for default (shared-tenancy) EC2 instances, Dedicated Instances, Dedicated Hosts, Spot Instances, and Spot Fleet, and Auto Scaling groups.\nRefer to https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html.\nOption A is incorrect.\nBecause AWS License Manager manages the BYOL licenses.\nAlthough AWS System Manager can work together with AWS License Manager to manage licenses for on-premises servers and non-AWS public clouds, it is not the central place to provide license management.\nOption B is CORRECT: Because AWS License Manager can define licensing rules, track license usage, and enforce controls on license use to reduce the risk of license overages.\nOption C is incorrect: Because the AWS License manager should be considered first for licensing management.\nOption D is incorrect: Because AWS License Manager can manage non-Microsoft licenses.\nAccording to https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html, license Manager tracks various software products from Microsoft, IBM, SAP, Oracle, and other vendors.\nOption B, \"Define license rules in AWS License Manager for the required licenses. Enforce the license rules in EC2 and track usage in the AWS License Manager console,\" is the correct option for the IT administrators to use.\nAWS License Manager is a service that helps organizations manage their software licenses, including those acquired through bring your own license (BYOL) agreements. By using AWS License Manager, IT administrators can create and enforce license rules for their software licenses in AWS. In this case, the company has several different licenses that they need to manage, so using AWS License Manager will help them to centralize and simplify their license management.\nTo use AWS License Manager, the company will need to create license configurations for each of their software licenses. These license configurations define the rules for how the licenses can be used, such as the number of instances or vCPUs that are allowed. Once the license configurations are created, they can be applied to the instances in EC2 to enforce the license rules.\nIT administrators can use the AWS License Manager console to track the usage of their software licenses. The console provides a dashboard that displays license usage and compliance status, as well as reports on license usage over time. This allows the administrators to monitor their license usage and ensure that they are in compliance with their license agreements.\nOption A, \"Create license rules in AWS System Manager for all BYOL licenses. Use the rules to make sure that there are no non-compliant activities. Link the rules when EC2 AMI is created. System Manager console has provided license usage status,\" is not the best option in this case because AWS License Manager is specifically designed for managing software licenses, while AWS System Manager is a more general-purpose management service. While it is possible to create license rules in AWS System Manager, it may not provide the same level of license management and tracking capabilities as AWS License Manager.\nOption C, \"Use a license management blueprint to create a dedicated Lambda to control license usage. Lambda outputs the usage status to Cloudwatch Metrics which can be used by the administrators to track the status,\" is not the best option because it requires more development effort than the other options. While it is possible to create a custom solution for managing licenses using AWS Lambda and CloudWatch, it may be more complex and time-consuming than using AWS License Manager.\nOption D, \"Define and enforce license rules in AWS License Manager for the Microsoft relevant licenses such as windows, SQL Server as only Microsoft licenses are supported. For the other licenses such as IBM Db2, track the license usage in AWS System Manager,\" is not the best option because it requires the company to use multiple tools to manage their licenses. While AWS License Manager may not support all of the company's licenses, it is still the best option for managing the licenses that it does support. Using AWS System Manager to manage the other licenses may not provide the same level of license management and tracking capabilities as AWS License Manager.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create license rules in AWS System Manager for all BYOL licenses. Use the rules to make sure that there are no non-compliant activities. Link the rules when EC2 AMI is created. System Manager console has provided license usage status.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Define license rules in AWS License Manager for the required licenses. Enforce the license rules in EC2 and track usage in the AWS License Manager console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a license management blueprint to create a dedicated Lambda to control license usage. Lambda outputs the usage status to Cloudwatch Metrics which can be used by the administrators to track the status.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Define and enforce license rules in AWS License Manager for the Microsoft relevant licenses such as windows, SQL Server as only Microsoft licenses are supported. For the other licenses such as IBM Db2, track the license usage in AWS System Manager.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 4,
  "query" : "An outsourcing company is working on a government project.\nSecurity is very important to the success of the application.\nThe application is developed mainly in EC2 with several application load balancers.\nCloudFront and Route53 are also configured.\nThe major concern is that it should be able to be protected against DDoS attacks.\nThe company decides to activate the AWS Shield Advanced feature.\nTo this effect, it has hired an external consultant to 'educate' its employees on the same.\nFor the below options, which ones help the company to understand the AWS Shield Advanced plan? Select 3.",
  "answer" : "E.\nF.\nG.\nH.\nCorrect Answer - A, D, E.\nAWS Shield has two plans - AWS Shield Standard and AWS Shield Advanced.\nAWS Shield Standard:\nAWS Shield Standard activates automatically at no additional charge.\nAWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your applications.\nAWS Shield Advanced:\nFor higher levels of protection against attacks.\nIt has a subscription fee which is $ 3000 per month.\nOption A is CORRECT.\nBecause Elastic Load Balancing (ELB), Amazon CloudFront, Amazon Route 53 are all covered by AWS Shield Advanced.\nOption B is incorrect.\nBecause AWS Shield Advanced has a subscription commitment of 1 year with a base monthly fee of 3000$.\nOption C is incorrect.\nBecause Route 53 is covered by AWS Shield Advanced.\nOption D is CORRECT.\nBecause 24*7 support by the DDoS Response team is a key feature of the advanced plan.\nOption E is CORRECT.\nBecause AWS Shield Advanced integrates with AWS CloudWatch and provides relevant reports.\nOption F is incorrect.\nBecause AWS Shield is not within AWS WAF.\nPlease note that both of them help protect the AWS resources.\nAWS WAF is a web application firewall service, while AWS Shield provides expanded DDoS attack protection for the AWS resources.\nThe outsourcing company is working on a government project where security is critical. The application is developed primarily in EC2 with several application load balancers. Additionally, CloudFront and Route53 are also used. The major concern is to protect against DDoS attacks, and the company decides to activate AWS Shield Advanced feature. The company has hired an external consultant to educate its employees on the same.\nThe AWS Shield Advanced plan is a comprehensive DDoS protection service offered by AWS. It helps to protect the applications hosted on EC2, ALB, CloudFront, and Route 53 against DDoS attacks. To understand the AWS Shield Advanced plan, the following three options can be considered:\n1.\nAWS Shield Advanced plan can protect application load balancers, CloudFront, and Route53 from DDoS attacks. This is the first important aspect to understand about AWS Shield Advanced plan. It is designed to provide protection to applications hosted on EC2, ALB, CloudFront, and Route53 against DDoS attacks.\n2.\nAWS Shield Advanced plan does not have a monthly base charge. The company only needs to pay the data transfer fee. Other than that, AWS WAF includes no additional cost. This is the second important aspect to understand about AWS Shield Advanced plan. There is no monthly base charge for AWS Shield Advanced plan. The company only needs to pay the data transfer fee. Additionally, AWS WAF is included with AWS Shield Advanced plan, and there is no additional cost for it.\n3.\nRoute 53 is not covered by AWS Shield Advanced plan. However, Route 53 is able to be protected under AWS WAF. This is the third important aspect to understand about AWS Shield Advanced plan. Route53 is not covered by AWS Shield Advanced plan. However, it can be protected under AWS WAF. AWS WAF is a web application firewall that is included with AWS Shield Advanced plan.\nThe remaining options are:\n1.\nA dedicated rule in WAF should be customized. This is not directly related to understanding the AWS Shield Advanced plan. However, it is an important aspect of customizing the AWS WAF, which is included with AWS Shield Advanced plan.\n2.\n24*7 support by the DDoS Response team. Critical and urgent priority cases can be answered quickly by DDoS experts. Custom mitigations during attacks are also available. This is an important aspect of the AWS Shield Advanced plan. AWS provides 24/7 support by the DDoS response team. Critical and urgent priority cases can be answered quickly by DDoS experts. Additionally, custom mitigations during attacks are also available.\n3.\nReal-time notification of attacks is available via Amazon CloudWatch. Historical attack reports are also provided. This is another important aspect of the AWS Shield Advanced plan. AWS Shield Advanced plan provides real-time notification of attacks via Amazon CloudWatch. Additionally, historical attack reports are also provided.\n4.\nAWS Shield is a sub-feature within AWS WAF. This is incorrect. AWS Shield is not a sub-feature within AWS WAF. AWS WAF is a web application firewall that is included with AWS Shield Advanced plan.\n5.\nAWS Shield Advanced can be activated in AWS WAF console, which also provides the near real-time metrics and packet captures for attack forensics. This is another important aspect of the AWS Shield Advanced plan. AWS Shield Advanced plan can be activated in AWS WAF console, which also provides the near real-time metrics and packet captures for attack forensics.\nTherefore, the correct options to understand the AWS Shield Advanced plan are A, B, and C.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Shield Advanced plan is able to protect application load balancers, CloudFront and Route53 from DDoS attacks.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Shield Advanced plan does not have a monthly base charge. The company only needs to pay the data transfer fee. Other than that, AWS WAF includes no additional cost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Route 53 is not covered by AWS Shield Advanced plan. However, Route 53 is able to be protected under AWS WA.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A dedicated rule in WAF should be customized.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "24*7 support by the DDoS Response team. Critical and urgent priority cases can be answered quickly by DDoS experts. Custom mitigations during attacks are also available.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Real-time notification of attacks is available via Amazon CloudWatch. Historical attack reports are also provided.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Shield is a sub-feature within AWS WA.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Shield Advanced can be activated in AWS WAF console, which also provides the near real-time metrics and packet captures for attack forensics.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 5,
  "query" : "Your organization is planning to shift one of the high-performance data analytics applications running on Linux servers purchased from the 3rd party vendor to the AWS.\nCurrently, the application works in an on-premises load balancer, and all the data is stored in a very large shared file system for low-latency and high throughput purposes.\nThe management wants minimal disruption to existing service and also wants to do stepwise migration for easy rollback.\nPlease select 3 valid options from below.",
  "answer" : "E.\nCorrect Answer: C, D, E.\nOptions C, D, and E are correct because network extension via VPN or Direct Connect will allow the on-premises instances to use the AWS resources like EFS.\nEFS is elastic file storage that can be mounted on EC2 and other instances.\nIt is inherently durable and scalable.\nEFS stores the data by default at multiple availability zones.\nWith Route 53 Weighted policy, the requests can be distributed to on-premise and AWS resources easily in a controlled manner.\nOption A is INCORRECT because S3 will work as shared, durable storage.\nBut it may not be a suitable choice for low-latency, high throughput load processing.\nAs the application cannot be easily modified, presenting the S3 as a local file system will be another task and has to be done via File Storage Gateway.\nOption B is INCORRECT because the purpose is to use a shared file system solution (EFS)\nRAID1 for EBS is not necessary as the application requires data from EFS rather than the local storage.\nOption A: Saving all the data on S3 and using it as shared storage and using an application load balancer with EC2 instances to share the processing load is a valid option. However, it may not be the best option for the given scenario as S3 is not a file system, and accessing data directly from S3 can result in high latencies. Also, it may not be cost-effective to store and access large amounts of data from S3.\nOption B: Creating a RAID 1 storage using EBS and running the application on EC2 with application-level load balancers to share the processing load is a valid option. EBS provides durable block-level storage, and RAID 1 offers data redundancy. Also, using an application-level load balancer, such as ELB or ALB, can help distribute traffic across EC2 instances. However, the drawback of this option is that EBS may not provide the same level of low-latency and high throughput as the current on-premises shared file system.\nOption C: Using VPN or Direct Connect to create a link between the company premise and AWS regional data center is a valid option. This option allows the organization to establish a secure, private connection between the on-premises data center and the AWS infrastructure. This option enables the organization to migrate data and applications to the cloud while minimizing disruptions to existing services.\nOption D: Creating an EFS with provisioned throughput and sharing the storage between on-premise instances and EC2 instances is a valid option. EFS provides a managed, elastic file system that is accessible from multiple EC2 instances and can scale automatically as the storage needs grow. By using EFS with provisioned throughput, the organization can ensure low-latency and high throughput for the application. However, this option may not be suitable if the application requires a specific file system, such as NFS or SMB.\nOption E: Setting up a Route 53 record to distribute the load between on-premises and AWS load balancer with the weighted routing policy is a valid option. Route 53 is a highly available and scalable DNS service that can route traffic to multiple resources based on different routing policies. The weighted routing policy can distribute the traffic between the on-premises load balancer and the AWS load balancer based on a set percentage. However, this option may not be the best option if the organization wants to minimize disruptions to the existing services.\nIn conclusion, the most suitable options for the given scenario are B, C, and D, as they can provide low-latency and high-throughput storage, secure connectivity, and scalable storage while minimizing disruptions to existing services.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Save all the data on S3 and use it as shared storage. Use an application load balancer with EC2 instances to share the processing load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a RAID 1 storage using EBS and run the application on EC2 with application-level load balancers to share the processing load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the VPN or Direct Connect to create a link between your company premise and AWS regional data center.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an EFS with provisioned throughput and share the storage between your on-premise instances and EC2 instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Setup a Route 53 record to distribute the load between on-premises and AWS load balancer with the weighted routing policy.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 6,
  "query" : "A communication company has deployed several EC2 instances in region ap-southeast-1 which are used to monitor user activities.\nThe AWS administrator has configured an EBS lifecycle policy to create a snapshot every day for each EBS volume to preserve data.\nThe retention is configured as 5, which means the oldest snapshot will be deleted after 5 days.\nThe administrator plans to copy some snapshots manually to another region ap-southeast-2 as these snapshots contain some important data.\nCan these snapshots be retained?",
  "answer" : "Correct Answer - C.\nCopying a snapshot to a new Region is commonly used for geographic expansion, migration, disaster recovery, etc.\nEBS snapshots' lifecycle policies contain some rules.\nOne of the rules is that when you copy a policy's snapshot, the new copy is not influenced by the retention schedule.\nOption A is incorrect: Because the new snapshots will be kept.\nOption B is incorrect: Because no matter the new snapshots are in the same region or not, they can be retained.\nOption C is CORRECT: Because the new snapshots are not affected by the original policy.\nReference is in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html.\nOption D is incorrect: Because there is no delete protection option for snapshots.\nThe correct answer is B: These new snapshots can be kept only when they are copied to another region. Otherwise, they may be deleted by the retention policy. In this case, the snapshots can be kept.\nExplanation: An EBS lifecycle policy is used to create snapshots of EBS volumes automatically and manage the retention of these snapshots. In this scenario, the retention period is set to 5 days, which means that after 5 days, the oldest snapshot will be deleted to make room for a new one.\nThe AWS administrator plans to copy some of the snapshots manually to another region, ap-southeast-2. When a snapshot is copied to another region, a new snapshot is created in the destination region. This new snapshot is a separate entity from the original snapshot, and the retention schedule of the original snapshot is not carried over to the copy.\nTherefore, if the administrator wants to keep the snapshots in the ap-southeast-2 region, they must be copied manually from the ap-southeast-1 region. If they are not copied, they will be deleted after the retention period of 5 days.\nOption A is incorrect because the retention policy applies to all snapshots, including the new snapshots created from the original snapshots.\nOption C is incorrect because the retention schedule is not carried over to the copy. Therefore, the copy will not be affected by the retention policy.\nOption D is incorrect because the new snapshots in region ap-southeast-2 are not subject to the retention policy of the original snapshots. However, they will be subject to the retention policy set in the destination region. By default, there is no retention policy in the destination region, so the snapshots will not be deleted automatically unless the delete protection option is enabled.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "These new snapshots may be deleted after the retention period, as they are still affected by the retention policy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "These new snapshots can be kept only when they are copied to another region. Otherwise, they may be deleted by the retention policy. In this case, the snapshots can be kept.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "These new snapshots can be kept as the retention schedule is not carried over to the copy.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The new snapshots in region ap-southeast-2 will be deleted after 5 days unless the delete protection option is enabled.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 7,
  "query" : "You are working for a large company.\nYou have set up the AWS consolidated billing with a Management account and several member accounts.\nHowever, the management account's cost allocation report does not use the AWS generated cost allocation tags to organize the resource costs.",
  "answer" : "Answer - D.\nAWS provides two types of cost allocation tags: AWS-generated tags and user-defined tags.\nAWS defines, creates, and applies the AWS-generated tags for you, and users define, create, and apply user-defined tags.\nTo use the AWS-generated tags, a management account owner must activate them in the Billing and Cost Management console.\nWhen a management account owner activates the tag, the tag is also activated for all member accounts.\nOption A is incorrect: Because AWS-generated tags should be activated.\nOption B is incorrect: Because AWS-generated tags can only be activated in the management account.\nOption C is incorrect: Same reason as Option.\nB.\nAlso, it is not user-defined tags.\nOption D is CORRECT: Because the tag can be activated in “Billing -&gt; Cost Management.\nReferences:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/aws-tags.html\nThe correct answer for this question is D. Log in to the AWS console using the Management account and activate the AWS-generated tags in the Billing and Cost Management console.\nExplanation: AWS provides several methods to manage cost allocation and tagging for resources. One of the ways to allocate costs for resources in AWS is through the use of AWS-generated cost allocation tags. Cost allocation tags can be used to organize resources based on their application, environment, owner, or any other attribute that makes sense for your organization.\nIn this scenario, the Management account's cost allocation report does not use the AWS generated cost allocation tags to organize the resource costs. To enable AWS-generated tags, you need to activate them in the Billing and Cost Management console using the Management account.\nThe Billing and Cost Management console is the central location for managing billing and cost allocation in AWS. To activate the AWS-generated tags, follow these steps:\n1. Log in to the AWS console using the Management account.\n2. Open the Billing and Cost Management console.\n3. Choose Cost allocation tags from the left navigation menu.\n4. Choose Activate AWS Cost Allocation Tags from the top right of the screen.\n5. Select the checkbox to confirm that you understand the cost allocation implications.\n6. Choose Activate.\nThis will activate the AWS-generated cost allocation tags for the Management account, and they will be applied to all the member accounts under consolidated billing.\nOption A is incorrect because user-defined tags are not relevant to this scenario. User-defined tags are created by users and applied to resources to help organize and categorize them. They are not used for cost allocation.\nOption B is incorrect because activating AWS-generated tags using the AWS CLI is not necessary for this scenario. Activating tags through the Billing and Cost Management console is the preferred method.\nOption C is incorrect because user-defined tags are not relevant to this scenario. Activating user-defined tags in the Cost Explorer will not enable AWS-generated cost allocation tags.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the Management account to log in to the AWS console and activate the user-defined tags in the Billing and Cost Management console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For both, the Management account and member accounts, use AWS CLI to activate AWS generated tags for Billing and Cost Management.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Log in to the AWS console of both Management account and member accounts, activate the user-defined tags in Billing -> Cost Explorer -> Cost Allocation Tags.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Log in to the AWS console using the Management account and activate the AWS-generated tags in the Billing and Cost Management console.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 8,
  "query" : "A company has run a major auction platform where people buy and sell a wide range of products.\nThe platform requires that transactions from buyers and sellers get processed in exactly the order received.\nAt the moment, the platform is implemented using RabbitMQ which is a light-weighted queue system.\nThe company consulted you to migrate the on-premise platform to AWS.\nHow should you design the migration plan? (Select TWO)",
  "answer" : "Correct Answer - A, C.\nSQS has 2 types - standard queue and FIFO queue.\nIn this case, the FIFO queue should be chosen as the order of message processing is critical to the application.\nFIFO queue has the below key features.\nOption A is CORRECT: Because the SQS FIFO queue can help with the message processing in the right order.\nOption B is incorrect: Because the SQS standard queue may have an issue that some messages are handled in the wrong sequence.\nOption C is CORRECT: Because the message group ID is a feature to help with the FIFO delivery.\nMessages that belong to the same message group are always processed one by one, in a strict order relative to the message group.\nOption D is incorrect: Because deduplication ID is a method to help on preventing messages to be processed duplicately, which is not used to guarantee the message order.\nSure, I'd be happy to help!\nWhen it comes to migrating the on-premise auction platform to AWS, there are a few things to consider in terms of maintaining the requirement that transactions are processed in the exact order received.\nOne of the main considerations is the choice of messaging service. RabbitMQ, the current messaging system being used, is a lightweight queue system that can be used to manage message queues. In AWS, there are two types of message queues that can be used to process messages: SQS standard and SQS FIFO.\nSQS standard is a distributed queue system that offers high throughput, allowing for multiple consumers to receive messages simultaneously. This means that messages can be processed in a different order than they were received. Therefore, it's not a good fit for this use case.\nOn the other hand, SQS FIFO is designed to guarantee that messages are processed in the order they are received. This makes it a good fit for the requirements of the auction platform.\nWith that in mind, here are the two correct options for designing the migration plan:\nA. When the bids are received, send the bids to an SQS FIFO queue before they are processed.\nThis option is a good solution because it utilizes SQS FIFO, which ensures that messages are processed in the order they are received. By sending the bids to the SQS FIFO queue before they are processed, the company can be sure that transactions will be processed in the exact order received.\nB. When the users have submitted the bids from the frontend, the backend service delivers the messages to an SQS standard queue.\nThis option is not a good solution because, as mentioned earlier, SQS standard is not designed to guarantee message processing order. If the company were to use SQS standard, there would be no way to ensure that transactions were processed in the exact order received. Therefore, this option should not be chosen.\nC. Add a message group ID to the messages before they are sent to the SQS queue so that the message processing is in a strict order.\nThis option is a feature of SQS FIFO and is not an alternative solution. SQS FIFO automatically orders messages based on their message group ID and ensures that messages within the same group are processed in the order they were received. While this option could be useful in combination with option A, it is not a separate solution.\nD. Use an EC2 or Lambda to add a deduplication ID to the messages before the messages are sent to the SQS queue to ensure that bids are processed in the right order.\nThis option is not a good solution because it doesn't address the requirement to process transactions in the exact order received. While deduplication IDs can help prevent duplicate messages from being processed, they don't have any impact on the order in which messages are processed. Therefore, this option should not be chosen.\nIn summary, the two correct options for designing the migration plan are to send bids to an SQS FIFO queue before they are processed and to add a message group ID to messages before they are sent to the SQS queue. By utilizing SQS FIFO and message group IDs, the company can ensure that transactions are processed in the exact order received.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "When the bids are received, send the bids to an SQS FIFO queue before they are processed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "When the users have submitted the bids from the frontend, the backend service delivers the messages to an SQS standard queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a message group ID to the messages before they are sent to the SQS queue so that the message processing is in a strict order.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use an EC2 or Lambda to add a deduplication ID to the messages before the messages are sent to the SQS queue to ensure that bids are processed in the right order.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 9,
  "query" : "Server-side encryption is about data encryption at rest.\nThat is, Amazon S3 encrypts your data at the object level as it writes it to disk in its data centers and decrypts it for you when you go to access it.\nA few different options are depending on how you choose to manage the encryption keys.\nOne of the options is called 'Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)'\nWhich of the following best describes how this encryption method works?",
  "answer" : "Answer - B.\nServer-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption.\nAmazon S3 encrypts each object with a unique key.\nAs an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.\nAmazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\nOption A is incorrect because there are no separate permissions to the key that protects the data key.\nOption B is CORRECT because as mentioned above, each object is encrypted with a strong unique key and that key itself is encrypted by a master key.\nOption C is incorrect because the keys are managed by the AWS.\nOption D is incorrect because there is no randomly generated key and the client does not do the encryption.\nFor more information on S3 encryption, please visit the links-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\nThe Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) is a server-side encryption method that automatically encrypts the data you store in Amazon S3 at the object level. This means that S3 encrypts the data as it writes it to disk in its data centers, and decrypts it for you when you access it.\nWith SSE-S3, Amazon S3 manages the encryption keys, which provides a simple and cost-effective encryption solution. This method uses Advanced Encryption Standard (AES)-256, a secure symmetric-key encryption standard. SSE-S3 provides strong encryption of data at rest and helps to protect against unauthorized access to the data.\nIn SSE-S3, each object is encrypted with a unique key that is randomly generated by Amazon S3. As an additional safeguard, the encryption key itself is encrypted with a master key that is also managed by Amazon S3. Amazon S3 regularly rotates the master key to further enhance the security of the encrypted data.\nThe encryption keys are stored separately from the encrypted data, which adds an additional layer of security. When you access your objects, Amazon S3 decrypts the data using the appropriate encryption key. You do not need to manage the encryption keys, and you cannot access or retrieve them.\nTherefore, Option B best describes how Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) works. It encrypts each object with a unique key employing strong encryption, and it encrypts the key itself with a master key that it regularly rotates. Amazon S3 manages the encryption keys, and you do not need to manage them.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "There are separate permissions for the use of an envelope key (a key that protects your data`s encryption key) that provides added protection against unauthorized access of your objects in S3 and also provides you with an audit trail of when your key was used and by whom.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Each object is encrypted with a unique key employing strong encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disk, and decryption when you access your objects.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A randomly generated data encryption key is returned from Amazon S3, which is used by the client to encrypt the object data.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 10,
  "query" : "You work in a video game company, and your team is working on a feature that tells how many times certain web pages have been viewed or clicked.\nYou also created an AWS Lambda function to show some key statistics of the data.\nYou tested the Lambda function, and it worked perfectly.",
  "answer" : "Correct Answer- A.\nPotentially, more than one option may work.\nHowever, this question asks the most cost-efficient and straightforward method that needs to be considered.\nOption A is CORRECT because the AWS CloudWatch Events rule is free and quite easy to begin with.\nTo schedule a daily event at 8:00 AM GMT, you just need to set up a cron rule, as given in the below screenshot.\nOption B is incorrect: Because launching a new EC2 instance for this task is not cost-efficient.\nOption C is incorrect: Because this is not something AWS Batch works.\nFor AWS Batch, it runs as a containerized application on an Amazon EC2 instance in your computing environment.\nOption D is incorrect: Because firstly, it should be “Create rule” rather than “Create Event”\nSecondly, the Cron expression of “ * ? * * 08 00” is incorrect.\nFor More information, Please check below AWS Docs:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\nOption A is the correct answer. You should create an AWS CloudWatch Events rule that is scheduled using a cron expression, and configure the target as the Lambda function.\nAWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. CloudWatch Events is a service that enables you to respond to events in the AWS ecosystem. By using CloudWatch Events, you can trigger actions in response to events in AWS services.\nIn this scenario, you want to track page views and clicks every day at 8:00 AM. Therefore, you need to schedule a CloudWatch Events rule to run at 8:00 AM every day.\nA cron expression is a string that specifies a set of times using the format:\nsqlCopy codeminute hour day-of-month month day-of-week year \nThe following cron expression will run the Lambda function every day at 8:00 AM:\nCopy code0 8 * * ? \nTo create a CloudWatch Events rule that triggers the Lambda function, follow these steps:\n1. Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/.\n2. In the navigation pane, choose \"Events,\" and then choose \"Create rule.\"\n3. On the \"Create Rule\" page, choose \"Schedule\" for the \"Event Source\" section.\n4. In the \"Schedule expression\" section, choose \"Cron expression,\" and then enter the cron expression \"0 8 * * ?\".\n5. Choose \"Add target,\" and then choose \"Lambda function.\"\n6. In the \"Function\" section, choose your Lambda function from the drop-down menu.\n7. Choose \"Configure details,\" and then enter a name and description for your rule.\n8. Choose \"Create rule\" to create the CloudWatch Events rule.\nOption B is incorrect because it involves setting up an EC2 instance and using AWS CLI to call the Lambda function. This approach requires managing and maintaining an EC2 instance, which can be time-consuming and less cost-effective compared to using a serverless approach with AWS Lambda.\nOption C is incorrect because Amazon Batch is not suitable for scheduling recurring tasks like this. Amazon Batch is designed for running batch computing workloads, which typically involve running jobs in parallel across multiple EC2 instances.\nOption D is incorrect because the cron expression provided is not correct. The expression \" * ? * * 08 00\" is not a valid cron expression. The correct expression to run the Lambda function at 8:00 AM every day is \"0 8 * * ?\".",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an AWS CloudWatch Events rule that is scheduled using a cron expression. Configure the target as the Lambda function.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an Amazon Linux EC2 T2 instance and set up a Cron job using Crontab. Use AWS CLI to call your AWS Lambda every 8:00 AM.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon Batch to set up a job with a job definition that runs every 8:00 AM for the Lambda function.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In AWS CloudWatch Events console, click “Create Event” using the cron expression “ * ? * * 08 00”. Configure the target as the Lambda function.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 11,
  "query" : "You decide to create a bucket on AWS S3 called 'mybucket' and then perform the following actions in the order that they are listed here.",
  "answer" : "Answer - B.\nObjects stored in your bucket before you set the versioning state have a version ID of null.\nWhen you enable versioning, existing objects in your bucket do not change.\nWhat changes is how Amazon S3 handles the objects in future requests.\nOption A is incorrect because the version ID for file1 would be null.\nOption B is CORRECT because the file1 was put in the bucket before the versioning was enabled.\nHence, it will have a null version ID.\nThe file2 will have two version IDs, and file3 will have a single version ID.Option C is incorrect because file2 cannot have a null version ID as the versioning was enabled before putting it in the bucket.\nOption D is incorrect because once the versioning is enabled, all the files put after that will not have a null version ID.\nBut file1 was put before versioning was enabled.\nSo it will have null as its version ID.For more information on S3 versioning, please visit the below link.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nIn AWS S3, versioning can be enabled for a bucket to preserve, retrieve, and restore every version of an object stored in the bucket. Versioning can help you to protect against accidental overwrites, deletions, and malicious actions.\nFor the given scenario, assuming versioning is enabled on the bucket mybucket, the following actions are performed:\n1. An object named file1 is uploaded to the bucket mybucket.\n2. file2 is uploaded to the bucket mybucket, but with the same key as file1.\n3. file2 is uploaded again to the bucket mybucket, but with a different key.\n4. file3 is uploaded to the bucket mybucket.\nBased on these actions, the following are the possible outcomes:\nA. There will be 1 version ID for file1, 2 version IDs for file2, and 1 version ID for file3. - This is not correct, as file2 was uploaded twice, so there should be 2 version IDs for file1.\nB. The version ID for file1 will be null. There will be 2 version IDs for file2, and 1 version ID for file3. - This is the correct answer, as file2 was uploaded twice, so there should be 2 version IDs for file2.\nC. There will be 1 version ID for file1, the version ID for file2 will be null, and there will be 1 version ID for file3. - This is not correct, as file2 was uploaded twice, so there should be 2 version IDs for file2.\nD. All file version IDs will be null because versioning must be enabled before uploading objects to mybucket. - This is not correct, as versioning can be enabled after objects are uploaded to a bucket. However, if versioning is not enabled, then all objects will have a null version ID.\nTherefore, the correct answer is B: The version ID for file1 will be null. There will be 2 version IDs for file2, and 1 version ID for file3.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "There will be 1 version ID for file1, 2 version IDs for file2, and 1 version ID for file3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The version ID for file1 will be null. There will be 2 version IDs for file2, and 1 version ID for file3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "There will be 1 version ID for file1, the version ID for file2 will be null, and there will be 1 version ID for file3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All file version ID`s will be null because versioning must be enabled before uploading objects to `mybucket`.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 12,
  "query" : "You are writing an AWS CloudFormation template, and you want to assign values to properties that will not be available until runtime.\nYou know that you can use intrinsic functions to do this but are unsure which part of the template they can use.\nWhich of the following is correct in describing how you can currently use intrinsic functions in an AWS CloudFormation template?",
  "answer" : "Answer - B.\nAs per AWS documentation:\nYou can use intrinsic functions only in specific parts of a template.\nCurrently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes.\nYou can also use intrinsic functions to create stack resources conditionally.\nHence, B is the correct answer.\nFor more information on intrinsic function, please refer to the below link.\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\nIntrinsic functions can be used in an AWS CloudFormation template to assign values to resource properties, outputs, metadata attributes, and update policy attributes that are not available until runtime. These functions allow you to dynamically set property values in your template based on conditions that may not be known at the time of template authoring.\nThe answer to the question is B. You can use intrinsic functions only in specific parts of a template. Specifically, intrinsic functions can be used in resource properties, outputs, metadata attributes, and update policy attributes.\nIntrinsic functions are denoted by special syntax, which starts with a double curly brace \"{{\" and ends with a double curly brace \"}}\". The function name is enclosed within the curly braces, along with any arguments that the function may require.\nHere are some examples of intrinsic functions and their usage:\n1. Fn::Sub - This function substitutes variables in a string with their corresponding values. It can be used in the metadata attribute of a resource.\nExample:\nyamlCopy codeMetadata:   AWS::CloudFormation::Init:     config:       commands:         01_hello:           command: !Sub echo \"Hello ${UserName}!\" \n1. Fn::Join - This function joins a list of strings into a single string. It can be used in resource properties.\nExample:\nvbnetCopy codeResources:   MyBucket:     Type: \"AWS::S3::Bucket\"     Properties:       BucketName: !Join [\"-\", [\"my\", \"bucket\", \"name\"]] \n1. Fn::ImportValue - This function imports the value of an output from another stack. It can be used in resource properties or in outputs.\nExample:\nyamlCopy codeResources:   MyInstance:     Type: \"AWS::EC2::Instance\"     Properties:       UserData:         Fn::Base64:           !Sub |             #!/bin/bash             export MY_DB_HOST=$(Fn::ImportValue \"MyDBInstanceEndpoint\")             # rest of the script \nIn conclusion, intrinsic functions are a powerful tool in AWS CloudFormation templates that allow you to dynamically set property values based on runtime conditions. They can be used in resource properties, outputs, metadata attributes, and update policy attributes. The correct answer to the question is B.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You can use intrinsic functions in any part of a template.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use intrinsic functions only in specific parts of a template. You can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can use intrinsic functions only in the resource properties part of a template.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use intrinsic functions in any part of a template, except AWSTemplateFormatVersion and Description.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 13,
  "query" : "Your application uses an ELB in front of an Auto Scaling group of web/application servers deployed across two AZs and a Multi-AZ RDS Instance for data persistence.\nThe database CPU is often above 80% usage, and 90% of I/O operations on the database are reads.\nTo improve the performance, you recently added a single-node Memcached ElastiCache Cluster to cache frequent DB query results.\nIn the next weeks, the overall workload is expected to grow by 30%\nDo you need to change anything in the architecture to maintain the high availability of the application with the anticipated additional load, and why?",
  "answer" : "Answer - A.\nOption A is CORRECT because having two clusters in different AZs provides high availability of the cache nodes, removing the single point of failure.\nIt will help caching the data; hence, reducing the overload on the database, maintaining the availability, and reducing the impact.\nOption B is incorrect because, even though AWS will automatically recover the failed node, there are no other nodes in the cluster once the failure happens.\nSo, the data from the cluster would be lost once that single node fails.\nFor higher availability, there should be multiple nodes.\nAlso, once the cache node fails, all the cached read load will go to the database, which will not be able to handle the load with a 30% increase to current levels.\nThis means there will be an availability impact.\nOption C is incorrect because provisioning the nodes in the same AZ does not tolerate an AZ failure.\nFor higher availability, the nodes should be spread across multiple AZs.\nOption D is incorrect because the very purpose of the cache node was to reduce the impact on the database by not overloading it.\nIf the cache node fails, the database will not be able to handle the 30% increase in the load; so, it will have an availability impact.\nMore information on this topic from AWS Documentation:\nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/BestPractices.html\nMitigating Node Failures.\nTo mitigate the impact of a node failure, spread your cached data over more nodes.\nBecause Memcached does not support replication, a node failure will always result in some data loss from your cluster.\nWhen you create your Memcached cluster, you can create it with 1 to 20 nodes or more by special request.\nPartitioning your data across a greater number of nodes means you'll lose less data if a node fails.\nFor example, if you partition your data across 10 nodes, any single node stores approximately 10% of your cached data.\nIn this case, a node failure loses approximately 10% of your cache which needs to be replaced when a replacement node is created and provisioned.\nMitigating Availability Zone Failures.\nTo mitigate the impact of an availability zone failure, locate your nodes in as many availability zones as possible.\nIn the unlikely event of an AZ failure, you will lose only the data cached in that AZ, not the data cached in the other AZs.\nThe correct answer is A. Yes. You should deploy two Memcached ElastiCache clusters in different AZs with a change in application logic to support both clusters because the RDS instance will not be able to handle the load if the cache node fails.\nExplanation:\nThe architecture has an ELB in front of an Auto Scaling group of web/application servers deployed across two Availability Zones (AZs) and a Multi-AZ RDS Instance for data persistence. The database CPU usage is above 80%, and 90% of I/O operations on the database are reads. To improve performance, a single-node Memcached ElastiCache Cluster was added to cache frequent DB query results.\nHowever, as the overall workload is expected to grow by 30%, the current setup might not be enough to handle the increased load while maintaining high availability. The Memcached ElastiCache Cluster can be a single point of failure, and if it fails, the RDS instance will not be able to handle the load. Therefore, a solution that provides high availability is necessary.\nOption A suggests deploying two Memcached ElastiCache clusters in different AZs and changing the application logic to support both clusters. This option provides redundancy and high availability because if one cache node fails, the other one can take over, ensuring that the application continues to function without any availability impact. By deploying the clusters in different AZs, the application can handle an AZ failure without impacting availability. The change in application logic is necessary to ensure that the application can use both clusters, and the cache data is consistent across both clusters.\nOption B suggests that the automated ElastiCache node recovery feature will prevent any availability impact if the cache node fails. Although this feature can restore the cache node automatically, it does not provide high availability, as there is only one cache node.\nOption C suggests deploying the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS DB master instance. This option does not provide high availability because if the AZ fails, both cache nodes and the RDS instance will be impacted.\nOption D suggests that if the cache node fails, the application can always get the same data from the DB without having any availability impact. This option does not provide high performance as most of the I/O operations are reads, and accessing the database for each read operation can slow down the application.\nIn conclusion, to maintain high availability and performance with the anticipated additional load, Option A is the best solution, as it provides redundancy and high availability by deploying two Memcached ElastiCache clusters in different AZs and changing the application logic to support both clusters.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Yes. You should deploy two Memcached ElastiCache clusters in different AZ`s with a change in application logic to support both clusters because the RDS instance will not be able to handle the load if the cache node fails.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "No. If the cache node fails, the automated ElastiCache node recovery feature will prevent any availability impact.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Yes. You should deploy the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS DB master instance to handle the load if one cache node fails.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No. If the cache node fails, you can always get the same data from the DB without having any availability impact.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 14,
  "query" : "You work in a DevOps team, and your team maintains several applications deployed in AWS.\nAt the moment, there are dozens of server certificates stored in IAM.\nThese certificates are used for different purposes and have different expiry date.\nYou have to renew the certificates before they expire.\nOtherwise, the services will be impacted.\nYou want to use another approach to renew and manage these certificates.\nWhich method is the best?",
  "answer" : "Correct Answer - B.\nCheck https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html on how to manage server certificates in IAM and ACM.\nOption A is incorrect: Because for the imported server certificates in IAM, there is no IAM console to manage them.\nThis is one major disadvantage of managing certificates in IAM.\nOption B is CORRECT: Because ACM is a preferred solution.\nCertificates requested by ACM are free and automatically renew.\nOption C is incorrect: Because you cannot migrate the certificates from IAM to ACM directly.\nThere is no such console to do that.\nFor ACM, you can import third-party certificates to the service.\nOption D is incorrect: Because ACM cannot automatically renew imported third-party certificates.\nYou are responsible for monitoring the expiration date.Please check the reference in.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html\nOut of the four options provided, the best method for renewing and managing server certificates in AWS is to use AWS Certificate Manager (ACM), as stated in option B.\nAWS Certificate Manager (ACM) is a managed service that provides SSL/TLS certificates for use with AWS services and resources. It simplifies the process of provisioning, managing, and deploying SSL/TLS certificates for use with AWS services and resources.\nOption A of adding a new strategy for server certificates to renew one month before the expiry date automatically in the IAM console can be a viable solution, but it does not eliminate the need to manually manage and monitor the certificates.\nOption C of migrating the certificates from IAM to ACM, and then having ACM automatically renew the certificates one month before the expiry date can be a valid solution. However, this option requires additional steps to migrate the certificates, and it may not be feasible for all scenarios.\nOption D of importing all third-party certificates into ACM can be an effective solution, but it may not be practical for organizations with a large number of certificates or those who have specific requirements for third-party certificates.\nTherefore, option B is the best method for managing and renewing server certificates in AWS. ACM automates the process of renewing certificates, eliminating the need for manual intervention. Additionally, ACM integrates with other AWS services, such as Elastic Load Balancing and Amazon CloudFront, making it easy to deploy certificates across multiple resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the IAM console, add a new strategy for server certificates to renew one month before the expiry date automatically.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Provision and manage the server certificates in AWS Certificate Manager (ACM). The certificates requested from ACM are automatically renewed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In IAM console, migrate the certificates from IAM to ACM then ACM can automatically renew the certificates one month before the expiry date.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Import all third-party certificates into ACM. ACM is responsible for the automatic renew for both third-party certificates and ACM provided certificates.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 15,
  "query" : "Your company asked you to create a mobile application.\nThe application is built to work with DynamoDB as the backend and Javascript as the frontend.\nDuring the application's usage, you notice that sometimes during the daytime, the write requests are throttled because of low provisioned write capacity.\nHow can you effectively resolve this problem of the DynamoDB in the easiest way?",
  "answer" : "Answer: A.\nOption A is correct because users can enable Auto Scaling in DynamoDB.\nPlease refer to the below link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\nOption B is incorrect because just increasing the write capacity of the DynamoDB table would not satisfy the requirement \"effectively manage DynamoDB\"\nThe problem only happens during the daytime.\nSo there is a waste of provisioned resources when there are few requests in the night.\nOption C is incorrect.\nBecause you have to create a new SQS queue and modify the related logic for DynamoDB.\nThis option is not the easiest one.\nOption D is incorrect because launching a DynamoDB table in a Multi-AZ configuration with a global index would only increase data availability and would not solve the write contention issue.\n---------------------------------------------------------------------------------------\nNote:\nThe question is asking \"....how to manage DynamoDB\"\nThe new feature that has been added recently is \"autoscaling of dynamodb\" which would help to solve this issue.\nPlease refer to the link- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html.\nAlso attached herewith is the screenshot of the same.\nThe issue you're facing is that your DynamoDB write capacity is being throttled during peak usage hours. This means that the current provisioned write capacity is not sufficient to handle the load being placed on the system. There are a few options available to address this issue:\nA. Enable DynamoDB Auto Scaling: This is likely the easiest solution to implement and should address the problem without requiring much effort on your part. DynamoDB Auto Scaling automatically adjusts provisioned capacity up or down based on the actual usage of the table. By enabling Auto Scaling, DynamoDB will automatically increase the provisioned write capacity during peak usage hours to ensure that the system is not throttled.\nB. Increase write capacity of DynamoDB: Another option would be to manually increase the provisioned write capacity of your DynamoDB table to handle the increased load. This option requires you to manually monitor the system during peak usage hours and make adjustments accordingly. However, it may result in higher costs and may not be as flexible as Auto Scaling.\nC. Use the SQS service: This option involves using the Simple Queue Service (SQS) to store write requests that are being throttled by DynamoDB. The requests can then be processed at a later time when the load on the system has decreased. However, this approach may introduce additional latency into the system and may not be suitable for real-time applications.\nD. Launch DynamoDB in Multi-AZ configuration: This option involves deploying your DynamoDB table across multiple Availability Zones (AZs) to improve availability and scalability. Additionally, using a global index can help distribute write requests across multiple partitions, which can help to balance the load and prevent throttling. However, this option may require more effort to implement and may result in higher costs.\nIn conclusion, the easiest and most flexible option for addressing your DynamoDB write capacity issue would be to enable DynamoDB Auto Scaling. This approach will automatically adjust the provisioned capacity up or down based on actual usage, ensuring that your system is not throttled during peak usage hours.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable DynamoDB Auto Scaling to meet the requirements.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Increase write capacity of DynamoDB to meet the peak loads.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the SQS service to read messages in the queue and write these to DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Launch DynamoDB in Multi-AZ configuration with a global index to balance writes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 16,
  "query" : "An agile team just starts using AWS, and the team leader wants them to move the legacy Java-based software to the AWS platform in 2 weeks.\nThe requirement is that the new environment must be highly available, and the infrastructure is managed as code and version controlled.\nBesides, the team has good experiences of Chef, so they want to use that knowledge during the migration.\nWhich actions should the team perform to meet the needs? Select 2.",
  "answer" : "E.\nCorrect Answer - A, D.\nOptions A, D are Correct:\nFirstly, OpsWorks is a supported service in CloudFormation according to https://aws.amazon.com/about-aws/whats-new/2014/03/03/aws-cloudformation-supports-aws-opsworks/\nThe AWS OpsWorks Resource Types in CloudFormation includes:\nAWS::OpsWorks::App.\nAWS::OpsWorks::ElasticLoadBalancerAttachment.\nAWS::OpsWorks::Instance.\nAWS::OpsWorks::Layer.\nAWS::OpsWorks::Stack.\nAWS::OpsWorks::UserProfile.\nAWS::OpsWorks::Volume.\nSecondly, EC2 application instances should be managed inside OpsWorks, and they do not belong to infrastructure preparations.\nOption A is more accurate than Option.\nC.Option A is CORRECT: Because infrastructure should be done via CloudFormation, and it can be easily version controlled.\nEither CodeCommit or GitHub is ok.\nOption B is incorrect: Because CloudFormation supports OpsWorks service.\nA nested template could be utilized to maintain OpsWorks related resources.\nOption C is incorrect: Because infrastructure refers to the resources which do not change frequently, such as VPC subnets and Bastion hosts.\nEC2 instances are application related and should be maintained within OpsWorks stacks.\nOption D is CORRECT: Because the team can use the Chef experiences in OpsWorks to build up the application.\nTogether with.\nOption A, a highly available environment can be built.\nOption E is incorrect: Because the OpsWorks service is supported in CloudFormation.\nIt makes more sense to create an OpsWorks stack as a CloudFormation resource.\nThe team's goal is to migrate their legacy Java-based software to AWS while ensuring high availability, infrastructure as code, and version control. Additionally, they have prior experience with Chef and would like to utilize it during the migration process.\nOut of the five options given, the two most suitable actions the team should perform are A and B.\nOption A recommends using CloudFormation templates to build up infrastructure such as VPC, NAT Gateway, Bastion, and Route53. Using infrastructure as code will enable the team to automate their infrastructure deployment and management. Version controlling the templates using CodeCommit will help them track changes made to the infrastructure over time, and they can also revert to earlier versions if needed.\nOption B suggests using Chef in EC2 to build up the web services. As the team has prior experience with Chef, it would be easier for them to manage and configure their application using existing cookbooks. They can add an auto-scaling group with an appropriate configuration to ensure high availability.\nOption C is similar to option A, where AWS infrastructure such as VPC, NAT Gateway, Bastion host, Security Groups, and EC2 instances are created using CloudFormation templates. However, this option suggests using GitHub for version control instead of CodeCommit.\nOption D recommends using a nested CloudFormation template to create an OpsWorks stack with a Java layer. Although this option meets the requirements, it might not be the best approach as it adds complexity to the infrastructure.\nOption E suggests using OpsWorks to build up the Java web services. However, as OpsWorks cannot be put into the CloudFormation template, it would be difficult to manage the infrastructure as code and version control it.\nIn summary, the team should use CloudFormation templates for infrastructure deployment and version control it using CodeCommit. They should also use Chef in EC2 to build up the web services and add an auto-scaling group for high availability.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use several CloudFormation templates to build up infrastructure such as VPC, NAT Gateway, Bastion, and Route53. Version control it using CodeCommit.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "As CloudFormation does not support the OpsWorks service, use Chef in EC2 to build up the web services. Existing cookbooks can be used. Add an auto-scaling group with a proper auto-scaling configuration to ensure high availability.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS infrastructure such as VPC, NAT Gateway, Bastion host, Security Groups, and EC2 instances are created using CloudFormation templates. Version control it using GitHub.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a nested CloudFormation template to create an OpsWorks stack. The resource type is “AWS::OpsWorks::Stack”. Add a Java layer in the stack. Make sure that the Scaling configuration is turned on.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use OpsWorks to build up the Java web services. Existing cookbooks can be used. However, OpsWorks cannot be put into the CloudFormation template as CloudFormation does not support it.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 17,
  "query" : "As AWS grows, most of your clients' main concerns seem to be about security, especially when all of their competitors also seem to be using AWS.\nOne of your clients asks you whether having a competitor who hosts their EC2 instances on the same physical host would make it easier for the competitor to hack into the client's data.\nWhich of the following statements would be the best choice to put your client's mind at rest?",
  "answer" : "Answer - C.\nOptions A and B are incorrect because 256-bit AES is used for encrypting the data.\nEnsuring the isolation of the VMs running on a hypervisor is not its responsibility.\nOption C is CORRECT because it is the hypervisor that hosts the VMs responsible for ensuring that the VMs are isolated from each other despite being hosted on the same underlying hypervisor.\nOption D is incorrect because IAM permissions have nothing to do with the isolation of the VMs running on a hypervisor.\nMore information on this topic:\nThe shared responsibility model for infrastructure services, such as Amazon Elastic Compute Cloud (Amazon EC2), for example, specifies that AWS manages the security of the following assets:\n• Facilities\n• Physical security of hardware\n• Network infrastructure\n• Virtualization infrastructure.\nhttps://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf\nThe correct answer to this question is C. Different instances running on the same physical machine are isolated from each other via the hypervisor.\nExplanation: In AWS, when a customer launches an EC2 instance, it runs on a physical host machine, which is managed by AWS. AWS uses a hypervisor to virtualize the physical host machine and create multiple virtual machines on it. Each virtual machine is isolated from the others and operates as if it were running on its own physical machine. This is called \"virtualization.\"\nAWS uses a combination of hardware and software-based security measures to ensure the isolation of each virtual machine on a physical host. The hypervisor is responsible for creating and managing the virtual machines and for providing each virtual machine with its own virtual resources, such as CPU, memory, and storage. The hypervisor ensures that the virtual machines are isolated from each other, so that a security breach in one virtual machine does not affect the others.\nTherefore, the correct answer is C. Different instances running on the same physical machine are isolated from each other via the hypervisor.\nOption A is incorrect because AES-256 is a form of encryption that is used to protect data at rest or in transit, but it does not provide isolation between virtual machines running on the same physical host.\nOption B is incorrect because while the hypervisor is responsible for isolating the virtual machines, AES-256 encryption is not involved in this process.\nOption D is also incorrect because IAM permissions are used to manage access to AWS resources, but they do not provide isolation between virtual machines running on the same physical host.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Different instances running on the same physical machine are isolated from each other via a 256- bit Advanced Encryption Standard (AES-256).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Different instances running on the same physical machine are isolated from each other via the hypervisor and via a 256-bit Advanced Encryption Standard (AES-256).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Different instances running on the same physical machine are isolated from each other via the hypervisor.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Different instances running on the same physical machine are isolated from each other via IAM permissions.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 18,
  "query" : "You are building a large-scale confidential documentation web server on AWS, and all of the documentation for it will be stored on S3\nOne of the requirements is that it cannot be publicly accessible from S3 directly.\nYou will need to use CloudFront to accomplish this.\nWhich of the methods listed below would satisfy the requirements as outlined? Choose an answer from the options below.",
  "answer" : "Answer - B.\nThere are two main points (1) the files should not be accessed directly via S3 as they are confidential, and (2) the files should be accessible via CloudFront.\nIf you want to use CloudFront signed URLs or signed cookies to provide access to objects in your Amazon S3 bucket, you probably also want to prevent users from accessing your Amazon S3 objects using Amazon S3 URLs.\nIf users access your objects directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies, for example, control over the date and time that a user can no longer access your content and control over which IP addresses can be used to access the content.\nIn addition, if users access objects both through CloudFront and directly by using Amazon S3 URLs, CloudFront access logs are less useful because they're incomplete.\nSee the image below:\nOption A is incorrect because it does not give CloudFront exclusive access to the S3 bucket.\nOption B is CORRECT because it gives CloudFront exclusive access to the S3 bucket and prevents other users from accessing the public content of S3 directly via S3 URL.\nOption C is incorrect because you do not need to create any individual policies for each bucket.\nOption D is incorrect because (a) creating a bucket policy is unnecessary and (b) it does not prevent other users from accessing the public content of S3 directly via S3 URL.\nFor more information on Origin Access Identity, please see the below link-\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\nThe correct answer is B. Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI only.\nHere is a detailed explanation of why this is the correct answer and why the other options are incorrect:\nA. Creating an Identity and Access Management (IAM) user for CloudFront and granting access to the objects in your S3 bucket to that IAM User does not provide a secure method of restricting access to S3 objects from CloudFront. This is because IAM users are intended for authentication of people or services accessing AWS resources, while CloudFront is a service that needs to access S3 objects on behalf of users who access your web server through CloudFront. Also, IAM users are intended to authenticate for AWS Management Console or AWS API calls and not for a public website.\nC. Creating individual policies for each bucket that stores documents and granting access to only CloudFront is not practical for large-scale environments, as you would need to create a new policy for every new S3 bucket that stores documents, making the process cumbersome and difficult to manage.\nD. Creating an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN) is not sufficient as it does not provide a secure method of restricting access to S3 objects from CloudFront. With this policy, anyone with knowledge of the CloudFront distribution ID could access the objects in your S3 bucket.\nB. Creating an Origin Access Identity (OAI) for CloudFront and granting access to the objects in your S3 bucket to that OAI only, is the best way to restrict access to the objects in your S3 bucket. When you create an OAI, you provide CloudFront with a special user that CloudFront can use to access the objects in your S3 bucket. This user can only be used by CloudFront, and not by anyone else. By granting access to the objects in your S3 bucket to this special user only, you ensure that the objects can only be accessed through CloudFront, and not directly from S3.\nIn summary, creating an Origin Access Identity (OAI) for CloudFront and granting access to the objects in your S3 bucket to that OAI only is the best way to restrict access to the objects in your S3 bucket and satisfy the requirement of not being publicly accessible from S3 directly.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM User.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI only.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create individual policies for each bucket that stores documents and in that policy grant access to only CloudFront.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN).",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 19,
  "query" : "A customer runs a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network.\nThey are connecting to the VPC over the Internet to manage all of their Amazon EC2 instances running in both the public and private subnets.\nThey have only authorized the bastion-security-group with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups.\nBut the company wants to limit further administrative access to all of the instances in the VPC.\nWhich of the following Bastion deployment scenarios will meet this requirement?",
  "answer" : "Answer - D.\nOption A is incorrect because a bastion host is deployed into the public subnet of a VPC with an Elastic IP address to allow inbound Secure Shell (SSH) access to EC2 instances, but the option does not say which subnet the Bastion host needs to be deployed into ( it just says corporate network )\nOption B is incorrect because only the corporate IP addresses should have SSH access to it, not from anywhere.\nOption C is incorrect because the bastion host needs to be placed in the public subnet, not private.\nOption D is CORRECT because (a) it places the bastion host in the public subnet, and (b) only the corporate IP addresses have RDP access to it.\nFor more information on controlling network access to EC2 instances using a bastion server, please see the link below.\nhttps://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/\nThe customer runs a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage all of their Amazon EC2 instances running in both the public and private subnets.\nTo further restrict administrative access to all of the instances in the VPC, the company needs to deploy a bastion host. A bastion host is a special-purpose instance that is designed to provide secure access to the instances in the VPC. The bastion host acts as a proxy to access the instances in the VPC, and all access to the instances must pass through the bastion host.\nLet's analyze each option:\nA. Deploy a Windows Bastion host on the corporate network that has RDP access to all instances in the VPC.\nThis option is not feasible as the VPC is not connected to the corporate network, and the customer is already accessing the VPC over the Internet.\nB. Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere.\nThis option is not recommended as it allows SSH access from anywhere, which is not secure. Also, the requirement is to restrict further administrative access to all instances in the VPC.\nC. Deploy a Windows Bastion host with an Elastic IP address in the private subnet and restrict RDP access to the bastion from only the corporate public IP addresses.\nThis option is a valid solution as it restricts RDP access to the bastion host from only the corporate public IP addresses, making it more secure. Also, by deploying the bastion host in the private subnet, it provides an additional layer of security, as the bastion host is not directly accessible from the Internet.\nD. Deploy a Windows Bastion host with an elastic IP address in the public subnet and allow RDP access to the bastion from only the corporate public IP addresses.\nThis option is not recommended as it allows RDP access from the public subnet, which is not secure. Also, the requirement is to restrict further administrative access to all instances in the VPC.\nTherefore, option C is the correct answer as it restricts RDP access to the bastion host from only the corporate public IP addresses and deploys the bastion host in the private subnet, which provides an additional layer of security.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy a Windows Bastion host on the corporate network that has RDP access to all instances in the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy a Windows Bastion host with an elastic IP address in the public subnet, and allow RDP access to the bastion from only the corporate public IP addresses.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 20,
  "query" : "You have been tasked with creating file-level restore on your EC2 instances.\nYou already have access to all the frequent snapshots of the EBS volume.\nYou need to be able to restore an individual lost file on an EC2 instance within 15 minutes of a reported loss of information.\nThe acceptable RPO is several hours.\nHow would you perform this on an EC2 instance? Choose an answer from the options below.",
  "answer" : "Answer - C.\nOption A is incorrect because there is an assumption that the EC2 instance can read files from S3.\nOption B is incorrect because the old volume connected to the EC2 could have different files compared to the snapshot that we are recovering from (i.e., some files may have been removed, some may have been newly added)\nIf that volume is removed, we may lose some files and the final volume will not be in the latest state.\nSo, the best solution is to copy the file from the recovery volume and add it to the old connected/backup volume.\nOption C is CORRECT because it mounts the EBS snapshot containing the file - as a volume and copies the file to the already attached volume.\nThis way, the already attached volume always stays up-to-date.\nOnce the file is copied, the volume - that was attached for copying the file - can be removed.\nWith this option, you can restore the individual lost files without removing any new files that the snapshot does not have.\nOption D is INCORRECT because the recent snapshot may not contain the lost file.\nAnd we only need to restore the lost file so that there is no need to copy all the system files.\nFor more information on EBS snapshots, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nThe correct answer to the question is C.\nTo restore an individual lost file on an EC2 instance within 15 minutes of a reported loss of information, and given that frequent snapshots of the EBS volume are available, the following steps can be taken:\n1.\nCreate a new EBS volume from the snapshot that contains the lost file. This can be done using the AWS Management Console or the AWS CLI.\n2.\nAttach the new EBS volume to the same EC2 instance that experienced the data loss, but at a different mount location. This is important as the original volume may still be in use by the application or operating system.\n3.\nBrowse the file system on the newly attached volume and select the lost file that needs to be restored.\n4.\nCopy the lost file from the new EBS volume to the original source volume. This can be done using a command-line tool like cp or a graphical file manager.\n5.\nVerify that the restored file is accessible and functional.\nNote that the acceptable RPO is several hours, which means that the frequency of the EBS snapshots can be adjusted to meet this requirement. It is also important to periodically test the restore process to ensure that it works as expected.\nOption A, setting up a cron job to copy files to S3, is not an ideal solution as it would require frequent copying of all files on the volume, which may not be practical or efficient. Option B, turning off frequent snapshots and creating a new volume from an old snapshot, would result in a longer RPO and potentially longer downtime for the application. Option D, creating a new EC2 instance and copying all system files, is also not an ideal solution as it would require more resources and time than necessary to restore a single lost file.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up a cron that runs aws s3 cp on the files and copy the EBS volume files to S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Turn off the frequent snapshots of EBS volumes. Create a volume from an EBS snapshot, attach the EBS volume to the EC2 instance at a different mount location, cutover the application to look at the new backup volume and remove the old volume.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a volume from the snapshot to be restored and attach the EBS volume to the same EC2 instance at a different mount location, browse the file system on the newly attached volume and select the file that needs to be restored, copy it from the new volume to the original source volume.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new EC2 instance from the recent snapshot and copy all the system files from the new EC2 instance to the old EC2 instance.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 21,
  "query" : "Your company is developing a next-generation pet collar that collects biometric information to assist families with promoting healthy lifestyles for their pets.\nEach collar will push 30kb of biometric data in JSON format every 2 seconds to a collection platform that will process and analyze the data providing health trending information back to the pet owners and veterinarians via a web portal.\nManagement has tasked you to architect the collection platform ensuring the following requirements are met.",
  "answer" : "Answer - B.\nThe main point to consider here is that the information is to be analyzed in real-time.\nThe solution should be highly durable, elastic, and processed in parallel.\nThe result should be persisted for data mining after the analysis.\nWhenever the question requires real-time processing of data, always think about using Amazon Kinesis.\nOption A is incorrect because (a) S3 is not efficient for collecting and storing real-time data, and (b) daily scheduled data pipeline is not a real-time analytics solution.\nOption B is CORRECT because (a) Amazon Kinesis and Kinesis Analytics is ideal for capturing and processing real-time data respectively captured by the sensor, (b) it also stores the result of analysis later, and (c) Redshift cluster can be used for processing (data mining) the information captured by the Kinesis and copied via EMR.\nOption C is incorrect because (a) S3 is not efficient for collecting and storing real-time data, and (b) MSSQL Server RDS is not ideal for storing the information for data mining.\nOption D is incorrect because (a) EMR alone is not ideal for capturing data and would need specific frameworks like Kafka to capture data for processing.\nAlso, real-time analytics needs to be done using Spark Streaming and not EMR alone, and (b) DynamoDB is not used for data mining.\nFor more information on Amazon Kinesis with Redshift, go to this link-\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\nThe best answer for this scenario is B. Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis Analytics, and save the results to a Redshift cluster using EMR.\nAmazon Kinesis is a data streaming service that enables real-time processing of streaming data at scale. It is suitable for collecting and processing large amounts of data in real-time, such as the biometric data generated by the pet collars. With Kinesis, you can easily ingest, buffer, and process data in real-time.\nKinesis Analytics is a service that enables real-time analysis of streaming data using SQL queries. It allows you to analyze data as it is being generated and provides real-time insights into the data. Using Kinesis Analytics, you can easily process the JSON data generated by the pet collars and extract the relevant information.\nEMR is a fully-managed Hadoop framework that makes it easy to process large amounts of data using distributed computing. EMR can be used in conjunction with Kinesis Analytics to save the results to a Redshift cluster. Redshift is a fully-managed data warehouse service that makes it easy to store and analyze large amounts of data. With Redshift, you can easily store and analyze the results generated by Kinesis Analytics.\nTherefore, B is the best answer as it combines Kinesis and Kinesis Analytics for processing the incoming data, and then saves the results to Redshift using EMR for storage and analysis.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Utilize S3 to collect the inbound sensor data, analyze the data from S3 with a daily scheduled Data Pipeline and save the results to a Redshift Cluster.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis Analytics and save the results to a Redshift cluster using EMR.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Utilize S3 to collect the inbound sensor data analyze the data from SQS with Amazon Kinesis and save the results to a Microsoft SQL Server RDS instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Utilize EMR to collect the inbound sensor data, analyze the data from EMR with Amazon Kinesis and save the results to DynamoD.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 22,
  "query" : "A developer is trying to get a new DevOps role and preparing for a technical task for the interview.\nThe requirement is that a simple pipeline should be built up within 1 week for a RESTful web service that contains several endpoints.\nFor the pipeline, he decides to use AWS CodePipeline.\nFor the application, he wants to use T2 Micro EC2 instances as they belong to free tier.\nIn order to show a breadth of skills, he would like to use certain orchestration tool such as OpsWorks or CloudFormation to deploy the App.\nHe has used Chef for some open source projects before.\nWhat below option is the best for him to do in a short time?",
  "answer" : "The best option for the developer to build a pipeline for a RESTful web service using AWS CodePipeline while demonstrating knowledge of orchestration tools such as OpsWorks or CloudFormation is option D:\nD. For CodePipeline, configure an S3 bucket as the source provider and configure the OpsWorks as the deployment provider. Then OpsWorks can create stack/layers and deploy APPs using artifacts in S3.\nThis option provides a good balance between ease of setup, speed of deployment, and demonstration of a breadth of skills.\nHere's how the developer can implement option D:\n1.\nSet up an S3 bucket: The developer can create an S3 bucket and upload the application artifacts (e.g., code, configuration files) to the bucket.\n2.\nCreate a CodePipeline: The developer can create a CodePipeline and configure the source provider to use the S3 bucket as the source location for the pipeline.\n3.\nConfigure the pipeline stages: The pipeline should have at least two stages: Build and Deploy.\n4.\nConfigure the Build stage: In the Build stage, the developer can choose a build provider (e.g., AWS CodeBuild, Jenkins) to build the application. The build provider should output the built artifacts to the S3 bucket.\n5.\nConfigure the Deploy stage: In the Deploy stage, the developer can choose OpsWorks as the deployment provider. OpsWorks can create the stack/layers and deploy the application using the artifacts from the S3 bucket.\nOpsWorks is a configuration management service that uses Chef, an open-source configuration management tool. OpsWorks supports deploying applications to EC2 instances, ECS clusters, and Lambda functions. By using OpsWorks, the developer can demonstrate familiarity with configuration management tools.\nOption A is incorrect because it suggests using OpsWorks to hook up CodePipeline in the build stage. This is not an optimal solution because CodePipeline already has built-in integration with many build providers, and the developer can choose a build provider that fits the application requirements.\nOption B is incorrect because it suggests configuring an OpsWorks stack, layer, and instance before setting up the pipeline. This approach adds unnecessary complexity and requires the developer to configure OpsWorks separately from CodePipeline.\nOption C is incorrect because it suggests using CloudFormation to build up EC2 instances with ELB and Autoscaling. While this is a valid approach, it requires more setup time and may not be necessary for a simple application that uses T2 Micro EC2 instances. Additionally, the question specifies that the developer should use an orchestration tool such as OpsWorks or CloudFormation to deploy the application, and CloudFormation does not qualify as an orchestration tool in this context.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use OpsWorks to hook up CodePipeline in the build stage. The artifacts can be put in an S3 bucket, and OpsWorks will use the newest code in S3 to deploy the applications.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Firstly, configure an OpsWorks stack, layer and instance. Secondly, in CodePipeline, choose an S3 bucket as the source which can be a zip file for the app and set up the existing OpsWorks stack as the deployment provider. Then the app can be deployed to your stack automatically.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "As CodePipeline does not support OpsWorks, CloudFormation template must build up EC2 instance with ELB and Autoscaling. Configure CodePipeline to select CloudFormation as a deployment target in the deploy stage of the pipeline.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For CodePipeline, configure an S3 bucket as the source provider and configure the OpsWorks as the deployment provider. Then OpsWorks can create stack/layers and deploy APPs using artifacts in S3.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 23,
  "query" : "A manufacturing company whose head office is in Sydney, have plants in 16 locations across the world.\nEmployees based in these 16 locations have to send their daily development data to AWS for storage and further analysis.\nData size is expected to be in GBs.\nWhat is the best way to send the data to AWS?",
  "answer" : "Answer: C.\nOption A is Incorrect because direct connect is ideal for clients who want to establish private connectivity between their on-prem network and AWS for some location.\nThis is not the right solution for moving files from multiple locations.\nOption B is Incorrect because snowball is ideal for clients moving a large bunch of data at once.\nOption C is Correct because S3 Transfer acceleration gives the ability to write into the single S3 Bucket from various locations.\nIt uses edge locations to move the data.\nOption D is Incorrect because Transfer acceleration doesn't work with the Amazon Glacier storage class.\nReference:\nhttps://www.youtube.com/watch?v=J2CVnmUWSi4 https://aws.amazon.com/about-aws/whats-new/2016/04/transfer-files-into-amazon-s3-up-to- https://aws.amazon.com/s3/transfer-acceleration/ https://www.youtube.com/watch?v=HnUqH3hdz4I\nThe best way to send the data to AWS in this scenario would be to use option B, i.e., store the data in Amazon S3 and send it through Snowball.\nHere's why:\nOption A: AWS Direct Connect is a good choice if the data transfer needs to be fast, secure, and consistent. However, it requires a dedicated network connection from the customer's data center to AWS. This means that the customer would need to have a data center or a colocation facility near one of the Direct Connect locations to establish the connection. This is not the best option for a manufacturing company with plants in 16 locations across the world. Also, storing the data in Amazon S3 is a separate requirement, which is not met by this option alone.\nOption B: Amazon S3 is a highly scalable object storage service that can store and retrieve any amount of data from anywhere on the web. In this option, the data can be stored in S3 buckets created in each of the 16 locations. Once the data is ready to be transferred to AWS, it can be shipped using a Snowball device. Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. The Snowball device is shipped to the customer, and they can load the data onto it. Once the data transfer is complete, the device is shipped back to AWS, where the data is loaded into S3.\nOption C: S3 Transfer Acceleration is an Amazon S3 feature that enables faster transfer of files over the internet to Amazon S3. This option is not suitable for this scenario as it does not provide a way to physically transfer large amounts of data to AWS, which is a requirement given the expected data size in GBs.\nOption D: Amazon Glacier is a secure, durable, and low-cost cloud storage service for data archiving and long-term backup. However, it is not suitable for storing frequently accessed data, which is a requirement for daily development data. Also, S3 Transfer Acceleration is not an ideal way to transfer data in this scenario, as explained above.\nTherefore, based on the given scenario, the best option for the manufacturing company would be to store the data in Amazon S3 and send it through Snowball.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "They can send through AWS Direct Connect and store the data in Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They can store the data in Amazon S3 and send it through Snowball.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They can send through S3 Transfer acceleration and store the data in Amazon S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "They can store in Amazon Glacier and send through S3 Transfer acceleration.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 24,
  "query" : "A company runs its critical portal on its On-prem datacenter on docker containers with a PostgreSQL database of size 40 TB.\nThey are looking to migrate their existing portal to AWS to enhance user's experience with less burden on Infrastructure management.\nWhich of the following methods should they use? (Select TWO).",
  "answer" : "E.\nAnswer: B and D are correct.\nOption A is Incorrect because deploying container on ECS using EC2 will require management task.\nOption B is Correct because Fargate removes the management task of container on ECS.\nAlso, moving 40TB of data one-time using snowball will be the right strategy here as this is the one-time movement of large data.\nOption C is Incorrect because deploying a container on EC2 will require lots of management task and also moving 40TB using 1GBPS will not right strategy here as this is a one-time data movement.\nOption D is Correct because RDS Aurora PostgreSQL provides better performance.\nOption E is Incorrect because running PostgreSQL in EC2 will require lots of management task like installation, Patching etc.\nReference:\nhttps://aws.amazon.com/rds/aurora/postgresql-features/ https://www.youtube.com/watch?v=4xqOoRPrnAw https://aws.amazon.com/blogs/compute/migrating-your-amazon-ecs-containers-to-aws- https://d1.awsstatic.com/events/reinvent/2019/Managing_large-cale_offline_data_migrations_Best_practices_STG337.pdf\nThe best approach for migrating an on-premises datacenter with critical portal and 40 TB PostgreSQL database to AWS would be a combination of options A and D.\nOption A recommends choosing EC2 to deploy the containers on ECS and move the 40 TB data using Snowball on AWS. EC2 is a good option as it provides more control and flexibility in terms of container deployment. ECS is a fully-managed container orchestration service that makes it easy to run, stop, and manage Docker containers on a cluster. Snowball is a service that allows for secure, fast, and cost-effective transfer of large amounts of data into and out of AWS. With Snowball, it is possible to transfer up to 80TB of data into or out of AWS using a physical storage appliance shipped to the customer's datacenter.\nOption D recommends using the RDS Aurora PostgreSQL Database in AWS. RDS is a managed database service that simplifies database administration tasks and provides automated backups, software patching, and scaling capabilities. Aurora is a PostgreSQL-compatible database engine that is designed for high availability, scalability, and performance. Aurora provides up to five times the performance of a standard PostgreSQL database while being compatible with PostgreSQL.\nOption B suggests using Fargate to deploy the containers on ECS and move the 40 TB data using Snowball on AWS. Fargate is a serverless computing engine for containers that allows customers to run containers without having to manage the underlying infrastructure. However, Fargate has limitations in terms of container configuration and control, which makes it less ideal for critical workloads.\nOption C suggests choosing EC2 to deploy the containers on ECS and move the 40 TB data using 1Gbps Direct Connection. A Direct Connection provides a dedicated network connection between the customer's datacenter and AWS. While this can provide high bandwidth and low latency, it may be cost-prohibitive and requires additional infrastructure to set up.\nOption E recommends running PostgreSQL on top of EC2 Nitro-based systems for better database performance. While EC2 Nitro-based systems can provide better performance for workloads, it still requires the customer to manage and configure the underlying infrastructure, making it less ideal for critical workloads.\nIn summary, the best approach for migrating an on-premises datacenter with critical portal and 40 TB PostgreSQL database to AWS would be to use EC2 to deploy the containers on ECS, move the 40 TB data using Snowball, and use RDS Aurora PostgreSQL Database in AWS for high availability, scalability, and performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "They should choose EC2 to deploy the containers on ECS and move the 40TB data using snowball on AWS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They should choose Fargate to deploy the containers on ECS and move the 40TB data using snowball on AWS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "They should choose EC2 to deploy the containers on ECS and move the 40 TB data using 1Gbps Direct Connection Connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They should use the RDS Aurora PostgreSQL Database in AWS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "They should run the PostgreSQL on top of EC2 Nitro Based system for better Database Performance.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 25,
  "query" : "You have just developed a new mobile application that handles analytics workloads on large-scale datasets stored on Amazon Redshift.\nConsequently, the application needs to access Amazon Redshift tables.\nYour company is asking to expand the scope of the application.\nWhich of the following methods would be the best, both practically and security-wise, to access the tables? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nTip: When a service, user, or application needs to access an AWS resource, always prefer creating an IAM Role over creating an IAM User.\nOption A is incorrect because embedding keys in the application to access AWS resources is not a good architectural practice as it creates security concerns.\nOption B is incorrect because the Redshift cluster uses the HSM certificate to connect to the client's HSM to store and retrieve the keys used to encrypt the cluster databases.\nOption C is incorrect because the read-only policy is insufficient and embedding keys in the application to access AWS resource is not a good architectural practice as it creates security concerns.\nOption D is CORRECT because (a) IAM role allows the least privileged access to the AWS resource, (b) web identity federation ensures the identity of the user, and (c) the user is given temporary credentials to access the AWS resource.\nFor more information on IAM policies, please refer to the below link-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\nFor more information on web identity federation, please refer to the below link-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\nThe best approach for accessing Amazon Redshift tables from a mobile application, both from a practical and security standpoint, is to use web identity federation with roles that grant access to Redshift tables using temporary credentials. Therefore, option D is the correct answer.\nHere is a more detailed explanation of each of the options and why option D is the best choice:\nA. Create an IAM user and generate encryption keys for that user. Create a policy for Redshift read-only access. Embed the keys in the application. This option is not the best choice because it requires embedding encryption keys in the application, which is not secure. If someone gains access to the keys, they could use them to access the Redshift tables. In addition, this option would require the application to manage encryption keys, which could be complex and difficult to maintain.\nB. Create an HSM client certificate in Redshift and authenticate using this certificate. This option is not practical for a mobile application because it would require a client certificate to be installed on the mobile device. This would be difficult to manage and not very secure, as certificates can be easily copied and distributed.\nC. Create a Redshift read-only access policy in IAM and embed those credentials in the application. This option is not as secure as option D because it requires embedding IAM credentials in the application. If someone gains access to the credentials, they could use them to access the Redshift tables. In addition, this option would require the application to manage IAM credentials, which could be complex and difficult to maintain.\nD. Use roles that allow a web identity federated user to assume a role that allows access to the Redshift table by providing temporary credentials. This option is the best choice because it uses web identity federation with roles that grant access to Redshift tables using temporary credentials. This approach is both practical and secure. The mobile application can authenticate users using web identity federation with Amazon Cognito, which allows the application to grant access to the Redshift tables without the need to manage IAM credentials or encryption keys. Instead, temporary credentials are issued to the application on behalf of the authenticated user, and those credentials can be used to access the Redshift tables. This approach minimizes the risk of credential compromise and simplifies the management of access to Redshift tables.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an IAM user and generate encryption keys for that user. Create a policy for Redshift read-only access. Embed the keys in the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an HSM client certificate in Redshift and authenticate using this certificate.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Redshift read-only access policy in IAM and embed those credentials in the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use roles that allow a web identity federated user to assume a role that allows access to the Redshift table by providing temporary credentials.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 26,
  "query" : "Your company has HQ in Tokyo and branch offices worldwide and uses logistics software with a multi-regional deployment on AWS in Japan, Europe and USA.\nThe logistic software has a 3-tier architecture and currently uses MySQL 5.6 for data persistence.\nEach region has deployed its own database.\nIn the HQ region, you run an hourly batch process reading data from every region to compute cross-regional reports sent by email to all offices.\nThis batch process must be completed as fast as possible to optimize logistics quickly.\nHow do you build the database architecture to meet the requirements?",
  "answer" : "E.\nAnswer - D.\nThe problem in the scenario is that an hourly batch process is currently run at the HQ region that reads the data from every region to compute cross-regional reports.\nThis is a slow process and needs to be quickened.\nThe most ideal scenario would be to have the replicated database in the HQ region updated asynchronously.\nOption A is incorrect because copying the data hourly to HQ region would be slow compared to the best option, which is.\nD.Option B is incorrect because (a) taking hourly EBS snapshots would affect the database's performance in its master region, and (b)copying the snapshots hourly across the region would be a slow process.\nOption C is incorrect because (a) taking hourly RDS snapshots would affect the database's performance in its master region, and (b)sending the snapshots hourly across the region would be a slow and very costly process.\nOption D is CORRECT because (a) it creates a read replica in the HQ region updated asynchronously.\nThis way, generating the reports would be very quick, and (b) it does not affect the databases' performance in their respective master region.\nOption E is incorrect because AWS Direct Connect is useless when there is no on-premise datacenter involved.\nFor more information on cross-region read replicas, please visit the link below.\nhttps://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/\nThe requirement is to optimize logistics quickly by completing the hourly batch process as fast as possible. The current architecture uses MySQL 5.6 for data persistence and has a 3-tier architecture with regional deployments in Japan, Europe, and USA. Each region has its own database. The data needs to be aggregated from all regions in the HQ region to compute cross-regional reports sent by email to all offices.\nOption A suggests using MySQL on EC2 with a master in each regional deployment and using S3 to copy data files hourly to the HQ region. This option requires managing EC2 instances, replication, and backups. Moreover, it does not provide a real-time data replication mechanism, and copying data files hourly can cause data loss and inconsistency.\nOption B suggests using MySQL on EC2 with a master in each regional deployment and sending hourly EBS snapshots to the HQ region. This option also requires managing EC2 instances, replication, and backups. Sending EBS snapshots is faster than copying data files, but it can still cause data loss and inconsistency.\nOption C suggests using RDS MySQL with a master in each regional deployment and sending hourly RDS snapshots to the HQ region. RDS provides managed database services, which eliminates the need for managing EC2 instances, replication, and backups. RDS snapshots are faster and provide better data consistency than EBS snapshots or copying data files. However, sending RDS snapshots can cause a network overhead and may not provide real-time data replication.\nOption D suggests using RDS MySQL with a master in each regional deployment and a read replica in the HQ region. This option provides real-time data replication with low network overhead. RDS read replicas can handle read traffic and improve query performance, which can help optimize logistics quickly. However, RDS read replicas lag behind the master, and this can cause data inconsistency.\nOption E suggests using Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process. Direct Connect provides a dedicated network connection between regions, which can reduce network latency and improve data transfer speed. However, Direct Connect requires additional configuration and may not provide real-time data replication.\nOut of these options, the best choice is Option D. Using RDS MySQL with a master in each regional deployment and a read replica in the HQ region provides real-time data replication with low network overhead, read traffic handling, and query performance improvement. It also eliminates the need for managing EC2 instances, replication, and backups. The RDS service takes care of all the heavy lifting, including backups, software patching, and maintenance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data files hourly to the HQ region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS snapshots to the HQ region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS snapshots to the HQ region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 27,
  "query" : "A legacy application with a license is attached to a single MAC address.\nAn EC2 instance can receive a new MAC address while launching new instances.\nHow can you ensure that your EC2 instances can maintain a single MAC address for licensing? Choose the correct option.",
  "answer" : "Answer - A.\nTip: Whenever a question has a scenario where you need to use a fixed MAC address for EC2 instances, always think about using Elastic Network Interface (ENI).\nIf a static MAC address is assigned to an ENI, it remains unchanged.\nAs long as the EC2 has that ENI, its MAC address will not change.\nOption A is CORRECT because, as mentioned above, as ENI with static MAC address can be assigned to the EC2 instance.\nIf the instance becomes unavailable or needs to be replaced, the ENI can be detached and re-attached to another EC2 while maintaining the same MAC address.\nOption B is incorrect because subnets have CIDR, not static MAC addresses.\nOption C is incorrect because if the EC2 instance fails or becomes unavailable, its MAC address cannot be reused with another EC2 instance.\nOption D is incorrect because you can avail of ENI in order to have a static MAC address for the EC2 instances.\nMore information on ENI on AWS Documentation:\nCreate a Low Budget High Availability Solution.\nIf one of your instances serving a particular function fails, its network interface can be attached to a replacement or hot standby instance pre-configured for the same role in order to recover the service rapidly.\nFor example, you can use a network interface as your primary or secondary network interface to a critical service such as a database instance or a NAT instance.\nIf the instance fails, you (or more likely, the code running on your behalf) can attach the network interface to a hot standby instance.\nBecause the interface maintains its private IP addresses, Elastic IP addresses, and MAC address, network traffic begins flowing to the standby instance as soon as you attach the network interface to the replacement instance.\nUsers experience a brief loss of connectivity between the time the instance fails and when the network interface is attached to the standby instance, but no changes to the VPC route table or your DNS server are required.\nBest Practices for Configuring Network Interfaces.\nFor more information on elastic network interfaces, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\nThe correct answer to ensure that EC2 instances can maintain a single MAC address for licensing is Option A: Create an ENI and assign it to the EC2 instance. The ENI will have a static MAC address and can be detached and reattached to a new instance if the current instance becomes unavailable.\nExplanation: When launching an EC2 instance, the MAC address of the instance will be randomly generated. This can be problematic if a legacy application requires a fixed MAC address for licensing purposes. The solution to this problem is to use an Elastic Network Interface (ENI).\nAn ENI is a virtual network interface that can be attached to an EC2 instance. When an ENI is created, it is assigned a static MAC address, which can be specified during the creation of the ENI. Once an ENI is created and attached to an EC2 instance, the instance will use the static MAC address assigned to the ENI.\nIf the EC2 instance needs to be replaced or terminated, the ENI can be detached from the instance and attached to a new instance. This ensures that the new instance will use the same MAC address as the previous instance, which is required for licensing purposes.\nOption B is incorrect because private subnets do not have static MAC addresses. A subnet is a range of IP addresses in a VPC, and each instance in the subnet will have its own MAC address.\nOption C is not a feasible solution because configuring a manual MAC address for each EC2 instance is not practical, especially in large-scale deployments.\nOption D is also incorrect because creating a VPN or Virtual Private Gateway (VGW) has nothing to do with maintaining a fixed MAC address for licensing purposes. A VPN or VGW is used to establish a secure and encrypted connection between an on-premises network and a VPC in AWS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an ENI and assign it to the EC2 instance. The ENI will have a static MAC address and can be detached and reattached to a new instance if the current instance becomes unavailable.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Private subnets have static MAC addresses. Launch the EC2 instance in a private subnet and, if required, use a NAT to serve data over the internet.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a manual MAC address for each EC2 instance and report that to the licensing company.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS cannot have a fixed MAC address; the best solution is to create a dedicated VPN/VGW gateway to serve data from the legacy application.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 28,
  "query" : "Your company has just set up a new document server on its AWS VPC, and it has four very important clients that it wants to give access to.\nThese clients also have VPCs on AWS, and it is through these VPCs, they will be given access to the document server.\nIn addition, each of the clients should not have access to any of the other clients' VPCs.\nChoose the correct answer from the options below.",
  "answer" : "E.\nAnswer - A.\nIn this scenario, you are asked how resources from 4 VPCs can access resources from another VPC.\nThis is a use case of \"Star-Shaped\" VPC peering shown in the image below.\nIn this configuration, VPCs that have non-overlapping CIDR with your VPC, are peered for the intent of accessing the resources using their private IP addresses.\nOption A is CORRECT because, as mentioned above, the peered VPCs can share and access the resources within each other via their private IP addresses.\nOption B is incorrect because you do not need to block any IP addresses in this scenario.\nOption C is incorrect because the peering among the client VPCs is unnecessary.\nThe only peering that is needed is between each client and your VPC.Option D is incorrect because, for VPC Peering, the VPCs should not have overlapping CIDRs.\nSo, VPCs having the same CIDR cannot peer.\nFor more information on VPC Peering, please see the below link.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html\nNote:\nThe scenario in question is describing an architecture similar to the one given below.\nVPC A is your company and the rest of the other 6 companies are your clients and you have set up separate VPC peering connection with each of your clients, allowing only them to have communication with your company VPC.\nVPC D can communicate with VPC A.\nVPC B can communicate with VPC A in the above configuration.\nBut VPC D cannot communicate with VPC B through VPC peering connection.\nThe correct answer for this scenario is option B: Set up VPC peering between your companys VPC and each of the clients VPCs, but block the IPs from the CIDR of the clients' VPCs to deny access between each other.\nExplanation: VPC peering is a way to connect two VPCs together and enable traffic to flow between them as if they were part of the same network. With VPC peering, you can enable communication between your company's VPC and each of the clients' VPCs, and at the same time, restrict access between the clients' VPCs.\nHowever, it's important to note that by default, VPC peering allows traffic to flow between the peered VPCs, which means that without any additional configuration, the clients would have access to each other's VPCs. To prevent this, option B proposes to block the IPs from the CIDR of the clients' VPCs, which means that traffic will be blocked between the clients' VPCs while allowing access to your company's VPC.\nOption A suggests setting up VPC peering between your company's VPC and each of the clients' VPCs, which is correct. However, it doesn't take into account the need to restrict access between the clients' VPCs.\nOption C is similar to option A and doesn't provide any additional security measures to restrict access between the clients' VPCs.\nOption D is incorrect because it suggests setting up VPC peering between the clients' VPCs, which would create a mesh network that could potentially create security and routing issues.\nOption E is also incorrect because it suggests setting up all the VPCs with the same CIDR, which is not possible. A CIDR (Classless Inter-Domain Routing) block is a range of IP addresses that defines the size of the network. Each VPC must have a unique CIDR block to avoid conflicts.\nIn summary, the best solution for this scenario is to set up VPC peering between your company's VPC and each of the clients' VPCs, but block the IPs from the CIDR of the clients' VPCs to deny access between each other.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up VPC peering between your company`s VPC and each of the clients` VPCs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up VPC peering between your company`s VPC and each of the clients` VPCs, but block the IPs from CIDR of the clients` VPCs to deny access between each other.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up VPC peering between your company`s VPC and each of the clients` VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Each client should have VPC peering set up between each other to speed up access time.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up all the VPCs with the same CIDR but have your company`s VPC as a centralized VP.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 29,
  "query" : "The manufacturing based company is using SAP S/4 HANA as part of their digital transformation journey.\nThey have deployed SAP S/4 HANA on AWS for better performance and to get the cost benefits.\nThe On-prem network is connected with AWS via AWS Direct Connect with 500Mbps Connection.\nCurrently, 600 employees access the SAP Application using SAP GUI on their local computers from various locations.\nThey are facing latency issues in uploading and downloading the data using SAP GUI.\nSAP Admin is also facing challenges to maintain the same version of SAP GUI in all Systems.\nBesides, you want users to only use up-to-date versions of SAP GUI.\nThe company wants to overcome this issue very quickly.\nWhat should they do?",
  "answer" : "Answer: D.\nOption A is Incorrect because increasing the direct connect speed will not solve the upload and download speed issue and managing the same SAP GUI Version issue as the users are located at various locations.\nOption B is Incorrect because increasing the direct connect speed will not solve the upload and download speed issue and managing the same SAP GUI Version issue as the users are located at various locations.\nOption C is incorrect because it will take time to deploy the same version in 600 systems.\nOption D is Correct because SAP GUI runs on AWS next to the SAP environment, increasing the employee's experience with SAP.\nSAP GUI also helps in maintaining the same version across the systems.\nWith this option, users only access up-to-date versions of SAP GUI, and you don't have to deploy updates to every user's computer.Reference:\nhttps://aws.amazon.com/appstream2/enterprises/?nc=sn&amp;loc=2&amp;dn=3 https://www.youtube.com/watch?v=qAFlv1m3MX0 https://aws.amazon.com/blogs/desktop-and-application-streaming/deploying-sap-gui-on- https://aws.amazon.com/appstream2/features/\nThe manufacturing company is facing latency issues while accessing SAP applications using SAP GUI from various locations. In addition, SAP admin is facing challenges in maintaining the same version of SAP GUI in all systems. The company wants to overcome these issues quickly.\nOption A: Increasing Direct Connect speed from 500Mbps to 1Gbps. Increasing Direct Connect speed may improve the network performance, but it may not solve the issue of SAP GUI version mismatch and maintaining the same version across different systems.\nOption B: Increasing Direct Connect speed from 500Mbps to 1Gbps and providing AWS WorkSpace to individual employees. Providing AWS WorkSpace may improve the user experience by providing a centralized location to access SAP GUI, which can be updated by SAP admin easily. However, increasing the Direct Connect speed may not solve the issue of maintaining the same version of SAP GUI.\nOption C: Deploying the SAP GUI the same version on individual systems. Deploying the same version of SAP GUI on all systems may solve the issue of version mismatch, but it may not solve the performance issue, which may still persist.\nOption D: Deploying the SAP GUI on Amazon AppStream 2.0. Deploying SAP GUI on Amazon AppStream 2.0 may solve both the issues of version mismatch and performance. Amazon AppStream 2.0 is a fully managed application streaming service that provides on-demand access to desktop applications. With Amazon AppStream 2.0, the SAP admin can manage a single version of SAP GUI, and users can access it from anywhere using a web browser or supported client. Amazon AppStream 2.0 also provides a scalable infrastructure that can handle variable demand, reducing the need for expensive hardware.\nTherefore, Option D is the best solution for the manufacturing company to overcome the issues they are facing.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "They should increase the Direct Connect speed from 500Mbps to 1Gbps.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They should increase the Direct Connect speed from 500Mbps to 1Gbps and also provide AWS WorkSpace to individual employees.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They should deploy the SAP GUI the same version on individual systems.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They should deploy the SAP GUI on Amazon AppStream 2.0.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 30,
  "query" : "A company needs to configure a NAT instance for its internal AWS applications to be able to download patches and package software.\nCurrently, they are running a NAT instance that is using the floating IP scripting configuration to create fault tolerance for the NAT.\nWhat is the best way to configure the NAT instance with fault tolerance?",
  "answer" : "Answer - C.\nOption A is incorrect because you would need at least two NAT instances for fault tolerance.\nOption B is incorrect because if you put both NAT instances in a single public subnet and that subnet becomes unavailable or unreachable to the other instances, the architecture would not be fault tolerant.\nOption C is CORRECT because you should place two NAT instances in two separate public subnets, and create routes from instances via each NAT instance for achieving fault tolerance.\nOption D is incorrect because you should not be putting the NAT instances in the private subnet as they need to communicate with the internet.\nThey should be in the public subnet.\nMore information on NAT instances:\nOne approach to this situation is to leverage multiple NAT instances that can take over for each other if the other NAT instance should fail.\nThis walkthrough and associated monitoring script (nat_monitor.sh) provide instructions for building an HA scenario where two NAT instances in separate Availability Zones (AZ) continuously monitor each other.\nIf one NAT instance fails, this script enables the working NAT instance to take over outbound traffic and attempts to fix the failed instance by stopping and restarting it.\nBelow is a diagram for fault tolerant NAT instances.\nFor more information on fault-tolerant NAT gateways, please see the below link-\nhttps://aws.amazon.com/articles/2781451301784570\nSure, I'll be happy to explain in detail the options provided and which one is the best way to configure the NAT instance with fault tolerance.\nA. Create one NAT instance in a public subnet and create a route from the private subnet to the NAT instance. This option involves creating a single NAT instance in a public subnet, which will act as a gateway for the private subnet to download patches and package software. The private subnet will have a route to the NAT instance, allowing traffic to be forwarded to the internet. This approach has a single point of failure since there is only one NAT instance. If the instance fails, the private subnet will not be able to download patches and package software, and there will be no fault tolerance.\nB. Create two NAT instances in a public subnet and create a route from the private subnet to each NAT instance for fault tolerance. This option involves creating two NAT instances in a public subnet, and the private subnet will have a route to both NAT instances. This approach provides fault tolerance since if one instance fails, the other instance can take over. However, managing two NAT instances can be complex, and there is a possibility of asymmetric routing.\nC. Create a NAT instance in a public subnet with the application running in a private subnet in an AZ. Create a similar architecture in another AZ and create a route from the private subnet to each NAT instance residing in these AZ's for fault tolerance. This option involves creating a NAT instance in a public subnet and running the application in a private subnet in one AZ. The same architecture is created in another AZ, and the private subnet has a route to both NAT instances. This approach provides fault tolerance since if one AZ goes down, the other AZ can take over. Additionally, there is no need to manage two NAT instances, which simplifies the architecture.\nD. Create two NAT instances in two separate private subnets. This option involves creating two NAT instances in two separate private subnets, which will act as gateways for the private subnet to download patches and package software. However, this approach is not a recommended solution since the NAT instances will not have a direct connection to the internet, and additional routing configuration is required to allow traffic to be forwarded to the internet. Additionally, there is no fault tolerance since both NAT instances are in separate subnets.\nIn conclusion, the best way to configure the NAT instance with fault tolerance is option C. This option provides fault tolerance, and there is no need to manage two NAT instances, which simplifies the architecture. Additionally, this option provides a direct connection to the internet, eliminating the need for additional routing configuration.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create one NAT instance in a public subnet and create a route from the private subnet to the NAT instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create two NAT instances in a public subnet and create a route from the private subnet to each NAT instance for fault tolerance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a NAT instance in a public subnet with application running in private subnet in an AZ. Create a similar architecture in another AZ and create a route from the private subnet to each NAT instance residing in these AZ`s for fault tolerance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create two NAT instances in two separate private subnets.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 31,
  "query" : "Your company has received a contract to augment a legacy enterprise file sharing system for a large media house.\nAs of now, the company is using an on-premises private file sharing solution that is integrated with its directory service and uses a web-based intranet application to share files.\nWith growing remote staff, the company needs support for mobile devices so that remote staff can work offline as well.\nPlease suggest a valid option to architect the system with the given requirements.",
  "answer" : "Correct Answer: C.\nOption A and B are INCORRECT because this will require a full-scale development of web and mobile application.\nOption C is CORRECT as AWS WorkDocs provides all the necessary resources to create an enterprise-level file sharing solution.\nIt can integrate with Enterprise Directory Service as well.\nOption D is INCORRECT because with S3 bucket level policies and static website hosting, it will not be sufficient to build the required solution.\nThe requirement is to augment a legacy enterprise file sharing system for a large media house that uses an on-premises private file sharing solution integrated with its directory service and a web-based intranet application to share files. The company needs support for mobile devices, allowing remote staff to work offline. Based on this requirement, the best option is to use AWS WorkDocs.\nAWS WorkDocs is a fully managed, secure content creation, storage, and collaboration service that provides a centralized location to store and manage documents. WorkDocs integrates with existing directory services, such as AD Connector, to provide a familiar user and administrative experience. WorkDocs can be accessed through web browsers, mobile devices, and desktop clients, making it suitable for remote staff to work offline.\nOption A suggests migrating all files to S3 and building a web and mobile application to share the files using IAM for user access management. This option doesn't take into account the integration with the existing directory service and the web-based intranet application. Migrating all files to S3 may also require additional effort to maintain directory services and file access controls.\nOption B suggests migrating the existing system to EC2 with Autoscaling and saving all files to S3. This option doesn't provide an offline capability for remote staff to work. It also requires more effort to set up and maintain the EC2 instances and autoscaling group.\nOption D suggests using Simple AD integration with IAM and using S3 bucket level policies to share files via S3. On S3, enable Static website hosting for web access. This option also doesn't take into account the offline capability for remote staff to work. It may also require additional effort to maintain directory services and file access controls.\nTherefore, option C, using AWS WorkDocs and integrating it with the on-premise directory server using the AD Connector, is the best option. AWS WorkDocs provides a familiar user and administrative experience, integrates with the existing directory service, and allows remote staff to work offline using mobile devices. It also provides easy-to-use web access and eliminates the need to manage infrastructure or file access controls.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Migrate all the files to S3 and build a web and mobile application to share the files. Use IAM for user access management.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Migrate the existing system to EC2 with Autoscaling and save all the files to S3. Use AD Connector for user access management.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the AWS WorkDocs and integrate it with the on-premise directory server using the AD Connector.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Simple AD integration with IAM, and use S3 bucket level policies to share the files via S3. On S3, enable the Static website hosting for web access.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 32,
  "query" : "A 3-tier e-commerce web application is currently deployed on-premises and will be migrated to AWS for greater scalability and elasticity.\nThe web server currently shares read-only data using a network distributed file system.\nThe app server tier uses a clustering mechanism for discovery and shared session state that depends on IP multicast.\nThe database tier uses shared-storage clustering to provide database failover capability and uses several read replicas for scaling.\nData on all servers and the distributed file system directory is backed up weekly on a separate backup server.\nWhich of the following AWS storage and database architectures meets the requirements of the application?",
  "answer" : "Answer - A.\nThe main requirements of this scenario are: (1) the application should be scalable and elastic, (2) app servers should be able to share the state, (3) need read replicas, and (4) weekly backup of the data.\nOption A is CORRECT because (a) the overall architecture is highly available, elastic, and scalable, (b) web servers share state using DynamoDB and IP unicast that is supported by AWS, (c) it supports read replicas, and (d) weekly backup for servers using AMIs and data using DB snapshots.\nOption B is incorrect because you cannot backup data to Glacier using snapshots.\nOption C is incorrect because it does not address the requirement of having read replicas for elasticity and scalability.\nOption D is incorrect because it is not suitable to store the read-only data in EC2\nS3 should be selected as it is a more available and durable storage service.\nFor more information on this topic, please visit the link below.\nhttps://d0.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf\nOption A is the correct answer. This option provides a combination of AWS storage and database services that meet the requirements of the application.\nWeb servers store read-only data in S3, and this data is copied from S3 to the root volume at boot time. This ensures that each web server has the necessary data to operate. This approach allows for greater scalability and fault tolerance, as new web servers can be launched and terminated easily, and they will automatically have the required data available.\nThe app servers share state using DynamoDB and IP unicast. DynamoDB is a highly available and scalable NoSQL database service that can be used to store session data. IP unicast is a unidirectional communication protocol that can be used to share state across app servers.\nThe database tier uses RDS with a multi-AZ deployment and one or more read replicas. RDS is a managed database service that provides high availability and durability. The multi-AZ deployment ensures that the database is available even if one of the availability zones fails. The read replicas provide scalability for read-heavy workloads.\nWeb and app servers are backed up weekly via snapshots. Snapshots are point-in-time copies of an Amazon EBS volume, and they are stored in Amazon S3. These snapshots can be used to create Amazon Machine Images (AMIs). The AMIs can then be used to launch new instances of the web and app servers.\nThe database is backed up via DB snapshots. DB snapshots are also point-in-time copies, but they are specific to RDS instances. These snapshots can be used to restore the database to a specific point in time.\nOption B is incorrect because it proposes backing up the web servers, app servers, and database to Glacier using snapshots. Glacier is a low-cost storage service designed for long-term backup and archive storage. It is not suitable for regular backups, as it has a longer retrieval time than S3.\nOption C is incorrect because it proposes backing up the web and app servers via AMIs. While AMIs can be used to launch new instances of the servers, they are not ideal for regular backups. This is because creating and storing AMIs is more time-consuming and resource-intensive than creating snapshots.\nOption D is incorrect because it proposes using an EC2 NFS server to store read-only data for the web servers. This approach does not provide the same level of scalability and fault tolerance as S3. Additionally, it proposes using IP multicast to share state across app servers. IP multicast is not supported in VPCs, which can limit the ability to scale the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Web servers store read-only data in S3, and copy it from S3 to root volume at boot time. App servers share state using a combination of DynamoDB and IP unicast. Database use RDS with multi-AZ deployment and one or more Read Replicas. Backup web and app servers weekly via snapshots. Use the snapshots to create the AMIs ( data copied to EC2 instances from the dedicated backup servers ). Back up the database via DB snapshots.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Web servers store read-only data in S3, and copy it from S3 to root volume at boot time. App servers share state using a combination of DynamoDB and IP unicast. Database use RDS with multi-AZ deployment and one or more Read Replicas. Backup web servers, app servers, and database weekly to Glacier using snapshots.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Web servers store read-only data in S3 and copy it from S3 to root volume at boot time. App servers share state using a combination of DynamoDB and IP unicast. Database use RDS with multi-AZ deployment. Backup web and app servers weekly via AMIs. Back up the database via DB snapshots.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Web servers store read-only data in an EC2 NFS server and mount to each web server at boot time. App servers share state using a combination of DynamoDB and IP multicast. Database use RDS with multi-AZ deployment and one or more Read Replicas. Backup web and app servers weekly via AMIs, and back up the database via DB snapshots.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 33,
  "query" : "You have a legacy application running that uses an m4.large instance size and cannot scale with Auto Scaling, but only has peak performance 5% of the time.\nThis is a huge waste of resources and money.\nSo your Senior Technical Manager has set you the task of trying to reduce costs while still keeping the legacy application, having lesser memory requirements, running in the long-term as it should.\nWhich of the following will best accomplish the task your manager has assigned you? Choose the correct answer from the options below.",
  "answer" : "Answer - A.\nThe AWS documentation clearly indicates using T2 EC2 instance types for those instances that often don't use CPU.\nT2\nT2 instances are Burstable Performance Instances that provide a baseline level of CPU performance with the ability to burst above the baseline.\nT2 Unlimited instances can sustain high CPU performance for as long as a workload needs it.\nFor most general-purpose workloads, T2 Unlimited instances will provide ample performance without any additional charges.\nIf the instance needs to run at higher CPU utilization for a prolonged period, it can also do so at a flat additional charge of 5 cents per vCPU-hour.\nCPU Credits govern the baseline performance and ability to burst.\nT2 instances receive CPU Credits continuously at a set rate depending on the instance size, accumulating CPU Credits when they are idle, and consuming CPU credits when active.\nT2 instances are a good choice for various general-purpose workloads, including microservices, low-latency interactive applications, small and medium databases, virtual desktops, development, build and stage environments, code repositories, and product prototypes.\nFor more information, see Burstable Performance Instances.\nFor more information on EC2 instance types, please see the below link:\nhttps://aws.amazon.com/ec2/instance-types/\nOption B is INCORRECT because using a \"C4\" instance would incur more costs and as per the requirements of the question, we need to reduce the costs.\nPlease refer to the below link for instances pricing.\nhttps://aws.amazon.com/ec2/pricing/on-demand/\nOption C is INCORRECT because although \"t2.nano\" instance pricing is less, it has only 0.5 GiB of allocated memory, making the legacy application run too slow.\nOption D is INCORRECT because there are chances that the spot instances would not be available when there is a need.\nTherefore, there are chances for the legacy application to break down for want of more instances when needed.\nThe best option for reducing costs while keeping the legacy application running with lesser memory requirements is to use a \"t2.medium - 3 yr RI\" burstable performance instance (Option A).\nExplanation:\nOption A suggests using a \"t2.medium\" instance with a \"3 yr RI\" (Reserved Instance). A \"t2.medium\" instance has 4GB of memory and 2 vCPUs, which is less than the current \"m4.large\" instance (8GB memory and 2 vCPUs). However, the \"t2.medium\" instance has a feature called \"burstable performance,\" which means that it can use CPU credits to burst above its baseline performance when needed. This makes it ideal for workloads that have peak performance requirements for a short period of time, as in the case of the legacy application that only requires peak performance 5% of the time.\nOption B suggests using a C4.large instance with enhanced networking. This option does not address the issue of peak performance and may not be cost-effective as the C4 instance family is optimized for compute-intensive workloads that require high performance processors, which may be unnecessary for the legacy application.\nOption C suggests using two t2.nano instances with single Root I/O Virtualization. This option may be cost-effective, but it may not provide enough memory and processing power for the legacy application.\nOption D suggests using a t2.nano instance and adding spot instances when they are required. This option may be cost-effective, but it does not guarantee that the required computing resources will be available when needed, which could impact the performance of the legacy application.\nTherefore, the best option is to use a \"t2.medium - 3 yr RI\" burstable performance instance (Option A) as it provides a balance of cost and computing resources required for the legacy application, with the added benefit of being able to burst when peak performance is required.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a \"t2.medium - 3 yr RI\" burstable performance instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a C4.large instance with enhanced networking.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use two t2.nano instances that have single Root I/O Virtualization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use t2.nano instance and add spot instances when they are required.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 34,
  "query" : "The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network.\nYou can have multiple sets of DHCP options, but you can associate only one set of DHCP options with a VPC at a time.\nYou have just created your first set of DHCP options, associated it with your VPC but now realize that you have made an error in setting them up and you need to change the options.\nWhich of the following options do you need to take to achieve this? Choose the correct answer from the options below.",
  "answer" : "E.\nAnswer - C.\nOption A, B, and D are incorrect because you cannot modify the DHCP options - neither via the console nor via CLI.\nOption C is CORRECT because once you create a set of DHCP options, you cannot modify them.\nYou must create a new set of DHCP options and associate it with your VPC.AWS Document says:\nChanging DHCP Options.\nAfter you create a set of DHCP options, you can't modify them.\nIf you want your VPC to use a different set of DHCP options, you must create a new set and associate them with your VPC.\nYou can also set up your VPC to use no DHCP options at all.\nYou can have multiple sets of DHCP options, but you can associate only one set of DHCP options with a VPC at a time.\nIf you delete a VPC, the DHCP options set associated with the VPC are also deleted.\nAfter you associate a new set of DHCP options with a VPC, any existing instances and all new instances that you launch in the VPC use these options.\nYou don't need to restart or relaunch the instances.\nThey automatically pick up the changes within a few hours, depending on how frequently the instance renews its DHCP lease.\nIf you want, you can explicitly renew the lease using the operating system on the instance.\nFor more information on DHCP Options set, please see the below link-\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html\nThe correct answer is C - you can modify the options from the console or the CLI.\nWhen you create a VPC in AWS, you can associate a set of DHCP options with it to provide network configuration information to instances launched within that VPC. DHCP options typically include information such as the DNS server to use, the domain name, and other network settings.\nAccording to the question, you have associated a set of DHCP options with your VPC, but you need to make changes to them. Fortunately, you can modify the DHCP options associated with a VPC at any time.\nTo modify the DHCP options associated with your VPC, you can do so from the AWS Management Console or the AWS Command Line Interface (CLI). In the console, you can navigate to the DHCP options set you wish to modify and make changes to its settings. Alternatively, you can use the modify-dhcp-options command in the CLI to make the necessary changes.\nYou do not need to stop all the instances in the VPC, create a new set of DHCP options, or modify the options from the CLI only. Instead, you can simply modify the existing DHCP options associated with your VPC to update the network configuration information passed to your instances.\nIt's worth noting that changes to DHCP options can take some time to propagate to all instances in the VPC, so you may need to wait a few minutes for the changes to take effect.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You need to stop all the instances in the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can then change the options, and they will take effect when you start the instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can modify the options from the console or the CLI.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You must create a new set of DHCP options and associate them with your VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can modify the options from the CLI only, not from the console.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 35,
  "query" : "Your company is running a microservice-based application.\nThey are using DynamoDB to store the data and AWS API Gateway for managing the Rest APIs.\nThey are also using Lambda non-proxy integration.\nThe development team recently made a change to one Rest API method.\nAfter that, the API does not seem to work as expected.\nYou have been asked to troubleshoot the issue.\nWhich is the correct statement?",
  "answer" : "Answer: B.\nOption A is incorrect because we can make changes in the Rest APIs in AWS API Gateway.\nOption B is Correct because updates to the method require Redeployment of the API.\nOption C is Incorrect because an update to the stage doesn't require the Redeployment of the API.\nOption D is incorrect because it's not mentioned in the question that they are using the caching.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/updating-api.html https://www.youtube.com/watch?v=9ElpSPXk-g8\nThe correct statement in this scenario is B. As the development team has recently made a change in the method of the Rest API, they should redeploy the API.\nAWS API Gateway is a fully managed service that makes it easy for developers to create, deploy, and manage APIs. It allows developers to create Rest APIs, WebSocket APIs, and HTTP APIs that can connect to AWS services or any publicly available HTTP endpoint.\nDynamoDB is a fast and flexible NoSQL database service provided by AWS. It allows developers to store and retrieve any amount of data, and it automatically scales up or down to handle any amount of traffic.\nLambda is a serverless computing service provided by AWS that allows developers to run code without provisioning or managing servers. With Lambda, developers can write code in different programming languages, such as Python, Java, and Node.js, and execute it in response to triggers, such as changes in data in DynamoDB or events from API Gateway.\nIn this scenario, the development team has made a change to one Rest API method, and the API does not seem to work as expected. This could be due to a number of reasons, such as an incorrect configuration of the API Gateway or Lambda, an error in the Lambda code, or a problem with the data in DynamoDB.\nTo troubleshoot the issue, the first step is to identify the root cause of the problem. This could involve checking the API Gateway logs, the Lambda logs, and the DynamoDB logs to see if there are any errors or issues. Once the root cause is identified, the appropriate action can be taken to resolve the issue.\nIn this scenario, the correct statement is B. As the development team has recently made a change in the method of the Rest API, they should redeploy the API. Redeploying the API will update the API Gateway configuration and ensure that the new method is correctly configured and integrated with the Lambda function. This will also ensure that any changes to the Lambda code are properly deployed and executed.\nChanging the cache time-to-live (TTL) value and updating the Lambda code, as suggested in option D, may not be the correct action to take in this scenario, as the issue may be related to the API Gateway configuration or the way the API is integrated with the Lambda function. Therefore, it is important to identify the root cause of the issue before taking any action.\nOption A, stating that you can't change Rest APIs in AWS API Gateway, is incorrect. API Gateway allows developers to create, deploy, and manage APIs, including making changes to the Rest APIs as needed.\nOption C, stating that the development team should redeploy the API because they made a change in the stage of the Rest API, may not be the correct action to take in this scenario, as it is unclear whether the change was made in the stage or the method of the API. However, if the change was made in the stage, redeploying the API would be the correct action to take.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You can’t change the Rest APIs in AWS API Gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "As the development team has recently done the change in the method of the Rest API. They should Redeploy the API.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "As the development team has recently done the change in the stage of the Rest API. They should Redeploy the API.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You should change the Cache time-to-live (TTL) value and update the Lambda Code.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 36,
  "query" : "You are a software engineer and are creating a new web service in AWS.\nThe service is about daily schedules where end users can configure and fetch.\nIt contains an AngularJs front end that deals with data in a DynamoDB table called \"UserScheduleData\" with read and write permissions.\nYou plan to use API gateway and Lambda to handle the backend service.\nDuring development, you also need to do integration testings frequently using curl for the API endpoints.\nYou have created a role “ScheduleRoleLambda” for the Lambda itself.\nWhat below options should you perform to ensure that the Lambda contains the necessary permissions in the service role? (Select TWO).",
  "answer" : "E.\nF.\nCorrect Answer - B, F.\nFirstly, the CloudWatch Logs permission for Lambda is required at a minimum.\nRefer to the below Lambda settings:\nEvery lambda needs this permission to create a log group, log stream, and put log events.\nBesides, access for DynamoDB is required for the Lambda.\n\"dynamodb:GetItem\" and \"dynamodb:PutItem\" are necessary.\nLastly, it needs to add permission to the permissions policy associated with the Lambda function.\nRun the add-permission AWS Lambda command to grant the Amazon API Gateway service principal (apigateway.amazonaws.com) permissions to invoke the Lambda function.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https-example.html\nOption A is incorrect, although AWS X-Ray can trace AWS Lambda functions, it is not mandatory.\nOption B is CORRECT because the Lambda function needs access to Amazon CloudWatch Logs for log streaming.\nOption C is incorrect, although this permission is needed, it does not belong to the Lambda function's service role.\nOption D is incorrect, SNS may help with error handling; however, it is optional and only needed depending on specific requirements.\nOption E is incorrect because, for the permissions of DynamoDB, the resource should be arn name of the DynamoDB table in accordance with the principle of least privilege.\n\"dynamodb:FetchItem\" is incorrect as well.\nThe below is an example:\nOption F is CORRECT because it correctly describes the permissions for DynamoDB.References:\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html https://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https-example.html\nSure, I can provide a detailed explanation of the options for you.\nA. AWSXrayWriteOnlyAccess policy is needed for “ScheduleRoleLambda” so that a segment record with details about the function invocation and execution can be saved for tracking and debug purpose.\nAWS X-Ray is a service that allows you to debug and analyze your serverless applications in production environments. This policy provides the Lambda function with permission to write to X-Ray, which is useful for debugging and tracing function invocations. This is particularly important if you are running a large-scale production system where you need to monitor and debug your functions.\nB. “ScheduleRoleLambda” should have a policy for CloudWatch Logs including CreateLogGroup, CreateLogStream and PutLogEvents.\nCloudWatch Logs is a service that allows you to store and monitor logs from your applications and systems. This policy provides the Lambda function with permission to create and write logs to CloudWatch Logs. This is important for debugging and monitoring the Lambda function. The CreateLogGroup, CreateLogStream, and PutLogEvents actions allow the Lambda function to create log groups, log streams, and put log events in CloudWatch Logs.\nC. Invoke permissions are needed to the permissions policy associated with your Lambda function so that the API Gateway can call the lambda function.\nWhen you create a Lambda function, you need to specify a permission policy that allows other AWS services to invoke the function. In this case, you need to add the API Gateway as a service that can invoke the Lambda function. This policy allows the API Gateway to call the Lambda function by granting the necessary permissions to the function's IAM role.\nD. “sns:publish” allow inline policy should be added into “ScheduleRoleLambda” for error handlings. For example, when exception appears, the message can be put into a dead letter queue via SNS publish.\nAmazon SNS is a messaging service that allows you to publish messages to topics that can be subscribed to by other AWS services. This policy provides the Lambda function with permission to publish messages to SNS topics. You can use SNS to send notifications or to handle errors in your Lambda function by putting messages into a dead letter queue. This policy allows the Lambda function to publish messages to SNS topics by granting the necessary permissions to the function's IAM role.\nE. “ScheduleRoleLambda” should contain an inline policy to allow DynamoDb access. The resource should be “*” and the action should contain \"dynamodb:FetchItem\", \"dynamodb:PutItem\" and \"dynamodb:Query\".\nThis policy provides the Lambda function with permission to read and write to a DynamoDB table. The resource specifies the table name and the actions allow the function to fetch, put and query items in the table. The resource is set to \"*\" which means all tables in DynamoDB are accessible. It is important to note that this policy grants broad access to all DynamoDB tables, so you should be careful when using it in production environments.\nF. An IAM policy to allow DynamoDb access is needed for “ScheduleRoleLambda”. The resource should be the arn of “UserScheduleData” and the actions should contain \"dynamodb:GetItem\" and \"dynamodb:PutItem\".\nThis policy provides the Lambda function with permission to read and write to a specific DynamoDB table. The resource is set to the ARN of the \"UserScheduleData\" table, which means the Lambda function only has access to this specific table. The actions allow the function to get and put items in the table. This policy provides more specific access control than the previous option, which grants access to all DynamoDB tables.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWSXrayWriteOnlyAccess policy is needed for “ScheduleRoleLambda” so that a segment record with details about the function invocation and execution can be saved for tracking and debug purpose.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "“ScheduleRoleLambda” should have a policy for CloudWatch Logs including CreateLogGroup, CreateLogStream and PutLogEvents.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Invoke permissions are needed to the permissions policy associated with your Lambda function so that the API Gateway can call the lambda function.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "“sns:publish” allow inline policy should be added into “ScheduleRoleLambda” for error handlings. For example, when exception appears, the message can be put into a dead letter queue via SNS publish.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "“ScheduleRoleLambda” should contain an inline policy to allow DynamoDb access. The resource should be “*” and the action should contain \"dynamodb:FetchItem\", \"dynamodb:PutItem\" and \"dynamodb:Query\".",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An IAM policy to allow DynamoDb access is needed for “ScheduleRoleLambda”. The resource should be the arn of “UserScheduleData” and the actions should contain \"dynamodb:GetItem\" and \"dynamodb:PutItem\".",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 37,
  "query" : "Your company is running an e-commerce application in On-prem.\nYour CIO has asked you to build a highly available, low cost, and easy solution to collect the user clickstream data in real-time.\nHow are you going to design the solution on AWS?",
  "answer" : "Answer: A.\nOption A is CORRECT because the Kinesis agent helps collect the clickstream data in a Kinesis data stream and store it in Amazon S3 Bucket.\nOption B is Incorrect because DynamoDB will not be the right storage for clickstream data.\nOption C is Incorrect because IoT Core can't help in collecting the clickstream data.\nOption D is Incorrect because Glacier is not the right storage class for storing clickstream data.\nReference:\nhttps://www.youtube.com/watch?v=TAkcRD6OxPw https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run- https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html\nThe best solution for collecting user clickstream data in real-time in a highly available, low-cost, and easy way on AWS would be to use Kinesis Data Streams in combination with an Amazon S3 bucket for further analysis. Therefore, option A is the correct answer.\nHere's how it would work:\n1.\nKinesis Agent: You would install and configure the Kinesis agent on your servers, which would allow you to capture user clickstream data as it's generated. This agent would be responsible for sending the data to the Kinesis Data Stream.\n2.\nKinesis Data Stream: The Kinesis Data Stream is a real-time, scalable data stream that can handle large volumes of data. It would receive the user clickstream data from the Kinesis agent and store it in a buffer until it's ready to be processed. This stream can also enable real-time processing of the data and its further distribution to multiple destinations.\n3.\nAmazon S3 Bucket: You would configure an S3 bucket to store the user clickstream data for further analysis. This would be a cost-effective way of storing the data, and it would also make it easy to retrieve and process the data later.\n4.\nData Analysis: You can use various AWS services like Amazon EMR, Amazon Athena, or Amazon Redshift to analyze the clickstream data and generate insights. Additionally, you can configure alerts, triggers, and dashboards using Amazon CloudWatch or Amazon QuickSight.\nOverall, this solution is scalable, highly available, and cost-effective as you only pay for what you use. It also provides you with flexibility as you can store data in various formats and process it using different tools.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a Kinesis agent to collect the clickstream data and send data to a Kinesis data stream. Store in Amazon S3 bucket for further analysis.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Kinesis agent to collect the clickstream data in Kinesis data stream and store it in DynamoDB for further analysis.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the IoT Core to collect the clickstream data in the Kinesis data stream and store the logs in Dynamo D.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the Kinesis agent to collect the clickstream data in the Kinesis data stream and store it in Amazon Glacier.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 38,
  "query" : "You're building a mobile application game.\nThe application needs permissions for each user to communicate and store data in DynamoDB tables.\nWhat is the best method for granting each mobile device that installs your application to access DynamoDB tables for storage when required? Choose the correct answer from the options below.",
  "answer" : "Answer - C.\nOption A is incorrect because IAM Roles are preferred over IAM Users because IAM Users have to access the AWS resources using access and secret keys, which is a security concern.\nOption B is not a feasible configuration.\nOption C is CORRECT because it (a) creates an IAM Role with the needed permissions to connect to DynamoDB, (b) it authenticates the users with Web Identity Federation, and (c) the application accesses the DynamoDB with temporary credentials that are given by STS.\nOption D is incorrect because creating the Active Directory (AD) server and using AD for authenticating are unnecessary and costly.\nSee the note below for more information on AssumeRoleWithWebIdentity API.\nWhen you write such an app, you'll make requests to AWS services that must be signed with an AWS access key.\nHowever, we strongly recommend that you do not embed or distribute long-term AWS credentials with apps that users download to a device, even in an encrypted store.\nInstead, build your app to request temporary AWS security credentials dynamically when needed using web identity federation.\nThe supplied temporary credentials map to an AWS role that has only the permissions needed to perform the tasks required by the mobile app.\nFor more information on web identity federation, please refer to the below link-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\nThe best method for granting each mobile device that installs your application to access DynamoDB tables for storage when required is option C, which involves creating an IAM role with the proper permission policy to communicate with the DynamoDB table and using web identity federation to grant temporary security credentials using STS when the user signs in.\nExplanation:\nOption A is not recommended because IAM users are intended for long-term use and not suited for mobile devices that may change frequently. Additionally, creating IAM credentials for each user during game configuration can be time-consuming and difficult to manage.\nOption B may be a viable solution, but it requires the developer to manage the mapping of device IDs to IAM users, which can be challenging and error-prone.\nOption C is the recommended approach as it uses web identity federation, which allows your application to authenticate users through popular identity providers such as Amazon, Facebook, and Google, among others. By assuming an IAM role with the proper permissions to communicate with DynamoDB, the user is granted temporary security credentials that can be used to access DynamoDB. This approach ensures that the mobile device can access the DynamoDB table without the need for a long-lived IAM user, and it is secure as the temporary security credentials are limited in time.\nOption D is not a recommended solution for mobile applications as it involves creating an Active Directory server and an AD user for each mobile application user. This approach is more suited for enterprise-level applications and not mobile games. Additionally, it involves more complexity and maintenance overhead.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "During the install and game configuration process, each user creates an IAM credential and assigns the IAM user to a group with proper permissions to communicate with DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM group that only gives access to your application and the DynamoDB tables. Then, when writing to DynamoDB, simply include the unique device ID to associate the data with that specific user.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM role with the proper permission policy to communicate with the DynamoDB table. Use web identity federation, which assumes the IAM role using AssumeRoleWithWebIdentity, when the user signs in, granting temporary security credentials using STS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an Active Directory server and an AD user for each mobile application user. When the user signs in to the AD sign-on, allow the AD server to federate using SAML 2.0 to IAM and assign a role to the AD user which is assumed with AssumeRoleWithSAML.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 39,
  "query" : "In an attempt to cut costs, your accounts manager has come to you and tells you that he thinks that if the company starts to use consolidated billing, it will save some money.\nHe also wants the billing set up in such a way that it is relatively simple, and it gives insights into each of the VPC environments regarding the utilization of the corresponding VPC resources.\nWhich of the following setups would satisfy your account manager's needs?",
  "answer" : "Answer - A and C.\nEach organization in AWS Organizations has a master account that pays the charges of all the member accounts.\nIf you have access to the master account, you can see a combined view of the AWS charges incurred by the member accounts.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\nWe can have multiple VPC's serving various departments, and we can use tags to define them and have one billing account.\nThe tags associated with the VPC's will distinguish each department or environment.\nOption A is CORRECT because VPC helps you segregate and organize your resources as per the functionality or domain, thus enabling the account owner to get insight into the resources' costing within the logical grouping of the resources.\ne.g., If an organization has a separate VPC for each department - Finance, Development, Sales etc.\nIt will be convenient to get the billing details per department.\nOption B is incorrect because if all the resources are created under a single account, it will be difficult for the accounts manager to get insights into the utilization of resources as per the domains or functionality.\ne.g., Instead of having a separate department such as Finance, Development, Sales, etc., if an organization has a single account, it will be tedious to get the details on the billing of each departmental resource.\nOption C is CORRECT as having a linked account would enable the accounts manager to leverage the Consolidated Billing for multiple AWS accounts.\nWith Consolidated Billing, you can see a combined view of AWS charges incurred by all accounts, as well as get a cost report for each account associated with your payer account.\nOption D is INCORRECT because only IAM User access will not be sufficient for Consolidated Billing.\nFor more information on consolidated billing, please refer to the below link-\nhttp://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\nYou can also have the option of segregating the resources via multiple VPC's and have the billing estimates done via each VPC.For more information on AWS VPC, please refer to the below link-\nhttps://aws.amazon.com/vpc/\nConsolidated billing is a feature of AWS that allows a single AWS account, known as the payer account, to consolidate billing information across multiple AWS accounts, known as linked accounts. This means that charges for all AWS accounts within the organization are grouped together on a single bill, making it easier to track and manage costs.\nOption A, using multiple VPCs for different departments and tagging resources, would not satisfy the requirement for consolidated billing. While it may be useful for tracking costs within each department, it does not consolidate billing information across multiple accounts.\nOption B, using one payer account with no linked accounts, is also not a suitable solution. This setup would not allow for consolidation of billing information, as there would be only one account in use.\nOption C, using one payer account with many linked accounts, is the correct answer. This setup allows for the consolidation of billing information across multiple linked accounts, making it easier to track costs and manage expenses. Each linked account is still responsible for paying its own charges, but the payer account is responsible for paying the consolidated bill.\nOption D, IAM user access to the Billing and Cost Management console, is not a solution for consolidated billing. While it may allow users to view billing information, it does not consolidate billing information across multiple accounts.\nIn summary, the correct answer is option C: Use one Payer Account and many linked accounts. This setup allows for consolidated billing and provides insights into each of the VPC environments regarding the utilization of the corresponding VPC resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use multiple VPC`s for the different departments ( eg: sales, marketing etc.) and tag the resources within the department.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use one Payer Account and no linked accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use one Payer Account and many linked accounts.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "IAM user access to the Billing and Cost Management console.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 40,
  "query" : "A startup company is designing an application that needs to handle an unexpected amount of load and allow site visitors to read data from a DynamoDB table, which contains the results of an online polling system.\nAt any given time, as many as 5,000 requests need to be handled by the application.\nThe company has limited budget.\nHow can this application be developed most cost-effectively?",
  "answer" : "Answer - A.\nThe most important design consideration of this question is to have a cost-effective architecture that provides an application that can communicate with DynamoDB.Option A is CORRECT because (a) to show the polling results, a static HTML page that is stored in S3 bucket is sufficient (b) CloudFront and Route53 are AWS managed services that are highly available and scalable, and (c) it uses the JavaScript to communicate with DynamoDB.Option B is incorrect because (a)it will require many EC2 instances to handle the load of incoming traffic, and (b) setting up the EC2 instances and ELB is not a cost-effective solution compared to the static web page in S3.\nOption C is incorrect because architecting this with ELB and EC2 instances will not be as cost-effective as the static HTML page - that communicates with DynamoDB - hosted in S3.\nOption D is incorrect.\nLambda uses a default safety throttle for the number of concurrent executions across all functions in a given region per account.\nCurrently, the concurrent lambda execution limit is 1000 (soft limit ), and we need to handle ten times the traffic at any point in time.\nPlease note that this soft limit can be increased by contacting AWS support.\nThis would take some time after AWS approval and implementing the same and therefore involves cost.\nBesides, using a Lambda function to create a static page per user may have some performance issues.\nFor more information on AWS s3, please refer to the below link-\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\nThe most cost-effective way to develop the application that can handle an unexpected amount of load and allow site visitors to read data from a DynamoDB table with limited budget is to use the JavaScript SDK and build a static HTML page, hosted inside an Amazon S3 bucket; use CloudFront and Route 53 to serve the website, which uses JavaScript client-side language to communicate with DynamoDB.\nOption A is the most cost-effective solution for the following reasons:\n1.\nStatic HTML page: A static HTML page does not require a server-side component to generate the page dynamically. Instead, the content is pre-built and delivered as is, which reduces the load on the servers and speeds up the delivery of the website.\n2.\nAmazon S3 bucket: Hosting the website in an S3 bucket is cost-effective because S3 is a low-cost object storage service that can scale to accommodate large amounts of data. Additionally, S3 integrates with CloudFront, which can cache and distribute content globally, reducing the load on the application servers and improving the user experience.\n3.\nJavaScript SDK: Using the JavaScript SDK allows the application to communicate directly with DynamoDB from the client-side, eliminating the need for a server-side SDK. This further reduces the cost of the application by minimizing the server-side infrastructure required to handle the load.\n4.\nCloudFront and Route 53: CloudFront and Route 53 can be used to distribute the website globally and route traffic to the closest available server, further improving the user experience and reducing the load on the application servers.\nOption B is not cost-effective because it requires an Auto Scaling ELB application pointing to EC2 instances. EC2 instances are more expensive than S3 and require server-side infrastructure, which adds to the cost of the application.\nOption C is not cost-effective because it requires an Auto Scaling application with Elastic Load Balancer pointing to EC2 instances that use a server-side SDK to communicate with the DynamoDB table. Again, this requires more server-side infrastructure than option A, which increases the cost of the application.\nOption D is not cost-effective because it requires a Lambda script to be executed for each user request, which can quickly become expensive as the number of requests increases. Additionally, the cost of generating custom HTML pages dynamically can quickly add up, further increasing the cost of the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the JavaScript SDK and build a static HTML page, hosted inside an Amazon S3 bucket; use CloudFront and Route 53 to serve the website, which uses JavaScript client-side language to communicate with DynamoD.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution that serves the HTML web page but sends the visitors to an Auto Scaling ELB application pointing to EC2 instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy an Auto Scaling application with Elastic Load Balancer pointing to EC2 instances that use a server-side SDK to communicate with the DynamoDB table.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Lambda script that pulls the most recent DynamoDB polling result and creates a custom HTML page in S3 as per the user request ( one lambda execution per each user request ) and use CloudFront and Route 53 to serve the static website.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 41,
  "query" : "You are migrating an existing application to AWS cloud that would be communicating with EC2 instances in the VPC.\nYou need to make this application highly available.\nThe application currently relies on hard-coded host names for communication between the various tiers.\nYou have migrated the application and configured multi-tier using the internal elastic load balancer for serving traffic.\nThe load balancer host name is \"demo-app.us-east-1.elb.amazonaws.com\"\nThe current hard coded host name in your application for internal communication between your multi-tier application is \"demolayer.example.com\"\nHow can you architect a solution for high availability?",
  "answer" : "Answer - B.\nSince demolayer.example.com is an internal DNS record, the best way is Route 53 to create an internal resource record.\nOne can then point the resource record to the create ELB.While ordinary Amazon Route 53 resource record sets are standard DNS resource record sets, alias resource record sets provide an Amazon Route 53-specific extension to DNS functionality.\nInstead of an IP address or a domain name, an alias resource record set contains a pointer to a CloudFront distribution, an Elastic Beanstalk environment, an ELB Classic or Application Load Balancer, an Amazon S3 bucket that is configured as a static website, or another Amazon Route 53 resource record set in the same hosted zone.\nOption A is incorrect because it does not mention how to map between the existing hard-coded hostname and the ELB hostname.\nOption B is CORRECT because it creates an internal ALIAS recordset where it defines the mapping between the hard-coded hostname and the ELB hostname that is to be used.\nOptions C and D are incorrect because they should create a private recordset, not public since the mapping between the hard-coded hostname and ELB hostname should be done internally.\nFor more information on alias and non-alias records, please refer to the below link-\nhttp://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\nThe best answer for this question would be B. Create a PRIVATE resource record set using Route53 with a host name of \"demolayer.example.com\" and an ALIAS record to \"demo-app.us-east-1.elb.amazonaws.com\".\nExplanation:\nThe scenario is that an existing application is being migrated to AWS cloud and needs to be made highly available. The application is multi-tiered and currently relies on hard-coded host names for communication between tiers.\nTo make the application highly available, an Elastic Load Balancer (ELB) is configured to serve traffic. The load balancer's hostname is \"demo-app.us-east-1.elb.amazonaws.com\". However, the existing hard-coded hostname in the application is \"demolayer.example.com\". Therefore, a solution is needed to ensure that communication between tiers of the application continues to function properly.\nOption A suggests passing the ELB hostname to the EC2 instances using \"user-data\". This approach could work but has limitations. It requires modifying the configuration of each EC2 instance, which may not be scalable or practical. Additionally, if the ELB's hostname changes for any reason, all the EC2 instances would need to be updated accordingly.\nOption B suggests creating a PRIVATE resource record set in Route53. This approach is preferred because it allows for decoupling the hostname of the ELB from the application itself. The PRIVATE resource record set is not publicly accessible and is only visible within the VPC. This approach ensures that the communication between tiers remains private and secure.\nOption C suggests creating a PUBLIC resource record set in Route53. This approach is not recommended since it would expose the hostname of the ELB publicly. This could be a security concern, and it is not necessary since the communication between tiers is internal to the VPC.\nOption D suggests adding a CNAME record to an existing on-premise DNS server. This approach is not recommended since it would introduce additional complexity to the architecture. It is preferable to use Route53 since it is a fully managed service that integrates well with other AWS services.\nIn conclusion, the best approach for ensuring high availability in this scenario is to create a PRIVATE resource record set using Route53 with a host name of \"demolayer.example.com\" and an ALIAS record to \"demo-app.us-east-1.elb.amazonaws.com\". This approach ensures that communication between tiers remains private, secure, and decoupled from the ELB hostname.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an environment variable passed to the EC2 instances using \"user-data\" with the ELB host name \"demo-app.us-east-1.elb.amazonaws.com\"",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a PRIVATE resource record set using Route53 with a host name of \"demolayer.example.com\" and an ALIAS record to \"demo-app.us-east-1.elb.amazonaws.com\"",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a PUBLIC resource record set using Route53 with a host name of \"demolayer.example.com\" and an ALIAS record to \"demo-app.us-east-1.elb.amazonaws.com\"",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a CNAME record to the existing on-premise DNS server with a value of \"demo-app.us-east-1.elb.amazonaws.com\". Create a PUBLIC resource record set using Route53 with a host name of \"applayer.example.com\" and an ALIAS record to \"demo-app.us-east-1.elb.amazonaws.com\"",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 42,
  "query" : "When it comes to KMS, which of the following best describes how the AWS Key Management Service works? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nAWS KMS supports two types of keys - Master Keys and Data Keys.\nA Data Key is used to encrypt and decrypt the actual data; whereas, the Master Key is used to protect (encrypt and decrypt) the data key and some data up to 4Kb.\nSee the image below:\nBased on this, option B is CORRECT.\nFor more information on the AWS KMS Concepts, please refer to the link below-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\nAWS Key Management Service (KMS) is a fully managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. KMS allows you to create, import, and manage cryptographic keys that protect sensitive data across AWS services and your own applications.\nAWS KMS supports two types of keys, which are master keys and data keys. Master keys are the root keys that are used to protect your data keys, and data keys are used to encrypt and decrypt your data.\nThe correct answer is option B: AWS KMS supports two kinds of keys - master keys and data keys. Master keys can be used to encrypt and decrypt up to 4 kilobytes of data directly and can also be used to protect data keys. The data keys are then used to encrypt and decrypt customer data.\nMaster keys can be used to encrypt and decrypt up to 4 kilobytes of data directly, but it's recommended that you use data keys for encrypting larger amounts of data. Data keys are used to encrypt and decrypt customer data, and they are protected by master keys.\nWhen you want to encrypt your data, you generate a data key from KMS, and then use that data key to encrypt your data. You can then securely store the encrypted data, knowing that the data key used to encrypt it is protected by KMS. When you need to decrypt the data, you send the encrypted data and the data key to KMS. KMS then uses the appropriate master key to decrypt the data key and returns it to you. You can then use the data key to decrypt the encrypted data.\nKMS integrates with many AWS services, such as Amazon S3, Amazon EBS, Amazon Redshift, and Amazon RDS, to make it easy to encrypt your data. KMS also supports customer-managed CMKs (Customer Master Keys), which allow you to create and manage your own master keys.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS KMS supports two kinds of keys — master keys and data keys. Master keys can be used to encrypt and decrypt up to 4 kilobytes of data directly and can also be used to protect data keys. The master keys are then used to encrypt and decrypt customer data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS KMS supports two kinds of keys — master keys and data keys. Master keys can be used to encrypt and decrypt up to 4 kilobytes of data directly and can also be used to protect data keys. The data keys are then used to encrypt and decrypt customer data.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS KMS supports two kinds of keys — master keys and data keys. Master keys can be used to encrypt and decrypt up to 4 kilobytes of data directly and can also be used to protect data keys. The data keys are then used to decrypt the customer data, and the master keys are used to encrypt the customer data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS KMS supports two kinds of keys — master keys and data keys. Master keys can be used to encrypt and decrypt up to 4 kilobytes of data directly and can also be used to protect data keys. The data keys are then used to encrypt the customer data, and the master keys are used to decrypt the customer data.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 43,
  "query" : "Your company is planning to move an existing portal to AWS.\nCurrently, it's running in on-premises.\nIt is a 5-year-old portal developed on Java and MySQL 5.6\nYour company is looking to dockerize the application and deploy it in a highly available environment in AWS.\nYou also need a serverless compute engine for containers so that you do not need to provision and manage servers.\nWhich of the following methods is the most suitable?",
  "answer" : "Answer: D.\nOption A is incorrect because the Client is looking to dockerize the App in a serverless environment.\nAmazon ECS with EC2 Launch Type is not the best case here.\nOption B is incorrect because the Client is looking for dockerizing the App in a serverless environment.\nAmazon EC2 is not the best case here.\nOption C is incorrect because ECS with EC2 Launch Type is not a serverless environment.\nOption D is CORRECT because ECS with AWS Fargate Launch Type provides a highly available and serverless environment.\nThis option meets the requirements of the question.\nReference:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-dg.pdf https://www.youtube.com/watch?v=4xqOoRPrnAw https://aws.amazon.com/blogs/compute/migrating-your-amazon-ecs-containers-to-aws-\nBased on the requirements provided, option D appears to be the most suitable solution for deploying the portal in a highly available environment in AWS. Here's why:\nOption A involves deploying the application on Amazon ECS with EC2 launch type and leveraging Amazon RDS MySQL for the database. Although ECS can be used to deploy containers and RDS can provide a managed MySQL database service, it still requires managing the underlying EC2 instances for the ECS cluster. This is not a serverless compute engine, which is one of the requirements.\nOption B involves deploying the application on Amazon EC2 and leveraging Amazon Aurora MySQL for the database. While Aurora provides a highly available and scalable MySQL-compatible database, it still requires managing the underlying EC2 instances for the application server. This is not a serverless compute engine, which is one of the requirements.\nOption C involves containerizing the Java-based application, storing the container image in Docker Hub, and deploying it in ECS with EC2 launch type. This is similar to option A, with the difference being that the container image is stored in Docker Hub instead of Amazon ECR. While this is an option, it does not provide a serverless compute engine, which is one of the requirements.\nOption D involves containerizing the Java-based application, storing the container image in Amazon ECR, and deploying it in ECS with AWS Fargate launch type. Fargate is a serverless compute engine for containers, which means that it eliminates the need to manage the underlying EC2 instances. It also provides automatic scaling, load balancing, and high availability. Using Amazon Aurora MySQL as the database provides a highly available and scalable database solution that is fully managed by AWS. This is the most suitable option based on the given requirements.\nIn summary, option D (Containerize the Java based application, store Container image in ECR and deploy it in ECS with AWS Fargate Launch Type. Leverage Amazon Aurora MySQL for the database) is the most suitable option for deploying the portal in a highly available environment in AWS because it provides a serverless compute engine for containers and a highly available and scalable database solution that is fully managed by AWS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy the Application on Amazon ECS with EC2 Launch Type and leverage Amazon RDS MySQL for the database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy the Application on Amazon EC2 and leverage the Amazon Aurora MySQL for the database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Containerize the Java based application, store Container image in Docker Hub and deploy it in ECS with EC2 Launch Type. Leverage Amazon Aurora MySQL for the database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Containerize the Java based application, store Container image in ECR and deploy it in ECS with AWS Fargate Launch Type. Leverage Amazon Aurora MySQL for the database.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 44,
  "query" : "You have acquired a new contract from a client to move all of their existing infrastructures onto AWS.\nYou notice that they are running some of their applications using multicast, and they need to keep it running as such when it is migrated to AWS.\nYou discover that multicast is not available on AWS, as you cannot manage multiple subnets on a single interface on AWS, and a subnet can only belong to one availability zone.\nWhich of the following would enable you to deploy legacy applications on AWS that require multicast? Choose 2 options.",
  "answer" : "Answer - A and B.\nOption A is CORRECT because first, we create Elastic Network Interfaces to communicate between the various subnets.\nOption B is CORRECT because overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).\nOption C is incorrect because it would disrupt multicast, involve additional time and cost, and not add any value.\nOption D is incorrect because VPC peering and multicast are not the same.\nFor more information on Overlay Multicast in Amazon VPC, please visit the URL below-\nhttps://aws.amazon.com/articles/6234671078671125\nMulticast is a network protocol that allows a single stream of data to be sent to multiple recipients at once, which is commonly used in some legacy applications. Unfortunately, multicast is not natively supported on AWS, as multiple subnets cannot be managed on a single interface, and a subnet can only belong to one availability zone. However, there are still some options to deploy legacy applications that require multicast on AWS, which are:\nA. Provide Elastic Network Interfaces to communicate between the subnets: Elastic Network Interfaces (ENIs) are virtual network interfaces that can be attached to an instance in a VPC. By creating ENIs for each subnet that requires multicast communication and attaching them to instances in each subnet, multicast traffic can be routed between the instances via the ENIs. However, this approach can be complicated to manage, especially when there are many subnets that require multicast communication.\nB. Create a virtual overlay network that runs on the OS level of the instance: A virtual overlay network can be created using a software-defined network (SDN) solution, such as VXLAN or GRE, which can run on the operating system (OS) level of the instance. By creating an overlay network that spans multiple subnets, multicast traffic can be routed between the instances that are connected to the overlay network. This approach provides more flexibility and easier management, but it may require additional configuration and setup time.\nC. Create all the subnets in a single VPC: By creating all the subnets in a single VPC, they can communicate with each other directly without the need for multicast. This approach simplifies the network architecture and management, but it may not be feasible in some scenarios, such as when there are geographic or regulatory requirements to use multiple VPCs.\nD. Create all the subnets on a different VPC and use VPC peering between them: By creating each subnet in a separate VPC and establishing VPC peering connections between them, multicast traffic can be routed between the instances in different VPCs. This approach provides more isolation and security between the subnets, but it may require additional setup time and configuration, and it may incur additional network costs.\nIn summary, options A and B are the two options that can enable the deployment of legacy applications that require multicast on AWS. Option A uses ENIs to enable multicast communication between subnets, while option B uses a virtual overlay network that runs on the OS level of the instance. Option C and D are alternative approaches that may not require multicast but may be feasible in some scenarios.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Provide Elastic Network Interfaces to communicate between the subnets.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a virtual overlay network that runs on the OS level of the instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create all the subnets in a single VPC",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create all the subnets on a different VPC and use VPC peering between them.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 45,
  "query" : "A company has three accounts under consolidated billing.\n\"Production\" is the payer account, and \"Development\" and \"Staging\" are the linked accounts and they have the reserved instances sharing enabled between them.\nThe development account has purchased three reserved instances with instance type of m4.large in Availability Zone us-east-1a.\nHowever, no instance is running on the development account, but has five m4.large instances running in the staging account and Availability Zone 1a.\nWho can receive the benefit of the reserved instance pricing? Choose the correct answer from the options below.",
  "answer" : "Answer - C.\nOption A is incorrect because the benefit of reserved instance pricing will apply to any three EC2 instances across all the accounts in the Consolidated Billing group.\nSince the staging account - which is part of the \"account family\" - has 5 EC2 instances running, only 3 of those will receive the reserved pricing benefit.\nOption B is incorrect because even though no EC2 instances are running in the development account, the instances running in the staging account will still receive the reservation pricing benefit since it is part of the Consolidated Billing group.\nOption C is CORRECT because the reserved instance pricing will be applied to three of the m4.large instances in the staging account as they are part of the Consolidated Billing group.\nThis needs Reserved Instance sharing to be enabled at the account.\nOption D is incorrect because the reserved Consolidated Billing advantage is applied to all the accounts linked to the primary account, not just the primary account.\nMore information on Consolidated Billing Group.\nFor more information on consolidating billing, please visit the below links-\nhttp://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html\nIn this scenario, the company has three AWS accounts under consolidated billing. The \"Production\" account is the payer account, and \"Development\" and \"Staging\" accounts are linked accounts. The linked accounts have enabled reserved instance sharing.\nThe development account has purchased three reserved instances of the m4.large instance type in the us-east-1a Availability Zone. However, currently, no instances are running on the development account. The staging account has five m4.large instances running in the us-east-1a Availability Zone.\nThe goal is to determine which account(s) can receive the benefit of the reserved instance pricing. Here are the options:\nA. All the instances in all the accounts running the m4.large will receive the pricing even if there is only one reserved instance purchase.\nThis option is incorrect. Although reserved instance sharing is enabled, it only shares the unused reserved instance capacity between the accounts that are part of the consolidated billing family. In this scenario, there are three reserved instances purchased in the development account, and no instances in that account match the reserved instance criteria, so this option cannot apply.\nB. No account will receive the reservation pricing because the reservation was purchased on the development account, and no instances that match the reservation are running in the development account.\nThis option is also incorrect. AWS reserves the capacity for the purchased reserved instances, regardless of whether there are matching instances running in the same account. As long as the reserved instances are not being used by any instances, the accounts within the consolidated billing family can benefit from the reserved instance pricing.\nC. The reserved instance pricing will be applied to three m4.large instances in the staging account because the staging account runs an instance that matches the reserved instance type.\nThis option is correct. Although the reserved instances were purchased in the development account, the staging account has instances that match the criteria of the reserved instances (m4.large instances in the us-east-1a Availability Zone). Therefore, the reserved instance pricing will apply to the matching instances in the staging account.\nD. Only the primary account (the consolidated billing primary account) will receive the discounted pricing if the instance is running in the primary billing account.\nThis option is incorrect. As mentioned earlier, reserved instance sharing allows the unused reserved instance capacity to be shared between the accounts in the consolidated billing family. Therefore, if a linked account has instances that match the reserved instance criteria, it can benefit from the reserved instance pricing.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "All the instances in all the accounts running the m4.large will receive the pricing even if there is only one reserved instance purchase.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No account will receive the reservation pricing because the reservation was purchased on the development account, and no instances that match the reservation are running in the development account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The reserved instance pricing will be applied to three m4.large instances in the staging account because the staging account runs an instance that matches the reserved instance type.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Only the primary account (the consolidated billing primary account) will receive the discounted pricing if the instance is running in the primary billing account.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 46,
  "query" : "A company has developed a Ruby on Rails content management platform.\nCurrently, OpsWorks has several stacks for dev, staging, and production to deploy and manage the application.\nNow, the company wants to start using Python instead of Ruby.\nHow should the company manage the new deployment so that it should revert back to the old application with Ruby if the new deployment starts adversely impacting the existing customers? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nOption A is incorrect because it mentions how the code would be deployed using the deploy lifecycle event.\nHowever, it does not mention how the system can revert back to the old application deployment stack if there is any failure.\nOption B is CORRECT because it deploys the new stack via the canary deployment method where the new stack is tested only on a small portion of production traffic first.\nIf the new deployment has any errors, it reverses back to the old deployment stack.\nOption C is incorrect.\nEven though you create the new stack, you should always test a small portion of production traffic with the new stack rather than route all the production traffic.\nOption D is incorrect because updating all the production instances at once is risky, and it does not give an option to revert back to the old stack in case of any errors.\nMore information on Canary Deployment.\nCanary deployments are a pattern for rolling out releases to a subset of users or servers.\nThe idea is first to deploy the change to a small subset of servers, test it, and then roll the change out to the rest of the servers.\nThe canary deployment serves as an early warning indicator with less impact on downtime: if the canary deployment fails, the rest of the servers aren't impacted.\nThe best option for the company to manage the deployment of the new Python application while still having the option to revert back to the old Ruby application if necessary is option B: Create a new stack that contains a new layer with the Python code. Route only a small portion of the production traffic to use the new deployment stack. Once the application is validated, slowly increase the production traffic to the new stack using the Canary Deployment. Revert to the old stack if the new stack deployment fails or does not work.\nThis option provides a controlled deployment process that minimizes the risk of negatively impacting customers. It involves creating a new stack that includes a new layer with the Python code, allowing for a separate deployment process. Initially, only a small portion of production traffic should be routed to the new stack to validate its functionality. This process is called canary deployment, where a small percentage of users are directed to the new stack and monitored for any issues. Once it has been verified that the new application is working correctly, more production traffic can be directed to the new stack until all users are using it. If at any point during this process, issues are detected, the company can quickly revert back to the old stack with Ruby.\nOption A, creating a new stack for Python with separate deployments, does not provide enough control over the deployment process and may cause disruption to customers if there are any issues with the new deployment. Option C, routing all traffic to the new stack at once, poses a significant risk to customer experience if there are issues with the new deployment. Option D, updating the existing host instances with the new Python code, may not be the best option since it eliminates the ability to revert to the old application if there are issues with the new deployment. Additionally, it may not be feasible to update all host instances simultaneously, which could result in downtime and negatively impact customer experience.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a new stack that contains the Python application code and manages separate deployments of the application via the secondary stack using the deploy lifecycle action to implement the application code.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new stack that contains a new layer with the Python code. Route only a small portion of the production traffic to use the new deployment stack. Once the application is validated, slowly increase the production traffic to the new stack using the Canary Deployment. Revert to the old stack, if the new stack deployment fails or does not work.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new stack that contains the Python application code. Route all the traffic to the new stack at once so that all the customers get to access the updated application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Update the existing host instances of the application with the new Python code. This will save the cost of having to maintain two stacks, hence cutting down on the costs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 47,
  "query" : "Your application has very high traffic.\nSo you have enabled autoscaling in multi-availability zone to suffice your application's needs.\nBut you observe that one of the availability zones is not receiving any traffic.\nWhat can be wrong here?",
  "answer" : "Answer - C.\nIn order to make sure that all the EC2 instances behind a cross-zone ELB receive the requests, make sure that all the applicable availability zones (AZs) are added to that ELB.Option A is incorrect because autoscaling can work with multiple AZs.\nOption B is incorrect because autoscaling can be enabled for multi-AZ in any single region, not just N.\nVirginia.\n(see the image below)\nOption C is CORRECT because most likely, the reason is that the AZ - whose EC2 instances are not receiving requests - is not added to the ELB.\nOption D is incorrect because instances need not be added manually to AZ (they should already be there!).\nMore information on adding AZs to ELB.\nWhen you add an Availability Zone to your load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone.\nLoad balancer nodes accept traffic from clients and forward requests to the healthy registered instances in one or more Availability Zones.\nFor more information on adding AZ's to ELB, please refer to the below URL-\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html\nThe correct answer to this question is C. Availability zone is not added to Elastic load balancer.\nWhen autoscaling is enabled in multi-availability zone, it means that AWS will automatically launch and terminate instances across multiple availability zones based on the demand of the application. This ensures high availability and fault tolerance of the application.\nIn order for the traffic to be distributed across all the availability zones, the instances launched by the autoscaling group should be registered with an Elastic Load Balancer (ELB). The ELB is responsible for distributing the traffic across all the registered instances in all availability zones.\nIf one of the availability zones is not receiving any traffic, it may be because the instances in that zone are not registered with the ELB. This could be due to a misconfiguration of the ELB or the autoscaling group. To resolve this issue, you should check if the instances in the affected availability zone are registered with the ELB, and if not, add them to the ELB target group. You should also verify that the autoscaling group is configured to launch instances in all the required availability zones.\nOption A, \"Autoscaling only works for single availability zone\", is incorrect because autoscaling can be enabled across multiple availability zones for high availability and fault tolerance.\nOption B, \"Autoscaling can be enabled for multi AZ only in north Virginia region\", is incorrect because autoscaling can be enabled for multi AZ in any region that supports multiple availability zones.\nOption D, \"Instances need to manually added to availability zone\", is incorrect because instances are automatically launched in the availability zones that are specified in the autoscaling group configuration.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Autoscaling only works for single availability zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Autoscaling can be enabled for multi AZ only in north Virginia region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Availability zone is not added to Elastic load balancer.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Instances need to manually added to availability zone.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 48,
  "query" : "A company has hired a third-party security auditor, and the auditor needs read-only access to the required AWS resources and logs of all VPC records and events that will occur on AWS.\nHow can the company meet the auditor's requirements without compromising with the security in the AWS environment?",
  "answer" : "Answer - D.\nOption A is incorrect.\nIAM roles are a secure way to grant permissions to entities that you trust.But the entities should be an IAM user in another account or a User from a corporate directory who use identity federation with SAML.\nNone of these are specified in the question.\nOption B is incorrect because sending the logs via email is not a good architecture.\nOption C is incorrect because granting the auditor access to AWS resources is not AWS's responsibility.\nIt is the AWS user or account owner's responsibility.\nAWS CloudTrail is now enabled by default for ALL CUSTOMERS and will provide visibility into the past seven days of account activity without the need for you to configure a trail in the service to get started.\nBut if you want to access your CloudTrail log files directly or archive your logs for auditing purposes, you can still create a trail and specify the S3 bucket for your log file delivery.\nhttps://aws.amazon.com/blogs/aws/new-amazon-web-services-extends-cloudtrail-to-all-aws-customers/\nMore information on AWS CloudTrail.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure.\nCloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.\nThis history simplifies security analysis, resource change tracking, and troubleshooting.\nFor more information on CloudTrail, please visit the below URL-\nhttps://aws.amazon.com/cloudtrail/\n<h2>Option A: Creating a role with required permissions for the auditor</h2><p>This option is a valid approach to provide read-only access to the third-party auditor. A role with read-only access to the required AWS resources can be created in AWS IAM, and the auditor can assume that role to access the resources. This approach is recommended because it allows the company to manage the access of the third-party auditor through IAM roles. By creating a role, the company can grant temporary access to the auditor, which will automatically expire at a specified time. Additionally, the company can revoke the access of the auditor anytime by removing the IAM role.</p><h2>Option B: Creating an SNS notification that sends the CloudTrail log files to the auditor's email</h2><p>This option is not a good choice for the given scenario. It involves sending log files to the auditor's email address, which could compromise the confidentiality and security of the data. Email is an unsecured communication channel, and the logs may contain sensitive information such as access keys and other credentials. Therefore, this option is not a recommended approach.</p><h2>Option C: Contacting AWS to grant required access to the third-party auditor</h2><p>This option is not a feasible approach because AWS does not provide direct access to third-party auditors. AWS follows the shared responsibility model, where the company is responsible for securing its AWS resources, and AWS is responsible for the security of the cloud infrastructure. Therefore, the company needs to provide access to the third-party auditor using the IAM role or any other suitable approach.</p><h2>Option D: Enabling CloudTrail and specifying the S3 bucket for log file delivery</h2><p>This option is a good approach to provide read-only access to the third-party auditor. By enabling AWS CloudTrail and specifying the S3 bucket for log file delivery, the company can create an audit trail of all the activities in the AWS environment, including VPC records and events. The company can then create an IAM user with read-only permission to the required AWS resources, including the VPC logs and the bucket containing CloudTrail logs. The auditor can then access the logs by assuming the IAM role. This approach is recommended because it provides a secure and auditable way to monitor the AWS environment without compromising the security of the data.</p>",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a role that has the required permissions for the auditor.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an SNS notification that sends the CloudTrail log files to the auditor`s email when CloudTrail delivers the logs to S3 but does not allow the auditor access to the AWS environment.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The company should contact AWS as part of the shared responsibility model, and AWS will grant required access to the third-party auditor.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable CloudTrail and specify the S3 bucket for your log file delivery.Create an IAM user who has read-only permission to the required AWS resources, including the VPC logs and the bucket containing CloudTrail logs.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 49,
  "query" : "After configuring a whole site CDN on CloudFront, you receive the following error.",
  "answer" : "Answer - B.\nThe AWS documentation also states that \"CloudFront caches responses to GET and HEAD requests\" and, optionally, OPTIONS requests.\nCloudFront does not cache responses to requests that use the other methods.\nAs per AWS documentation,\nAllowed HTTP Methods.\nSpecify the HTTP methods that you want CloudFront to process and forward to your origin:\nGET, HEAD: You can use CloudFront only to get objects from your origin or to get object headers.\nGET, HEAD, OPTIONS: You can use CloudFront only to get objects from your origin, get object headers, or retrieve a list of the options that your origin server supports.\nGET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE: You can use CloudFront to get, add, update, and delete objects, and to get object headers.\nIn addition, you can perform other POST operations, such as submitting data from a web form.\nNote:\nCloudFront caches responses to GET and HEAD requests and, optionally, OPTIONS requests.\nCloudFront does not cache responses to requests that use the other methods.\nBased on this, Option B seems to be a better choice than.\nC.For more information, please visit:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesAllowedHTTPMethods\nWhen the CloudFront Distribution supports only \"cacheable requests\", it means that it supports only GET and HEAD HTTP requests (read-only)\nFor the HTTP requests such as OPTIONS, PUT, POST, PATCH AND DELETE, the CloudFront will give an error \"The distribution supports only cacheable requests\".\nHence, option B is the correct answer.\nThere is a good question posted on StackOverflow which explains what should be done in this situation.\nhttp://stackoverflow.com/questions/31253694/this-distribution-is-not-configured-to-allow-the-http-request\nThe error indicates that there is a problem with the allowed HTTP methods on the specific origin configured for the CloudFront distribution. The allowed HTTP methods on the origin do not match the HTTP methods used by the client, which results in the error.\nOption A, \"The CloudFront distribution is configured to the wrong origin,\" is not likely to be the issue because the error message specifically mentions HTTP methods.\nOption B, \"Allowed HTTP methods on that specific origin is only accepting GET, HEA,\" is also not correct because \"HEA\" is not a valid HTTP method. The correct method is \"HEAD.\"\nOption C, \"Allowed HTTP methods on that specific origin is only accepting POST, PATCH,\" is also not correct because the error message suggests that the client is using a method that is not allowed by the origin, and POST and PATCH are commonly used methods that are allowed by most origins.\nOption D, \"Allowed HTTP methods on that specific origin is only accepting GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE,\" is the correct answer because it lists all of the commonly used HTTP methods that are allowed by most origins. If the origin is only accepting a subset of these methods, then the client may receive an error if it attempts to use a method that is not allowed.\nIn summary, the correct answer is option D because it correctly identifies the allowed HTTP methods on the origin as the likely cause of the error.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The CloudFront distribution is configured to the wrong origin.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Allowed HTTP methods on that specific origin is only accepting GET, HEA.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Allowed HTTP methods on that specific origin is only accepting POST, PATCH.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Allowed HTTP methods on that specific origin is only accepting GET, HEAD, OPTIONS, PUT, POST, PATCH, DELET.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 50,
  "query" : "You're running a financial application on an EC2 instance.\nData stored in the EBS volumes is critical and you want to make it fault-tolerant.\nWhich of the following options provides the most fault-tolerant configuration?",
  "answer" : "Answer - A.\nOption A is CORRECT because, as mentioned above, RAID 1 configuration maintains the exact copy of the data (via mirroring) in a backup EBS volume that can be used in case of the failure of the main volume, providing redundancy and fault tolerance.\nOption B is incorrect because, although each Amazon EBS volume is automatically replicated within its Availability Zone, it is done so to protect it from any component failure from the AWS side.\nIt does not withstand any failures at the user level.\nIt is the user's responsibility to replicate the data that is stored on the EBS volume.\nEBS volumes are not automatically backed up to additional hardware in the same availability zone.\nYou will need to set up your backup strategy when using EBS volumes.\nEBS Snapshot Doc: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html.\nOption C is incorrect because RAID 5 does not improve the fault tolerance.\nRAID 5 and RAID 6 are not recommended for Amazon EBS because these RAID modes' parity write operations consume some of the IOPS available to your volumes.\nOption D is incorrect because RAID 0 configuration helps improve the performance but does not provide redundancy or mirroring of the data across disks.\nAs a result of having data striped across all disks, any failure will result in total data loss.\nMore information on RAID Configurations with EBS volumes:\nAs per the AWS documentation, it is clearly given to use RAID 1 configuration for fault tolerance of EBS volumes.\nWith Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as the operating system for your instance supports that particular RAID configuration.\nThis is because all RAID is accomplished at the software level.\nFor greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together.\nFor more information on RAID configuration, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\nOption A is partially correct in that RAID 1 configuration provides fault tolerance by mirroring the data across two EBS volumes. However, it is not the most fault-tolerant option because it only protects against the failure of one EBS volume. If both EBS volumes fail at the same time, the data will be lost.\nOption B is incorrect. Although EBS volumes are durable and backed up to additional hardware in the same availability zone, this does not provide fault tolerance in case of a failure of the EBS volume or the EC2 instance itself.\nOption C is incorrect because RAID 5 configuration does not provide fault tolerance on EBS volumes. RAID 5 is a striping method that provides improved I/O performance by distributing data across multiple EBS volumes. It does offer some protection against the failure of one EBS volume, but not against the failure of multiple EBS volumes.\nOption D is incorrect because RAID 0 does not provide any fault tolerance. It is a striping method that combines multiple EBS volumes into one logical volume, providing improved I/O performance but no protection against failure.\nThe most fault-tolerant configuration for critical data stored in EBS volumes would be to use a combination of RAID 1 mirroring and a multi-Availability Zone (multi-AZ) deployment. Multi-AZ deployment involves running the application and storing the data on EBS volumes in multiple availability zones. In case of a failure in one availability zone, the application can automatically switch to the EBS volumes in another availability zone, ensuring continuous availability of the application and data.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Mirror the data using RAID 1 configuration, which provides fault tolerance on EBS volumes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Nothing is required since EBS volumes are durability backed up to additional hardware in the same availability zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Mirror the data using RAID 5 configuration to improve the I/O performance on EBS volumes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Stripe multiple EBS volumes together with RAID 0, which provides fault tolerance on EBS volumes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 51,
  "query" : "",
  "answer" : "Answer - D.\nOption A is incorrect because the HTTPS endpoint is incorrect as RDS uses another port.\nThis option also does not use VPN and it is not the most secure way.\nOption B is incorrect because replicating via EC2 instances is very time consuming and very expensive cost-wise.\nOption C is incorrect because Data Pipeline is for batch jobs and not suitable for replicating the RDS DB to an on-premises database.\nOption D is CORRECT because it is feasible to set up the secure IPSec VPN connection between the on-premises server and AWS VPC.\nThe mysqldump utility can be used to transfer the database.\nFor more information on VPN connections, please visit the below URL:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\nNote:\nAWS docs state that,\nConfigure an egress rule for the external instance to operate as a Read Replica during the export.\nThe egress rule will allow the MySQL Read Replica to connect to the MySQL DB instance during replication.\nSpecify an egress rule that allows TCP connections to the port and IP address of the source Amazon RDS MySQL DB instance.\nPlease refer to the following links for more information.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Exporting.NonRDSRepl.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-ug.pdf\nThe question describes a scenario where a company has an on-premises MySQL database server, and they want to replicate their MySQL data to an Amazon RDS instance running MySQL in the cloud. The replication must be secure, and the company wants to ensure the integrity and availability of their data.\nA. Configure the RDS instance as the master and enable replication over the open internet using a secure HTTPS endpoint to the on-premises server.\nThis option is not secure, as it involves transmitting sensitive data over the public internet, which is prone to hacking and other security risks. Additionally, HTTPS is a protocol for secure communication over the internet, but it doesn't provide end-to-end encryption, which means data could still be intercepted or compromised in transit.\nB. RDS cannot replicate to an on-premises database server directly. Instead, first, configure the RDS instance to replicate to an EC2 instance with core MySQL, and then configure replication over a secure VPN/VPG connection.\nThis option is more secure as it involves using an EC2 instance with core MySQL to replicate data from RDS to an on-premises database server. The replication is over a secure VPN/VPG connection, which ensures that the data is encrypted and secure during transmission.\nC. Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint.\nThis option involves exporting data from the MySQL database to S3 each night, and then downloading the data securely from an S3 HTTPS endpoint. While this option is secure, it may not be ideal for scenarios where real-time replication is required.\nD. Create an IPSec VPN connection between AWS VPC and the on-premise server. Use the mysqldump utility to transfer the database from the Amazon RDS database to the external MySQL database in the on-premises data center.\nThis option is also secure, as it involves using an IPSec VPN connection between AWS VPC and the on-premise server to transfer data using the mysqldump utility. This option is suitable for scenarios where real-time replication is not required, and nightly backups are sufficient.\nIn conclusion, the best option for replicating MySQL data from an Amazon RDS instance to an on-premises database server is Option B, which involves replicating data from RDS to an EC2 instance with core MySQL and then transferring the data over a secure VPN/VPG connection.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure the RDS instance as the master and enable replication over the open internet using a secure HTTPS endpoint to the on-premises server.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "RDS cannot replicate to an on-premises database server directly. Instead, first, configure the RDS instance to replicate to an EC2 instance with core MySQL, and then configure replication over a secure VPN/VPG connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IPSec VPN connection between AWS VPC and the on-premise server. Use the mysqldump utility to transfer the database from the Amazon RDS database to the external MySQL database in the on-premises data center.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 52,
  "query" : "You are setting up a video streaming service with the main components of the setup being S3, CloudFront, and Transcoder.\nYour video content will be stored on AWS S3, and it should only be viewed by the subscribers who have paid for the service.\nYour first job is to upload 10 videos to S3 and ensure that they are secure before you even begin to start thinking of streaming the videos.\nThe 10 videos have just finished uploading to S3, so you now need to secure them with encryption at rest.\nWhich of the following would be the best way to do this? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nThere are two main considerations in this scenario: (1) the data in S3 should be encrypted \"at rest\", and (2) only the authenticated users should be able to stream the video.\nOption A is incorrect because AWS CloudHSM is used to generate the user's own encryption keys on the AWS Cloud.\nIt does not encrypt any data on S3 at rest.\nOption B is incorrect because, even though it encrypts the data at rest, storing the keys in the CloudFront for private access to the authenticated users is an invalid solution.\nOption C is incorrect because using the IAM approach is not scalable and safe.\nOption D is CORRECT because (a) it uses KMS keys for encrypting and decrypting the data, and (b) it ensures that only the authenticated users will have access to the videos by using the signed URLs on the CloudFront distribution.\nPlease check the following references:\nhttps://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/#more-1038 https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\nFor securing the 10 videos on S3 with encryption at rest, the best option would be to use a KMS (Key Management Service) CMK (Customer Master Key) to encrypt the files and use signed URLs in a CloudFront distribution to serve the S3 contents.\nOption A is not the best choice for encrypting the videos as AWS CloudHSM (Hardware Security Module) is a high-end security option that is typically used for protecting sensitive workloads and data, such as those that are subject to regulatory compliance requirements. It can be overkill for the use case of securing videos on S3.\nOption B is not a good choice as it requires storing the encryption key on CloudFront, which is not secure. Additionally, it does not provide sufficient access control to ensure that only authenticated users can stream the videos.\nOption C is not the best choice for encrypting the videos as it only encrypts the data in S3 and does not provide any access control. IAM users can access the videos in S3, but this does not ensure that only authenticated users can stream the videos.\nTherefore, the best option is to use a KMS CMK to encrypt the files and signed URLs in a CloudFront distribution to serve the S3 contents. KMS provides a simple and secure way to manage keys for encrypting data at rest and in transit. With signed URLs, CloudFront can serve the videos securely and ensure that only authenticated users can access the content. Additionally, signed URLs provide time-limited access, so users cannot share the URL with unauthorized parties.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use AWS CloudHSM appliance with both physical and logical tamper detection and response mechanisms that trigger zeroization of the appliance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Encrypt your data using AES-256. After the object is encrypted, the encryption key you used needs to be stored on AWS CloudFront so that only authenticated users can stream the videos.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set an API flag, or check a box in the AWS Management Console, to have data encrypted in Amazon S3. Create IAM Users to access the videos from S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a KMS CMK to encrypt the files. Also, use signed URLs in a CloudFront distribution to serve the S3 contents.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 53,
  "query" : "You currently have 9 EC2 instances running in a Cluster Placement Group.\nAll these nine instances were initially launched at the same time and seemed to be performing as expected.\nYou decide that you need to add two new instances to the group.\nHowever, when you attempt to do this, you receive a 'capacity error.' Which of the following actions will most likely fix this problem? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nOption A is incorrect because to benefit from the enhanced networking, all the instances should be in the same Placement Group.\nLaunching the new ones in a new Placement Group will not work in this case.\nOption B is CORRECT because the most likely reason for the \"Capacity Error\" is that the underlying hardware may not have the capacity to launch any additional instances on it.\nIf the instances are stopped and restarted, AWS may move the instances to a hardware that has the capacity for all the requested instances.\nOption C is incorrect because there is no such limit on the number of instances in a Placement Group (however, you can not exceed your EC2 instance limit allocated to your account per region).\nOption D is incorrect because the capacity error is not related to the instance size and ensures that the instances are of the same size will not resolve the capacity error.\nMore information on Cluster Placement Group.\nIf you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again.\nRestarting the instances may migrate them to hardware that has the capacity for all the requested instances.\nCluster Placement Groups.\nA cluster placement group is a logical grouping of instances within a single Availability Zone.\nA placement group can span peered VPCs in the same region.\nCluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both.\nThe majority of the network traffic is between the instances in the group.\nTo provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type supporting enhanced networking.\nFor more information, see Enhanced Networking.\nWe recommend that you launch the number of instances you need in the placement group in a single launch request and use the same instance type for all instances in the placement group.\nIf you try to add more instances to the placement group later or try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error.\nIf you stop an instance in a placement group and then start it again, it still runs in the placement group.\nHowever, the start fails if there isn't enough capacity for the instance.\nIf you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group and try the launch again.\nRestarting the instances may migrate them to hardware that has capacity for all the requested instances.\nFor more information on this, please refer to the below URL.\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nNote:\nIn this scenario, we are discussing the insufficient capacity error happening within a placement group.\nAs per AWS docs,\n\"If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again.\nRestarting the instances may migrate them to hardware that has capacity for all the requested instances.\"\nIn our scenario, we already have 9 EC2 instances running on a placement group.\nWhen we tried to add 2 more to the group, we have encountered this error.\nSo if we stop and restart all the instances, it will help resolve this issue as the restarting instances will migrate to a new hardware that will have the capacity for all the instances.\nFor more information, please refer to:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nThe correct answer to this question is C. Request a capacity increase from AWS as you are initially limited to 10 instances per Placement Group.\nExplanation:\nA Cluster Placement Group is a logical grouping of instances within a single Availability Zone. Instances in a placement group benefit from low-latency, high-bandwidth network connections to other instances in the same placement group.\nIn AWS, there is a limit of 10 instances per placement group, and this limit cannot be increased by default. Therefore, when attempting to add two new instances to a placement group that already has 9 instances, a capacity error will occur.\nOption A, creating another placement group and launching new instances in that group, will not resolve the issue because there is still a limit of 10 instances per placement group.\nOption B, stopping and restarting all instances in the placement group, will not resolve the issue because it will not increase the capacity of the placement group.\nOption D, ensuring that all instances are the same size, will not resolve the issue because it will not increase the capacity of the placement group.\nOption C, requesting a capacity increase from AWS, is the correct solution to the problem. You can request an increase in the limit by contacting AWS support. Once the limit is increased, you will be able to add the additional instances to the placement group.\nIn summary, the correct solution to the problem is to request a capacity increase from AWS, as there is a limit of 10 instances per placement group in AWS, and this limit cannot be increased by default.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create another placement group and launch new instances in that group. Make sure that both the placement groups are in the same subnet.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Stop and restart all the instances in the Placement Group and then try the launch again.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Request a capacity increase from AWS as you are initially limited to 10 instances per Placement Group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Make sure all the instances are the same size and then try the launch again.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 54,
  "query" : "A company has two batch processing applications that consume financial data about the day's stock transactions.\nEach transaction needs to be stored durably and the order of transactions needs to be guaranteed so that the billing and audit batch processing applications can process the data.\nThe billing application firstly processes the transaction information and after several hours, the audit application access to the same data.\nAfter the transaction information for the day is processed, the information no longer needs to be stored.\nWhat is the best way to architect this application so that the above requirements are achieved? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nThe main architectural considerations in this scenario are: (1) each transaction needs to be stored durably (no loss of any transaction), (2)they should be processed in serial order, (3) guaranteed delivery of each record to the audit and billing batch processing, and (4) the processing of the record by two application is done with a time gap of several hours.\nBased on the considerations above, it seems that we must use Amazon Kinesis Data Streams which enables real-time processing of streaming big data.\nIt provides ordering of records and the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.\nThe Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nOption A is incorrect because (a) the onus of ensuring that each message is copied to the audit queue is on the application, and (b) this option is not fail-proof.\ni.e.\nIf the application fails to copy the message between the queue, there is no logic to put the message back to the billing queue.\nOption B is incorrect because even though it uses SQS, there is an overhead of ensuring that the message is processed properly by the billing and audit processes.\nWhen the billing process is processing the message, the message is unavailable for the audit process.\nAlso, there is a possibility of processing the same record (message) multiple times by the instances processing it (there is no way to know if the record has been already processed).\nOption C is incorrect because it adds the overhead of delivery guarantee on the application itself.\nAlso, the use of DynamoDB in this scenario is not a cost-effective solution.\nTo store the transaction data in real-time data, Kinesis is a better choice.\nOption D is CORRECT because Amazon Kinesis is best suited for applications that need to process large real-time transactional records and have the ability to consume records in the same order a few hours later.\nFor example, you have a billing application and an audit application that runs a few hours behind the billing application.\nBecause Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.\nMore information on when Amazon Kinesis Data Streams and Amazon SQS should be used:\nAWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:\nRouting related records to the same record processor (as in streaming MapReduce)\nFor example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.\nOrdering of records.\nFor example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.\nAbility for multiple applications to consume the same stream concurrently.\nFor example, you have one application that updates a real-time dashboard and another archives data to Amazon Redshift.\nYou want both applications to consume data from the same stream concurrently and independently.\nAbility to consume records in the same order a few hours later.\nFor example, you have a billing application and an audit application that runs a few hours behind the billing application.\nBecause Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.\nAWS recommends Amazon SQS for use cases with requirements that are similar to the following:\nMessaging semantics (such as message-level ack/fail) and visibility timeout.\nFor example, you have a queue of work items and want to track each item's successful completion independently.\nAmazon SQS tracks the ack/fail, so the application does not have to maintain a persistent checkpoint/cursor.\nAmazon SQS will delete asked messages and redeliver failed messages after a configured visibility timeout.\nIndividual message delay.\nFor example, you have a job queue and need to schedule individual jobs with a delay.\nWith Amazon SQS, you can configure individual messages to have a delay of up to 15 minutes.\nLeveraging Amazon SQS's ability to scale transparently.\nFor example, you buffer requests and the load changes as a result of occasional load spikes or the natural growth of your business.\nBecause each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nThe best way to architect this application to meet the requirements is to use option D: Use Kinesis to store the transaction information. The billing application consumes data from the stream and then the audit application consumes the same data several hours later.\nHere's why:\nKinesis is a fully managed, scalable, and durable data streaming service that allows real-time processing of streaming data at scale. Kinesis Data Streams can collect and process large amounts of data in real-time and enables applications to read and analyze the data as it arrives. Kinesis is well-suited for this use case because it can provide durable storage and ordering guarantees, and can also be used to process and analyze the data in real-time.\nOption A (Use SQS for storing the transaction messages. When the billing batch process consumes each message, have the application create an identical message and place it in a different SQS for the audit application to use several hours later) is not an ideal solution. Although SQS provides durable storage of messages and can guarantee message order within a queue, it is not designed for real-time data processing. It is also not recommended to create identical messages for each application as this can lead to increased cost and complexity.\nOption B (Use SQS for storing the transaction messages; when the billing batch process performs first and consumes the message, write the code in a way that does not remove the message after consumed, so it is available for the audit application several hours later. The audit application can consume the SQS message and remove it from the queue when completed) is not an optimal solution either. Although this approach ensures that the message is available for both applications, it may lead to issues with message duplication or other data consistency problems. Additionally, it is not efficient or cost-effective to keep the messages in the queue for several hours after they have been processed.\nOption C (Store the transaction information in a DynamoDB table. The billing application can read the rows while the audit application will read the rows then remove the data) is not ideal because it does not provide ordering guarantees or real-time processing capabilities. Although DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability, it is not suitable for real-time data processing or data streams.\nOption D is the best solution because Kinesis provides the durability, ordering guarantees, and real-time processing capabilities required for this use case. The billing application can consume the data from the stream in real-time, and the audit application can consume the same data several hours later. Additionally, Kinesis allows applications to process and analyze the data in real-time, which can be useful for real-time monitoring, alerting, or other applications that need to respond quickly to changes in the data.\nIn summary, Kinesis is the best solution for this use case because it provides durable storage, ordering guarantees, real-time processing capabilities, and efficient data handling. It is designed for real-time data streaming and is well-suited for this type of use case.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use SQS for storing the transaction messages. When the billing batch process consumes each message, have the application create an identical message and place it in a different SQS for the audit application to use several hours later.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use SQS for storing the transaction messages; when the billing batch process performs first and consumes the message, write the code in a way that does not remove the message after consumed, so it is available for the audit application several hours later. The audit application can consume the SQS message and remove it from the queue when completed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the transaction information in a DynamoDB table. The billing application can read the rows while the audit application will read the rows then remove the data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Kinesis to store the transaction information. The billing application consumes data from the stream and then the audit application consumes the same data several hours later.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 55,
  "query" : "A company is considering integrating its on-premises resources with AWS in a hybrid architecture without any security threats posed by the internet.\nTheir goal is to run the customer-facing data collection processes in AWS.\nThey have to transfer a huge volume of data from their on-premises environment to the EC2 instances running in an AWS VPC (with the data being stored in the volumes of the EC2 instances) daily using a high bandwidth connection which may save costs too.\nHow can this be accomplished?",
  "answer" : "Answer - B.\nIn this question, the customer wants to transfer a large amount of data to VPC from the on-premises data center.\nThe main architectural considerations are (1) the cost should be low, and (2) the data being transferred is new data every time.\nOption A is incorrect.\nAlthough setting up a VPN is a cost-effective solution, it may not have sufficient bandwidth for the data being transferred, especially since the data is new every time.\nAlso, the data will be transferred over the internet.\nSo, the new data adds to the unpredictability of the performance that the VPN connection would yield.\nSo, the VPN connection may stay much longer than expected adding to the overall cost.\nOption B is CORRECT because (a) since it is a dedicated connection from on-premises data center to AWS, it takes out the unpredictable nature of the internet out of the equation, and (2) due to the high bandwidth availability, Direct Connect would most probably transfer the large amount of data quickly compared to VPN connection.\nHence, it may well save the cost for the customer.\nOption C is incorrect because AWS Import/Export is not an ideal option since the data being transferred is new every time, since Import/Export is preferred mainly for first-time data migration and using VPN/Direct Connect later on.\nOption D is incorrect because it is the requirement that the data must be transferred to AWS, hence ruling out the option of leaving the data on-premise.\nFor more information on AWS direct connect, browse to the below URL-\nhttps://aws.amazon.com/directconnect/\nNote: While I agree that the Direct Connect is costly, but compared to other options given, it is certainly the most viable.\nWith the dedicated network connectivity and higher bandwidth, the data transfer would get done quicker than the internet.\nWith Direct Connect, the initial setup cost would be more.\nBut in the long run, it would be the most suitable solution even with regards to keeping the cost low.\nTo transfer a large volume of data from on-premises to AWS, while ensuring that there are no security threats posed by the internet, a company can consider the following approaches:\nA. Provision a VPN connection between the on-premise data center and the AWS region using the VPN section of a VPC. This approach involves setting up a Virtual Private Network ( VPN) connection between the on-premises data center and the AWS region. This can be done using the VPN section of a Virtual Private Cloud (VPC) in AWS. Once the VPN connection is established, the company can transfer the data securely between the on-premises environment and the EC2 instances running in the AWS VPC. The VPN connection will ensure that the data is encrypted while in transit, and will help to prevent any unauthorized access to the data.\nB. Suggest provisioning a Direct Connect connection between the on-premise data center and the AWS region. Direct Connect is a dedicated network connection between the on-premises data center and an AWS region. This approach involves setting up a dedicated connection between the on-premises environment and the AWS region, which can provide a high-speed, low-latency connection for data transfer. This can be particularly useful for transferring large volumes of data, as it can be more cost-effective than using a VPN connection. Direct Connect also offers enhanced security, as the data does not travel over the internet, and is instead transmitted over a private network connection.\nC. Suggest using AWS import/export to transfer the TBs of data while synchronizing the new data as it arrives. AWS Import/Export is a service provided by AWS that allows customers to transfer large amounts of data to and from AWS using portable storage devices. This approach involves shipping the data to AWS using physical storage devices, such as hard drives or tapes, which are then imported into AWS using the Import/Export service. This can be a useful option for transferring large volumes of data, as it can be faster than transferring the data over the internet. Once the initial transfer is complete, the company can synchronize any new data as it arrives using a different transfer method, such as a VPN or Direct Connect connection.\nD. Suggest leaving the data required for the application on-premise and use a VPN to query the on-premise database data from EC2 when required. This approach involves leaving the data required for the application on-premises and using a VPN to query the data from EC2 instances running in the AWS VPC when required. This approach can be useful if the company does not need to transfer all of the data to AWS, or if the data is not required to be stored in AWS. However, it may not be the best option if the company requires high-performance access to the data, as accessing the data over a VPN connection can be slower than accessing it locally.\nIn conclusion, there are several options available for transferring large volumes of data from an on-premises environment to AWS in a secure manner, including VPN, Direct Connect, AWS Import/Export, and leaving the data on-premises and accessing it using a VPN connection. The best approach will depend on the specific requirements of the company, including the volume of data to be transferred, the level of security required, and the performance requirements of the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Provision a VPN connection between the on-premise data center and the AWS region using the VPN section of a VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Suggest provisioning a Direct Connect connection between the on-premise data center and the AWS region.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Suggest using AWS import/export to transfer the TBs of data while synchronizing the new data as it arrives.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Suggest leaving the data required for the application on-premise and use a VPN to query the on-premise database data from EC2 when required.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 56,
  "query" : "A multi-tier application is being hosted on a single EC2 instance in a VPC without an ELB.\nYou have been instructed to set it up with separate SSL certificates for each tier.\nWhich of the following would be the best method to achieve this while leaving the application running on a single EC2 instance?",
  "answer" : "Answer - A.\nIt can be useful to assign multiple IP addresses to an instance in your VPC to do the following.\n(1) Host multiple websites on a single server by using multiple SSL certificates on a single server and associating each certificate with a specific IP address.\n(2) Operate network appliances, such as firewalls or load balancers, that have multiple IP addresses for each network interface.\n(3) Redirect internal traffic to a standby instance if your instance fails by reassigning the secondary IP address to the standby instance.\nOption A is CORRECT because, as mentioned above, if you have multiple elastic network interfaces (ENIs) attached to the EC2 instance, each network IP can have a component running with a separate SSL certificate.\nOption B is incorrect because having separate rules in the security group as well as NACL does not mean that the instance supports multiple SSLs.\nOption C is incorrect because an EC2 instance cannot have multiple subnets.\nOption D is incorrect because the NAT address is not related to supporting multiple SSLs.\nFor more information on Multiple IP Addresses, please refer to the link below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html\nTo set up separate SSL certificates for each tier of a multi-tier application running on a single EC2 instance in a VPC without an ELB, the best method would be to use option A - create an EC2 instance that has multiple network interfaces with multiple elastic IP addresses.\nOption A: Create an EC2 instance that has multiple network interfaces with multiple elastic IP addresses\nThis option involves creating multiple network interfaces and assigning each interface a separate Elastic IP address. Each network interface can be assigned to a separate security group, which can be configured to allow traffic to the specific tier of the application that it is serving.\nSSL certificates can then be configured for each network interface, which will allow traffic to be encrypted between the client and the corresponding tier of the application.\nOption B: Create an EC2 instance that has both an ACL and the security group attached to it and have separate rules for each IP address\nThis option involves using a single network interface for the EC2 instance and attaching both an ACL and a security group to it. Separate rules can be configured for each IP address to allow traffic to the specific tier of the application that it is serving.\nHowever, SSL certificates cannot be configured separately for each IP address in this setup.\nOption C: Create an EC2 instance that has multiple subnets attached to it and each will have a separate IP address\nThis option involves creating multiple subnets and attaching them to a single EC2 instance. Each subnet will have a separate IP address assigned to it.\nHowever, this setup does not allow for separate security groups to be assigned to each subnet, and SSL certificates cannot be configured separately for each subnet.\nOption D: Create an EC2 instance with a NAT address\nThis option involves using a Network Address Translation (NAT) instance to forward traffic to the appropriate tier of the application. SSL certificates can be configured for the NAT instance to encrypt traffic between the client and the NAT instance.\nHowever, this setup requires additional configuration and adds complexity to the infrastructure, and may not be the most efficient or cost-effective solution.\nIn summary, option A - creating an EC2 instance with multiple network interfaces and multiple elastic IP addresses - is the best method for setting up separate SSL certificates for each tier of a multi-tier application running on a single EC2 instance in a VPC without an ELB.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an EC2 instance that has multiple network interfaces with multiple elastic IP addresses.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an EC2 instance that has both an ACL and the security group attached to it and have separate rules for each IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an EC2 instance that has multiple subnets attached to it and each will have a separate IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an EC2 instance with a NAT address.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 57,
  "query" : "You are working as a consultant for a company designing a new hybrid architecture to manage part of their application infrastructure in the cloud and on-premise.\nAs part of the infrastructure, they need to transfer high amounts of data consistently.\nThey require a low latency and high consistency traffic to AWS.\nThe company is looking to keep costs as low as possible and is willing to accept slow traffic in the event of primary failure.\nGiven these requirements, how would you design a hybrid architecture? Choose the correct answer from the options below.",
  "answer" : "Answer - A.\nAWS Direct Connect makes it easy to establish a dedicated network connection from your on-premises data center to AWS.\ni.e.\nYou can establish private connectivity between AWS and the on-premises data center, which helps to reduce the overall network cost, increase bandwidth throughput, and provide a more consistent network experience than the internet-based connection.\nA VPN connection is a low-cost, appliance-based access to the AWS resources given to the on-premises resources via gateways.\nCompared to AWS Direct Connect, the VPN connection may experience slow connection speed and limited bandwidth due to unpredictability of the internet.\nSince cost is a factor for the backup and the company does not mind the reduced traffic, the backup option can a VPN connection instead of another direct connect solution.\nOption A is CORRECT because it sets up a Direct Connect as the primary connection that provides consistent bandwidth for transferring high amounts of data.\nIn case of failure, it sets up a VPN which is a low-cost solution.\nOption B is incorrect because VPN (although set up as dual) does not provide a low latency connection as it still has network unpredictability, and consistency as the Direct Connect would do.\nOption C is incorrect because there is no automatic failover or redundancy in Direct Connect.\nOption D is incorrect because setting up two Direct Connect connections would be costly.\nFor more information on AWS direct connect, browse to the below URL-\nhttps://aws.amazon.com/directconnect/\nThe requirement for low latency and high consistency traffic to AWS while keeping costs as low as possible suggests the need for a hybrid architecture that leverages both on-premise and cloud resources. Additionally, the requirement for consistent data transfer suggests the need for a reliable and resilient connectivity solution.\nOption A: Provision a Direct Connect connection to an AWS region using a Direct Connect partner. Provision a VPN connection as a backup in the event of Direct Connect connection failure. This option suggests using Direct Connect, a dedicated network connection between the on-premise data center and the AWS region. This would provide a consistent and reliable connectivity solution with low latency. Direct Connect partner connections can be cost-effective compared to Direct Connect provider connections. However, Direct Connect partner connections may have limitations in terms of bandwidth and may not provide automatic failover in case of primary failure. In this option, a VPN connection is suggested as a backup, which would provide a redundant and secure connection to AWS in case of Direct Connect connection failure. However, the VPN connection may have higher latency and lower consistency than the Direct Connect connection.\nOption B: Create a dual VPN tunnel for private connectivity, which increases network consistency and reduces latency. The dual tunnel provides a backup VPN in the case of primary failover. This option suggests using dual VPN tunnels to provide private connectivity between the on-premise data center and the AWS region. This would provide a reliable and resilient connectivity solution at a lower cost compared to Direct Connect. However, VPN connections may have higher latency and lower consistency compared to Direct Connect. This option also provides a backup VPN tunnel in the case of primary failover, which increases reliability and resilience.\nOption C: Provision a Direct Connect connection which has automatic failover and backup built into the service. This option suggests using Direct Connect, but with automatic failover and backup built into the service. This would provide a highly reliable and resilient connectivity solution with low latency and high consistency. However, this option may be more expensive than the other options.\nOption D: Provision a Direct Connect connection to an AWS region using a Direct Connect provider. Provision a secondary Direct Connect connection as a failover. This option suggests using Direct Connect with a Direct Connect provider. This would provide a reliable and resilient connectivity solution with low latency and high consistency. However, this option may be more expensive than the other options. This option also provides a secondary Direct Connect connection as a failover, which increases reliability and resilience.\nOverall, each option has its advantages and disadvantages, and the choice would depend on the specific requirements and constraints of the project. Options A and B may be more cost-effective but may have limitations in terms of reliability and resilience. Options C and D would provide high reliability and resilience but may be more expensive.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Provision a Direct Connect connection to an AWS region using a Direct Connect partner. Provision a VPN connection as a backup in the event of Direct Connect connection failure.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a dual VPN tunnel for private connectivity, which increases network consistency and reduces latency. The dual tunnel provides a backup VPN in the case of primary failover.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Provision a Direct Connect connection which has automatic failover and backup built into the service.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Provision a Direct Connect connection to an AWS region using a Direct Connect provider. Provision a secondary Direct Connect connection as a failover.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 58,
  "query" : "You have a massive social networking application that is already deployed on the N.Virginia region ( with newly created key pairs ) with around 100 EC2 instances.\nYou want to deploy your application to multiple regions for better availability.\nYou don't want to handle multiple key pairs and reuse existing key pairs for the N.Virginia region.\nHow will you accomplish this?",
  "answer" : "Answer - C.\nKey pairs across regions are not possible.\nIn order to use key pairs across regions, you need to import the key pairs in the respective regions.\nYou need to go to the respective region, and from the EC2 dashboard, click on Import Key pair and choose the relevant key pair.\nOption A is incorrect because the key pair is region specific - not global.\nOption B is incorrect because keys cannot be copied across different regions.\nThey need to be imported.\nOption C is CORRECT because import key pair functionality enables migrating an EC2 instance from one region to another and use the same PEM key.\nOption D is incorrect because PEM keys cannot be copied to another region as part of the AMI.\nFor more information on bringing your own key pair, please refer to the below URL-\nhttps://aws.amazon.com/blogs/aws/new-amazon-ec2-feature-bring-your-own-keypair/\nOption A is correct. Key pairs in AWS are not tied to any specific region. Instead, they are global resources that can be used across all regions. Therefore, you do not need to create new key pairs for each region where you want to deploy your application.\nOptions B, C, and D are not relevant for this scenario since they all deal with copying or importing key pairs or machine images between regions. While these features are useful in some cases, they do not address the specific question of how to reuse an existing key pair across multiple regions.\nTo summarize, if you want to deploy your application to multiple regions and use the same key pair, you can simply use the existing key pair that you created in the N.Virginia region. You do not need to create new key pairs for each region, as key pairs are a global resource in AWS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Key pair is not a region level concept, all the keys are available globally.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use copy key command line API to transfer key to different regions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Using import key-pair feature using AWS web console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Copy AMI of your EC2 machine between regions and start an instance from that AMI.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 59,
  "query" : "A third-party auditor is being brought in to review security processes and configurations for all of a company's AWS accounts.\nCurrently, the company does not use any on-premises identity provider.\nInstead, they rely on IAM accounts in each of their AWS accounts.\nNow the auditor needs read-only access to all AWS resources for each AWS account.\nThe auditor has an IAM user in his AWS account.\nGiven the requirements, what is the most secure and easiest method for architecting access for the security auditor? Choose the correct answer from the options below.",
  "answer" : "Answer - C.\nOption A is incorrect because creating an IAM User for each AWS account is an overhead and less preferred way than creating IAM Role.\nOption B is incorrect because the scenario says that the company does not have any on-premises identity provider.\nThis method is not straightforward.\nOption C is CORRECT because it creates an IAM Role with all the necessary permission policies that allow the auditor to assume it while accessing the resources.\nOption D is incorrect because using the IAM Role with the required permissions is the preferred and more secure way of accessing the AWS resources than using the Amazon credentials.\nAlso, this option does not use the Security Token Service that gives temporary credentials to log in.\nHence this is a less secure way of accessing the AWS resources.\nFor more information on IAM roles, please refer to the below URL-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nThe most secure and easiest method for architecting access for the security auditor in this scenario is to create an IAM role with read-only permissions to all AWS services in each AWS account and allow the auditor IAM user to assume the ARN role for each AWS account. This is option C.\nExplanation:\nOption A: Create an IAM user for each AWS account with read-only permission policies for the auditor, and disable each account when the audit is complete.\nThis option requires creating a separate IAM user for each AWS account and granting read-only access to each user for their respective account. However, this approach is not scalable and may result in a lot of administrative overheads, especially if the company has a large number of AWS accounts. Additionally, there is a risk of human error when disabling each account after the audit is complete, which could result in a security breach.\nOption B: Configure an on-premise AD server and enable SAML identity federation for single sign-on to each AWS account.\nThis option involves setting up an on-premise AD server and enabling SAML identity federation for single sign-on to each AWS account. However, this approach requires additional infrastructure and management overheads, which may not be practical for the company. Additionally, setting up and configuring SAML identity federation requires additional technical expertise, which the company may not have.\nOption C: Create an IAM role with read-only permissions to all AWS services in each AWS account. Allow the auditor IAM user to assume the ARN role for each AWS account.\nThis option involves creating an IAM role with read-only permissions to all AWS services in each AWS account and allowing the auditor IAM user to assume the ARN role for each AWS account. This approach is scalable, easy to manage, and provides granular control over access. Additionally, the company can revoke access to the IAM role once the audit is complete, ensuring no residual access is left behind.\nOption D: Create a custom identity broker application that allows the auditor to use existing Amazon credentials to log into the AWS environments.\nThis option involves creating a custom identity broker application that allows the auditor to use existing Amazon credentials to log into the AWS environments. However, this approach requires additional infrastructure and management overheads, which may not be practical for the company. Additionally, creating a custom application introduces additional security risks, which may not be acceptable to the auditor.\nTherefore, option C is the most secure and easiest method for architecting access for the security auditor in this scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an IAM user for each AWS account with read-only permission policies for the auditor, and disable each account when the audit is complete.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an on-premise AD server and enable SAML identity federation for single sign-on to each AWS account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM role with read-only permissions to all AWS services in each AWS account. Allow the auditor IAM user to assume the ARN role for each AWS account.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a custom identity broker application that allows the auditor to use existing Amazon credentials to log into the AWS environments.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 60,
  "query" : "An auditor needs access to logs that record all the API events on AWS.\nThe auditor only needs read-only access to the log files and does not need access to each AWS account.\nThe company has multiple AWS accounts, and the auditor needs access to all the logs for all the accounts.\nWhat is the best way to configure access for the auditor to view event logs from all accounts? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nYou can have CloudTrail deliver log files from multiple AWS accounts into a single Amazon S3 bucket.\nFor example, you have four AWS accounts with account IDs 111111111111, 222222222222, 333333333333, and 444444444444, and you want to configure CloudTrail to deliver log files from all four of these accounts to a bucket belonging to account 111111111111\nTo accomplish this, complete the following steps in order:\nTurn on CloudTrail in the account where the destination bucket will belong (111111111111 in this example)\nDo not turn on CloudTrail in any other accounts yet.\nUpdate the bucket policy on your destination bucket to grant cross-account permissions to CloudTrail.\nTurn on CloudTrail in the other accounts you want (222222222222, 333333333333, and 444444444444 in this example)\nConfigure CloudTrail in these accounts to use the same bucket belonging to the account that you specified in step 1 (111111111111 in this example).\nThe AWS CloudTrail service provides an option to deliver the log files for all the regions in a single S3 bucket.\nThis feature is very useful when you need to access the logs related to all the resources in multiple regions for all the AWS accounts via a single S3 bucket.\nPlease see the images below.\nOption A is incorrect because delivering the logs in multiple buckets is an unnecessary overhead.\nInstead, you can have CloudTrail deliver the logs to a single S3 bucket.\nOption B is incorrect because consolidated billing will not help the auditor to get the records of all the API events in AWS.\nOption C is incorrect because there is no consolidated logging feature in AWS CloudTrail.\nOption D is CORRECT because it delivers the logs pertaining to different AWS accounts to a single S3 bucket in the primary account and grants the auditor access to it.\nMore information on this topic regarding CloudTrail:\nYou can have CloudTrail deliver log files from multiple AWS accounts into a single Amazon S3 bucket.\nFor example, if you have four AWS accounts with account IDs A, B, C, and D, and you can configure CloudTrail to deliver log files from all four of these accounts to a bucket belonging to account A.\nSee the link below-\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html\nThe correct answer is D. Configure the CloudTrail service in each AWS account and have the logs delivered to a single S3 bucket in a separate account. Provide the auditor with access only to this bucket.\nExplanation: The CloudTrail service is used to track API events on AWS. It can be configured in each AWS account to log all API events. However, since the company has multiple AWS accounts, it may be challenging to provide the auditor with access to all the logs for all the accounts. Here are the reasons why the other options are not the best way to configure access for the auditor to view event logs from all accounts:\nOption A: Configure the CloudTrail service in each AWS account, and make the logs delivered to an S3 bucket on each account, while granting the auditor permissions to the bucket via roles in the secondary accounts and a single primary IAM account that can assume a read-only role in the secondary AWS accounts.\nThis option requires granting access to the auditor for each S3 bucket on each account, which can be cumbersome and difficult to manage. Also, it can be difficult to keep the access permissions consistent across all the accounts.\nOption B: Configure the CloudTrail service in the primary AWS account and configure consolidated billing for all the secondary accounts. Then grant the auditor access to the S3 bucket that receives the CloudTrail log files.\nConsolidated billing is a feature that allows you to consolidate billing for multiple AWS accounts. However, it does not provide access to CloudTrail logs. Also, since CloudTrail is not enabled in each account, it will not provide comprehensive logs for all the accounts.\nOption C: Configure the CloudTrail service in each AWS account and enable consolidated logging inside of CloudTrail.\nThis option enables you to consolidate logs from multiple AWS accounts into a single S3 bucket, but it does not provide access to the auditor. Additionally, this option requires configuring CloudTrail in each account and enabling consolidated logging, which can be time-consuming and complicated to manage.\nOption D: Configure the CloudTrail service in each AWS account and have the logs delivered to a single S3 bucket in a separate account. Provide the auditor with access only to this bucket.\nThis option enables you to consolidate logs from multiple AWS accounts into a single S3 bucket and provides access to the auditor to view logs from all the accounts. The logs can be delivered to a single S3 bucket in a separate account, which makes it easier to manage access permissions for the auditor. Also, it provides comprehensive logs for all the accounts, which is crucial for auditing purposes.\nIn summary, the best way to configure access for the auditor to view event logs from all accounts is to configure the CloudTrail service in each AWS account and have the logs delivered to a single S3 bucket in a separate account. Provide the auditor with access only to this bucket.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure the CloudTrail service in each AWS account, and make the logs delivered to an S3 bucket on each account, while granting the auditor permissions to the bucket via roles in the secondary accounts and a single primary IAM account that can assume a read-only role in the secondary AWS accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the CloudTrail service in the primary AWS account and configure consolidated billing for all the secondary accounts. Then grant the auditor access to the S3 bucket that receives the CloudTrail log files.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the CloudTrail service in each AWS account and enable consolidated logging inside of CloudTrail.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the CloudTrail service in each AWS account and have the logs delivered to a single S3 bucket in a separate account. Provide the auditor to access only to this bucket.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 61,
  "query" : "An employee unknowingly keeps terminating EC2 instances on the production environment.\nYou want to restrict the user from terminating the production instances or add an extra layer of defense before he tries to do that next time.\nWhich of the following options are suitable? (Select TWO.)",
  "answer" : "Correct Answer: A &amp; D.\nThe key point to this question is the “extra layer of defence against terminating the instances”.\nOption A is CORRECT because with tags you can explicitly deny an action.\nCheck the following example:\n{\n\"Sid\": \"DenyDelete\",\n\"Action\": [\n\"ec2:TerminateInstances\"\n],\n\"Effect\": \"Deny\",\n\"Resource\": \"*\",\n\"Condition\": {\n\"StringLikeIfExists\": {\n\"ec2:ResourceTag/Production\": \"true\"\n}\n}\n}\nFrom the article below:\n“In some contexts, you may optionally choose to explicitly deny a group of users the ability to manage specific instances.\nExplicit denial policies are not generally required since IAM is deny-all by default.\nBut the use of an explicit deny policy can provide an additional layer of protection since the presence of a deny statement will cause the user to be denied the ability to act even if another policy statement would have allowed it.”\nFor more information.\nhttps://aws.amazon.com/blogs/security/resource-level-permissions-for-ec2-controlling-management-access-on-specific-instances/\nOption B is incorrect.\nWhile it starts correctly by tagging production instances, it does not use the production tag in the IAM policy.\nBesides, the IAM policy should contain an explicit deny instead of an implicit deny.\nOption C is incorrect because disabling MFA removes layers of protections rather than adds them.\nOption D is CORRECT because it ensures that an AWS MFA device authenticates the user before the employee can delete objects (termination protection for instances)\nThe following condition can be added to the IAM policy:\n\"Condition\": {\"Bool\": {\"aws:MultiFactorAuthPresent\": \"true\"}}\nFrom the article below:\n“You can also set conditions that require the use of SSL or MFA (multi-factor authentication)\nFor example, you can require that a user has authenticated with an MFA device in order to be allowed to terminate an Amazon EC2 instance.”\nFor more information.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-policy-conditions https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html#MFAProtectedAPI-resource-policies\nThe situation described in the question highlights the need to prevent an employee from terminating EC2 instances on the production environment. This can be accomplished by implementing additional security measures and access controls. Two possible solutions are discussed below:\nOption A: Tagging instances with a production-identifying tag and adding resource-level permissions\nThis solution involves tagging all production instances with a unique identifier (e.g., \"production\") and then modifying the user's IAM policy to include an explicit deny statement for the terminate API call to instances with the production tag. This ensures that the employee user is not allowed to terminate production instances, but can still terminate non-production instances if necessary. This solution requires resource-level permissions to be added, which provide fine-grained control over specific resources, in this case, EC2 instances.\nOption B: Tagging instances with a production-identifying tag and giving the IAM user an implicit deny\nThis solution involves tagging all production instances with a unique identifier and then modifying the user's IAM policy to include an implicit deny statement for the EC2 terminate API call for all EC2 instances. This ensures that the employee user is not allowed to terminate any EC2 instances, regardless of whether they are production or non-production.\nOption C: Modifying the IAM policy to require MFA before deleting EC2 instances and disabling MFA access\nThis solution involves modifying the IAM policy on the user to require Multi-Factor Authentication (MFA) before deleting EC2 instances. Additionally, MFA access to the employee can be disabled to ensure that they are unable to perform the termination action. This solution is a general approach that can be applied to other AWS services and not just EC2 instances.\nOption D: Authenticating with an MFA device to terminate an Amazon EC2 instance\nThis solution involves requiring the user to authenticate with an MFA device before they can terminate an Amazon EC2 instance. This can be done using IAM policies, which allow for granular access control. This solution provides an additional layer of defense against unauthorized termination of EC2 instances, but does not prevent the employee user from terminating non-production instances.\nIn summary, Option A and Option B are suitable solutions for preventing an employee from terminating production EC2 instances, while Option C and Option D are more general approaches that can be used to prevent unauthorized access to AWS services.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Tag the instances with a production-identifying tag and add resource-level permissions to the employee user with an explicit deny on the terminate API call to instances with the production tag.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Tag the instances with a production-identifying tag. Give the IAM user an implicit deny on the EC2 terminate API call for all the EC2 instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the IAM policy on the user to require MFA before deleting EC2 instances and disable MFA access to the employee.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user should be authenticated with an MFA device in order to be allowed to terminate an Amazon EC2 instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 62,
  "query" : "A company is managing a customer's application which currently includes a three-tier application configuration.\nThe first tier manages the web instances and is configured in a public subnet in an AWS VPC.\nThe second layer is the application layer.\nAs part of the application code, the application instances upload large amounts of data to Amazon S3\nCurrently, the private subnets that the application instances are running on have a route to a single t2.micro NAT instance.\nThe application, during peak loads, becomes slow, and customer uploads from the application to S3 are not completing and taking a long time.\nWhich steps might you take to solve the issue using the most cost-efficient method? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nIn this scenario, the NAT instance is the bottleneck which during the peak loads becomes slow.\nThe possible solutions are either to scale up or scale out the NAT instance or setup S3 as the endpoint of the VPC.\nSo that the VPC can privately and securely connect to the S3 buckets.\nSee the images below for setting up the S3 as VPC Endpoint.\nOption A is incorrect because even though placing NAT instances in auto-scale would handle the increase in load, the addition of the NAT instances would not be as cost-efficient as creating an S3 endpoint.\nOption B is CORRECT because, with the S3 Endpoint, the VPC can privately and securely connect to the S3 buckets.\nNo additional infrastructure provisioning such as NAT or Gateway is needed, hence saving the cost.\nOption C is incorrect because increasing in NAT instance size would add to the cost.\nOption D is incorrect because provisioning additional NAT instances would add to the cost.\nFor more information on VPC endpoints, please refer to the below URLs-\nhttps://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/ https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html\nThe issue that the company is facing is that the private subnet instances are experiencing performance degradation during peak loads, and the uploads from the application to S3 are slow or not completing. The current configuration has a single t2.micro NAT instance that acts as a gateway for traffic going out to the internet. This NAT instance has a limited amount of network bandwidth, which can become a bottleneck during peak loads.\nTo solve the issue, we need to consider the most cost-efficient method that will improve the network throughput for the private subnet instances and enable faster uploads to S3. Let's look at the available options:\nOption A: Configure Auto Scaling for the NAT instance to handle an increase in load.\nThis option involves setting up Auto Scaling for the NAT instance. This approach can help handle increased load during peak periods by launching additional instances of the NAT instance. However, this solution may not address the underlying issue of limited network bandwidth, and additional instances may not improve network throughput.\nOption B: Create a VPC S3 endpoint.\nA VPC S3 endpoint is a highly available and scalable service that enables direct connectivity between VPCs and S3 buckets. By creating a VPC endpoint for S3, traffic from the private subnet instances to S3 can bypass the NAT instance and go directly to the S3 service, resulting in faster uploads. This option is a cost-efficient and straightforward solution to the problem.\nOption C: Increase the NAT instance size; network throughput increases with an increase in instance size.\nThis option involves upgrading the NAT instance to a larger instance size with better network throughput capabilities. While this option may increase network throughput, it may not be cost-efficient as the company would have to pay for the increased instance size continuously, regardless of the traffic load.\nOption D: Launch an additional NAT instance in another subnet and replace one of the routes in a subnet with the new instance.\nThis option involves launching an additional NAT instance in a different subnet and replacing one of the routes in a subnet with the new instance. This solution can improve network throughput by distributing traffic across multiple NAT instances. However, this solution may not be cost-efficient as it requires launching and managing additional instances.\nConclusion:\nOut of the four available options, Option B is the most cost-efficient solution for the issue. It involves creating a VPC S3 endpoint that enables traffic from the private subnet instances to bypass the NAT instance and go directly to the S3 service. This solution can significantly improve the upload speed to S3 and is a simple and cost-effective approach to solving the problem.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure Auto Scaling for the NAT instance to handle an increase in load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a VPC S3 endpoint.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Increase the NAT instance size; network throughput increases with an increase in instance size.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Launch an additional NAT instance in another subnet and replace one of the routes in a subnet with the new instance.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 63,
  "query" : "A company has employees who need to run internal applications that access the company's AWS resources.\nThese employees already have user credentials in the company's current identity authentication system, based on their roles, supported by SAML2.0\nHow should the SSO setup be designed?",
  "answer" : "Answers - B and C.\nOption A is incorrect because already a role-based setup is in place.\nOption B is CORRECT because (a) it creates a custom identity broker application for authenticating the users using their existing credentials, (b) it gets temporary access credentials using STS, and (3) it uses federated access for accessing the AWS resources.\nOption C is CORRECT because (a) it creates a custom identity broker application for authenticating the users using their existing credentials, and (b) it uses AssumeRole API for accessing the resources using a temporary role.\nOption D is INCORRECT as the DecodeAuthorizationMessage API call only decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request.\nMore information on AssumeRole and GetFederatedToken:\nAssume Role - Returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) that you can use to access AWS resources that you might not normally have access to.\nTypically, you use AssumeRole for cross-account access or federation.\nFor more information, please visit the below URL-\nhttp://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\nGetFederationToken - Returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user.\nA typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network.\nFor more information, please visit the below URL-\nhttp://docs.aws.amazon.com/STS/latest/APIReference/API_GetFederationToken.html\nThe most appropriate option for designing the SSO setup for this scenario would be Option C: Create a custom identity broker application that authenticates employees using the existing system and uses the AssumeRole API call to gain temporary, role-based access to AWS.\nOption A, creating an IAM user to share based on employee roles, is not recommended because IAM users require a direct association with AWS accounts and cannot be used to authenticate external users.\nOption B, creating a custom identity broker application that uses the GetFederationToken API call, may work but is not the best option as it requires more complex code and additional permissions that may be difficult to manage.\nOption D, configuring an AD server and using the DecodeAuthorizationMessage API call, is also not the best option as it involves more setup and configuration, and can be more difficult to maintain.\nOption C involves creating a custom identity broker application that authenticates employees using the company's existing authentication system (SAML2.0) and then uses the AssumeRole API call to gain temporary, role-based access to AWS resources. This approach allows for more fine-grained control over access permissions and is easier to maintain and manage.\nThe custom identity broker application can be designed to integrate with the company's existing SAML2.0-based identity authentication system. When an employee logs in, the custom identity broker application can authenticate the user and use the AssumeRole API call to request temporary AWS credentials that are assigned a role based on the user's SAML2.0 attributes. These temporary credentials can then be used to access AWS resources for a specified duration, after which they expire.\nBy using the AssumeRole API call, the custom identity broker application can provide users with temporary credentials that are scoped down to a specific set of permissions or resources, reducing the risk of unauthorized access to AWS resources.\nOverall, Option C provides a secure, scalable, and manageable solution that meets the requirements of the scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an IAM user to share based on employee roles in the company.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a custom identity broker application that authenticates the employees using the existing system, uses the GetFederationToken API call and passes a permission policy to gain temporary access credentials from STS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a custom identity broker application that authenticates employees using the existing system and uses the AssumeRole API call to gain temporary, role-based access to AWS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure an AD server that synchronizes from the company`s current Identity Provider and configures SAML based Single-Sign-On which will then use the DecodeAuthorizationMessage API call to generate credentials for the employees.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 64,
  "query" : "You have created a mobile application that serves data stored in an Amazon DynamoDB table.\nYour primary concern is scalability of the application and ability to handle millions of visitors and data requests.\nAs part of your application, the customer needs access to the data located in the DynamoDB table.\nGiven the application requirements, what would be the best method to design the application? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nThe AssumeRolewithWebIdentity returns a set of temporary security credentials for users who have been authenticated in a mobile or web application with a web identity provider, such as Amazon Cognito, Login with Amazon, Facebook, Google, or any OpenID Connect-compatible identity provider.\nOut of options C and D, Option C is invalid because S3 is used to host static websites and not server side language websites.\nFor more information on AssumeRolewithWebIdentity, please visit the below URL-\nhttp://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html\nThe best method to design the application, given the requirements of scalability and ability to handle millions of visitors and data requests, would be option D:\nD. Let the users sign in to the app using a third-party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the static webpage in an S3 bucket.\nThis option allows for a scalable and secure approach to accessing data stored in an Amazon DynamoDB table through a mobile application. Here's why:\n1.\nThird-party identity provider: Using a third-party identity provider like Amazon, Google, or Facebook allows users to authenticate with the application without needing to create a separate set of credentials. This provides a streamlined user experience and reduces the risk of password-related security issues.\n2.\nAssumeRoleWithWebIdentity API: This API allows the mobile application to request temporary security credentials for the user, based on the identity provider they used to authenticate. These temporary credentials can be used to access the DynamoDB table while still maintaining the security of the application and data.\n3.\nJavaScript and S3: Writing the application in JavaScript and hosting it in an S3 bucket provides a scalable solution that can handle millions of visitors and data requests. S3 can also be configured to serve the static content via CloudFront, which provides a content delivery network (CDN) for increased performance and availability.\nOverall, this solution leverages the security and scalability of AWS services to provide a reliable and secure mobile application that can handle a large volume of traffic and data requests.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an on-premise AD server utilizing SAML 2.0 to manage the application users inside the on-premise AD server and write code that authenticates against the LD serves. Grant a role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Let the users sign into the app using a third-party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWith API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the static webpage in an S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Let the users sign into the app using a third-party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in a server-side language using the AWS SDK and host the application in an S3 bucket for scalability.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Let the users sign in to the app using a third-party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the static webpage in an S3 bucket.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 65,
  "query" : "Singapore-based College is closed due to covid 19\nCollege wants to continue Online classes.\nThey are also looking for a solution to offer college-based applications/software/Labs to students easily.\nWhat device-agnostic solution for all college students will you suggest?",
  "answer" : "Click on the arrows to vote for the correct answer\nA.\nB.\nA.\nD.\nE.\nF.\nAnswer: B.\nOption A is incorrect because using Amazon WorkSpace is not the right solution in this case.\nOption B is Correct because Amazon AppStream offers a desktop-based application through streaming.\nPrice is based on a monthly fee per streaming user and stream resource required for provisioning.\nOption C is incorrect becauseAmazon WorkSpace is not the right solution in this case.\nOption D is incorrect because the hourly fee per streaming user is the wrong statement here.\nReference:\nhttps://aws.amazon.com/appstream2/pricing/?nc=sn&amp;loc=4 https://aws.amazon.com/blogs/publicsector/fife-school-districts-use-amazon-appstream-2-0-to-provide-equitable-access/\nThe best device-agnostic solution for a college to offer applications/software/Labs to students would be to use Amazon AppStream 2.0. Amazon AppStream 2.0 is a fully managed application streaming service that enables students to access applications and software from any device, regardless of their location.\nOption A and F suggest using Amazon AppStream 2.0 in combination with either a virtual private network ( VPN) or a browser on the student's personal system. The difference between the two options is the pricing model. Option B mentions a monthly fee per streaming user and stream resource required for provisioning, while option F mentions an hourly fee. The pricing is determined by the number of students using the service and the resources required to stream the applications.\nOption D suggests using Amazon AppStream 2.0 through the browser on the student's personal system, which is a cost-effective solution compared to option B and F. However, it may not be the best solution for resource-intensive applications that require more processing power.\nOption E suggests using Amazon WorkSpaces, a virtual desktop infrastructure (VDI) solution that provides students with a persistent desktop experience. However, this option requires an \"always on\" mode which may lead to increased costs.\nOverall, the best solution for a college to offer device-agnostic access to applications, software, and labs to students is to use Amazon AppStream 2.0. The college can choose the pricing model that best suits their needs and the solution is scalable to accommodate increasing demand from students.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You can suggest using Amazon AppStream 2.0 in VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AmazonAppStream 2.0 price is based on a monthly fee per streaming user and stream resource required for provisioning. Students can access the application through the Amazon WorkSpace.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can suggest using Amazon AppStream 2.0 in VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon AppStream 2.0 price is based on a monthly fee per streaming user and stream resource required for provisioning. Students can access the application through the browser in their personal system.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can suggest to use Amazon WorkSpace and run them in always ON mode.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can suggest using Amazon AppStream 2.0. Amazon AppStream 2.0 price is based on an hourly fee per streaming user and stream resource required for provisioning. Students can access the application through the browser in their personal system.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 66,
  "query" : "A customer has established an AWS Direct Connect connection to AWS.\nThe link is up and routes are being advertised from the customer's end.\nHowever, the customer cannot connect from EC2 instances inside its VPC to servers residing in its data center.",
  "answer" : "E.\nAnswers - B &amp; E.\nOption A is incorrect because adding an option of VPN is unnecessary.\nOption B is CORRECT because VGW is the other side of the connection (on the AWS side) and the route propagation needs to be enabled for the Direct Connect to work.\nOption C is incorrect because there is no such configuration in the Virtual Private Gateway.\nOption D is incorrect because there is no “route” command available on the instances in the VPC.Option E is correct because to advertise prefixes to Amazon, for Prefixes you want to advertise, enter the IPv4 CIDR destination addresses (separated by commas) to which traffic should be routed over the virtual interface.\n( under Additional Settings )\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/routing-and-bgp.html\nThe scenario described in the question suggests that the customer has established a Direct Connect (DX) connection between their on-premises data center and AWS. However, even though the DX link is up and routes are being advertised from the customer's end, EC2 instances inside the VPC are unable to connect to servers in the customer's data center. To resolve this issue, we need to identify the correct solution from the options provided.\nOption A suggests adding a route to the route table with an IPsec VPN connection as the target. This option is not suitable in the given scenario because the customer has established a Direct Connect connection, and adding an IPsec VPN connection would not be necessary.\nOption B suggests enabling route propagation to the Virtual Private Gateway (VGW). This option may help in resolving the issue because VGW is the logical representation of the customer gateway that connects their VPC with their on-premises network. By enabling route propagation, the routes advertised by the customer's on-premises router can be propagated to the VGW and subsequently to the VPC route tables. Therefore, this option is a potential solution to the issue described in the question.\nOption C suggests opening port 80 in the security group of the Virtual Private Gateway (VGW). However, VGW does not have a security group associated with it, so this option is not applicable in the given scenario.\nOption D suggests modifying the route table of all instances using the \"route\" command. While this option may work, it is not a scalable solution, and it would require manual intervention each time a new instance is launched in the VPC. Therefore, this option is not recommended.\nOption E suggests entering the IPv4 destination addresses for routing the traffic over VGW. This option is not suitable because the customer is already advertising routes from their on-premises network over the Direct Connect connection, and the goal is to enable EC2 instances in the VPC to access resources in the on-premises network.\nIn summary, the correct answer to the given question is Option B, which suggests enabling route propagation to the Virtual Private Gateway (VGW). By doing so, the routes advertised by the customer's on-premises router can be propagated to the VGW and subsequently to the VPC route tables, enabling EC2 instances in the VPC to access resources in the on-premises network.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Add a route to the route table with an IPsec VPN connection as the target.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable route propagation to the Virtual Private Gateway (VGW).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Open the port 80 in the security group of the Virtual Private Gateway (VGW).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the route table of all Instances using the ‘route’ command.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enter the IPv4 destination addresses for routing the traffic over VGW.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 67,
  "query" : "You are setting up a website for a small company.\nThis website serves up images and is very resource intensive.\nYou have decided to serve up the images using Cloudfront.\nThere is a requirement though the content should be served up using a custom domain and should work with https.",
  "answer" : "Answer - B and C.\nCustom SSL certificate support lets you deliver content over HTTPS using your own domain name and your own SSL certificate.\nThis gives visitors to your website the security benefits of CloudFront over an SSL connection that uses your own domain name in addition to lower latency and higher reliability.\nNote: Please note that some older browsers do not support SNI and will not be able to establish a connection with CloudFront to load the HTTPS version of your content.\nhttps://aws.amazon.com/cloudfront/custom-ssl-domains/\nOption C is correct.\nIf we want to use our own domain name, we need to use Amazon Route 53 to create an alias record that points to our CloudFront distribution.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html\nOption A is incorrect because a custom SSL certificate or third-party certificate can not be configured in Route53.\nOption D is incorrect because Origin Access identity(OAI) does not deal with custom SSL.\nIt is only used to ensure that the origin is accessible with CloudFront distribution only.\nMore information on Custom SSL Domains:\nAWS Cloudfront can use IAM certificates.\nReference Link:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-custom-certificate/\nAlso, there is a discussion forum on the same topic \"\"ssl certificate IAM\" in the Amazon CloudFront Discussion Forum\"\nIt is helpful in understanding this topic further.\nFor more information on CloudFront custom SSL domains, please visit the below URL-\nhttps://aws.amazon.com/cloudfront/custom-ssl-domains/\nSure, I'd be happy to explain each answer option in detail:\nA. You must provision and configure your own SSL certificate in Route 53 and associate it to your CloudFront distribution.\nThis option suggests that you would need to obtain your own SSL certificate and configure it to use with your CloudFront distribution. SSL certificates are used to secure communications between clients (such as web browsers) and servers (such as CloudFront). Route 53 is a DNS service offered by AWS, and it can be used to manage the domain name for your website.\nTo use this option, you would first need to obtain an SSL certificate from a certificate authority (CA) and upload it to AWS. Once you have the certificate, you can create a new CloudFront distribution or update an existing one to use the certificate. In addition, you would need to configure your Route 53 DNS settings to associate the custom domain name with the CloudFront distribution.\nB. You must provision Server Name Indication (SNI) Custom SSL for your CloudFront Distribution.\nThis option suggests that you should use Server Name Indication (SNI) to provide a custom SSL certificate for your CloudFront distribution. SNI is an extension to the TLS protocol (which is used to secure communications between clients and servers) that allows multiple SSL certificates to be used on a single IP address.\nTo use this option, you would need to obtain an SSL certificate and upload it to AWS. When creating or updating your CloudFront distribution, you can select the SNI Custom SSL option and choose the certificate you uploaded. You would also need to configure your Route 53 DNS settings to associate the custom domain name with the CloudFront distribution.\nC. You must provision and configure an ALIAS in Route 53 and associate it to your CloudFront distribution.\nThis option suggests that you should use an ALIAS record in Route 53 to associate your custom domain name with your CloudFront distribution. An ALIAS record is a special type of DNS record that can be used to map a domain name to another AWS resource, such as a CloudFront distribution.\nTo use this option, you would need to create an ALIAS record in Route 53 and point it to your CloudFront distribution. You would also need to ensure that your CloudFront distribution is configured to accept requests for the custom domain name.\nD. You must create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket where the images are stored.\nThis option suggests that you should use an Origin Access Identity (OAI) to restrict access to the objects in your S3 bucket. An OAI is a special CloudFront user that is used to access objects in an S3 bucket. By using an OAI, you can ensure that only requests from CloudFront are able to access the objects in your S3 bucket.\nTo use this option, you would need to create an OAI and grant it access to the objects in your S3 bucket. You would also need to update your CloudFront distribution to use the OAI when accessing the objects in your S3 bucket. Finally, you would need to ensure that your CloudFront distribution is configured to accept requests for the custom domain name and that the domain name is associated with the CloudFront distribution.\nIn conclusion, all of these options are valid ways to serve up images using CloudFront and a custom domain with HTTPS. The best option depends on your specific requirements and preferences.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You must provision and configure your own SSL certificate in Route 53 and associate it to your CloudFront distribution.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You must provision Server Name Indication (SNI) Custom SSL for your CloudFront Distribution.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You must provision and configure an ALIAS in Route 53 and associate it to your CloudFront distribution.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You must create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket where the images are stored.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 68,
  "query" : "Your company is developing a Serverless application with Lambda Proxy Integration in API Gateway.\nCaching is also used for the Rest APIs.\nYour company is worried about the security of the APIs.\nWhich of the following security considerations should you NOT recommend for your application? (Select TWO).",
  "answer" : "E.\nAnswer: B and E.\nOption A is Incorrect as CloudTrail helps in logging the user and API Activity.\nOption B is Correct because AWS Inspector is used for EC2 and cannot be used to inspect API Gateway.\nOption C is Incorrect because creating Private Rest APIs and using CloudWatch logs or Amazon Kinesis Data Firehose helps with the logging and security.\nOption D is Incorrect because implementing Least Privilege is the right security strategy.\nOption E is Correct because JWTs are used for restricting client access to your APIs and cannot be used to encrypt cache.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-jwt-authorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/security.html https://d1.awsstatic.com/whitepapers/AWS_Serverless_Multi-Tier_Architectures.pdf\nAs a language model, I don't have the ability to predict the exact questions or answer choices that might appear in the AWS Certified Solutions Architect - Professional exam, but I can provide you with an explanation of the security considerations for a Serverless application with Lambda Proxy Integration in API Gateway.\nA. Suggest setting up API and user activity logging with AWS CloudTrail: API and user activity logging with AWS CloudTrail is a recommended security consideration for any AWS application. CloudTrail provides a history of API calls made to AWS services, which can be used for auditing, compliance, and troubleshooting purposes. It helps to detect unauthorized access, changes to resources, and any other suspicious activity in your AWS account. Therefore, this is a recommended security consideration for your application.\nB. Suggest setting up AWS Inspector to perform security assessments on the Rest APIs: AWS Inspector is a security assessment service that helps you improve the security and compliance of your applications running on AWS. It automatically assesses applications for vulnerabilities and generates findings that include detailed remediation steps. Therefore, setting up AWS Inspector to perform security assessments on the Rest APIs is a recommended security consideration for your application.\nC. Suggest creating private Rest APIs and using CloudWatch Logs or Amazon Kinesis Data Firehose to log requests to your APIs: Creating private Rest APIs and using CloudWatch Logs or Amazon Kinesis Data Firehose to log requests to your APIs is also a recommended security consideration for your application. This helps to restrict access to your APIs to only authorized users or applications and provides visibility into who is accessing the APIs and when. It helps you to identify and investigate any suspicious activities, and also provides you with a trail of events that can be used for auditing purposes.\nD. Use IAM policies to implement least privilege access for creating, reading, updating, or deleting Rest APIs in API Gateway: Using IAM policies to implement least privilege access for creating, reading, updating, or deleting Rest APIs in API Gateway is a recommended security consideration for your application. It helps you to enforce the principle of least privilege and ensures that only authorized users or applications can make changes to your APIs. It also helps to reduce the risk of accidental or intentional misconfiguration or deletion of your APIs.\nE. Suggest using JSON Web Tokens (JWTs) for encrypting cache: JSON Web Tokens (JWTs) are a way of securely transmitting information between parties. They are commonly used for authentication and authorization purposes, but they are not typically used for encrypting cache. Therefore, this is not a recommended security consideration for your application.\nIn summary, the security considerations that you should NOT recommend for your application are:\nUsing JSON Web Tokens (JWTs) for encrypting cache. All the other security considerations mentioned above are recommended for a Serverless application with Lambda Proxy Integration in API Gateway.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Suggest setting up API and user activity logging with AWS CloudTrail.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Suggest setting up AWS Inspector to perform security assessments on the Rest APIs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Suggest creating private Rest APIs and using CloudWatch Logs or Amazon Kinesis Data Firehose to log requests to your APIs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use IAM policies to implement least privilege access for creating, reading, updating, or deleting Rest APIs in API Gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Suggest using JSON Web Tokens (JWTs) for encrypting cache.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 69,
  "query" : "Japan-based fintech company is running applications in AWS.\nThose are mission-critical applications, and they want to analyze the application logs using Amazon Redshift.\nThe applications forward the logs to a Kinesis Data Firehose.\nWhat do you suggest to send the records from Kinesis Data Firehose to Redshift?",
  "answer" : "Answer: A.\nOption A is Correct because a Kinesis Firehose Delivery Stream can directly configure Amazon Redshift as its destination.\nOption B is incorrect because LOAD is not a valid command to move the data to Amazon Redshift.\nOption C is incorrect because SQS cannot be used to send the data from Kinesis Firehose to Amazon Redshift.\nOption D is incorrect because Lambda cannot be used to send the data from Kinesis Firehose to Amazon Redshift.\nReference:\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-redshift\nFor analyzing application logs using Amazon Redshift, the data must first be ingested into the Redshift cluster. Since the applications are forwarding logs to Kinesis Data Firehose, we need to decide on the best method for sending data from Kinesis Data Firehose to Redshift.\nOption A: Directly configure Amazon Redshift as the destination of the Kinesis Data Firehose Delivery Stream. This option is not recommended because direct delivery to Redshift is not supported. Also, it is not a best practice to send data directly from Kinesis Data Firehose to Redshift as it can cause issues such as data inconsistency, data loss, and latency.\nOption B: Use Amazon S3 to store raw data and send the data to Amazon Redshift using Load Command. This option is recommended because it is a best practice to first store raw data in S3 before loading it into Redshift. Kinesis Data Firehose can be configured to store data in S3, and then Redshift can be loaded using the Redshift COPY command to load data from S3. This approach provides greater flexibility and scalability, allowing for multiple data sources to be loaded into Redshift using the same process.\nOption C: Directly send the data to Amazon Redshift using an SQS queue. This option is not recommended because Amazon Redshift does not support direct ingestion from SQS.\nOption D: Directly send the data to Amazon Redshift using a custom Lambda function. This option is not recommended because it requires more development effort to implement and maintain a custom Lambda function. Also, it can result in additional latency in the data pipeline.\nIn summary, the best approach for sending data from Kinesis Data Firehose to Redshift is to use Amazon S3 to store raw data and then load the data into Redshift using the COPY command.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Directly configure Amazon Redshift as the destination of the Kinesis Data Firehose Delivery Stream.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Amazon S3 to store raw data and send the data to Amazon Redshift using Load Command.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Directly send the data to Amazon Redshift using an SQS queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Directly send the data to Amazon Redshift using a custom Lambda fuction.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 70,
  "query" : "Your supervisor tells you of a client who needs a two-tier web application that is publicly accessible and configured on AWS.\nThe most important requirement is that MySQL must be used as the database, and it must be configured at the client's location in the most secure fashion.\nWhich of the following solutions would be the best to ensure that the client's requirements are met?",
  "answer" : "Answer - A.\nIn this scenario, the main architectural consideration is that the database should remain on the client's data center.\nSince the database should not be hosted on the cloud, you cannot put the database in the public subnet in AWS.\nOption A is correct because it puts the application servers in the public subnet ( because it should be publicly accessible ) and keeps the database server at the client's data center.\nThis is also a valid two-tiered architecture.\nOption B is INCORRECT because VPC peering cannot establish the connection between on-premises and AWS VPC.Option C is INCORRECT because the database cannot be on the public subnet.\nIt is rather located at the client's data center as per the question.\nOption D is INCORRECT because the database cannot be on the public subnet due to the posed security risk (against the question's requirements).\nThe best option is to create a VPN connection for securing traffic, as shown below.\nFor more information on VPN connections, please visit the below URL-\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\nThe best solution to meet the client's requirements for a two-tier web application using MySQL as the database on AWS would be option A. Building the application server on a public subnet and the database at the client's data center, and connecting them using a VPN connection that uses IPsec.\nOption A provides a secure solution for the client's requirement for a two-tier web application. By building the application server on a public subnet, it can be accessed from the internet. At the same time, the database is located in the client's data center, which ensures that the client has complete control over the database and its security.\nConnecting the application server and the database with a VPN connection that uses IPsec ensures that the connection between the two is encrypted, providing a secure connection between the two systems. Additionally, by building the application server on a public subnet, the client can configure the necessary security groups to allow traffic only from authorized sources.\nOption B, building the application server on a private subnet and the database at the client's data center, and connecting them with a VPC peering connection, does not meet the requirement of the database being publicly accessible. This option does not provide a public IP address to the database, and hence the database cannot be accessed from the internet.\nOption C, building the application server on a private subnet and the database on the public subnet with a NAT instance between them, is not the best solution for the client's requirement. This option involves using a NAT instance to enable the application server to access the internet, and hence the database. However, this introduces additional latency, and the NAT instance can become a single point of failure.\nOption D, building the application server on the private subnet and the database on the public subnet with a secure SSH connection to the public subnet from the client's data center, is not the best solution. This option involves using a secure SSH connection to access the public subnet, which introduces additional security risks. Additionally, the database is publicly accessible, which may not meet the client's security requirements.\nIn summary, option A, building the application server on a public subnet and the database at the client's data center, and connecting them with a VPN connection that uses IPsec, is the best solution to meet the client's requirement for a two-tier web application using MySQL as the database on AWS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Build the application server on a public subnet and the database at the client’s data center. Connect them with a VPN connection that uses IPsec.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Build the application server on the private subnet and the database at the client`s data center. Connect them with a VPC peering connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Build the application server on a private subnet and the database on the public subnet with a NAT instance between them.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Build the application server on the private subnet and the database on the public subnet with a secure SSH connection to the public subnet from the client`s data center.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 71,
  "query" : "Regarding encryption on data stored on your databases, namely Amazon RDS, which of the following statements is true? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nTip for the exam: You can encrypt your Amazon RDS instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instance (only certain EC2 instance types support encryption, more information is given below)\nData that is encrypted at rest includes the underlying storage for a DB instance, its automated backups, Read Replicas, and snapshots.\nOption A is incorrect because the RDS instance encryption can be done through the AWS console.\nOption B is CORRECT because once the encryption is enabled, its automated backups, read replicas, and snapshots are automatically encrypted without the need for any additional settings.\nOption C is incorrect because no additional configurations need to be made from the client side, once the encryption is enabled.\nOption D is incorrect because, as mentioned above, the snapshots get automatically encrypted once the encryption is turned-on on the RDS instance.\nFor more information on RDS encryption, please visit the below URL-\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\nSee the list of instance types that support the encryption-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Availability\nThe correct answer is B: Encryption can be enabled on RDS instances to encrypt the underlying storage. This will, by default, also encrypt snapshots as they are created. No additional configuration needs to be made on the client-side for this to work.\nAmazon RDS (Relational Database Service) is a managed database service provided by Amazon Web Services (AWS). It allows users to easily deploy, operate, and scale relational databases in the cloud. RDS supports various popular database engines, such as MySQL, PostgreSQL, Oracle, and SQL Server.\nEncryption is a security feature that protects sensitive data by converting it into an unreadable format that can only be deciphered with a secret key. AWS provides various encryption options for RDS to help customers protect their data at rest and in transit.\nTo enable encryption on RDS, users can choose to use either AWS-managed keys or customer-managed keys. AWS-managed keys are managed by AWS Key Management Service (KMS), a fully managed service that makes it easy to create and control encryption keys. Customer-managed keys, on the other hand, are managed by the user and can be imported into KMS for use with RDS.\nOption A is incorrect because encryption can be enabled on RDS instances using the AWS console. In fact, it is one of the easiest ways to enable encryption on RDS. When creating an RDS instance, users can select the option to enable encryption and choose the encryption key to use.\nOption C is incorrect because there is no additional configuration needed on the client-side for encryption to work. Once encryption is enabled on an RDS instance, it will automatically encrypt all data stored on the underlying storage, including backups and snapshots.\nOption D is incorrect because encryption can also be enabled on snapshots as they are created. By default, RDS will encrypt snapshots using the same encryption key used for the underlying storage. Users can also choose to use a different encryption key for snapshots.\nIn summary, encryption can be enabled on RDS instances to encrypt the underlying storage, and this will automatically encrypt backups and snapshots. Users can choose to use either AWS-managed or customer-managed keys. There is no additional configuration needed on the client-side for encryption to work.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Encryption cannot be enabled on RDS instances using the AWS console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Encryption can be enabled on RDS instances to encrypt the underlying storage. This will, by default, also encrypt snapshots as they are created. No additional configuration needs to be made on the client-side for this to work.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Encryption can be enabled on RDS instances to encrypt the underlying storage, and this will by default also encrypt snapshots as they are created. However, some additional configuration needs to be made on the client-side for this to work.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Encryption can be enabled on RDS instances to encrypt the underlying storage, but you cannot encrypt snapshots as they are created.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 72,
  "query" : "Your company has built a workload management solution for a deepwater drilling company.\nThe application sends the workload detail before each shift starts.\nDue to the recent change in the regulations, there has to be a worklog file that needs to be sent to each Shift Manager at the time of shift start.\nAs per the requirements, the file can only be accessed by the targeted user and it should only be allowed to access within a given shift timeline and needs to be password protected as well.\nBecause there is a very short deadline, the management has tasked you to come up with a reliable, secure and cost-effective solution.",
  "answer" : "Correct Answer: B.\nOption A is INCORRECT because bucket policy and static web hosting will not be enough to cater to all the requirements like restricted time-based access.\nOption B is CORRECT because Workdocs supports features to share restricted, time-based files.\nAnd with the downloadable clients, it would be easy for the users to access those files.\nOptions C and D are INCORRECT.\nAlthough it is possible to build a solution with the given requirements, it would be a waste of resources to build a custom application that can already be served with AWS Workdocs.\nAmong the given options, the best solution for this scenario would be to upload the files to S3 and develop a custom application that can be hosted on S3. AWS Cognito should be used to allow the restricted, time-based access.\nExplanation: Option A is not a good choice because while S3 allows you to store and control access to your data, it is not a good fit for hosting websites or web applications, especially ones that require custom authentication and authorization. Furthermore, using the S3 website hosting feature would expose the data to the internet, which is not recommended for this use case.\nOption B is also not the best choice because it requires manual sharing of files and setting expiration dates and passwords on the link. This approach is not scalable, and it can be time-consuming, especially if there are many shift managers to share files with.\nOption C is not a good choice either because EBS is designed for block-level storage for EC2 instances, and it is not ideal for file storage and sharing. Furthermore, building a custom application on EC2 for sharing files can be complex and time-consuming, especially if the application needs to be secure and scalable.\nOption D is the best choice because it leverages the strengths of S3 for storing files, Cognito for user authentication and authorization, and the ability to develop a custom application to meet the specific needs of the use case. The custom application can be built using AWS Lambda and API Gateway, which are serverless services that can scale automatically and have built-in security features. The application can also be configured to only allow access during the specific shift timeline, ensuring that the data is protected and available only to authorized users.\nIn conclusion, Option D is the best solution for this scenario because it provides a scalable, secure, and cost-effective solution for storing, sharing, and controlling access to the worklog files for the shift managers.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use S3 to store the files inside individual folders for shift managers in a single bucket. Enable the S3 Website Hosting and control the access via bucket policy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon WorkDocs to share files with shift managers via email. Set the expiration date and password on the link.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Upload the files to EBS and build an application which can run on EC2 to share the files with AWS Simple Email Service.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Upload the files to S3 and develop a custom application which can be hosted on S3. Use the AWS Cognito to allow the restricted, time-based access.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 73,
  "query" : "Your security officer has told you that you need to tighten up the logging of all events that occur on your AWS account.\nHe wants to be able to access all events that occur on the account across all regions quickly and in the simplest possible manner.\nHe also wants to make sure that he is the only person who can access these events in the most secure way possible.\nWhich of the following would be the best solution to assure his requirements are met? Choose the correct answer from the options below.",
  "answer" : "Answer - A.\nThe main points to consider in this scenario is: (1)the security officer needs to access all events that occur on the account across all the regions, and (2) only that security officer should have the access.\nOption A is CORRECT because it configures only one S3 bucket for all the CloudTrail log events on the account across all the regions.\nIt also restricts access to the security officer only via the bucket policy.\nSee the images below:\nOption B is incorrect because it uses Amazon Glacier vaults, an archival solution and is not used to store the CloudTrail logs.\nOption C is incorrect because sending the API calls to CloudWatch is unnecessary.\nAlso, notifying the security officer via email is not a good nor a secure architecture.\nOption D is incorrect because CloudTrail provides an option where are all the logs get delivered to a single S3 bucket.\nPutting all the logs in separate buckets is an overhead .\nMore information on AWS CloudTrail.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure.\nCloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.\nThis history simplifies security analysis, resource change tracking, and troubleshooting.\nYou can design CloudTrail to send all logs to a central S3 bucket.\nFor more information on CloudTrail, please visit the below URL-\nhttps://aws.amazon.com/cloudtrail/\nThe best solution to meet the security officer's requirements is option A: Use CloudTrail to log all events to one S3 bucket. Make this S3 bucket only accessible by your security officer with a bucket policy that restricts access to his user only and adds MFA to the policy for a further security level.\nCloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It provides a history of all events that occur within an AWS account, including API calls made by users, AWS Management Console sign-ins, and AWS Management Console actions. By default, CloudTrail logs events in the region where the event occurred.\nOption A proposes to use one S3 bucket to store all CloudTrail logs, making it easier for the security officer to access all the events that occur in the account across all regions in a single location. By using a single bucket, it is easier to manage and secure the logs since you do not have to manage multiple buckets.\nTo ensure that only the security officer can access the logs, a bucket policy that restricts access to the security officer's user should be applied. MFA should also be added to the policy to provide an additional layer of security. With MFA, the security officer would have to provide an additional authentication factor in addition to their password to access the S3 bucket, making it difficult for unauthorized access.\nOption B proposes to log all events to an Amazon Glacier Vault, which is a low-cost storage option for long-term data retention. However, Glacier is not optimized for fast access, and accessing logs may take some time. Additionally, restricting access to the vault by IP address is not a secure method since IP addresses can be spoofed or changed, which may lead to unauthorized access.\nOption C proposes to send all API calls to CloudWatch and send an email to the security officer every time an API call is made. While this option may provide an email notification to the security officer for every event, it may generate a large volume of emails, making it difficult to manage. Additionally, email may not be a secure method of communication, and there is a risk that the logs may be intercepted by unauthorized users.\nOption D proposes to log all events to separate S3 buckets in each region, which may lead to more significant overhead in managing the logs, as there would be a need to manage multiple buckets. Additionally, since CloudTrail cannot write to a bucket in a different region, it may not provide the security officer with a complete view of all events that occur across all regions in the account.\nIn summary, Option A is the best solution to meet the security officer's requirements, as it provides a centralized and secure location for storing logs, with fine-grained access controls to ensure only the security officer can access the logs, and MFA for added security.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use CloudTrail to log all events to one S3 bucket. Make this S3 bucket only accessible by your security officer with a bucket policy that restricts access to his user only and adds MFA to the policy for a further security level.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use CloudTrail to log all events to an Amazon Glacier Vault. Make sure the vault access policy only grants access to the security officer`s IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudTrail to send all API calls to CloudWatch and send an email to the security officer every time an API call is made. Make sure the emails are encrypted.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudTrail to log all events to a separate S3 bucket in each region as CloudTrail cannot write to a bucket in a different region. Use MFA and bucket policies on all the different buckets.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 74,
  "query" : "You have created a temporary application that accepts image uploads, stores them in S3, and records information about the image in RDS.\nAfter building this architecture and accepting images for the duration required, it's time to delete the CloudFormation template.\nHowever, your manager has informed you that the RDS data needs to be stored and the S3 bucket with the images needs to remain for archival reasons.\nYour manager has also instructed you to ensure that the application can be restored by a CloudFormation template and run next year during the same period.",
  "answer" : "Answer - D.\nThe main points in this scenario are: even if the CloudFormation stack is deleted,(1) the RDS data needs to be stored, and (2) the S3 bucket with the images should remain (not be deleted).\nOption A is incorrect because even if the images are backed up to another bucket, the original bucket would be deleted if the CloudFormation stack is deleted.\nOne of the requirements is to retain the S3 bucket.\nOption B is incorrect because the DeletionPolicy attribute for RDS should be snapshot, not retain.\nAfter all, with snapshot option, the backup of the RDS instance would be stored in the form of snapshots (which is the requirement)\nWith retain option, CF will keep the RDS instance alive which is unwanted.\nOption C is incorrect because the DeletionPolicy of the S3 bucket should be retain, not snapshot.\nOption D is CORRECT because it correctly sets the DeletionPolicy of retain on the S3 bucket and snapshot on the RDS instance.\nMore information on DeletionPolicy on CloudFormation.\nDeletionPolicy options include:\nRetain: You retain the resource in the event of a stack deletion.\nSnapshot: You get a snapshot of the resource before it's deleted.\nThis option is available only for resources that support snapshots.\nDelete: You delete the resource along with the stack.\nThis is the default outcome if you don't set a DeletionPolicy.\nAWS Document says:\nTo keep or copy resources when you delete a stack, you can specify either the Retain or Snapshot policy options.\nWith the DeletionPolicy attribute, you can preserve or (in some cases) backup a resource when its stack is deleted.\nYou specify a DeletionPolicy attribute for each resource that you want to control.\nIf a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.\nTo keep a resource when its stack is deleted, specify Retain for that resource.\nYou can use retain for any resource.\nFor example, you can retain a nested stack, Amazon S3 bucket, or EC2 instance so that you can continue to use or modify those resources after you delete their stacks.\nNote.\nIf you want to modify resources outside of AWS CloudFormation, use a retain policy and then delete the stack.\nOtherwise, your resources might get out of sync with your AWS CloudFormation template and cause stack errors.\nFor resources that support snapshots, such as AWS::EC2::Volume, specify Snapshot to have AWS CloudFormation create a snapshot before deleting the resource.\nFor more information on Cloudformation deletion policy, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\nThe correct answer to this scenario is D. Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, set the RDS resource declaration DeletionPolicy to snapshot.\nExplanation:\nThe requirement is to delete the CloudFormation template but retain the S3 bucket with images and RDS data for archival purposes, and ensure that the application can be restored by a CloudFormation template and run next year during the same period. This means that we need to retain the S3 bucket and RDS data while deleting the CloudFormation stack.\nOption A is incorrect because S3 bucket replication is not needed since the requirement is to keep the existing S3 bucket with images. Also, setting the deletion policy of the RDS instance to snapshot is not enough to retain the data; we need to ensure that the RDS instance is not deleted with the CloudFormation stack.\nOption B is incorrect because setting the DeletionPolicy to Retain for both the S3 and RDS resource types would not allow the deletion of the CloudFormation stack, which is required.\nOption C is incorrect because setting the DeletionPolicy on the S3 resource to snapshot and the DeletionPolicy on the RDS resource to snapshot is not a valid option. Snapshot is not a valid value for DeletionPolicy.\nOption D is the correct answer. We need to set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, which will ensure that the S3 bucket with images is not deleted with the CloudFormation stack. For the RDS resource declaration, we need to set the DeletionPolicy to snapshot, which will create a snapshot of the RDS instance before deletion, allowing us to restore it later.\nTherefore, by choosing option D, we can retain the S3 bucket with images and RDS data while ensuring that the CloudFormation stack can be deleted and the application can be restored by a CloudFormation template next year during the same period.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects, set the deletion policy for the RDS instance to snapshot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to Retain.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set the DeletionPolicy on the S3 resource to snapshot and the DeletionPolicy on the RDS resource to snapshot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, set the RDS resource declaration DeletionPolicy to snapshot.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 75,
  "query" : "A software team builds up a feature that needs a new RESTful endpoint that returns greetings to customers.\nThe endpoint contains several path variables and query string parameters.\nThis HTTP endpoint is supposed to be hit millions of times per month.\nHowever, the hit rate may change dramatically.\nThe team decides to use API gateway/lambda to save some cost.\nThe team needs to deploy the feature quickly.\nHowever, the team members have little experience with lambda.\nWhich below options CAN NOT help the team? Select 2.",
  "answer" : "Correct Answer - C, D.\nThe feature development is under time pressure.\nSo we should find the options that make things unnecessarily complicated.\nOne thing to note is that this question asks for the options that CAN NOT help.\nTo build an API with Lambda integrations, either Lambda proxy integration or Lambda custom integration can be used.\n(https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html):\nIn Lambda proxy integration, the input to the integrated Lambda function can be expressed as any combination of request headers, path variables, query string parameters, and body.\nIn addition, the Lambda function can use API configuration settings to influence its execution logic.\nFor an API developer, setting up a Lambda proxy integration is simple.\nOther than choosing a particular Lambda function in a given region, you have little else to do.\nAPI Gateway configures the integration request and integration response for you.\nIn Lambda custom integration, you must ensure that input to the Lambda function is supplied as the integration request payload.\nThis implies that you, as an API developer, must map any input data the client supplied as request parameters into the proper integration request body.\nYou may also need to translate the client-supplied request body into a format recognized by the Lambda function.\nAccording to the above, the simpler way is to use Lambda proxy integration.\nThe below is an example when creating a resource in API gateway:\nOption A is incorrect: Because the “microservice-http-endpoint” blueprint can help build the whole system quickly, which is also straightforward to use.\nRefer to https://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https-example-configure-event-source_1.html on how to use an API gateway/Lambda blueprint.\nOption B is incorrect: Because Lambda proxy integration is simpler for API to integrate with Lambda.\nOption C is CORRECT: Because Lambda custom integration has introduced unnecessary complicity.\nConsidering the limited time, it is not a proper way and brings less help on the project.\nOption D is CORRECT: Although lambda authorizer can help with the authorization, it is not required in this case.\nIt has added extra effort to the project and is not a necessary step either.\nOption A and D are the options that cannot help the team.\nOption A suggests using a blueprint in the Lambda console to create a lambda-microservice under the selected API. While this is a quick way to create a RESTful endpoint, it may not be the best option for the team since the endpoint is expected to be hit millions of times per month. Using a blueprint may not offer the scalability and customization required to handle the expected traffic.\nOption B suggests building an API gateway with a proxy resource for a Lambda function. This is a valid option and can be a quick way to deploy a lambda function through the API gateway. This can be done by selecting the “Configure as proxy resource” option when creating an API resource. The team can then use API Gateway to define the RESTful endpoint and proxy the requests to the Lambda function. This is a valid and scalable option.\nOption C suggests using the API Gateway console to build an API that enables a client to call Lambda functions through the Lambda custom integration. This option is also valid and can be used to quickly deploy Lambda functions through the API Gateway. The user can choose a particular Lambda function in a given region, and the API Gateway console will take care of the integration. This is a valid and scalable option.\nOption D suggests implementing a lambda authorizer for the API gateway to grant permissions. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML, which secures access to the API gateway and back-end lambda. While this is a valid security option, it does not help the team with their primary concern of quickly deploying the feature using Lambda and API Gateway.\nIn summary, options B and C are valid options for the team to quickly deploy the feature using Lambda and API Gateway. Options A and D may not be the best options for the team given the expected hit rate and their limited experience with Lambda.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the Lambda console, choose a blueprint such as “microservice-http-endpoint” to create a lambda-microservice under the selected API.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Build an API gateway with a proxy resource for a Lambda function. This can be done by selecting the “Configure as proxy resource” option when creating an API resource.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the API Gateway console to build an API that enables a client to call Lambda functions through the Lambda custom integration. Lambda custom integration is very simple to use, and the user has little else to do except choosing a particular Lambda function in a given region.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement a lambda authorizer for the API gateway to grant permissions. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML, which secures access to the API gateway and back-end lambda.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 76,
  "query" : "You are launching your first ElastiCache cache cluster and start using Memcached.\nWhich of your following requirement is NOT supported by Memcached? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nOption B is CORRECT because it is Redis, not Memcached, which supports advanced/complexdata types such as strings, hashes, lists, sets, sorted sets, and bitmaps.\nOptions A, C and D are all incorrect because these are the main features of Memcached.\nFor the exam, it is very important to remember the differences between Memcached and Redis.\nBoth are excellent solutions, but used for different scenarios.\nPlease see the notes given below by the AWS documentation:\nChoose Memcached if the following apply to your situation:\nYou need the simplest model possible.\nYou need to run large nodes with multiple cores or threads.\nYou need the ability to scale out/in, adding and removing nodes as demand on your system increases and decreases.\nYou need to cache objects, such as a database.\nChoose Redis 2.8.x or Redis 3.2.4 (non-clustered mode) if the following apply to your situation:\nYou need complex data types, such as strings, hashes, lists, sets, sorted sets, and bitmaps.\nYou need to sort or rank in-memory data-sets.\nYou need persistence of your key store.\nYou need to replicate your data from the primary to one or more read replicas for read-intensive applications.\nYou need automatic failover if your primary node fails.\nYou need publish and subscribe (pub/sub) capabilities-to inform clients about events on the server.\nYou need backup and restore capabilities.\nYou need to support multiple databases.\nFor more information on the various caching engines, please visit the below URL-\nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/SelectEngine.Uses.html\nThe correct answer is B. \"You can use more advanced data types, such as lists, hashes, and sets.\"\nMemcached is an in-memory key-value store that is designed to cache small, frequently accessed data sets. It is a simple caching system that is easy to set up and use, making it a popular choice for caching in many applications. However, it has some limitations.\nA. Memcached supports horizontal scaling, which means you can add more cache nodes to the cluster as your traffic increases. This allows you to handle more traffic without overloading a single node.\nB. Memcached only supports simple data types such as strings and integers. It does not support more advanced data types such as lists, hashes, and sets.\nC. Memcached has a simple caching model, where you set a key-value pair and retrieve it later. This makes it easy to use and understand.\nD. Memcached is primarily designed for object caching, which means it stores the result of expensive database queries or other expensive computations. This allows you to offload these computations from your database and serve the results from the cache, which can improve the performance of your application.\nIn conclusion, while Memcached is a simple and effective caching system, it has some limitations. It does not support more advanced data types such as lists, hashes, and sets.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Your ability to scale your cache horizontally as you grow.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use more advanced data types, such as lists, hashes, and sets.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Your need for as simple a caching model as possible.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Object caching is your primary goal to offload your database.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 77,
  "query" : "A new client may use your company to move some of their existing Data Center applications and infrastructure to AWS.\nYou need to provide an initial scope to this possible new client.\nOne of the things you notice concerning the existing infrastructure is that it has a few legacy applications that you are almost certain will not work on AWS.\nWhich of the following would be the best strategy to employ regarding the migration of these legacy applications? Choose the correct answer from the options below.",
  "answer" : "Answer - C.\nOption A is incorrect because there is some legacy application that will not work on the AWS platform.\nSo creating VPC for such applications will not be possible.\nOption B is incorrect because the scenario explicitly mentions that there are some components of the application (legacy part) that will not work with AWS.\nSo, it is highly presumptuous that the legacy application can be run by an AWS Machine Image (legacy application may consist of more than just AMIs).\nOption C is CORRECT because it uses a hybrid approach - where the legacy application stays on-premises.\nIt should definitely work as the remaining infrastructure would be on AWS.\nThe communication between the two infrastructures would be taken care of by establishing a VPN connection.\nThis is certainly the most viable, time, and cost-saving solution among the given options.\nOption D is incorrect because it is the least feasible solution.\nFirst of all, de-commissioning the legacy application may not be possible for the client, especially when the scenario says that the legacy application will almost surely not work on AWS.\nStill, even if they agree, it would be a big impact on the client in terms of time, cost, and efforts to re-architect the solution to replace the legacy application.\nMore information on the hybrid setup:\nThe best option is to have a dual-mode wherein you have the legacy apps running on-premise and start migrating the apps which have compatibility in the cloud.\nHave a VPN connection from the on-premise to the cloud for ensuring communication can happen from each environment to the other.\nFor the full fundamentals of AWS networking options, please visit the URL-\nhttps://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-one/\nThe best strategy to employ when faced with legacy applications that may not work on AWS when migrating a client's existing Data Center applications and infrastructure to AWS would be to create a hybrid cloud by configuring a VPN tunnel to the on-premises location of the Data Center.\nOption A, creating two VPCs, is not an ideal solution because it may create unnecessary complexity, increase management overhead, and reduce the flexibility of the overall solution. Additionally, VPC peering has its limitations, such as maximum bandwidth and distance limitations. It's better to have a single VPC to manage and have a consistent security posture.\nOption B, moving the legacy applications onto AWS first before building any infrastructure, may not be a viable option because there is no guarantee that there will be an AWS Machine Image that can run the legacy application. Additionally, this option is not efficient because it creates extra work, as it requires the creation of infrastructure after moving the legacy applications to AWS.\nOption D, convincing the client to look for another solution by decommissioning these applications and seeking new ones that will run on AWS, is not an ideal solution because it may not be possible or feasible for the client to replace these legacy applications. Additionally, it may be more expensive for the client to purchase new applications that can run on AWS compared to migrating the existing ones.\nTherefore, creating a hybrid cloud by configuring a VPN tunnel to the on-premises location of the Data Center is the best option because it provides a secure, reliable, and scalable solution that can accommodate both the legacy applications and the other applications that can be migrated to AWS. This approach also allows the client to gradually migrate their applications and infrastructure to AWS at their own pace, minimizing the risk of disruption to their operations. The VPN tunnel will provide secure connectivity between the on-premises data center and the AWS environment, allowing applications and data to be accessed securely and easily from both locations.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create two VPCs. One containing all the legacy applications and the other containing all the other applications. Make a connection between them with VPC peering.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Move the legacy applications onto AWS first before you build any infrastructure. There is sure to be an AWS Machine Image that can run this legacy application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a hybrid cloud by configuring a VPN tunnel to the on-premises location of the Data Center.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Convince the client to look for another solution by de-commissioning these applications and seeking new ones that will run on AWS.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 78,
  "query" : "A very big company has provided financial consulting services to end-users.\nIt uses the traditional MySQL database inside EC2 t2.medium instances which mainly deal with legacy services.\nAs a business grows, read contention is becoming more and more frequent for the database.\nThe AWS architect suggests using Aurora to scale as it is much cheaper and provides great read scalability.\nHow would you use Aurora to scale up and ease the read contention issue? (Select THREE.)",
  "answer" : "E.\nCorrect Answer - B, C, E.\nAmazon Aurora can be used with the MySQL DB instance to take advantage of the read scaling capabilities of Amazon Aurora and expand the read workload for the MySQL DB instance.\nTo use Aurora to read scale the MySQL DB instance, create an Amazon Aurora MySQL DB cluster and make it a replication slave of your MySQL DB instance.\nThis applies to an Amazon RDS MySQL DB instance, or a MySQL database running external to Amazon RDS.\nPlease refer to https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf.\nOne thing to note is that Aurora has the concept of DB cluster and can scale up to 15 read replicas.\nThe below is a screenshot of when DB cluster identifier is defined:\nOption A is incorrect: Because it is unnecessary to use db.r5.16xlarge which is not cost-efficient.\nAlso, Multi-AZ deployment is for availability instead of scalability.\nOption B is CORRECT: Because an Amazon Aurora MySQL DB cluster needs to be set up first so that the source MySQL database will use its endpoint.\nOption C is CORRECT: Because when replication starts, it is important to monitor if the data is successfully synchronized.\nThis is also stated in https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf.\nOption D is incorrect: Aurora can scale from the MySQL instances outside of RDS MySQL.\nOption E is CORRECT: The same reason as options.\nD.\nRefer to “Start Replication Between an External Master Instance and a MySQL DB Instance on Amazon RDS” in https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf page 575.\nThe correct answers are A, B, and C.\nAurora is a MySQL-compatible database that is built for the cloud. It is designed to be highly scalable and available, with automatic failover, and read scalability.\nTo use Aurora to scale up and ease the read contention issue, the following steps should be taken:\nA. Create an Amazon Aurora MySQL DB db.r5.16xlarge instance. Make sure that Multi-AZ deployment is enabled in different zones. Enable Aurora global tables.\nThis step involves creating an Aurora MySQL DB cluster with a large instance type to support the high read traffic. Multi-AZ deployment should be enabled to provide automatic failover in the event of a primary instance failure. Aurora global tables should also be enabled to enable read scaling across multiple regions.\nB. Create an Amazon Aurora MySQL DB cluster. The Aurora DB cluster endpoint address will be used when referenced by the MySQL source instances to scale up.\nThis step involves creating an Aurora MySQL DB cluster that will act as the read replica for the MySQL database. The Aurora DB cluster endpoint address will be used when referenced by the MySQL source instances to scale up. The read replica will be used to offload the read traffic from the MySQL database, reducing the read contention issue.\nC. Set up replication between a MySQL DB instance and an Amazon Aurora MySQL DB cluster. Monitor the replication to ensure that it remains healthy and repair it if necessary.\nThis step involves setting up replication between the MySQL database and the Aurora MySQL DB cluster. The replication should be monitored to ensure that it remains healthy, and any issues should be repaired promptly.\nD. As Aurora does not support MySQL database running external to Amazon RDS, migrate the database from EC2 MySQL to RDS MySQL first and then use Aurora to scale the database.\nThis option is incorrect because Aurora supports MySQL-compatible databases, including those running outside of Amazon RDS.\nE. Make the Aurora DB a replication slave of MySQL DB instances. This applies to the Amazon RDS MySQL DB instance, or a MySQL database running in EC2.\nThis option is incorrect because the Aurora DB cluster should be the read replica for the MySQL database, not the other way around.\nIn summary, to use Aurora to scale up and ease the read contention issue, a large Aurora MySQL DB instance should be created with Multi-AZ deployment enabled, Aurora global tables should be enabled, an Aurora MySQL DB cluster should be created to act as the read replica, and replication should be set up between the MySQL database and the Aurora MySQL DB cluster.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an Amazon Aurora MySQL DB db.r5.16xlarge instance. Make sure that Multi-AZ deployment is enabled in different zones. Enable Aurora global tables.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an Amazon Aurora MySQL DB cluster. The Aurora DB cluster endpoint address will be used when referenced by the MySQL source instances to scale up.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up replication between a MySQL DB instance and an Amazon Aurora MySQL DB cluster. Monitor the replication to ensure that it remains healthy and repair it if necessary.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "As Aurora does not support MySQL database running external to Amazon RDS, migrate the database from EC2 MySQL to RDS MySQL first and then use Aurora to scale the database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Make the Aurora DB a replication slave of MySQL DB instances. This applies to the Amazon RDS MySQL DB instance, or a MySQL database running in EC2.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 79,
  "query" : "You have been given the task of designing a backup strategy for your organization's on-premise storage with the only caveat being that you must use the AWS Storage Gateway.\nThere is no requirement for the file protocol.\nWhich of the following is the correct/appropriate statement surrounding the most cost-effective storage strategy? Choose the correct answer from the options below.",
  "answer" : "Answer - A.\nOption A is CORRECT becasue the VTL is most cost-effective storage option here when compared to Gateway-Cached or the Gateway-Stored Volumes while the user is looking for a backup storage for the on-premises data.\nOption B is incorrect because this is NOT a cost-effective option it keeps only the frequently accessed data (not the entire data) on the on-premises server to get quick access.\nOption C is incorrect because although both Gateway-Cached Volumes and Gateway-Stored Volume can be independently deployed as storage/backup options these are not as cost-effective as the VTL.\nOption D is incorrect because this is also NOT a cost-effective option for storage of data.\nMore information on AWS Storage Gateway.\nVolume Gateway - A volume gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI) devices from your on-premises application servers.\nThe gateway supports the following volume configurations:\nCached volumes - You store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally.\nCached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises.\nYou also retain low-latency access to your frequently accessed data.\nStored volumes - If you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally.\nThen asynchronously back up point-in-time snapshots of this data to Amazon S3\nThis configuration provides durable and inexpensive off-site backups that you can recover to your local data center or Amazon EC2\nFor example, if you need replacement capacity for disaster recovery, you can recover the backups to Amazon EC2.\nTape Gateway - With a tape gateway, you can cost-effectively and durably archive backup data in Amazon Glacier.\nA tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.\nFor more information on Storage gateways, please visit the below URLs-\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html https://aws.amazon.com/storagegateway/faqs/\nThe correct answer for this question is A. You should use Gateway-Virtual Tape Library (VTL) as it is the most cost-efficient.\nAWS Storage Gateway is a hybrid cloud storage service that enables on-premises applications to use cloud storage. It provides three types of storage interfaces: file, volume, and tape. The file interface allows the use of the Amazon S3 object storage service, while the volume and tape interfaces enable the use of Amazon EBS and Amazon S3 Glacier, respectively.\nIn this scenario, the requirement is to use AWS Storage Gateway for designing the backup strategy for on-premise storage, without specifying any preference for the file protocol. Therefore, we can use either volume or tape interfaces for backup storage.\nGateway-Cached Volumes and Gateway-Stored Volumes are both volume interfaces of AWS Storage Gateway. Gateway-Cached Volumes store frequently accessed data on-premises, while storing all data backups on Amazon S3, and offer faster access to the frequently accessed data. Gateway-Stored Volumes, on the other hand, store all data backups on-premises, providing low-latency access to data.\nGateway-Virtual Tape Library (VTL) is a tape interface of AWS Storage Gateway that enables on-premises applications to use Amazon S3 Glacier and S3 Glacier Deep Archive as a backup target. It emulates a physical tape library with a virtual tape changer and virtual tape drives, allowing customers to store backup data in Amazon S3 Glacier or S3 Glacier Deep Archive using their existing backup workflows.\nFor the given scenario, the requirement is for a cost-effective storage strategy. Gateway-Virtual Tape Library (VTL) is the most cost-efficient option for backup storage since it allows the use of Amazon S3 Glacier or S3 Glacier Deep Archive, which are low-cost storage options for data backups. In contrast, Gateway-Cached Volumes and Gateway-Stored Volumes use Amazon S3, which is a higher-cost storage option for data backups.\nTherefore, the correct answer is A. You should use Gateway-Virtual Tape Library (VTL) as it is the most cost-efficient option for backup storage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You should use Gateway-Virtual Tape Library (VTL) as it is the most cost-efficient.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You should use Gateway-Cached Volumes. You will have quicker access to the data, and it is a more preferred storage solution than Gateway-Stored Volumes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It doesn`t matter whether you use Gateway-Cached Volumes or Gateway-Stored Volumes as long as you also combine either of these solutions with the Gateway-Virtual Tape Library (VTL).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You should use Gateway-Stored Volumes as it is preferable to Gateway-Cached Volumes as a storage medium.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 80,
  "query" : "Your company has an e-commerce platform that is expanding all over the globe.\nYou have EC2 instances deployed in multiple regions.\nYou want to monitor the performance of all of these EC2 instances.\nHow will you set up CloudWatch to monitor EC2 instances in multiple regions?",
  "answer" : "Answer - C.\nYou can monitor AWS resources in multiple regions using a single CloudWatch dashboard.\nFor example, you can create a dashboard that shows CPU utilization for an EC2 instance located in the us-west-2 region with your billing metrics, which are located in the us-east-1 region.\nPlease see the following snapshot which shows how a global CloudWatch Dashboard looks.\nFor more information on the Cloudwatch dashboard, please refer to the below URLs-\nNov 8 2019 new anouncement:https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/\nCross-region cross-account console:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Cross-Account-Cross-Region.html.\nCross-Account Cross-Region Dashboards with Amazon CloudWatch: https://aws.amazon.com/blogs/aws/cross-account-cross-region-dashboards-with-amazon-cloudwatch/\nTo monitor the performance of EC2 instances in multiple regions, you can set up CloudWatch in the following ways:\n1.\nRegister instances running on different regions to CloudWatch: You can register your EC2 instances to CloudWatch to monitor their performance. By doing this, you can collect and analyze metrics from all of your EC2 instances in different regions from a single console. You can also use CloudWatch alarms to alert you when certain thresholds are breached. However, to register your instances with CloudWatch, you need to ensure that you have installed the CloudWatch agent on your EC2 instances. This agent collects metrics from the operating system and applications on the instance and sends them to CloudWatch.\n2.\nCreate separate dashboards in every region: You can create separate dashboards in each region to monitor the performance of EC2 instances in that region. This approach can help you to focus on the specific performance metrics of instances in a particular region. However, this approach can lead to dashboard sprawl and make it difficult to correlate data across regions.\n3.\nHave one single dashboard that reports the metrics from CloudWatch pertaining to different regions: You can create a single dashboard that reports the metrics from CloudWatch pertaining to different regions. This approach can help you to get an overall picture of the performance of your EC2 instances across multiple regions. However, you need to ensure that you have selected the correct regions while creating the dashboard to avoid confusion.\n4.\nThis is not possible: This answer is not correct since CloudWatch allows you to monitor EC2 instances in multiple regions through the above-mentioned methods.\nTherefore, the correct answer is B - Register instances running on different regions to CloudWatch.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create separate dashboards in every region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Register instances running on different regions to CloudWatch.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Have one single dashboard that reports the metrics from CloudWatch pertaining to different regions.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "This is not possible.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 81,
  "query" : "What does the below custom IAM Policy achieve?",
  "answer" : "Answer - A.\nOption A is CORRECT because, although the policy given in the question allows the access to launch the EC2 instance by including \"ec2:RunInstances\" in the Actions, it will not allow the user to launch the EC2 instances.\n(Try creating the same policy, attach it to a new user.\nYou can log in using that user credentials and see if you can launch an EC2 instance.\nYou will not be able to do so.\nYou will get the error shown below.)\nIn order to allow users to launch an instance, the policy needs to be updated to grant the user more privileges: access to launch using an EC2 key pair, a security group, an Elastic Block Store (EBS) volume, and an Amazon Machine Image (AMI)\nTo do this, you will have to create a separate statement for the RunInstances action.\nOption B is incorrect because, as mentioned above, the user will not be able to launch an EC2 instance and will get an error (shown below) about not having the permission to do so.\nOption C is incorrect because the user can start, stop, and terminate existing instances with this policy.\nOption D is incorrect because the user will be able to start, stop and terminate existing EC2 instances.\nFor more information on EC2 resource-level permissions, please visit the below URL and for further explanation as to why only the TerminateInstances, StopInstances, and StartInstances actions are allowed, please visit the below URL-\nhttps://aws.amazon.com/blogs/security/demystifying-ec2-resource-level-permissions/\nThe provided IAM policy allows us to control access to various AWS resources and actions for users and groups within our AWS account.\nThe policy in question contains four actions that are allowed for EC2 instances: StartInstances, StopInstances, TerminateInstances, and DescribeInstances.\nHere is an explanation of each answer choice:\nA. This answer choice is incorrect because it only allows the user to perform actions on existing instances but doesn't allow the user to launch a new instance.\nB. This answer choice is correct because it allows the user to perform all actions on EC2 instances, including launching a new instance as well as starting, stopping, and terminating existing instances.\nC. This answer choice is incorrect because it only allows the user to describe instances but doesn't allow the user to perform any other actions on EC2 instances.\nD. This answer choice is incorrect because one of the answer choices is correct.\nTherefore, the correct answer is B, which permits the user to launch a new instance as well as start, stop, and terminate the existing instances.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Permits the user start, stop, terminate and describe the existing instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Permits the user to launch a new instance as well as start, stop, and terminate the existing instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Permits the user only to describe the instances (read-only), and will not be able to start, stop, or terminate instances, since it overrides the allowed actions of TerminateInstances, RunInstances, StartInstances, and StopInstances in the policy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "None of the above.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 82,
  "query" : "A company is running a production load Redshift cluster for a client.\nThe client has an RTO objective of one hour and an RPO of one day.\nWhile configuring the initial cluster, what configuration would best meet the client's recovery needs for this specific Redshift cluster configuration? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nOption A is incorrect because it copies the snapshot from the destination region (disaster recovery region).\nOption B is CORRECT because it copies the snapshot from the source region (production) to the destination region (disaster recovery region).\nOption C is incorrect because you do not need to copy the manual snapshots (as it is an overhead)\nRedshift copies the snapshot from source to destination region.\nOption D is incorrect because Redshift replicates the data to another region using snapshots.\nOnce the snapshot is copied from the source to the destination region, you need to restore the cluster from the snapshot and use its DSN.\nMore information on Amazon Redshift Snapshots.\nSnapshots are point-in-time backups of a cluster.\nThere are two types of snapshots: automated and manual.\nAmazon Redshift stores these snapshots internally in Amazon S3 using an encrypted Secure Sockets Layer (SSL) connection.\nIf you need to restore from a snapshot, Amazon Redshift creates a new cluster and imports data from the snapshot that you specify.\nWhen you restore from a snapshot, Amazon Redshift creates a new cluster and makes the new cluster available before all of the data is loaded, so you can begin querying the new cluster immediately.\nThe cluster streams data on demand from the snapshot in response to the active queries then loads the remaining data in the background.\nAmazon Redshift periodically takes snapshots and tracks incremental changes to the cluster since the last snapshot.\nAmazon Redshift retains all of the data required to restore a cluster from a snapshot.\nFor more information on RedShift snapshots, please visit the below URL-\nhttp://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\nTo meet the recovery needs of the client for the Redshift cluster, it is essential to ensure that the data is available for recovery in the event of a disaster. In this scenario, the client has an RTO objective of one hour and an RPO of one day. RTO stands for Recovery Time Objective, which is the maximum acceptable downtime for the system, and RPO stands for Recovery Point Objective, which is the maximum amount of data loss that is acceptable.\nAmong the given options, the best configuration that meets the client's recovery needs for this specific Redshift cluster configuration is option B, which is to enable automatic snapshots and configure automatic snapshot copy from the current production cluster to the disaster recovery region.\nEnabling automatic snapshots will ensure that the data in the Redshift cluster is backed up regularly, and in the event of a disaster, the cluster can be restored to a recent point in time using the latest snapshot. Configuring automatic snapshot copy to the disaster recovery region will ensure that the snapshots are available in the secondary region and can be launched in the event of a disaster.\nThis configuration meets the client's RPO objective of one day as automatic snapshots are taken regularly, and data loss would be limited to the period between the last snapshot and the time of the disaster. Additionally, the RTO objective of one hour can be met by launching the snapshot in the disaster recovery region and redirecting traffic to the secondary region.\nOption A is not the best configuration as it only copies snapshots to the disaster recovery region and does not configure automatic snapshot copy. This means that if a disaster occurs, the snapshots will need to be manually copied from the primary region to the secondary region, which may result in additional downtime.\nOption C is also not the best configuration as it requires manual copying of the snapshot to the secondary region in the event of a disaster, which may result in additional downtime and potentially impact the RTO objective.\nOption D involves creating a secondary cluster and replicating data from the primary cluster to the secondary cluster. While this provides an additional layer of redundancy, it is not necessary to meet the client's recovery needs as it can be accomplished by configuring automatic snapshot copy to the disaster recovery region. Additionally, this option may be more complex and costly to implement.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable automatic snapshots on the cluster in the production region FROM the disaster recovery region. So snapshots are available in the disaster recovery region and can be launched in the event of a disaster.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable automatic snapshots and configure automatic snapshot copy from the current production cluster to the disaster recovery region.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Enable automatic snapshots on a Redshift cluster. In the event of a disaster, a failover to the backup region is needed. Manually copy the snapshot from the primary region to the secondary region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create the cluster configuration and enable Redshift replication from the cluster running in the primary region to the cluster running in the secondary region. In the event of a disaster, change the DNS endpoint to the secondary cluster’s leader node.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 83,
  "query" : "A company runs its current application that has a million views per day, entirely on-premises.\nHowever, they expect a big boost in traffic and need to figure out a way to decrease the load to handle the scale.\nUnfortunately, they cannot migrate their application to AWS.\nWhat could they do with their current on-premises application to help offload some of the traffic and scale to meet the demand cost-effectively? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nThe main point to consider is that the application should entirely stay on the on-premises server but still leverage AWS offerings for handling the peak traffic and scale on demand.\nCloudFront is the best suited for such a situation because it can use the on-premises server as the custom origin.\nOption A is incorrect because even though OpsWork can work with on-premises servers, setting up the EC2 instances with Auto Scaling would be a costly solution.\nOption B is incorrect because moving static files to S3 is not sufficient ( since the application has a million views per day ) to improve the scalability to handle the peak load.\nHow to handle the dynamic contents is not mentioned in this option.\nOption C is incorrect because the requirement explicitly mentions that the application cannot be migrated to AWS.\nOption D is CORRECT because CloudFront - which is an AWS managed - is a highly available, scalable service that can use the on-premises server as the origin.\nBy enabling the \"Query String Forwarding\" with a value of \"None\" will increase the likelihood that CloudFront can serve a request from the cache, which improves performance and reduces the load on your origin.\nAmazon CloudFront can speed up the distribution of both the static and dynamic web content as it delivers the content through a worldwide network (edge locations).\nRefer to page 52 on the below link-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AmazonCloudFront_DevGuide.pdf\nThe correct answer is D. Create a CloudFront CDN and enable query string forwarding. Offload the DNS to AWS to handle CloudFront CDN traffic but use on-premise load balancers as the origin.\nExplanation: Since the company cannot migrate their application to AWS, they will have to find other ways to decrease the load on their on-premises infrastructure. The key challenge is to handle the expected increase in traffic cost-effectively. One way to do this is to use Amazon CloudFront, a content delivery network (CDN) service provided by AWS.\nCDNs help offload traffic by caching content at edge locations around the world, closer to the end-user, reducing the load on the origin infrastructure. CloudFront can be used to cache and serve both static and dynamic content. In this scenario, we can use CloudFront to cache the static content of the application, such as images, videos, and CSS files, and serve them from edge locations closer to the end-users.\nTo implement this solution, the company should first create a CloudFront distribution and enable query string forwarding. Query string forwarding allows CloudFront to cache and serve different versions of the same URL, based on query parameters. This is particularly useful for dynamic content that generates different responses based on query parameters, such as search results or user preferences.\nNext, the company should offload the DNS to AWS by transferring their domain name to Amazon Route 53, AWS's DNS service. Route 53 offers a range of routing policies, including weighted routing, which allows traffic to be split between different endpoints based on weight values. In this scenario, the company can configure a weighted-based DNS routing policy to send a portion of the traffic to CloudFront and the rest to their on-premises infrastructure.\nFinally, the company can use their on-premises load balancers as the origin for CloudFront. This means that CloudFront will fetch the dynamic content from the load balancers, which can distribute the load across the on-premises infrastructure.\nDeploying OpsWorks on-premise (option A) can help automate the management and configuration of the on-premises infrastructure, but it will not offload traffic or help scale the infrastructure cost-effectively.\nUploading static files to S3 and creating a CloudFront distribution serving those static files (option B) can offload traffic for static content, but it will not help scale the dynamic content of the application.\nDuplicating half of the web infrastructure on AWS and configuring weighted-based DNS routing (option C) can help offload traffic, but it will require duplicating the infrastructure, which may not be cost-effective or feasible for the company.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy OpsWorks on-premise to manage the instance to configure on-premise auto-scaling to meet the demand.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Upload all static files to Amazon S3 and create a CloudFront distribution serving those static files.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Duplicate half your web infrastructure on AWS, offload the DNS to Route 53 and configure weighted-based DNS routing to send half the traffic to AWS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CloudFront CDN and enable query string forwarding. Offload the DNS to AWS to handle CloudFront CDN traffic but use on-premise load balancers as the origin.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 84,
  "query" : "Like the benefits Lambda has, Amazon Aurora has provided an on-demand, autoscaling serverless configuration, which automatically starts up, shuts down, and scales up or down capacity based on the application's needs.\nAurora Serverless provides a relatively simple, cost-effective option.\nWhich below scenarios are suitable for Aurora serverless to be used? Select 3 Options.",
  "answer" : "E.\nCorrect Answer - A, B, D.\nAurora Serverless provides the following advantages.\nSimpler - Aurora Serverless removes much of the complexity of managing DB instances and capacity.\nScalable - Aurora Serverless seamlessly scales compute and memory capacity as needed, with no disruption to client connections.\nCost-effective - When you use Aurora Serverless, you pay for only the database resources that you consume on a per-second basis.\nHighly available storage - Aurora Serverless uses the same fault-tolerant, distributed storage system with six-way replication as Aurora to protect against data loss.\nFor details about Aurora Serverless, please refer to page 100 in.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf\nTo create an Aurora serverless, make sure the Capacity type is selected as Serverless:\nOption A is CORRECT: Because it is a new application, and the load is unknown.\nIt is suitable for Aurora Serverless since it is hard to provision an instance size.\nOption B is CORRECT: Because Aurora Serverless is suitable for development and test databases.\nWith Aurora Serverless, the database automatically shuts down when it's not in use.\nIt can save costs without impacting the service.\nOption C is incorrect: Because Aurora Serverless does not support MariaDB.\nOnly MySQL is supported in production.\nRefer to “Limitations of Aurora Serverless” in.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf.\nNote: Aurora Serverless is also available in preview for the Aurora PostgreSQL-compatible edition since Nov 2018.\nOption D is CORRECT: Because Aurora Serverless is suitable for unpredictable workloads.\nThe database auto scales its capacity to meet the needs of the application's peak load and scales back down when the surge of activity is over.\nOption E is incorrect: One limitation for Aurora Serverless is that you can't access an Aurora Serverless DB cluster's endpoint through an AWS VPN connection or an inter-region VPC peering connection.\nAlso, the workload is stable so that a provisioned capacity works for this case.\nNote:\nAs of now, the WRT exam options are good to go for Amazon Aurora Serverless.\nhttps://aws.amazon.com/rds/aurora/serverless/\nAurora Serverless is a configuration of Amazon Aurora relational database engine that automatically starts, stops, and scales capacity on demand based on application needs, making it a cost-effective option. The suitability of Aurora Serverless for a particular scenario depends on factors such as workload patterns, resource requirements, and cost considerations. Let's examine the given scenarios and determine which ones are suitable for Aurora Serverless:\nA. Startup company B is deploying a new application for an online trading system, which needs a database to store customers' transactions. It is a totally new application; therefore, the team is still unsure what the load may look like at first.\nThis scenario is suitable for Aurora Serverless since the startup company does not know what the initial workload will be like, and Aurora Serverless can automatically scale up or down based on the application's needs. It provides a cost-effective option for the startup since they don't need to provision a fixed amount of capacity upfront.\nB. For an internal project, a developer needs to use a database during work hours but does not need it on nights or weekends. He decides to use Aurora to save some costs as required by the team lead.\nThis scenario is also suitable for Aurora Serverless since the developer only needs the database during work hours, and Aurora Serverless can automatically shut down the database when not in use, saving on costs. However, since the workload pattern is predictable, it may be more cost-effective to use an instance-based Aurora configuration that provides a fixed amount of capacity.\nC. A middle-size company C is considering migrating its legacy on-premise MariaDB database to AWS RDS. The database has a dramatically higher workload on weekends than weekdays.\nThis scenario is not suitable for Aurora Serverless since the workload pattern is predictable, with higher usage on weekends. It may be more cost-effective to use an instance-based Aurora configuration that provides a fixed amount of capacity to handle the expected peak workload.\nD. A development team runs an IOT monitor system where there is database usage throughout the day and peaks of activity that are hard to predict. When the peaks happen, the total activities may reach 10 times of the normal level.\nThis scenario is suitable for Aurora Serverless since the workload pattern is unpredictable, with peaks of activity that are hard to predict. Aurora Serverless can automatically scale up or down based on the application's needs, making it a cost-effective option.\nE. A company has EC2 applications in the Sydney region. Due to the market increase in Singapore, it decides to add an RDS database on the Singapore region. Because of some security considerations, AWS VPN is needed for the database to talk with several on-premise applications. The workload is expected to be high and stable.\nThis scenario is not suitable for Aurora Serverless since the workload is expected to be high and stable, and Aurora Serverless is optimized for unpredictable workloads that require automatic scaling. It may be more cost-effective to use an instance-based Aurora configuration that provides a fixed amount of capacity to handle the expected workload.\nIn summary, the scenarios suitable for Aurora Serverless are those with unpredictable workload patterns that require automatic scaling to save on costs. For predictable workload patterns, instance-based Aurora configurations may be more cost-effective.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Startup company B is deploying a new application for an online trading system, which needs a database to store customers’ transactions. It is a totally new application; therefore, the team is still unsure what the load may look like at first.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "For an internal project, a developer needs to use a database during work hours but does not need it on nights or weekends. He decides to use Aurora to save some costs as required by the team lead.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A middle-size company C is considering migrating its legacy on-premise MariaDB database to AWS RDS. The database has a dramatically higher workload on weekends than weekdays.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A development team runs an IOT monitor system where there is database usage throughout the day and peaks of activity that are hard to predict. When the peaks happen, the total activities may reach 10 times of the normal level.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A company has EC2 applications in the Sydney region. Due to the market increase in Singapore, it decides to add an RDS database on the Singapore region. Because of some security considerations, AWS VPN is needed for the database to talk with several on-premise applications. The workload is expected to be high and stable.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 85,
  "query" : "A company has a Redshift cluster for petabyte-scale data warehousing.\nThe data within the cluster is easily reproducible from additional data stored on Amazon S3\nThe company wants to reduce the overall cost of running this Redshift cluster.\nWhich scenario would meet best for the needs of the running cluster? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nSnapshots are point-in-time backups of a cluster.\nThere are two types of snapshots: automated and manual.\nAmazon Redshift stores these snapshots internally in Amazon S3 using an encrypted Secure Sockets Layer (SSL) connection.\nIf you need to restore from a snapshot, Amazon Redshift creates a new cluster and imports data from the snapshot that you specify.\nOption A is incorrect because AWS Redshift does not have the concept of read replica.\nOption B is CORRECT because\n\"There is no additional charge for backup storage up to 100% of your provisioned storage for an active data warehouse cluster\"\nTherefore, if we reduce to a 1-day retention period for the backup, we can save costs too.\nRefer link: https://aws.amazon.com/redshift/pricing/\nOption C is incorrect because implementing daily backup is going to be more expensive than option.\nB.Option D is incorrect because we cannot get disabled manual snapshots.\nIn this scenario, we need to reconfigure the automated snapshots.\nFor more information on Redshift snapshots, please visit the below URL-\nhttp://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\nThe correct answer for reducing the overall cost of running a Redshift cluster is Option B: Enable automated snapshots but set the retention period to a lower number to reduce storage costs.\nExplanation: Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. It allows you to store and analyze large amounts of data using SQL queries. Redshift provides various features like backups, snapshots, and replication to ensure high availability, durability, and disaster recovery.\nIn this scenario, the company wants to reduce the overall cost of running the Redshift cluster. The data within the cluster is easily reproducible from additional data stored on Amazon S3. Therefore, the company can use automated snapshots to reduce storage costs.\nOption A, to disable automated backups and create a read-replica in another region for disaster recovery, is not a cost-effective solution. Read replicas can be used to improve performance, but they do not reduce storage costs. Moreover, disabling automated backups can put data at risk.\nOption C, to implement daily backups but do not enable multi-region copy to save data transfer costs, is not a recommended solution. Multi-region copy is important for disaster recovery and business continuity. It ensures that your data is available even if there is a disruption in the primary region.\nOption D, to disable manual snapshots on the cluster, is not recommended because manual snapshots can be used to capture a point-in-time backup of the cluster. This is important for disaster recovery and to protect against accidental data loss.\nTherefore, the best solution to reduce the overall cost of running the Redshift cluster is to enable automated snapshots but set the retention period to a lower number to reduce storage costs. This will ensure that the data is protected against accidental deletion or corruption while also reducing storage costs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Disable automated backups. Create a read-replica in another region for disaster recovery.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable automated snapshots but set the retention period to a lower number to reduce storage costs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement daily backups but do not enable multi-region copy to save data transfer costs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Disable manual snapshots on the cluster.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 86,
  "query" : "A company is running a web application that has a high amount of dynamic content.\nThe application has MySQL as its database.\nThe company is looking to implement a caching solution for clients requesting the application.\nWhat is the best possible solution and why?",
  "answer" : "Answer - B.\nThe scenario requires a caching solution which should help to reduce the load time of the dynamic content.\nElastiCache is the proper caching solution for this use case.\nOption A is INCORRECT because Route 53 helps improving the resolving of the DNS queries for the multi-region application.\nIt does not help caching of the dynamic content.\nOption B is CORRECT because ElastiCache is most suited for caching dynamic content.\nThis will make sure the read-intensive load is reduced on the database instance.\nOption C is INCORRECT because setting the TTL to 0 in CloudFront does not cache content and is not suited for caching dynamic content.\nOption D is INCORRECT because it is the same answer as above, except query string forwarding is disabled.\nThis would block the dynamic content generated at the origin server using the query string parameters.\nhttps://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/\nThe best solution for implementing a caching solution for a web application that has a high amount of dynamic content and is using MySQL as its database is to create an ElastiCache cluster and place it in front of the RDS dynamic content.\nOption A is not a suitable solution for implementing a caching solution for clients requesting the application as it is related to DNS resolution and not caching.\nOption C and D suggest creating a CloudFront distribution with a TTL of 0 and keeping TCP connections live from CloudFront to the origin, reducing the time it takes for a TCP handshake to occur. However, these options are not the best fit for a web application with high amounts of dynamic content since CloudFront caches content based on the object's cache key, which is either the URL without query parameters (if query string forwarding is disabled) or the URL with query parameters (if query string forwarding is enabled). Since the web application has a high amount of dynamic content, the URL would change frequently, which makes it less suitable for caching with CloudFront.\nTherefore, the best solution is option B, which involves creating an ElastiCache cluster, writing code that caches the correct dynamic content, and placing it in front of the RDS dynamic content. ElastiCache is a fully managed in-memory data store service that is compatible with popular open-source in-memory data stores like Redis and Memcached. By placing the cache in front of the RDS dynamic content, it will reduce the amount of time it takes to request the dynamic content since it is cached. This solution is ideal for web applications that have high amounts of dynamic content, as it can cache frequently requested data, reducing the load on the database, and improving application performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Offload the DNS to Route 53; Route 53 has DNS servers worldwide and routes the request to the closest region which reduces DNS latency.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an ElastiCache cluster, write code that caches the correct dynamic content and places it in front of the RDS dynamic content. This will reduce the amount of time it takes to request the dynamic content since it is cached.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution, enable query string forwarding and set the TTL to 0. This will keep TCP connections live from CloudFront to the origin, reducing the time it takes for a TCP handshake to occur.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution, disable query string forwarding and set the TTL to 0. This will keep TCP connections live from CloudFront to the origin, reducing the time it takes for a TCP handshake to occur.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 87,
  "query" : "You are running an online gaming server, with one of its requirements being a need for a high IOPS of write performance on its EBS volumes.\nGiven the fact that each EBS volume cannot provision the required IOPS, which of the following would be a reasonable solution if instance bandwidth is not an issue? Choose the correct answer from the options below.",
  "answer" : "Answer - A.\nOption A is CORRECT because creating a RAID 0 array allows you to achieve a higher performance level for a file system than you can provision on a single Amazon EBS volume, and the resulting size of a RAID 0 array is the sum of the sizes of the volumes within it.\nThe bandwidth is the sum of the available bandwidth of the volumes within it.\nOption B is incorrect because ephemeral storage may not always have consistent and reliable I/O performance given by PIOPS EBS Volumes.\nOption C is incorrect because (a) instance bandwidth is not an issue, and (b) auto-scaling with spot instances will not increase the IOPS of the EBS volumes.\nOption D is incorrect because launching the instances in a placement group does not increase the IOPS of the EBS volumes.\nIt only increases the overall network performance.\nMore information on EBS with RAID Configuration.\nWith Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as the operating system for your instance supports that particular RAID configuration.\nThis is because all RAID is accomplished at the software level.\nFor greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together.\nAn example of better throughout with RAID 0 configuration is also given in the AWS documentation.\nThe resulting size of a RAID 0 array is the sum of the sizes of the volumes within it, and the bandwidth is the sum of the available bandwidth of the volumes within it.\nThe resulting size and bandwidth of a RAID 1 array are equal to the size and bandwidth of the volumes in the array.\nFor example, two 500 GiB Amazon EBS io1 volumes with 4,000 provisioned IOPS each will create a 1000 GiB RAID 0 array with an available bandwidth of 8,000 IOPS and 1,000 MB/s of throughput or a 500 GiB RAID 1 array with an available bandwidth of 4,000 IOPS and 500 MB/s of throughput.\nFor more information on RAID configuration, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\nThe correct answer to the given scenario is A. Create a RAID 0 configuration with several EBS volumes.\nExplanation:\nEBS volumes are used as the primary storage device for EC2 instances in AWS. EBS volumes can deliver high-performance storage for workloads that require high IOPS. However, there are limits to the amount of IOPS that a single EBS volume can provision.\nTo achieve high IOPS write performance, one of the solutions is to create a RAID 0 configuration with several EBS volumes. RAID 0 is a disk configuration that uses multiple disks in parallel to achieve high performance. When you create a RAID 0 configuration with several EBS volumes, the IOPS performance of the volumes is added together, providing high IOPS write performance for your application.\nUsing ephemeral storage (option B) can be an alternative for storing data that can be easily recreated in case of failure or can be replicated across multiple instances. However, ephemeral storage is not a suitable option for storing critical data as it is volatile and can be lost if the instance is terminated.\nAuto Scaling (option C) can be used to scale your application horizontally by adding or removing EC2 instances based on demand. However, Auto Scaling does not provide a solution for achieving high IOPS write performance.\nCreating a Placement Group (option D) is used to deploy EC2 instances in a single Availability Zone to achieve low-latency network performance. Placement Groups do not provide a solution for achieving high IOPS write performance on EBS volumes.\nTherefore, the correct solution to achieve high IOPS write performance is to create a RAID 0 configuration with several EBS volumes.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a RAID 0 configuration with several EBS volumes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use ephemeral storage which gives a much larger IOPS write performance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Auto Scaling to use spot instances when required to increase the IOPS write performance when required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Placement Group with several EBS volumes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 88,
  "query" : "While implementing cost-cutting measurements in your organization, you have been told that you need to migrate some of your existing resources to another region.\nThe first task you have been given is to copy all of your Amazon Machine Images from Asia Pacific (Sydney) to US West (Oregon)\nOne of the things that you are unsure of is how the SSH keys on your Amazon Machine Images need to be migrated.\nWhich of the following best describes how your SSH keys are affected when AMIs are migrated between regions? Choose the correct answer from the options below.",
  "answer" : "Answer - D.\nOption A is incorrect because, as mentioned above, the SSH Keys are private keys and are never copied across the regions.\nOption B is incorrect because the SSH keys are not user-specific.\nOption C is incorrect because the authorization keys are copied across the region.\nOption D is CORRECT because the authorization key is included in the AMI, hence copied across the region.\nHowever, the SSH keys are not copied and need to be imported explicitly if you still want to use the same SSH key for the instances in the new region.\nSee the AWS Console option for importing the SSH key.\nFor more information on EC2 key pairs, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\nFor more information on this subject, please visit the below forums on AWS-\nhttps://forums.aws.amazon.com/thread.jspa?threadID=52654\nNote:\nYou specify the name of the key pair when you launch an EC2 instance and provide the private key when you connect to that instance.\nAuthorization Key here is the public key content (of the key pair) placed in an entry within ~/.ssh/authorized_keys of that EC2 instance.\nThis gets copied as part of the AMI.\nWhen you migrate Amazon Machine Images (AMIs) from one region to another, there are several factors that you need to consider, one of which is how the SSH keys are affected. SSH (Secure Shell) is a network protocol used for secure communication between two computers, allowing you to connect to and manage your EC2 instances remotely.\nThe correct answer to the question is D. The SSH keys will not be copied to the new region, but the authorization keys will still be in the operating system of the AMI. This means that when you launch the new EC2 instances in the US West (Oregon) region using the migrated AMIs, the authorization keys will still be present in the operating system, but the SSH keys themselves will not be copied over to the new region.\nTo connect to the new EC2 instances using SSH, you will need to import the SSH keys to the new region. This can be done using a few different methods, including:\n1.\nManually copying the SSH keys: You can manually copy the SSH keys from the old region to the new region by downloading them from the old EC2 instances and then uploading them to the new instances.\n2.\nUsing AWS System Manager: AWS System Manager is a service that allows you to manage EC2 instances remotely, and it includes a feature called Session Manager that allows you to connect to EC2 instances without needing to use SSH keys. This means that you can use Session Manager to connect to your old EC2 instances in the Sydney region, and then use it to copy the SSH keys to the new instances in the US West (Oregon) region.\n3.\nUsing AWS DataSync: AWS DataSync is a service that allows you to transfer large amounts of data between different storage systems, including EC2 instances. You can use AWS DataSync to transfer the SSH keys from the old region to the new region.\nIt's also worth noting that if you need new users to access the migrated instances in the new region, you will need to generate new SSH keys for them. The authorization keys will still be present in the operating system of the AMI, but new users will not have access to the existing SSH keys.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The SSH keys will also be copied across, so you don`t need to do anything except launch the new instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The SSH keys will also be copied across. However, they will only work for users who have already accessed them in the old region. If you need new users to access the instances, then new keys will need to be generated.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Neither the SSH key nor the authorized key is copied and consequently you need to create new keys when you launch the new instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The SSH keys will not be copied to the new region but the authorization keys will still be in the operating system of the AMI. You need to import the SSH keys to the new region if you want to launch the new EC2 instances with the same SSH keys.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 89,
  "query" : "You're working as a consultant for a company that has a three-tier application.\nThis architecture's application layer sends over 20Gbps of data per second during peak hours to and from Amazon S3\nCurrently, you're running two NAT gateways in two subnets to transfer the data from your private application layer to Amazon S3\nYou will also need to ensure that the instances receive software patches from a third-party repository.\nWhat architecture changes should be made, if any? Choose the correct answer from the options below.",
  "answer" : "Answer - B.\nVPC Endpoints for Amazon S3 are easy to configure, highly reliable, and provide a secure connection to S3 that does not require a gateway or NAT instances.\nThe EC2 instances running in private subnets of a VPC can now have controlled access to S3 buckets, objects, and API functions in the same region as the VPC.\nYou can use an S3 bucket policy to indicate which VPCs and VPC Endpoints have access to your S3 buckets.\nOption A is incorrect because adding a third NAT Gateway for communicating with an S3 bucket is a costly solution compared to creating an S3 endpoint.\nOption B is CORRECT because (a) you can securely connect with S3 via the S3 endpoint, and (b) even though you can connect to the S3 endpoint without requiring a NAT gateway, you still need to keep it because the instances in the VPC needs to receive the software patches from the third-party repository.\nSee the image in the More information on VPC Endpoint for S3 section.\nOption C is incorrect because you need to connect to the Amazon S3 via the VPC endpoint as the current NAT gateways may not be sufficient to handle the peak load.\nOption D is incorrect because if you remove the NAT Gateway, the instances in the VPC will not be able to receive the software patches from the third-party repository.\nMore information on VPC Endpoint for S3\nVPC endpoints alleviate the need for everything to go through theNAT instance.\nNew VPC Endpoint for S3\nToday we are simplifying access to S3 resources from within a VPC by introducing the concept of a VPC Endpoint.\nThese endpoints are easy to configure, highly reliable, and provide a secure connection to S3 that does not require a gateway or NAT instances.\nEC2 instances running in private subnets of a VPC can now have controlled access to S3 buckets, objects, and API functions in the same region as the VPC.\nYou can use an S3 bucket policy to indicate which VPCs and which VPC Endpoints have access to your S3 buckets.\nFor more information on VPC endpoints, please refer to the below URL-\nhttps://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/\nThe correct answer is B: Create a VPC S3 endpoint which allows for higher bandwidth, throughput as well as tighter security and keep the NAT gateways to receive the software patches from the third party repository.\nExplanation: The architecture currently has two NAT gateways in two subnets to transfer the data from the private application layer to Amazon S3. However, during peak hours, the application layer sends over 20Gbps of data per second, which exceeds the network performance of the NAT gateway that supports only 10 Gbps. This means that the current architecture is not able to handle the data traffic effectively, and it may cause performance issues and delays.\nTo address this problem, we need to consider an architecture change that can support higher network performance and throughput. The recommended approach is to create a VPC S3 endpoint that allows the application layer to directly access S3 without having to go through a NAT gateway. The VPC S3 endpoint provides a highly available, scalable, and secure connection to Amazon S3 that can support up to 100 Gbps of data transfer per second.\nBy using a VPC S3 endpoint, we can eliminate the network bottleneck caused by the NAT gateway and significantly improve the application's performance. Additionally, using a VPC S3 endpoint provides tighter security as it removes the need for the application layer to have access to the internet to access S3.\nHowever, we need to keep the NAT gateways to receive software patches from a third-party repository. This is because the VPC S3 endpoint does not provide access to the internet, and we need a way to receive software patches from a third-party repository. The NAT gateways can be used to receive these patches and then distribute them to the application layer.\nTherefore, the correct answer is B: Create a VPC S3 endpoint which allows for higher bandwidth, throughput as well as tighter security and keep the NAT gateways to receive the software patches from the third party repository. This solution provides better performance, security, and reliability for the three-tier application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "NAT gateways support network performance of 10 Gbps and two of them are running: Add a third NAT Gateway to a separate subnet to allow for any increase in demand.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a VPC S3 endpoint which allows for higher bandwidth, throughput as well as tighter security and keep the NAT gateways to receive the software patches from the third party repository.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "NAT gateways support 10Gbps and two are running: No changes are required to improve this architecture.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Remove the NAT gateways and create a VPC S3 endpoint which allows for higher bandwidth throughput as well as tighter security.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 90,
  "query" : "BCJC is running Oracle DB workloads on AWS.\nCurrently, they are running the Oracle RAC (Real Application Cluster) configuration on the AWS public cloud.\nYou've been tasked with configuring snapshots on the RAC cluster to enable durability.\nWhat is the best method for configuring snapshots to become backups eventually?",
  "answer" : "Answer - C.\nCurrently, Oracle Real Application Cluster (RAC) is not supported as per the AWS documentation.\nHowever, you can deploy scalable RAC on Amazon EC2 using the recently-published tutorial and Amazon Machine Images (AMI)\nSo, to create the snapshots (for an eventual backup), you need to save a point-in-time view of the EC2 that is deployed for RAC in the form of a snapshot and create a script that runs snapshots against the EBS volumes, and run it against the snapshot.\nOptions A, B, and D are all incorrect because RDS does not support Oracle RAC.Option C is CORRECT because Oracle RAC is supported via the deployment using Amazon EC2\nHence, for the data backup, you can create a script that takes the snapshots of the EBS volumes.\nFor more information on Oracle RAC on AWS, please visit the below URLs-\nhttps://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/ https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/ https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/\nOption D is the correct answer for configuring snapshots on the RAC cluster to enable durability.\nExplanation:\nOracle RAC (Real Application Cluster) is a popular database clustering technology used to improve scalability, availability, and performance. It is a complex technology that involves multiple nodes and shared storage. To configure snapshots on the RAC cluster, we need to consider the following:\n1.\nData Durability: We need to ensure that the data is safe and can be recovered in the event of failure or disaster.\n2.\nRPO (Recovery Point Objective): It is the maximum acceptable data loss in the event of a failure. The lower the RPO, the better.\n3.\nRTO (Recovery Time Objective): It is the maximum acceptable downtime in the event of a failure. The lower the RTO, the better.\nOption A is not a recommended method for configuring snapshots. Manual snapshots require human intervention and are prone to errors. We need an automated method to ensure data durability and reduce RPO and RTO.\nOption B is not relevant to configuring snapshots. Multi-AZ failover is a feature of Amazon RDS that provides high availability and automatic failover capability. It does not address the snapshot configuration.\nOption C is not a recommended method for configuring snapshots. Creating snapshots against EBS volumes is a possible method for backing up data, but it does not take into account the complexities of the RAC cluster, such as shared storage.\nOption D is the recommended method for configuring snapshots on the RAC cluster. Enabling automated backups on the RDS RAC cluster provides a consistent and reliable backup mechanism. It also enables the auto snapshot copy feature to copy backups to a backup region, reducing RPO and RTO. With this configuration, backups are taken automatically at a defined interval, ensuring data durability and reducing the risk of data loss in the event of a failure or disaster.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create manual snapshots of the RDS backup and write a script that runs the manual snapshot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable Multi-AZ failover on the RDS RAC cluster to reduce the RPO and RTO in the event of disaster or failure.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a script that runs snapshots against the EBS volumes to create backups.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Enable automated backups on the RDS RAC cluster; enable auto snapshot copy to a backup region to reduce RPO and RTO.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 91,
  "query" : "You have multiple EC2 instances in three availability zones (AZs), with a load balancer configured for your application.\nAny instance can process the requests and there is no need to route the requests for a particular session to the same EC2 instance.\nHowever, you observe that only one of those AZs is receiving all the traffic.\nWhich of the following actions may help distribute the traffic in three AZs? (Select TWO).",
  "answer" : "Answer - A, C.\nSince the traffic is routed to only one availability zone (AZ) and none of the other AZs are receiving any, the ELB must have only one AZ registered in it.\nFirst, you have to ensure that the ELB is configured to support multiple AZs via Cross-Zone load balancing.\nEven after enabling the cross zone load balancing, if the traffic is routed to particular EC2 instances in an AZ, the users' sessions must have tied to those EC2 instances.\nThese symptoms seem to be related to the sticky sessions (session affinity)\nSo, the second thing you must ensure that the sticky sessions need to be either disabled or configured to be expiring after a specific period.\nOption A is CORRECT because, as mentioned above, sticky sessions could be a reason for traffic being routed to specific EC2 instances in a specific AZ.\nOption B is incorrect because reducing the health check frequency will not balance the traffic between different AZs.\nOption C is CORRECT because cross zone load balancing needs to be enabled on the ELB and the other AZs must be registered under this ELB.Option D is incorrect because there is no such recommendation from Amazon about ELB.More information on ELB, Sticky Sessions, and Cross Zone Load Balancing:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html\nThe issue described in this question suggests that the Elastic Load Balancer (ELB) is not distributing traffic evenly across all three Availability Zones (AZs) where your EC2 instances are deployed. To fix this, you should consider the following two options:\n1.\nEnable Cross-Zone Load Balancing: Enabling Cross-Zone Load Balancing allows the ELB to distribute traffic evenly across all the registered instances, regardless of the AZ they belong to. This means that traffic from any client will be distributed across all the instances in all the AZs that are registered with the ELB. To enable this feature, you should go to your Load Balancer's configuration and select the option \"Enable Cross-Zone Load Balancing\". This should help distribute the traffic more evenly across all the AZs.\n2.\nDisable Sticky Sessions: Sticky sessions is a feature of the ELB that allows the same client to be directed to the same instance in order to maintain session state. If you have disabled sticky sessions, the ELB will distribute traffic evenly across all instances regardless of their state. If you're not using sticky sessions and your traffic is still being directed to a single AZ, it's possible that some instances are not healthy or are underutilized, which is causing the ELB to direct traffic to the instances in the healthy AZ. To disable this feature, you should go to your Load Balancer's configuration and select the option \"Disable Sticky Sessions\".\nReducing the frequency of health checks may not help distribute traffic to all the AZs as the ELB will still direct traffic to the healthy instances regardless of their AZ. Additionally, Amazon recommends using at least two AZs for high availability and fault tolerance, but this is not relevant to the current issue. Therefore, the correct answers to this question are A and C.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Disable sticky sessions",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Reduce the frequency of the health checks",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable cross zone load balancer",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon recommends to use two availability zone behind ELB.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 92,
  "query" : "Your company has a branch office in Sydney (Australia) that has just purchased the AWS Direct Connect link to connect a local data center to AWS resources in the ap-southeast-2 Region.\nThe company also owns lots of AWS EC2 resources in VPCs in the US West Region.\nYou want to use the same Direct Connection for your local data center to communicate with these resources in us-west-1\nHow would you achieve the requirement?",
  "answer" : "Answer - D.\nA Direct Connect gateway is a globally available resource.\nYou can create the Direct Connect gateway in any Region and access it from all other Regions.\nA Direct Connect gateway does not allow gateway associations that are on the same Direct Connect gateway to send traffic to each other (for example, a virtual private gateway to another virtual private gateway).\nA Direct Connect gateway does not prevent traffic from being sent from one gateway association back to the gateway association itself (for example when you have an on-premises supernet route that contains the prefixes from the gateway association)\nOption A is incorrect because the Direct Connection can be used to access public or private services in a remote region.\nOption B is incorrect because you would need to create a Direct Connect gateway in the remote Region (us-west-1) for the connection.\nOption C is incorrect because a public virtual interface is required for accessing public services in the remote region.\nOption D is CORRECT because the Direct Connect gateway can be created as a globally available resource to communicate with private resources in VPCs.\nFor more information on accessing a remote AWS Region, please visit the below URL-\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/remote_regions.html https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\nThe correct answer is D: Create a Direct Connect gateway as a globally available resource. Use it to connect the AWS Direct Connect connection over a private virtual interface to VPCs in us-west-1.\nExplanation: Direct Connect is a network service provided by AWS that allows customers to establish a dedicated network connection between their data center and AWS. The connection is established over a private, high-speed, low-latency connection. Direct Connect is available in many regions around the world and is used by many customers to establish a dedicated connection to AWS.\nIn this scenario, the company has purchased a Direct Connect link to connect their local data center in Sydney to AWS resources in the ap-southeast-2 region. However, they also have many EC2 resources in VPCs in the us-west-1 region, and they want to use the same Direct Connect link to connect to those resources.\nOption A is incorrect because Direct Connect cannot be used for different regions. Each region requires its own Direct Connect connection.\nOption B is incorrect because a private virtual interface can only be used to connect to resources in the same region as the Direct Connect location. Therefore, a private virtual interface in the ap-southeast-2 region cannot be used to connect to resources in the us-west-1 region.\nOption C is incorrect because a public virtual interface is not secure enough to protect the data. A VPN should be established over a private virtual interface for better security.\nOption D is the correct answer because a Direct Connect gateway is a globally available resource that can be used to connect to resources in different regions. A Direct Connect gateway is a router that can be used to connect to VPCs in different regions over a private virtual interface. In this scenario, the company can create a Direct Connect gateway and use it to connect to their VPCs in the us-west-1 region over a private virtual interface. This would allow them to use the same Direct Connect link to communicate with both the resources in ap-southeast-2 and us-west-1 regions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The Direct Connection cannot be used for different Regions. You need to create a new connection in the us-west-1 Region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a private virtual interface in the ap-southeast-2 Region and use it to connect the AWS Direct Connect connection to the VPCs in us-west-1.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a public virtual interface in the us-west-1 Region and set up a VPN over the public virtual interface to protect the data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Direct Connect gateway as a globally available resource. Use it to connect the AWS Direct Connect connection over a private virtual interface to VPCs in us-west-1.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 93,
  "query" : "Company D has a legacy employee verification product which exists for many years.\nThe system has used some old facial recognition technologies by comparing new photos with original ones in a large file system to verify employees.\nIt is becoming harder and harder to maintain the system as all product architects have left and technical documents are not well maintained.\nThe company decides to migrate the system to AWS and use some new technologies if possible.\nHowever, they do not want to spend huge efforts on the migration and hope to finish it as soon as possible.\nWhich option is the best for the company to choose at a reasonable cost?",
  "answer" : "Correct Answer - B.\nAWS always recommends exploring new services that can help increase working efficiency.\nAmazon Rekognition is a service that makes it easy to add image analysis to your applications.\nWith Rekognition, you can detect objects, scenes, and faces in images.\nYou can also search and compare faces.\nRekognition's API lets you easily build powerful visual search and discovery into your applications.\nWith Amazon Rekognition, you only pay for the images you analyze and the face metadata you store.\nThere are no minimum fees, and there are no upfront commitments.\nAs the question asks for a migration with a reasonable cost, it is not proper to use the SAAS provider.\nThe below is a piece of an example response of a DetectFaces API call.\nLots of information can be returned by Rekognition:\n{\n\"FaceDetails\": [\n{\n\"AgeRange\": {\n\"High\": 43,\n\"Low\": 26\n},\n\"Beard\": {\n\"Confidence\": 97.48941802978516,\n\"Value\": true.\n},\n\"BoundingBox\": {\n\"Height\": 0.6968063116073608,\n\"Left\": 0.26937249302864075,\n\"Top\": 0.11424895375967026,\n\"Width\": 0.42325547337532043\n},\n\"Confidence\": 99.99995422363281,\n\"Emotions\": [\n{\n\"Confidence\": 0.042965151369571686,\n\"Type\": \"DISGUSTED\"\n},\n{\n\"Confidence\": 0.002022328320890665,\n\"Type\": \"HAPPY\"\n},\n{\n\"Confidence\": 0.4482877850532532,\n\"Type\": \"SURPRISED\"\n},\n…\n}\nOption A is incorrect: Because the price will be higher than Rekognition.\nIt is not the best choice.\nOption B is CORRECT: Because Rekognition can definitely meet the need with a reasonable cost.\nAbout the facial recognition function of Rekognition, please refer to https://aws.amazon.com/getting-started/tutorials/detect-analyze-compare-faces-rekognition/ The compare-faces CLI reference is in https://docs.aws.amazon.com/cli/latest/reference/rekognition/compare-faces.html.\nOption C is incorrect: Same reason as A.\nAlso, RDS is unsuitable for storing photos.\nOption D is incorrect: It is quite similar to option.\nB.\nHowever, for Rekognition CLI, compare-faces only support source and target pictures in S3\nRefer to the below in https://docs.aws.amazon.com/cli/latest/reference/rekognition/compare-faces.html:\n“If you use the AWS CLI to call Amazon Rekognition operations, passing image bytes isn't supported.\nThe image must be formatted as a PNG or JPEG file.”\nThe company D has a legacy employee verification system that uses outdated facial recognition technologies. As the system is becoming harder and harder to maintain, the company decides to migrate the system to AWS and use new technologies if possible. However, the company wants to minimize the cost and effort of migration.\nOption A suggests utilizing a popular facial recognition service that has new biometric matching technology. The application will be put in EC2 to handle facial recognition tasks, and the on-premise file system will be migrated to S3. This option will require a significant amount of work to integrate the facial recognition service with the existing application. The company will also need to consider the cost of the service and the EC2 instances required to handle the application workload.\nOption B suggests using the Rekognition CLI to develop an application that uses AWS Rekognition facial analysis service. The CLI can detect faces and compare them to see if they match. This option requires less effort to implement than option A since the company will be utilizing a pre-built service. However, the company will still need to consider the cost of the Rekognition service and the resources required to run the application.\nOption C suggests utilizing a modern facial recognition SAAS. The application will be put in EC2 or Lambda to communicate with the SAAS provider. The on-premise file system will be migrated to RDS, and the SAAS provider will need access to RDS. This option will require the least amount of effort to implement since the company will be utilizing a pre-built service. However, the company will need to consider the cost of the SAAS provider and the resources required to run the application.\nOption D suggests transforming all source or target pictures to base64-encoded bytes. The Rekognition CLI will be used to compare faces using the pictures in image bytes format. This option will require the least amount of cost, but it will require more effort than option B. The company will need to consider the resources required to transform the pictures to base64-encoded bytes and the resources required to run the application.\nIn conclusion, the best option for the company to choose at a reasonable cost would be option B. This option requires less effort to implement than option A, and it provides a pre-built service that can detect faces and compare them to see if they match. The company will still need to consider the cost of the Rekognition service and the resources required to run the application, but it will minimize the cost and effort of migration compared to other options.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Utilize a popular facial recognition service that has used new biometric matching technology. Put the application in EC2 to handle the facial recognition tasks. Also, migrate the on-premise file system to S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Rekognition CLI to develop an application to use AWS Rekognition facial analysis service. For example, “aws rekognition detect-faces” can return a JSON response that contains information for each detected face. It can also compare faces to see if they match.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Utilize a modern facial recognition SAAS. Put the application in EC2 or lambda to communicate with the SAAS service provider. Also, migrate the on-premise file system to RDS. Make sure that the SAAS provider has access to RDS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Transform all source or target pictures to base64-encoded bytes. Use Rekognition CLI to compare faces such as “aws rekognition compare-faces” by using the pictures with the format of image bytes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 94,
  "query" : "Your company is hosting a web application on AWS (with an autoscaling group already in place)\nAccording to the architectural best practices, the application must be highly available, scalable, cost-effective, with high-performance, and require minimal human intervention.\nYou have deployed the web servers and database servers in the public and private subnet of the VPC, respectively.\nWhile testing the application via a web browser, you noticed that the application is not accessible.",
  "answer" : "E.\nAnswers - C.\nOption A is incorrect because (a) NAT instance is ideally used to route traffic from a private subnet to the internet via a public subnet, (b) NAT instance is not managed by AWS and requires to be configured and maintained by the user; hence, adding to the overhead, and (c) if not scaled, can cause a performance bottleneck.\nNAT Gateway is a preferred option over NAT instances.\nOption B recommends using AWS CloudFront and configure the distributions Origin to the web server and then use an AWS Route 53 ‘CNAME' for the CloudFront Distribution.\nCloudFront is highly available and accessible to the Internet.\nIt would work better if the Origin for the AWS CloudFront Distribution were pointed to an AWS ELB rather than the Web Server itself.\nCloudFront doesn´t support IPs as origins.\nSince the Origin would only be a Web Server, if this server goes offline for a period of time, the website would become unavailable.\nThe content is not cached at the Edge location or if the TTL for the content expires.\nSo, Option B is incorrect as well.\nOption C is CORRECT.\nBecause, (a) if the web servers are behind an ELB, the load on the web servers will be uniformly distributed.\nHence, if any of the web servers goes offline or becomes non-responsive, the traffic would be routed to other online web servers; making the application highly available, and (b) You can use Route53 to set the ALIAS record that points to the ELB endpoint.\nOption D is incorrect.\nBecause the Auto Scaling group may terminate and relaunch a new instance.\nIt is not suitable to configure the recordset that points to EIPs.\nSure, I'd be happy to explain the options for resolving the issue you are facing with the web application not being accessible.\nOption A: Configure a NAT instance in your VPC and create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address.\nIn this option, a NAT instance will be deployed in your VPC and all traffic from the private subnet to the Internet will be routed through this NAT instance. A default route will be configured to point to the NAT instance, and all subnets will be associated with this route. In addition, a DNS A record will be created that points to the public IP address of the NAT instance.\nThis option will enable the web application to access the Internet and provide external users with access to the application. However, it does not provide a high level of availability since the NAT instance can be a single point of failure. Moreover, it may not be the most cost-effective option since you will need to pay for the NAT instance.\nOption B: Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution.\nIn this option, a CloudFront distribution will be created to cache and serve static content from the web application. The origin for the CloudFront distribution will be configured to point to the private IP addresses of the web servers. A Route53 CNAME record will be created to point to the CloudFront distribution.\nThis option provides a high level of availability since the CloudFront distribution is replicated across multiple availability zones. It also improves the performance of the application by caching static content closer to the users. However, it may not be the most cost-effective option since you will need to pay for the CloudFront distribution.\nOption C: Place all your web servers behind ELB.\nIn this option, all web servers will be placed behind an Elastic Load Balancer (ELB). The ELB will distribute incoming traffic across all web servers and provide health checks to ensure that only healthy web servers receive traffic.\nThis option provides a high level of availability since the ELB is replicated across multiple availability zones. It also improves the performance of the application by distributing traffic across multiple web servers. However, it may not be the most cost-effective option since you will need to pay for the ELB.\nOption D: Configure a Route53 ALIAS-Record to point to the ELB DNS name.\nIn this option, a Route53 ALIAS record will be created to point to the DNS name of the ELB. The ALIAS record will resolve to the IP address of the ELB, and traffic will be distributed across all web servers.\nThis option provides a high level of availability since the ELB is replicated across multiple availability zones. It also simplifies DNS management since the Route53 ALIAS record automatically resolves to the IP address of the ELB. However, it may not be the most cost-effective option since you will need to pay for the ELB.\nOption E: Assign EIPs to all web servers. Configure a Route53 A-Record set with all EIPs with health checks and DNS failover.\nIn this option, Elastic IP addresses (EIPs) will be assigned to all web servers. A Route53 A-Record set will be created with all EIPs and health checks will be configured to monitor the health of the web servers. DNS failover will be configured to automatically route traffic to healthy web servers in the event of a failure.\nThis option provides a high level of availability since it ensures that traffic is always routed to a healthy web server. It also provides cost savings since there is no need to pay for additional services like ELB or CloudFront.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure a NAT instance in your VPC and create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Place all your web servers behind EL.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure a Route53 ALIAS-Record to point to the ELB DNS name.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Assign EIP`s to all web servers. Configure a Route53 A-Record set with all EIPs with health checks and DNS failover.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 95,
  "query" : "You have developed an application that processes a massive amount of process logs generated by web site and mobile app.\nThis application requires the ability to analyze petabytes of unstructured data using Amazon Elastic MapReduce.\nThe resultant data is stored on Amazon S3\nYou have deployed the c4.8xlarge Instance type, whose CPUs are mostly idle during the data processing.\nWhich of the below options would be the most cost-efficient way to reduce the log processing job's runtime?",
  "answer" : "Answer - C.\nOption A is incorrect even though storing the files on an S3 storage class such as RRS would reduce the cost.\nThe problem in the scenario is that the provision of a large instance is wasted due to it being idle most of the time.\nOption B is incorrect as adding more of the c4.8xlarge instance type in the task instance group would create more idle resources, which is - in fact - more costly.\nOption C is CORRECT because, since the CPU's are mostly idle, it means that you have provisioned a larger instance that is under-utilized.\nA better cost-efficient solution would be to use smaller instances.\nFor batch processing jobs such as the one mentioned in this scenario, you can use multiple t2 instances - which support the concept of CPU bursts - are ideal for situations where there are bursts of CPU during certain periods of time only.\nOption D is incorrect even though storing the files on an S3 storage class such as RRS would reduce the cost.\nThe problem in the scenario is that the provision of a large instance is wasted due to it being idle most of the time.\nFor more information on resizing of the EC2 instances, please visit the URL given below-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\nThe most cost-efficient way to reduce the log processing job's runtime in this scenario would be to add additional c4.8xlarge instances by introducing a task instance group, as mentioned in option B.\nOption A suggests creating log files with smaller sizes and storing them on Amazon S3. Although this would help with data organization and lifecycle management, it would not necessarily reduce the processing time of the data. Additionally, moving the files to RRS and then to Amazon Glacier vaults might affect the data availability and increase retrieval time.\nOption C suggests using smaller instances that have higher aggregate I/O performance. While smaller instances might have higher I/O performance, they may not have enough resources to process the massive amount of data efficiently. Using more instances with the current instance type would be a more practical solution.\nOption D suggests creating fewer, larger log files and compressing and storing them on the Amazon S3 bucket. While compressing the files may reduce storage costs, it may increase processing time since the system would need to decompress the files before analyzing them. Additionally, this approach could lead to data availability issues and longer retrieval times when accessing compressed files from Glacier.\nTherefore, option B is the most cost-efficient way to reduce the log processing job's runtime in this scenario. Adding additional c4.8xlarge instances would increase the processing speed and reduce the load on the EMR cluster. The network performance of 10 Gigabit per EC2 instance would also further enhance data processing speed. This would allow the application to analyze petabytes of unstructured data faster, which is necessary for efficient log processing.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create log files with smaller sizes and store them on Amazon S3. Apply the life cycle policy to the S3 bucket such that the files would be first moved to RRS and then to Amazon Glacier vaults.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add additional c4.8xlarge instances by introducing a task instance group. The network performance of 10 Gigabit per EC2 instance would increase the processing speed; thus reducing the load on the EMR cluster.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use smaller instances that have higher aggregate I/O performance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create fewer, larger log files. Compress and store them on the Amazon S3 bucket. Apply the life cycle policy to the S3 bucket such that the files would be first moved to RRS and then to Amazon Glacier vaults.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 96,
  "query" : "Your department creates a regular analytics report from your company's log files stored in S3\nThe daily EMR jobs produce the PDF reports and the aggregated tables in CSV format as an input to the Redshift data warehouse, and these are accessed frequently.\nYou notice that the average EMR hourly usage is more than 25% but less than 50%.",
  "answer" : "Answer - B.\nOptions A, C, and D are invalid.\nWe need to access the PDF and CSV files daily.\nSo S3-IA and glacier are not suitable for this purpose as both of these storage options are not the best for frequent access to files.\nBased on the given scenario, we need to optimize the usage of AWS resources for storing and processing log files stored in S3.\nOption A suggests using Glacier to store PDF and CSV data. However, Glacier is a low-cost storage service designed for data archiving rather than frequently accessed data. It is not suitable for storing data that needs to be accessed frequently. Adding Spot Instances to Amazon EMR jobs will help reduce the cost of processing data, but using Reserved Instances for Amazon Redshift is not an optimal solution since we cannot predict the exact usage of Redshift. Redshift is a fully managed data warehouse service that offers on-demand and reserved instances pricing models. Using on-demand instances will be more cost-effective for Redshift.\nOption B suggests using standard S3 to store PDF and CSV data, which is a suitable solution since we need to access the data frequently. Using a combination of Spot Instances and Reserved Instances for EMR can help reduce the cost of processing data. However, using Reserved Instances for Redshift is not optimal since we cannot predict the exact usage of Redshift.\nOption C suggests using Glacier to store PDF and CSV data, which is not a suitable solution since we need to access the data frequently. Adding Spot Instances to Amazon EMR jobs will help reduce the cost of processing data, but using Spot Instances for Amazon Redshift is not an optimal solution since Redshift requires dedicated and persistent resources.\nOption D suggests using S3-IA to store PDF and CSV data, which is a suitable solution since we need to access the data frequently, and S3-IA is a cost-effective storage service for infrequently accessed data. Using Reserved Instances for Amazon EMR jobs can help reduce the cost of processing data. Using on-demand instances for Amazon Redshift is a more cost-effective solution since we cannot predict the exact usage of Redshift.\nTherefore, option D is the best solution for optimizing the usage of AWS resources for storing and processing log files stored in S3.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Glacier to store PDF and CSV Data. Add Spot Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use standard S3 to store PDF and CSV data. Use a combination of Spot Instances and Reserved Instances for EMR and Reserved Instances for RedShift.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Glacier to store PDF and CSV Data. Add Spot Instances to Amazon EMR jobs. Use Spot Instances for Amazon Redshift.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use S3-IA to store PDF and CSV Data. Use Reserved Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 97,
  "query" : "You are the new IT architect in a company that operates a mobile sleep tracking application.\nWhen activated at night, the mobile app sends collected data points of 1 KB every 5 minutes to your middleware.\nThe middleware layer takes care of authenticating the user and writing the data points into an Amazon DynamoDB table.\nEvery morning, you scan the table to extract and aggregate last night's data on a per-user basis and store the results in Amazon S3\nUsers are notified via Amazon SMS mobile push notifications that new data is available, parsed, and visualized by the mobile app.\nThe old data is not required by the end-users.\nCurrently, you have around 100k users.\nYou have been tasked to optimize the architecture of the middleware system to lower the cost.\nWhat would you recommend?\n (Select TWO)",
  "answer" : "E.\nAnswer - A and C.\nOption A is CORRECT because (a) The data stored would be old/obsolete anyways and need not be stored; hence, lowering the cost, and (b) Storing the data in DynamoDB is expensive.\nHence, you can set an expiry date so that the data gets deleted automatically.\nOption B is incorrect because (a) Storing the data in DynamoDB is more expensive than S3, and (b) giving the app access to DynamoDB to read the data is an operational overhead.\nOption C is CORRECT because (a) it uses SQS which reduces the provisioned output cutting down on the costs, and (b) acts as a buffer that absorbs sudden higher load, eliminating going over the provisioned capacity.\nOption D is incorrect because the data is only read once before it is stored to S3\nThe cache would only be useful if you read things multiple times.\nAlso, in this scenario optimizing \"write\" operations is most desired, not \"read\" ones.\nOption E is incorrect because (a) Amazon Redshift cluster is primarily used for OLAP transactions, not OLTP; hence, not suitable for this scenario, and (b) moving the storage to Redshift cluster means deploying a large number of EC2 instances that are continuously running, which is not a cost-effective solution.\nFor complete guidelines on working with DynamoDB, please visit the below URL-\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\nThe goal is to optimize the middleware system of a mobile sleep tracking application that sends data points of 1KB every 5 minutes from the mobile app to an Amazon DynamoDB table via middleware layer, and then aggregates the data on a per-user basis and stores the results in Amazon S3 for visualization and notification purposes.\nTo lower the cost, the following two recommendations can be made:\n1.\nStore the data in the DynamoDB table with a Time to Live (TTL) and the data will be deleted automatically (Option A). Using DynamoDB with a TTL feature will allow the automatic deletion of old data from the table. This way, users don't need to access or visualize old data that is not required. With TTL, there will be no additional cost for deleting old data, and the cost of storing the data will be minimized.\n2.\nIntroduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput (Option C). Using Amazon SQS can help to reduce provisioned write throughput by buffering the writes to the Amazon DynamoDB table. This way, the system can handle any spikes in data ingestion without the need for additional write capacity in DynamoDB. Additionally, by using an SQS queue, the middleware layer can ensure that every write operation is processed reliably, even in cases of high concurrency, data loss, or failure.\nOther options such as accessing DynamoDB directly from the mobile app (Option B) or using Elasticache to cache reads from DynamoDB (Option D) may not necessarily optimize the cost since the first option can increase the load on DynamoDB, and the second option can increase the cost by introducing an additional service.\nLastly, writing data directly into an Amazon Redshift cluster (Option E) is not a recommended option since it is a different use case than DynamoDB, and the requirements for data aggregation and visualization would require significant changes to the application architecture. Additionally, Redshift is not suitable for real-time data ingestion or low-latency data access.\nTherefore, options A and C are the recommended choices to optimize the middleware system's architecture and lower the cost.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Store the data in the DynamoDB table with a Time to Live (TTL) and the data will be deleted automatically.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Have the mobile app access Amazon DynamoDB directly instead of JSON files stored on Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 98,
  "query" : "Your website is serving on-demand training videos to your workforce.\nVideos are uploaded monthly in high-resolution MP4 format.\nYour workforce is distributed globally, often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video.\nYour company has no video transcoding expertise, and it required that you may need to pay for a consultant.\nHow would you implement the most cost-efficient architecture without compromising the high availability and quality of video delivery?",
  "answer" : "Answer - A.\nThere are four most important design considerations here: (a) video transcoding expertise, (b) global distribution of the content, (c) cost-effective solution, and (d) no compromise with the high availability and quality of the video delivery.\nAmazon Elastic Transcoder is a media transcoding service in the cloud.\nIt is designed to be a highly scalable, easy-to-use, and cost-effective way for developers and businesses to convert (or “transcode”) media files from their source format into versions that will playback on various devices like smartphones, tablets, and PCs.\nOption A is CORRECT because (a) it uses Amazon Elastic Transcoder that converts from MP4 to HLS, (b) S3 Object Lifecycle Management reduces the cost by archiving the files to Glacier, and (c) CloudFront - which is a highly available service - enables the global delivery of the video without compromising the video delivery speed or quality.\nOption B is incorrect because (a) it necessitates the overhead of infrastructure provisioning.\ni.e, deploying of EC2 instances, auto-scaling, SQS queue/pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost-efficient solution.\nOption C is incorrect because the use of EBS snapshots is not a cost-effective solution compared to S3 Object Lifecycle Management.\nOption D is incorrect because (a) it necessitates the overhead of infrastructure provisioning.\ni.e, deploying EC2 instances, auto-scaling, SQS queue/pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost-efficient solution, and (d) the use of EBS snapshots is not a cost-effective solution compared to S3 Object Lifecycle Management.\nFor more information on Elastic transcoder, please visit the below URL-\nhttps://aws.amazon.com/elastictranscoder/\nCloudFront can then be used to deliver the content to the users from its various edge locations.\nOption A is the most cost-efficient and effective architecture for delivering high-quality videos to a globally distributed workforce using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch videos.\nThe Elastic Transcoder service can be used to transcode the original high-resolution MP4 videos to HLS format, which is optimized for streaming and can adapt to varying network conditions, device capabilities, and screen sizes. This avoids the need for any video transcoding expertise and the cost of hiring a consultant.\nThe videos can be hosted in Amazon S3, which is a highly scalable and durable object storage service. S3 can automatically replicate data across multiple availability zones within a region for high availability and durability. This ensures that the videos are easily accessible and can be delivered reliably to users around the world.\nCloudFront can be used as a content delivery network (CDN) to serve the HLS transcoded videos from S3. CloudFront can cache the videos in edge locations around the world, reducing latency and improving the user experience. CloudFront also supports secure video streaming using SSL/TLS encryption and signed URLs, which can prevent unauthorized access and distribution of the videos.\nOverall, this architecture provides a highly scalable, available, and cost-efficient solution for delivering high-quality videos to a global workforce using HLS-enabled tablets without compromising on the user experience.\nOption B is not the best architecture as it requires a video transcoding pipeline running on EC2, which increases the cost and complexity of the solution. Additionally, archiving all files to Glacier after a few days can lead to longer retrieval times and higher costs, which may not be suitable for delivering on-demand videos to a global workforce.\nOption C is also not ideal as hosting videos on EBS volumes can limit the scalability and availability of the solution, and using EBS snapshots to incrementally backup original files can increase costs and complexity.\nOption D is similar to Option B, but hosting videos on EBS volumes instead of S3 can limit the scalability and availability of the solution, and using EBS snapshots to incrementally backup original files can increase costs and complexity.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. Use S3 to host videos and use CloudFront to serve HLS transcoded videos from S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. Use S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days. Use CloudFront to serve HLS transcoding videos from Glacier.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. Use CloudFront to serve HLS transcoded videos from EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. Use CloudFront to serve HLS transcoded videos from EC2.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 99,
  "query" : "You've been hired to enhance the overall security posture for a large e-commerce site.\nThey have a well-architected, multi-tier application running in a VPC that uses ELBs in front of both the web and the app tier with static assets served directly from S3\nThey are using a combination of RDS and DynamoDB for their dynamic data and then archiving nightly into S3 for further processing with EMR.\nThey are concerned because they found questionable log entries and a flood of superfluous requests for accessing the resources.\nYou suspect that someone is performing a DDoS attack.\nHow would you mitigate this kind of attack in the easiest and most cost-efficient way?",
  "answer" : "E.\nAnswer - C.\nIn such scenarios where you are designing a solution to prevent the DDoS attack, you can use a Web Application Firewall (WAF).\nAWS WAF is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.\nAWS WAF gives you control over which traffic to allow or block your web applications by defining customizable web security rules.\nYou can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules designed for your specific application.\nNew rules can be deployed within minutes, letting you respond quickly to changing traffic patterns.\nOption A is incorrect because, although this option could work, the setup is very complex and is not a cost-effective solution.\nOption B is incorrect because (a) even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and (b) it does not prevent the attack from the new source of threat.\nOption C is CORRECT because you can use AWS WAF web access control lists (web ACLs) to help minimize the effects of a distributed denial of service (DDoS) attack.\nReference can be found in https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html.\nOption D is incorrect because there is no such thing as the Advanced Protocol Filtering feature for ELB.For more information on WAF, please visit the below URL-\nhttps://aws.amazon.com/waf/\nThe scenario presented in this question is focused on enhancing the security posture of a large e-commerce site. The site uses a multi-tier application running in a Virtual Private Cloud (VPC) with Elastic Load Balancers (ELBs) in front of the web and app tiers, with static assets being served directly from Amazon S3. Dynamic data is stored in a combination of Amazon RDS and DynamoDB, with nightly archiving to Amazon S3 for further processing with Amazon EMR. The company is concerned about the possibility of a Distributed Denial of Service (DDoS) attack due to questionable log entries and an excess of requests for accessing resources.\nThere are multiple ways to mitigate a DDoS attack in AWS. However, the question is asking for the easiest and most cost-efficient solution, so we must evaluate each answer choice to determine which solution best fits the criteria.\nAnswer choice A recommends that the company lease space at a DirectConnect partner location and establish a 1G DirectConnect connection to their VPC. While this would provide a dedicated and reliable connection to the VPC, it is not the easiest or most cost-efficient solution for mitigating a DDoS attack.\nAnswer choice B suggests establishing internet connectivity into the space and filtering traffic with a hardware Web Application Firewall (WAF) before passing traffic through the DirectConnect connection into the application. This solution is more complex and expensive than necessary and therefore not the easiest or most cost-efficient.\nAnswer choice C proposes adding previously identified host file source IPs as an explicit inbound deny network access control list (NACL) to the web tier subnet. While this could help to block traffic from known sources of the attack, it is not an effective solution for mitigating a DDoS attack since the attacker could easily change the source IP addresses.\nAnswer choice D is the best answer for mitigating a DDoS attack in the easiest and most cost-efficient way. It recommends enabling AWS WAF to protect the application from the DDoS attack. AWS WAF is a web application firewall that helps protect web applications from common web exploits and attacks. It can be easily deployed with Amazon CloudFront and Application Load Balancer (ALB) to protect web applications running on EC2 instances or running on AWS services like S3.\nFinally, answer choice E recommends removing all but TLS 1 and 2 from the web tier ELB and enabling advanced protocol filtering. While this solution can help to protect against certain types of attacks, it is not as effective as using AWS WAF for mitigating a DDoS attack.\nIn summary, the best and easiest way to mitigate a DDoS attack in this scenario is to enable AWS WAF to protect the application from the attack.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Recommend that they lease space at a DirectConnect partner location and establish a 1G DirectConnect connection to their VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Then they would establish Internet connectivity into their space, filter the traffic in hardware Web Application Firewall (WAF) and then pass the traffic through the DirectConnect connection into their application running in their VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add previously identified host file source IPs as an explicit INBOUND DENY NACL to the web tier subnet.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Enable AWS WAF to protect the application from the DDoS attack.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Remove all but TLS 1 & 2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 100,
  "query" : "You currently operate a web application in the AWS US-East region.\nThe application runs on an auto-scaled layer of EC2 instances and an RDS Multi-AZ database.\nYour IT security compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to your EC2, IAM, and RDS resources.\nThe solution must ensure the integrity and confidentiality of your log data.\nWhich of the below solutions would you recommend?",
  "answer" : "Answer - A.\nFor the scenarios where the application is tracking (or needs to track) the changes made by any AWS service, resource, or API, always think about AWS CloudTrail service.\nAWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account.\nCloudTrail logs authenticated AWS API calls and AWS sign-in events and collected this event information in files delivered to Amazon S3 buckets.\nThe most important points in this question are (a) Use of a single S3 bucket, (b) CloudTrail with the option that applies trail to specific regions, (b) Data integrity, and (d) Confidentiality.\nOption A is CORRECT because (a) it uses AWS CloudTrail with the option that applies trail to the US-East region, (b) a single new S3 bucket and IAM Roles so that it has the confidentiality, (c) log file validation is enabled.\nSee the AWS CloudTrail setting below which sets the option that applies trail to all regions:\nOptions B is incorrect because (a) although it uses AWS CloudTrail, the option that applies trail to all regions is not enabled, and (b) SNS notifications can be overhead in this situation.\nOption C is incorrect because (a) as an existing S3 bucket is used, it may already be accessed to the user, hence not maintaining confidentiality, and (b) it is not using IAM roles.\nOption D is incorrect because (a) although it uses AWS CloudTrail, the option that applies trail to all regions is not enabled, and (b) three S3 buckets are not needed.\nFor more information on Cloudtrail, please visit the below URLs-\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\nThe recommended solution for this scenario would be option A: Create a new CloudTrail trail with one new S3 bucket to store the logs in the US-East region. Enable log file validation. Use IAM roles and S3 bucket policies to control the permissions on the S3 bucket.\nExplanation: CloudTrail is a service that allows you to log and monitor API calls made in your AWS account. It can be used to record changes to resources, such as EC2, IAM, and RDS. CloudTrail can be configured to deliver these logs to an S3 bucket, where they can be stored securely and accessed as needed. Here are the reasons why option A is the recommended solution:\n1.\nUse of a new S3 bucket: The first step is to create a new S3 bucket to store the logs. It is recommended to use a new S3 bucket to avoid any issues with pre-existing data or permissions. By doing this, the logs can be isolated and managed more easily.\n2.\nEnable log file validation: Enabling log file validation ensures the integrity of the log data. This feature creates a digital signature for each log file, making it tamper-proof. This ensures that the logs are not altered, deleted, or injected with malicious code during transit or storage.\n3.\nUse of IAM roles and S3 bucket policies: IAM roles and S3 bucket policies are used to control access to the S3 bucket that stores the logs. IAM roles are used to grant access to the CloudTrail service to write to the S3 bucket. S3 bucket policies are used to restrict access to the logs to specific IAM users or roles. This ensures that only authorized personnel can access the logs.\nOption B is not the recommended solution because it uses SNS to send log file delivery notifications to your management system. This can be an additional layer of complexity, and it may not be necessary for the logging solution.\nOption C is not the recommended solution because it uses an existing S3 bucket, which can increase the risk of accidental or unauthorized deletion of logs. Also, using S3 ACLs to control permissions is not as robust as using IAM roles and S3 bucket policies.\nOption D is not the recommended solution because it creates three new CloudTrail trails and S3 buckets, which can be difficult to manage and monitor. Having separate buckets for different types of logs can create unnecessary complexity and increase the risk of misconfiguration.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a new CloudTrail trail with one new S3 bucket to store the logs in the US-East region. Enable log file validation. Use IAM roles and S3 bucket policies to control the permissions on the S3 bucket.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new CloudTrail with one new S3 bucket to store the logs. Configure SNS to send log file delivery notifications to your management system. Use IAM roles and S3 bucket policies on the S3 bucket that stores your logs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the option that applies trail to all regions selected. Use S3 ACLs to control the permissions on the S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs, and one for command line tools. Use IAM roles and S3 bucket policies on the S3 buckets that store your logs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 101,
  "query" : "An enterprise wants to use a 3rd party SaaS application hosted by another AWS account.\nThe SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise's account.\nThe enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege.\nThere must be controls in place to ensure that any other third party cannot use the SaaS vendor's credentials.",
  "answer" : "Answer - C.\nWhen a user, a resource, an application, or any service needs to access an AWS service or resource, always prefer creating an appropriate role with the least privileged access or only required access, rather than using any other credentials such as keys.\nOption A is incorrect because you should never share your access and secret keys.\nOption B is incorrect because (a) when a user is created, even though it may have the appropriate policy attached to it, its security credentials are stored in the EC2 which can be compromised, and (b) creation of the appropriate role is always the better solution rather than creating a user.\nOption C is CORRECT because AWS's role creation allows cross-account access to the application to access the necessary resources.\nSee the image and explanation below.\nMany SaaS platforms can access AWS resources via Cross-account access created in AWS.\nIf you go to Roles in your identity management, you will see the ability to add a cross-account role.\nOption D is incorrect because the role is to be assigned to the application and its resources, not the EC2 instances.\nFor more information on the cross-account role, please visit the below URL-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nThe correct answer is C.\nExplanation: To allow a third-party SaaS application hosted by another AWS account to access Amazon EC2 resources running within the enterprise's account, while conforming to the principles of least privilege, we should create an IAM role for cross-account access.\nThis is because using IAM roles instead of IAM users provides better security and ensures that the credentials of the SaaS vendor are not shared with any other third party. The IAM role can be assumed by the SaaS provider's account, which will provide temporary security credentials that are automatically rotated and are limited in scope to only the actions required by the SaaS application.\nOption A is incorrect because it involves retrieving the access and secret key for the enterprise account, which would mean sharing the enterprise account credentials with the SaaS provider, which goes against the principle of least privilege.\nOption B is also incorrect because it involves creating an IAM user with access and secret keys, which again would mean sharing long-term credentials with the SaaS provider.\nOption D is also incorrect because it involves creating an IAM role for EC2 instances, which is not appropriate for granting access to an external third-party SaaS application.\nTherefore, the correct answer is C. Create an IAM role for cross-account access that allows the SaaS provider's account to assume the role and assign it a policy that allows only the actions required by the SaaS application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM user within the enterprise account and assign a user policy to the IAM user that allows only the actions required by the SaaS application. Create new access and secret key for the user and provide these credentials to the SaaS provider.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM role for cross-account access that allows the SaaS provider’s account to assume the role and assign it a policy that allows only the actions required by the SaaS application.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an IAM role for EC2 instances, assign it a policy that allows only the actions required for the Saas application to work, provide the role ARN to the SaaS provider to be used when launching their application instances.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 102,
  "query" : "You are designing a data leak prevention solution for your VPC environment.\nYou want your VPC Instances to be able to access software distribution servers on the Internet for product updates.\nThe servers are accessible via the third party via their DNS names.\nYou want to make sure that the instances can only access these known servers based on their URLs.\nWhich of the following options would you consider?",
  "answer" : "Answer - A.\nThere are 3 main considerations in this scenario: (a) the instances in your VPC need internet access, (b) the access should be restricted for product updates only, and (c) all other outbound connection requests must be denied.\nWith such scenarios, you should not put your instances in the public subnet as they would have access to the internet without any restrictions.\nSo, you should put them in a private subnet, and since there is a need for logic for filtering the requests from client machines, configure a proxy server.\nWhat is a Proxy Server?\nA proxy server is a server that acts as a mediator between the client(s) that sends requests and the server that receives the requests and replies back.\nIf any client requires any resources, it connects to the proxy server, and the proxy server evaluates the request based on its filtering rules.\nIf the requests are valid, it connects to the server which receives the request and replies.\nThe proxy server also maintains cache; i.e., if any subsequent requests from the same or other clients are received, it returns the result from the cache, saving the trip to and from the server.\nHence, proxy servers tend to improve performance.\nSee the diagram below.\nOption A is CORRECT because a proxy server (a) filters requests from the client and allows only those related to the product updates, and (b) in this case helps to filter all other requests except the ones for the product updates.\nOption B is incorrect because a security group cannot filter requests based on URLs and you cannot specify deny rules.\nSecurity groups are used only for IPs and NOT for static DNS names.\nOption C is incorrect because even though moving the instances in a private subnet is a good idea, the routing table does not have the filtering logic.\nOption D is incorrect.\nNACL is stateless.\nThe default network ACL is configured to allow all traffic to flow in and out of the subnets to which it is associated.\nOption D is only specifying an Inbound rule.\nBut for an Inbound rule, it should specify the Source rather than the destination.\nAn example of setting up a proxy server can be found via the below URL-\nhttps://aws.amazon.com/articles/6463473546098546\nOption A: This option suggests placing the EC2 instances in private subnets and directing their egress traffic to a web proxy server in the public subnet. The web proxy server can then enforce URL-based rules for outbound access. This solution would allow for URL-based filtering of outbound traffic while still allowing access to the necessary software servers. However, implementing and maintaining a web proxy server can be complex and costly.\nOption B: Option B suggests implementing security groups and configuring outbound rules to only permit traffic to software servers. This solution would be a simpler and more cost-effective approach than option A, as it does not require the implementation and maintenance of a web proxy server. However, this solution assumes that the software servers have static IP addresses and do not change frequently. If the software servers' IP addresses change frequently, this solution would require constant maintenance to update the security group rules.\nOption C: Option C suggests moving all instances into private VPC subnets, removing default routes from all routing tables, and adding specific routes to the software servers only. This solution would allow for fine-grained control over outbound traffic by restricting all traffic by default and only allowing traffic to the known software servers. However, this solution would be complex to manage, as it would require the constant addition of new routes for any new software servers that need to be accessed.\nOption D: Option D suggests implementing network access control lists (NACLs) to allow traffic from specific destinations, with an implicit deny as a rule. This solution would allow for URL-based filtering of outbound traffic and is a simpler approach than option C. However, this solution would require constant maintenance to update the NACL rules to include any new software servers that need to be accessed.\nOverall, Option A and Option B are the most viable options for implementing URL-based filtering of outbound traffic, depending on the complexity of the environment and the specific requirements of the solution. Option C and Option D could also be viable solutions but require more maintenance and are more complex to manage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Place EC2 instances in private subnets and direct the egress traffic to a web proxy server in the public subnet and enforce URL based rules for outbound access.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement security groups and configure outbound rules to only permit traffic to software servers.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Move all your instances into private VPC subnets. Remove default routes from all routing tables and add specific routes to the software servers only.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Implement network access control lists to allow traffic from specific destinations, with an implicit deny as a rule.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 103,
  "query" : "An administrator uses Amazon CloudFormation to deploy a three-tier web application that consists of a web tier and application tier that will utilize Amazon DynamoDB for storage.\nWhile creating the CloudFormation template, which of the following would allow the application instance access to the DynamoDB tables without exposing API credentials?",
  "answer" : "Answer - C.\nThe scenario requires the instance to have access to DynamoDB tables without having to use the API credentials.\nIn such scenarios, always think of creating IAM Roles rather than IAM Users.\nOption A is incorrect because the IAM Role needs both the read and the write access to the DynamoDB table.\nOption B is incorrect because (a) you should never expose the Access and Secret Keys while accessing the AWS resources, and (b) using IAM Role is the more secure way of accessing the resources than using IAM Users with security credentials.\nOption C is CORRECT.\nInstance profiles (with the correct roles) can be referenced in cloud formation templates.\nThese instance profiles are created manually with the correct roles associated with them and can be reused on multiple instances.\nPre-building instance profiles can provide standardization in naming across the entire infrastructure.\nOption D is incorrect because (a) you should never expose the Access and Secret Keys while accessing the AWS resources, (b) using IAM Role is the more secure way of accessing the resources than using IAM Users with security credentials.\nFor more information on granting access to AWS resources via EC2 instance profile property, please visit the below URL-\nhttps://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html\nFor more information on adding IAM roles in CloudFormation templates, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html\nThe best way to allow the application instance access to the DynamoDB tables without exposing API credentials is by creating an IAM role that has the necessary permissions to read and write from the required DynamoDB table and then associating that role to the application instances by referencing an instance profile.\nOption A: This is the recommended approach. Creating an IAM role with read-only permissions for the required DynamoDB table and associating the role to the application instances using an instance profile allows the application to access the table securely without exposing any API credentials.\nOption B: Using the Parameter section to have the user input Access and Secret Keys from an already created IAM user that has the permissions required to read and write from the required DynamoDB table is not recommended. This approach exposes the API credentials to the user, which is a security risk.\nOption C: Creating an IAM role with the required permissions to read and write from the required DynamoDB table and associating the role to the application instances by referencing an instance profile is the recommended approach.\nOption D: Creating an Identity and Access Management (IAM) user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, using the GetAtt function to retrieve the Access and secret keys, and passing them to the application instance through user-data is not recommended. This approach exposes the API credentials to the application instance, which is a security risk.\nIn conclusion, Option A is the best approach to allow the application instance access to the DynamoDB tables without exposing API credentials.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an IAM role that only has the read permissions for the required DynamoDB table and associate the Role to the application instances by referencing an instance profile.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the Parameter section in the Cloud Formation template to have the user input Access and Secret Keys from an already created IAM user that has permissions required to read and write from the required DynamoDB table.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM Role that has the required permission to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an Identity and Access Management user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret keys and pass them to the application instance through user-data.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 104,
  "query" : "An AWS customer is deploying an application in a separate, highly constrained execution environment ( enclaves ) composed of an auto-scaling group of EC2 Instances.\nThe customer's security policy requires that every outbound connection from these instances to any other service within the customer's Virtual Private Cloud must be authenticated using a unique X.509 certificate that contains the specific instance ID.",
  "answer" : "Answer - C.\nThis scenario requires (a) an x.509 certificate per instance created in the auto-scaling group, (b) the certificate should be unique that contains the instance id.\nOption A is incorrect because (a) storing the signed certificate in S3 is a bad idea as it will not be unique for each instance id, and (b) S3 is not a key management service and cannot generate such certificates.\nOption B is incorrect because you need to generate the instance id first before generating the certificate that will be unique for that instance id.\nTherefore, embedding a certificate in the image and then launching the instance will not be useful at all.\nOption C is correct because once the new instance is launched, the ASG is configured to send the notification through SNS to the ACM.\nThe ACM then generates the new certificate.\nNitro enclaves and ACM:\nManager (ACM) for Nitro Enclaves allows you to use public and private SSL/TLS certificates with your web applications and web servers running on Amazon EC2 instances with AWS Nitro Enclaves.\nSSL/TLS certificates are used to secure network communications and establish the identity of websites over the internet and resources on private networks.\nPreviously, when running a web server on an EC2 instance, you would have created SSL certificates and stored them as plaintext on your instance.\nWith ACM for Nitro Enclaves, you can now bind AWS Certificate Manager certificates to an enclave and use those certificates directly with your web server without exposing the certificates in plaintext form to the parent instance and its users.\nACM for Nitro Enclaves removes the time-consuming and error-prone manual process of purchasing, uploading, and renewing SSL/TLS certificates.\nACM for Nitro Enclaves creates secure private keys, distributes the certificate and its private key to your enclave, and manages certificate renewals.\nWith ACM for Nitro Enclaves, the certificate's private key remains isolated in the enclave, preventing the instance, and its users, from accessing it.\nPlease also check https://docs.aws.amazon.com/enclaves/latest/user/enclaves-user.pdf#nitro-enclave-refapp (page 39) on how to use ACM for Nitro Enclaves.\nOption D is incorrect because (a) the onus is on the EC2 instances to generate the signed certificate, (b) the requirement is to use a key management service to generate the signed certificate, and (c)AWS KMS does not have any feature to 'poll' any service.\nPlease check below AWS Docs for more details.\nhttps://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-refapp.html https://docs.aws.amazon.com/enclaves/latest/user/enclaves-user.pdf#nitro-enclave-refapp\nThe correct answer for this scenario would be option B: Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group. Have the launched instances generate a certificate signature request with the instance's assigned instance-id to the AWS KMS for signature.\nHere's a detailed explanation of why this is the correct option:\nThe requirement is that every outbound connection from these instances to any other service within the customer's Virtual Private Cloud must be authenticated using a unique X.509 certificate that contains the specific instance ID. To achieve this, we need to generate a certificate for each instance and embed it within the instance. This certificate must be signed by a trusted authority to ensure that it is authentic.\nOption A is incorrect because it involves fetching the certificate from Amazon S3 upon first boot, which could potentially be a security risk. The IAM role approach in this option does not provide a way to uniquely identify each instance using a certificate.\nOption C is incorrect because it uses ACM, which is not designed to issue instance-specific certificates. Additionally, the SNS notification approach does not provide a way to uniquely identify each instance using a certificate.\nOption D is incorrect because it involves configuring each instance to generate a new certificate upon first boot. This approach is not practical because it requires the KMS to poll the Auto Scaling group for associated instances, which could be slow and introduce latency issues.\nOption B is the correct approach because it involves embedding a certificate into the Amazon Machine Image that is used by the Auto Scaling group. Each launched instance can then generate a certificate signature request with its assigned instance ID to the AWS KMS for signature. This approach ensures that each instance has a unique certificate and that the certificate is authentic.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure an Auto Scaling group to launch instances with this role. Have the instances bootstrap, get the certificate from Amazon S3 upon first boot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group Have the launched instances, generate a certificate signature request with the instance’s assigned instance-id to the AWS KMS for signature.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the AutoScaling group to send an SNS notification of the launch of a new instance to the AWS Certificate Manager. Create a signed certificate using AWS Certificate Manager (ACM).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure the launched instances to generate a new certificate upon first boot. Have the AWS KMS poll the Auto Scaling group for associated instances and send new instances a certificate signature (that contains the specific instance-id).",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 105,
  "query" : "You are given the task of moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC.\nUnfortunately, this app requires access to several on-premises services, and no one who configured the app still works for your company.\nEven worse, there's no documentation for it.\nWhich of the following options can help you to move the application from on-premises to VPC?",
  "answer" : "E.\nF.\nAnswer: A, D, E and F.\nThe scenario requires you to connect your on-premise server/instance with Amazon VPC.\nWhen such scenarios are presented, always think about Direct Connect, VPN, and VM Import and Export as they help either connect the instances from a different location or import them from one location to another.\nOption A is CORRECT because Direct Connect sets up a dedicated connection between on-premises data-center and Amazon VPC and provides you with the ability to connect your on-premise servers with the instances in your VPC.Option B is incorrect as you normally create a VPN connection based on a customer gateway and a virtual private gateway (VPG) in AWS.\nOption C is incorrect as EIPs are not needed as the instances in the VPC can communicate with on-premise servers via their private IP address.\nOption D is CORRECT because there should not be a conflict between the IP address of on-premise servers and the instances in VPC for them to communicate.\nOption E is CORRECT because we need to configure Route-53 resolver to forward queries via Direct Connect to the On-Prem DNS server.\nRoute 53 alone will not be able to move the application from on-premises to VPC.Option F is CORRECT because the VM Import Export service helps you import the virtual machine images from the data center to the AWS platform as EC2 instances and export them back to your on-premises environment.\nThis offering allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances.\nOnce the VM import is done.\nThen the application running inside the VPC can reach out to on-premises services.\nNote:\nVMWare import can help us moving machines from on-premise to ec2 instances inside VPC.Recently there is an announcement from AWS regarding Route53 Support for resolving on-premise dependency:\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-route-53-announces-resolver-with-support-for-dns-resolution-over-direct-connect-and-vpn/\nAs you know, the latest features/announcements take around 6 months to get reflected in the actual exam.\nThe situation described in the question is that there is a legacy application running on a virtual machine in an on-premises datacenter, which needs access to several on-premises services. There is no documentation available on how the app is configured, and nobody who configured the app works at the company anymore. The task is to move this legacy application to an Amazon VPC (Virtual Private Cloud) environment.\nThere are several ways to move the application from on-premises to VPC, but we need to choose the option that would allow the application to access the required on-premises services. Let's examine each of the given options:\nA. An AWS Direct Connect link between the VPC and the datacenter network: This option can establish a dedicated network connection between the VPC and the on-premises datacenter. It can provide a private and high-bandwidth connection, which can be used to transfer large amounts of data securely between the VPC and the datacenter. However, it doesn't address the issue of the legacy application's requirement to access on-premises services.\nB. An Internet Gateway to allow a VPN connection: This option can create a virtual private network ( VPN) connection between the VPC and the datacenter, which can allow secure access to on-premises services. However, this option requires that the on-premises network is configured to allow VPN connections, and the legacy application might not be compatible with this configuration.\nC. An Elastic IP address on the VPC instance: Elastic IP addresses are static, public IP addresses that can be assigned to AWS resources. This option can provide a fixed public IP address to the VPC instance, which can allow on-premises services to access the VPC instance. However, it doesn't solve the issue of the legacy application's requirement to access on-premises services.\nD. An IP address space that does not conflict with the one on-premises: This option can ensure that the IP address space used in the VPC doesn't conflict with the one used in the on-premises network. However, it doesn't help in accessing the on-premises services.\nE. Configure Route-53 resolver and make entries in Amazon Route 53 that allow the Instances to resolve its dependencies' IP addresses: This option can allow the VPC instances to resolve the IP addresses of the on-premises services using Amazon Route 53 resolver. However, this option assumes that the on-premises services are accessible via DNS, and the legacy application might not be compatible with this configuration.\nF. A VM Import of the current virtual machine: This option can import the current virtual machine to the AWS environment, which can be launched as an EC2 instance. However, it doesn't address the issue of the legacy application's requirement to access on-premises services.\nBased on the above analysis, none of the given options can fully address the requirement of the legacy application to access on-premises services. However, the option that can provide the most flexibility is option B, which can create a VPN connection between the VPC and the datacenter. With this option, the legacy application can access the on-premises services securely. However, this option requires that the on-premises network is configured to allow VPN connections. If this is not possible, then other options might need to be explored, such as refactoring the legacy application to remove the requirement for on-premises services.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "An AWS Direct Connect link between the VPC and the datacenter network.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "An Internet Gateway to allow a VPN connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An Elastic IP address on the VPC instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An IP address space that does not conflict with the one on-premises.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure Route-53 resolver and make entries in Amazon Route 53 that allow the Instances to resolve its dependencies’ IP addresses.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A VM Import of the current virtual machine.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 106,
  "query" : "Your company has recently extended its on-premises data center into a VPC on AWS to add burst computing capacity as needed.\nThe data center has already used an identity provider (IDP)\nMembers of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary.\nYou don't want to create new IAM users for each member and make those users sign in again to the AWS Management Console.\nWhich option below will meet the needs of your NOC members?",
  "answer" : "Answer - C.\nThis scenario has two requirements: (a) temporary access to AWS resources be given to certain users or applications (NOC members in this case), and (b) you are not supposed to create new IAM users for the NOC members to log into AWS console.\nThis scenario is handled by a concept named \"Federated Access\"\nRead this for more information on federated access: https://aws.amazon.com/identity/federation/ .\nRead this article for more information on how to establish federated access to the AWS resources.\nhttps://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/\nOption A is incorrect because OAuth 2.0 is not applicable in this scenario as we are not using Web Identity Federation.\nIt is used with public identity providers such as Facebook, Google, etc.\nOption B is incorrect because the key point here is that you need to give access to AWS Management Console to only the members of your Network Operations Center using on-premises SSO to avoid signing in again.\nThe users should not be using Facebook or Google IDs to login.\nOption C is CORRECT because (a) it gives federated access to the NOC members to AWS resources by using SAML 2.0 identity provider, and (b) it uses on-premises single sign on (SSO) endpoint to authenticate users and gives them access tokens before providing the federated access.\nOption D is incorrect because, even though it uses SAML 2.0 identity provider, we need to grant SSO access to the AWS management console and retrieving temporary security credentials is irrelevant here.\nSee this diagram that explains the Federated Access using SAML 2.0.\nThe correct answer for this question is C. Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.\nExplanation: The scenario presented in the question requires a solution that allows members of the NOC to access and administer Amazon EC2 instances in the AWS Management Console without creating new IAM users for each member and requiring them to sign in again to the AWS Management Console.\nOption A is incorrect because OAuth 2.0 is an authorization protocol used for third-party applications to access user data on a resource server without the need for the user to share their credentials with the third-party application. While OAuth 2.0 can be used to retrieve temporary AWS security credentials, it is not the best solution for granting members of the NOC access to the AWS Management Console.\nOption B is incorrect because Web Identity Federation is used to grant access to AWS resources to users who have authenticated with a web identity provider, such as Facebook or Google. While Web Identity Federation can be used to retrieve AWS temporary security credentials, it is not the best solution for granting members of the NOC access to the AWS Management Console.\nOption D is incorrect because using an on-premises SAML 2.0-compliant IDP to retrieve temporary security credentials would require members of the NOC to sign in again to the AWS Management Console using those credentials. This is not what the scenario requires.\nOption C is the correct answer because it allows the on-premises SAML 2.0-compliant IDP to grant the members of the NOC federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint. Federated access means that members of the NOC can use their existing credentials to access the AWS Management Console without the need to create new IAM users or sign in again. AWS SSO allows organizations to manage access to multiple AWS accounts and business applications through a single sign-on experience. By configuring the on-premises SAML 2.0-compliant IDP with AWS SSO, the members of the NOC can use their existing credentials to access the AWS Management Console.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your members to sign in to the AWS Management Console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Web Identity Federation to retrieve AWS temporary security credentials to enable your members to sign in to the AWS Management Console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use your on-premises SAML 2.0-compliant identity provider (IDP) to retrieve temporary security credentials to enable members to sign in to the AWS Management Console.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 107,
  "query" : "You are designing an SSL/TLS solution that requires HTTPS clients to be authenticated by a web server using client certificate authentication.\nWhich of the following options would you consider for configuring the webserver infrastructure?",
  "answer" : "Answers - A and B.\nThis scenario requires you to set up the web servers so that the HTTPS clients must be authenticated by the client-side certificate (not the server-side certificate)\nThere are two ways of architecting this - with ELB and without ELB.\n(a) With ELB, if you use HTTPS listeners, you have to deploy the server-side certificate - which is not desired.\nSo, you need to use the TCP listener so that the HTTPS client requests do not terminate at the ELB.\nThey pass through ELB and terminate at the webserver instances.\n(b) Alternatively, without ELB, you can directly use the webserver to communicate with the clients or set up a Route53 Record Set with the public IP address of the webserver(s) such that the client requests would be directly routed to the webserver(s).\nOption A is CORRECT because it uses TCP (443) listener that relays requests to the backend instances as-is.\nThis terminates the client certificate on the webserver instances rather than the ELB.Option B is CORRECT because it uses Route53 Record Set with the public IP address of the webserver(s) such that the client requests would be directly routed to the webserver(s).\nOption C is INCORRECT because if you use HTTPS listeners, it does not support client certificates.\nOption D is incorrect because this setting should be an \"origin\" and not a \"target\"\nThe phrase should be \"Configure your web servers as the origins for a CloudFront distribution\"\nPlease check below AWS docs for more details.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-server-cert.html https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-create-https-ssl-load-balancer.html#create-https-lb-console\nPlease refer to the following link to TCP to pass through traffic to the ELB \"as-is\"\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/https-tcp-passthrough.html\nTo configure an SSL/TLS solution that requires HTTPS clients to be authenticated by a web server using client certificate authentication, there are several options to consider for configuring the web server infrastructure.\nA. Configure ELB with TCP listeners on TCP/443 and place the web servers behind it.\nThis option involves using the Elastic Load Balancer (ELB) to distribute incoming traffic across multiple web servers. With this approach, the web servers would sit behind the ELB, and the ELB would be configured with TCP listeners on TCP/443. This approach is useful if you don't need to terminate SSL/TLS at the load balancer level and prefer to keep it at the web server level. However, if you need to terminate SSL/TLS at the load balancer level, this approach may not be suitable.\nB. Configure your web servers with EIPs. Place the web servers' IPs as endpoints of Route53 Record Sets and configure health checks against all web servers.\nThis option involves configuring your web servers with Elastic IP addresses (EIPs) and placing the web servers' IPs as endpoints of Route53 Record Sets. You would then configure health checks against all web servers to ensure that traffic is only routed to healthy web servers. This approach is useful if you need to have more control over the web server infrastructure and prefer not to use a load balancer. However, this approach can be more challenging to manage and scale as the infrastructure grows.\nC. Configure ELB with HTTPS listeners, and place the web servers behind it.\nThis option involves using the ELB to terminate SSL/TLS and distribute incoming traffic across multiple web servers. With this approach, the ELB would be configured with HTTPS listeners, and the web servers would sit behind the ELB. The ELB would then handle SSL/TLS termination and forward the decrypted traffic to the web servers. This approach is useful if you need to terminate SSL/TLS at the load balancer level and want to have more control over the web server infrastructure.\nD. Configure your web servers as the targets for a CloudFront distribution. Use custom SSL certificates on your CloudFront distribution.\nThis option involves using Amazon CloudFront to distribute incoming traffic across multiple web servers. With this approach, the web servers would be configured as targets for a CloudFront distribution, and custom SSL certificates would be used on the CloudFront distribution. This approach is useful if you need to distribute traffic globally and want to take advantage of CloudFront's caching and edge locations. However, this approach can be more complex to set up and manage.\nIn summary, option C is the most suitable option for configuring an SSL/TLS solution that requires HTTPS clients to be authenticated by a web server using client certificate authentication. However, the best option would depend on your specific requirements and constraints.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure ELB with TCP listeners on TCP/443 and place the Web servers behind it.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure your Web servers with EIPs. Place the Web servers` IPs as endpoints of Route53 Record Sets and configure health checks against all Web servers.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure ELB with HTTPS listeners, and place the Web servers behind it.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure your web servers as the targets for a CloudFront distribution. Use custom SSL certificates on your CloudFront distribution.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 108,
  "query" : "You are designing a connectivity solution between on-premises infrastructure and Amazon VPC.\nYour servers on-premises will be communicating with your VPC instances.\nYou will be establishing IPSec tunnels over the internet.\nYou will be using Virtual Private Gateways and terminating the IPsec tunnels on AWS-supported customer gateways.",
  "answer" : "E.\nF.\nAnswer - C, D, E and F.\nIPSec is designed to provide authentication, integrity, and confidentiality of the data that is being transmitted.\nIPSec operates at the network layer of the OSI model.\nHence, it only protects the data that is in transit over the internet.\nFor the full security of the data transmission, it is essential that both the sender and receiver need to be IPSec-aware.\nSee the diagram of this scenario:\nOption A is incorrect because (a) IPSec operates at the network layer of the OSI model.\nHence, it only protects the data that is in transit ( encryption of data in transit ONLY) over the internet, and (b) both the source and the destination (client and server) may not be IPSec aware.\nOption B is incorrect because the identity authentication of the origin of the data has to be done at the application layer, not the network layer.\nOption C is CORRECT because the data that is transiting via the IPSec tunnel is encrypted.\nOption D is CORRECT because IPSec protects the data in transit over the internet (fundamental responsibility of IPSec tunnel).\nOption E is CORRECT because, in this scenario, it is a pre-requisite to have the Peer identity authentication between VP gateway and customer gateway for implementing an IPsec VPN tunnel.\nThe IPSec tunnel is established between the VP Gateway (VPG) and Customer Gateway (CGW) whose identity gets authenticated during the IPSec tunnel setup.\nSince it is a pre-requisite even for establishing this connection we cannot term that as an objective that we have achieved via IPSec implementation.\nOption F is CORRECT because - as mentioned earlier - the integrity of the data that is transiting via the IPSec tunnel is always preserved (fundamental responsibility of IPSec tunnel).\nFor more information on IPSec tunnel, please refer to.\nhttp://techgenix.com/securing_data_in_transit_with_ipsec/\nThe below link provides an article on the general working of an IPSec tunnel which outlines the advantages of an IPSec tunnel which includes:\nhttp://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html\nThe scenario described involves establishing IPSec tunnels over the internet between on-premises infrastructure and Amazon VPC. The solution requires a secure connectivity solution that provides protection of data in transit over the internet and data encryption across the internet.\nVirtual Private Gateways (VPGs) are the Amazon VPC side of a VPN connection. A VPG is a virtual router that is used to establish a secure IPsec tunnel between the on-premises network and Amazon VPC. AWS-supported customer gateways are the physical devices that terminate the IPsec tunnels on the on-premises network side.\nAnswer options A, D, and F are correct because they all relate to the protection of data in transit over the internet. IPSec provides end-to-end protection of data, including data encryption and data integrity protection. This ensures that data transmitted between on-premises infrastructure and Amazon VPC is protected from interception and tampering.\nAnswer option B is not required in this scenario as there is no mention of end-to-end identity authentication being required.\nAnswer option E is incorrect because peer identity authentication between Virtual Private Gateway and customer gateway is a requirement for establishing the VPN connection, not for the implementation of the solution.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "End-to-end protection of data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "End-to-end Identity authentication.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Data encryption across the Internet.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Protection of data in transit over the Internet.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Peer identity authentication between Virtual Private Gateway and customer gateway is achieved as it is imperative for its implementation.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Data integrity protection across the Internet.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 109,
  "query" : "You are designing an intrusion detection prevention (IDS/IPS) solution for a customer's web application in a single VPC.\nYou are considering the options for implementing IDS/IPS protection for traffic coming from the Internet.\nWhich of the following options would you consider?",
  "answer" : "When designing an intrusion detection prevention (IDS/IPS) solution for a customer's web application in a single VPC, there are various options to consider.\nOption A: Implement IDS/IPS agents on each instance running in the VPC. This option involves installing IDS/IPS agents on each instance running in the VPC. This would require installing the agents on all instances, which can be time-consuming and labor-intensive. Additionally, the approach will not protect traffic coming into the VPC and traffic within the VPC.\nOption B: Configure an instance in each subnet to switch its network interface card to promiscuous mode and analyze network traffic. This option requires configuring a dedicated instance in each subnet to switch its network interface card to promiscuous mode, allowing it to capture all network traffic within that subnet. However, it can be difficult to scale and manage, and it will not provide protection against traffic entering the VPC.\nOption C: Implement Elastic Load Balancing with SSL listeners in front of the web applications. This option involves implementing Elastic Load Balancing (ELB) with SSL listeners in front of the web applications. This provides load balancing and SSL offloading, ensuring that traffic coming into the VPC is secure. However, ELB does not provide IDS/IPS functionality, so additional measures would be required to provide intrusion detection and prevention.\nOption D: Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server. This option involves implementing a reverse proxy layer in front of the web servers, with IDS/IPS agents installed on each reverse proxy server. This provides a layer of protection for traffic coming into the VPC and traffic within the VPC. However, it can be complex to manage, and additional measures may be required to provide load balancing and SSL offloading.\nOverall, option D is the best choice because it provides protection for both incoming and internal traffic, while also allowing for load balancing and SSL offloading. Additionally, implementing a reverse proxy layer can provide additional benefits such as caching and content delivery. However, it is important to carefully consider the management overhead and potential complexity involved in this option.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Implement IDS/IPS agents on each Instance running In VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an instance in each subnet to switch its network interface card to promiscuous mode and analyze network traffic.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Implement Elastic Load Balancing with SSL listeners in front of the web applications.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 110,
  "query" : "You are designing a photo-sharing mobile app.\nThe application will store all pictures in a single Amazon S3 bucket.\nUsers will upload pictures from their mobile devices directly to Amazon S3 and will be able to view and download their own pictures directly from Amazon S3\nYou want to configure security to handle the potential users in the most secure manner possible.",
  "answer" : "E.\nF.\nAnswer - B.\nThis scenario requires the mobile application to have access to the S3 bucket.\nThere are potentially millions of users and a proper security measures should be taken.\nIt is suggested to set up a web identity federation through AWS Cognito.\nYou can let users sign in using a well-known third-party identity provider such as log in with Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider.\nYou can exchange the credentials from that provider for temporary permissions to use resources in your AWS account.\nThis is known as the web identity federation approach to temporary access.\nWhen you use web identity federation for your mobile or web application, you don't need to create custom sign-in code or manage your own user identities.\nUsing a web identity federation helps you keep your AWS account secure because you don't have to distribute long-term security credentials, such as IAM user access keys, with your application.\nOption A is incorrect because you should always grant the short-term or temporary credentials for the mobile application.\nThis option asks to create long-term credentials.\nOption B is CORRECT because it configures the web identity federation through AWS Cognito by generating temporary security credentials using STS \"AssumeRole\" function.\nOption C is incorrect because, even though the setup is very similar to option B, it does not create IAM Role with proper permissions, which is essential.\nOption D is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution.\nYou should create an IAM Role to access the AWS Resource via the \"AssumeRole\" function.\nOption E is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution.\nYou should create an IAM Role to access the AWS Resource via the \"AssumeRole\" function.\nFor more information on AWS temporary credentials, please refer to the below links-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html\nThe most secure way to handle potential users in a photo-sharing mobile app that uses Amazon S3 to store photos is to use web identity federation through Amazon Cognito. Option B is the correct answer. Here's why:\nOption A is not secure because it involves storing long-term credentials in the mobile app, which can be easily compromised. If these credentials are compromised, the attacker can gain access to all the photos stored in the S3 bucket.\nOption C is not a valid option as it involves recording the user's information in Amazon DynamoDB, which is a NoSQL database service. Although it is possible to store user information in DynamoDB, it does not provide any security mechanism for controlling access to the S3 bucket.\nOption D involves generating temporary credentials using AWS STS, which is a valid approach. However, storing the credentials in the mobile app's memory is not secure, as they can be easily retrieved by an attacker who gains access to the mobile device.\nOption E involves creating an IAM user and generating access keys and secret keys for the user, which are then stored in the mobile app. This approach is not recommended because access keys and secret keys are long-term credentials that can be easily compromised if they are stored on the mobile device.\nOption F is similar to option E, but it involves updating the bucket policy to grant the IAM user access to the S3 bucket. While this is a valid approach, it is not the most secure way to handle potential users in the mobile app.\nWeb identity federation through Amazon Cognito (option B) is the most secure approach because it uses temporary security credentials to access the S3 bucket. This approach involves authenticating users using popular identity providers such as Facebook, Google, or Amazon. Once a user is authenticated, Amazon Cognito issues a temporary token that can be used to request temporary security credentials from AWS STS. These temporary credentials are then used to access the S3 bucket. This approach is more secure because it does not involve storing long-term credentials on the mobile device.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a set of long-term credentials using the AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up web identity federation through Amazon Cognito for the mobile app. Use Cognito API operations to get a Cognito token and request temporary security credentials from AWS STS. Use the temporary credentials to access Amazon S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Record the user’s Information In Amazon DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "When the user uses their mobile app create temporary credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app’s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM user. Assign appropriate permissions to the IAM user Generate an access key and secret key for the IAM user, store them in the mobile app, and use these credentials to access Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM user. Update the bucket policy with appropriate permissions for the IAM user. Generate an Access Key and Secret Key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 111,
  "query" : "You have an application running on an EC2 Instance which will allow users to download files from a private S3 bucket using a pre-signed URL.\nBefore generating the URL, the application should verify the existence of the file in S3\nHow should the application use AWS credentials to access the S3 bucket securely?",
  "answer" : "Answer - C.\nAn IAM role is similar to a user.\nIn that, it is an AWS identity with permission policies that determine what the identity can perform in AWS.\nHowever, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.\nIf a user is assigned a role, access keys are created dynamically and provided to the user.\nYou can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.\nWhenever the question presents you with a scenario where an application, user, or service wants to access another service, always prefer creating IAM Role over IAM User.\nThe reason being that when an IAM User is created for the application, it has to use the security credentials such as access key and secret key to use the AWS resource/service.\nThis has security concerns.\nWhereas, when an IAM Role is created, it has all the necessary policies attached to it.\nSo, the use of the access key and the secret key is not needed.\nThis is the preferred approach.\nOption A is incorrect because you should use the IAM Role.\nSee the explanation given above.\nOption B is incorrect because instead of IAM User, you should use the IAM Role.\nSee the explanation given above.\nOption C is CORRECT because (a) it creates the IAM Role with appropriate permissions, and (b) the application accesses the AWS Resource using that role.\nOption D is incorrect because instead of IAM User, you should use the IAM Role.\nSee the explanation given above.\nFor more information on IAM roles, please visit the below URL-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nThe correct answer for this scenario is C. Create an IAM role for EC2 that allows permission to list objects in the S3 bucket. Launch the instance with that role and assume the role's credentials from the EC2 Instance metadata.\nExplanation: In this scenario, we have an application running on an EC2 Instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. To access the S3 bucket securely, we need to follow the best practice of using IAM Roles for EC2 instances, which provides temporary AWS security credentials that the EC2 instances can use to access AWS services. IAM roles are more secure than using access keys because they are temporary, and the AWS SDK automatically retrieves them from the EC2 Instance metadata service.\nOption A: Use the AWS account access Keys. The application retrieves the credentials from the source code of the application. This option is not recommended because storing AWS access keys in source code can lead to security vulnerabilities. If a hacker gains access to the source code, they will have access to the AWS account.\nOption B: Create an IAM user for the application with permissions that allow list access to the S3 bucket, launch the instance as the IAM user, and retrieve the IAM user's credentials from the EC2 instance user data. This option is not recommended because the application would need to retrieve the IAM user's credentials from the EC2 instance user data, which is also not secure.\nOption D: Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user. This option is not recommended because the application would need to retrieve the IAM user's credentials from a temporary directory, which is not secure. Additionally, using IAM users is not the best practice for EC2 instances.\nOption C: Create an IAM role for EC2 that allows permission to list objects in the S3 bucket. Launch the instance with that role and assume the role's credentials from the EC2 Instance metadata. This is the best practice for accessing AWS services from EC2 instances. By creating an IAM role for EC2, we can assign permissions to the role to access S3, and the EC2 instance can assume the role and access S3 securely. This method eliminates the need to manage access keys or IAM user credentials on the EC2 instance. The application can simply assume the role using the AWS SDK, and the SDK will automatically retrieve the temporary security credentials from the EC2 instance metadata service.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the AWS account access Keys. The application retrieves the credentials from the source code of the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM user for the application with permissions that allow list access to the S3 bucket, launch the instance as the IAM user, and retrieve the IAM user’s credentials from the EC2 instance user data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM role for EC2 that allows permission to list objects in the S3 bucket. Launch the instance with that role and assume the role’s credentials from the EC2 Instance metadata.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 112,
  "query" : "You are designing a social media site and are considering how to mitigate distributed denial-of-service (DDoS) attacks.\nWhich of the below can be used to notify and mitigate the attacks? (Select THREE).",
  "answer" : "E.\nAnswer - C, D, and E.\nThis question is asking you to select some of the most recommended and widely used DDoS mitigation techniques.\nWhat is a DDoS Attack?\nA Distributed Denial of Service (DDoS) attack is an attack orchestrated by distributed multiple sources that make your web application unresponsive and unavailable for the end-users.\nDDoS Mitigation Techniques.\nSome of the recommended techniques for mitigating the DDoS attacks are\n(i) building the architecture using the AWS services and offerings that can protect the application from such attacks.\ne.g.\nCloudFront, WAF, Autoscaling, Route53, VPC, etc.\n(ii) defending the infrastructure layer by over-provisioning capacity and deploying DDoS mitigation systems.\n(iii) defending the application layer by using WAF and operating at scale by using autoscale so that the application can withstand the attack by scaling and absorbing the traffic.\n(iv) minimizing the surface area of attack.\n(v) obfuscating the AWS resources.\nOption A is incorrect because ENIs do not help in increasing the network bandwidth.\nOption B is incorrect because having dedicated instances performing at maximum capacity will not help mitigate the DDoS attack.\nWhat is needed is instances behind auto-scaling so that the traffic can be absorbed while actions are being taken on the attack and the application can continue responding to the clients.\nOption C is CORRECT because WAF can protect against DDoS attacks and users can define rules to allow or block traffic.\nOption D is CORRECT because ELB helps distribute the traffic to the auto-scaling instances (helps to absorb the traffic).\nOption E is CORRECT because CloudWatch alarms can be used to trigger an SNS notification so that the team can be alerted of the high traffic.\nNote: Advanced Shield would be a better solution.\nThere is a cost factor attached to it.\nIt is very important to read the AWS Whitepaper on Best Practices for DDoS Resiliency.\nhttps://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\nDDoS attacks are a common problem for web-based applications, and it is important to take measures to mitigate the impact of such attacks. Here are the three options that can be used to notify and mitigate DDoS attacks:\n1.\nDeploy AWS WAF on an Amazon CloudFront distribution: AWS WAF (Web Application Firewall) is a service that helps protect web applications from common web exploits that can affect the availability, security, or consumption of resources. By deploying AWS WAF on an Amazon CloudFront distribution, you can protect your application from DDoS attacks that target your application's origin servers. AWS WAF can also block malicious traffic that tries to exploit known vulnerabilities or patterns.\n2.\nUse an Elastic Load Balancer with Auto Scaling Groups: An Elastic Load Balancer (ELB) can distribute incoming traffic across multiple EC2 instances, improving availability and fault tolerance. Auto Scaling Groups (ASGs) can dynamically scale the number of EC2 instances based on the incoming traffic load. By using ELB with ASGs, you can distribute traffic across multiple instances and scale up or down based on the traffic load, ensuring that the application can handle large spikes in traffic during a DDoS attack.\n3.\nCreate Amazon CloudWatch alarms: Amazon CloudWatch is a monitoring service that can monitor various metrics related to your AWS resources. By creating CloudWatch alarms, you can monitor metrics such as network in and CPU utilization, and receive notifications when certain thresholds are exceeded. You can also configure these alarms to trigger an SNS notification to notify the team of the high traffic, allowing them to take appropriate action.\nThe other two options mentioned are not directly related to mitigating DDoS attacks:\n1.\nAdding multiple elastic network interfaces (ENIs) to each EC2 instance to increase network bandwidth: While adding multiple ENIs can increase network bandwidth, it does not directly mitigate DDoS attacks. DDoS attacks can saturate the network bandwidth, making additional ENIs ineffective.\n2.\nUsing dedicated instances to ensure that each instance has the maximum performance possible: Using dedicated instances can improve performance, but it does not directly mitigate DDoS attacks. DDoS attacks can overwhelm the instance, making it unavailable regardless of whether it is dedicated or not.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Add multiple elastic network interfaces (ENIs) to each EC2 instance to increase the network bandwidth.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use dedicated instances to ensure that each instance has the maximum performance possible.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy AWS WAF on an Amazon CloudFront distribution to prevent DDoS attacks to the application.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use an Elastic Load Balancer with Auto Scaling Groups at the Web Application layer to scale up when the traffic is increasing.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create Amazon CloudWatch alarms to alert for a high network in and CPU utilization and notify the team with an SNS notification.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 113,
  "query" : "A benefits enrollment company hosts a 3-tier web application running in a VPC on AWS which includes a NAT (Network Address Translation) instance in the public Web tier.\nThere is enough provisioned capacity for the expected workload for the new fiscal year benefit enrollment period plus some extra overhead.\nEnrollment proceeds nicely for two days, but the web tier becomes unresponsive upon investigation using CloudWatch and other monitoring tools.\nIt is discovered that there is a huge and unanticipated amount of inbound traffic coming from a set of 15 specific IP addresses over port 80 from a country where the benefits company has no customers.\nThe web tier instances are so overloaded that benefit enrollment administrators cannot even SSH into them.\nWhich activity would be useful in defending against this attack?",
  "answer" : "Answer - D.\nIn this scenario, the attack comes from a set of certain IP addresses over a specific port from a specific country.\nYou are supposed to defend against this attack.\nIn such questions, always think about two options: Security groups and Network Access Control List (NACL)\nSecurity Groups operate at the individual instance level, whereas NACL operates at the subnet level.\nYou should always fortify the NACL first, as it is encounter first during the communication with the instances in the VPC.Option A is incorrect because IP addresses cannot be blocked using the route table or IGW.\nOption B is incorrect because changing the NAT instance's EIP cannot block the incoming traffic from a particular IP address.\nOption C is incorrect because (a) you cannot deny port access using security groups, and (b) by default all requests are denied; you open access for a particular IP address or range.\nYou cannot deny access to particular IP addresses using security groups.\nOption D is CORRECT because (a) you can add deny rules in NACL and block access to certain IP addresses.\nSee an example below.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html\nThe correct answer for this scenario is D. Create an inbound NACL (Network Access Control List) associated with the web tier subnet with deny rules to block the attacking IP addresses.\nExplanation: In this scenario, the web tier instances are experiencing an overload due to a huge and unanticipated amount of inbound traffic coming from a set of 15 specific IP addresses over port 80. The first step in defending against this attack is to block the attacking IP addresses. There are different ways to do this, but the most effective approach is to use a Network Access Control List (NACL) associated with the web tier subnet.\nA Network Access Control List is a stateless firewall that controls traffic in and out of a subnet. NACLs operate at the subnet level, which means that any traffic that passes through the subnet must first go through the NACL. In this case, we can create an inbound NACL associated with the web tier subnet with deny rules to block the attacking IP addresses. This will prevent the traffic from reaching the web tier instances, thus freeing up resources and allowing the benefit enrollment administrators to SSH into the instances and investigate further.\nCreating a custom route table associated with the web tier and blocking the attacking IP addresses from the Internet Gateway (IGW) is not a viable option because the NAT instance in the web tier subnet is already acting as a gateway to the Internet. Changing the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and updating the Main Route Table with the new EIP is also not a viable option because it does not address the root cause of the problem, which is the unanticipated inbound traffic from the attacking IP addresses. Creating 15 Security Group rules to block the attacking IP addresses over port 80 is also not a viable option because Security Groups operate at the instance level, not at the subnet level, and creating 15 rules can be tedious and error-prone. It is better to use a Network Access Control List in this case.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (Internet Gateway).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Change the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and update the Main Route Table with the new EIP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create 15 Security Group rules to block the attacking IP addresses over port 80.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an inbound NACL (Network Access Control List) associated with the web tier subnet with deny rules to block the attacking IP addresses.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 114,
  "query" : "Your fortune 500 company has undertaken a TCO analysis evaluating the use of Amazon S3 versus acquiring more hardware.\nThe outcome was that all employees would be granted access to use Amazon S3 for storage of their personal documents.\nWhich of the following will you need to consider so that you can set up a solution that incorporates a single sign-on from your corporate AD or LDAP directory?",
  "answer" : "E.\nAnswer - A, B, and D.\nIn questions like this where an application or user needs to be given access using Single Sign On (SSO), the following steps are very important.\n(i) setting up an identity provider for federated access.\n(ii) authenticating users using corporate data store or active directory-user-attributes.\n(iii) getting temporary access tokens/credentials using AWS STS.\n(iv) creating the IAM Role that has access to the needed AWS Resources.\nOption A is CORRECT because as mentioned above, setting up an identity provider for federated access is needed.\nOption B is CORRECT because as mentioned above, getting temporary access tokens/credentials using AWS STS is needed.\nOption C is incorrect because tagging each folder in a bucket does not help in this scenario.\nOption D is CORRECT because as mentioned above, creating the IAM Role that has access to the needed AWS Resources is needed.\nOption E is incorrect because you should be creating IAM Roles rather than IAM Users.\nThe diagram below showcases how authentication is carried out when having an identity broker.\nThis is an example of a SAML connection, but the same concept holds true for getting access to an AWS resource.\nFor more information on federated access, please visit the below link-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\nTo enable single sign-on (SSO) for your corporate Active Directory (AD) or LDAP directory users, you will need to set up a federation proxy or identity provider. This will allow your users to authenticate once with their corporate credentials and then be granted access to Amazon S3.\nAnswer A is correct: Setting up a federation proxy or identity provider is the correct approach to enable SSO from your corporate AD or LDAP directory to Amazon S3. A federation proxy acts as a mediator between your corporate directory and AWS, while an identity provider is a service that authenticates users and provides temporary security credentials to access AWS resources.\nAnswer B is not correct: Using AWS Security Token Service (STS) to generate temporary tokens is a method of enabling cross-account access or allowing users to assume temporary roles in your AWS account. It is not related to setting up SSO with your corporate directory.\nAnswer C is not correct: Tagging each folder in the bucket is a method of organizing and categorizing objects in Amazon S3, but it is not related to enabling SSO with your corporate directory.\nAnswer D is not complete: Configuring IAM roles with suitable permissions is a crucial step in granting access to AWS resources, including Amazon S3. However, it is not sufficient to enable SSO with your corporate directory. You will need to set up a federation proxy or identity provider to authenticate your users with their corporate credentials.\nAnswer E is not correct: Setting up a matching IAM user for every user in your corporate directory is not a scalable or secure approach. IAM users are intended for managing access to AWS resources within your own account, not for authenticating external users. Additionally, creating and managing a large number of IAM users can be time-consuming and error-prone.\nIn summary, the correct answer is A: Setting up a federation proxy or identity provider is necessary to enable SSO with your corporate AD or LDAP directory. This will allow your users to authenticate once with their corporate credentials and then be granted access to Amazon S3.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Setting up a federation proxy or identity provider.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Using AWS Security Token Service to generate temporary tokens.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Tagging each folder in the bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configuring IAM roles that have suitable permissions to the related S3 resources.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Setting up a matching IAM user for every user in your corporate directory that needs access to a folder in the bucket.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 115,
  "query" : "Your company policies require encryption of sensitive data at rest.\nYou consider the possible options for protecting data while storing it at rest on an EBS data volume attached to an EC2 instance.\nWhich of these options would allow you to encrypt your data at rest?",
  "answer" : "E.\nAnswer - A, C,and D.\nYou can encrypt the data at rest by either using native data encryption, using a third-party encrypting tool or just encrypt the data before storing it on the volume.\nOption A CORRECT because it uses a third-party volume encryption tool.\nOption B is incorrect because EBS volumes are not encrypted by default.\nOption C is CORRECT as it encrypts the data before storing it on EBS.\nOption D is CORRECT as it uses native data encryption.\nOption E is incorrect as SSL/TLS is used to secure the data in transit, not at rest.\nThe correct answer is A. Implement third party volume encryption tools.\nExplanation:\nOption A is the most appropriate answer for this question. It suggests using third-party volume encryption tools to encrypt sensitive data at rest. There are several third-party encryption tools that can be used to encrypt data at rest, such as BitLocker, VeraCrypt, and dm-crypt.\nOption B is incorrect because EBS volumes are not encrypted by default. While it is true that EBS volumes can be encrypted using AWS KMS or other encryption options, this is not the default behavior.\nOption C is also incorrect because it suggests encrypting data inside the application before storing it on EBS. While this is a viable option, it is not the most efficient or effective way to encrypt data at rest.\nOption D is incorrect because it suggests using native data encryption drivers at the file system level. While this is a possible solution, it is not the most practical one, as it requires significant expertise in file system encryption.\nOption E is also incorrect because it suggests implementing SSL/TLS for all services running on the server. While SSL/TLS can be used to encrypt data in transit, it does not provide protection for data at rest.\nIn conclusion, the most appropriate solution for encrypting sensitive data at rest on an EBS data volume attached to an EC2 instance is to implement third-party volume encryption tools.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Implement third party volume encryption tools",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Do nothing as EBS volumes are encrypted by default",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Encrypt data inside your applications before storing it on EBS",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Encrypt data using native data encryption drivers at the file system level",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement SSL/TLS for all services running on the server.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 116,
  "query" : "You are migrating a legacy client-server application to AWS.\nThe application responds to a specific DNS domain (e.g., www.example.com) and has a 2-tier architecture, with multiple application servers and a database server.\nRemote clients use TCP to connect to the application servers.\nThe application servers need to know the clients' IP address to function properly and are currently taking that information from the TCP socket.\nA decision is made to use a multi-AZ RDS MySQL instance for the database.\nDuring the migration, you can change the application code.\nBut you have to file a change request.",
  "answer" : "Answer - A.\nAWS ELB has support for Proxy Protocol.\nIt simply depends on a humanly readable header with the client's connection information to the TCP data sent to your server.\nAs per the AWS documentation, the Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections.\nBecause load balancers intercept traffic between clients and your instances, the access logs from your instance contain the load balancer's IP address instead of the originating client.\nYou can parse the first line of the request to retrieve your client's IP address and the port number.\nOption A is CORRECT because it implements the proxy protocol and uses ELB with a TCP listener.\nA change request is needed to support the proxy protocol in the application.\nWith the proxy protocol in ELB, the backend servers can get the IP addresses of the clients.\nOption B is incorrect because, although implementing cross-zone load balancing provides high availability, it states to use TCP forwarding, which does not support X-Forwarded-For.\nOption C is incorrect because Route53 latency-based routing does not give the IP address of the clients.\nOption D is incorrect because the Route53 Alias record does not give the IP address of the clients.\nFor more information on ELB enabling support for TCP, please refer to the links given below.\nhttps://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/ https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html\nThe best answer for this question would be A. File a change request to implement Proxy Protocol Support in the application. Use an ELB with a TCP Listener and Proxy Protocol enabled to distribute the load on two application servers in different AZs.\nExplanation: When migrating a legacy client-server application to AWS, it is important to consider the new infrastructure in place and how it will affect the application's behavior. In this scenario, the application needs to know the clients' IP address to function properly and is currently taking that information from the TCP socket.\nWith the migration to AWS, a decision is made to use a multi-AZ RDS MySQL instance for the database. Therefore, to distribute the load on two application servers in different AZs, we need to implement an Elastic Load Balancer (ELB). An ELB is a service that automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones.\nFor this scenario, we need to use a TCP listener with Proxy Protocol enabled to ensure that the application servers receive the clients' IP addresses. Proxy Protocol is a Layer 4 protocol that is supported by many load balancers and allows the load balancer to pass the client's IP address to the application server in a TCP packet.\nOption A recommends using a TCP listener with Proxy Protocol enabled to distribute the load on two application servers in different AZs. This approach ensures that the application servers receive the clients' IP addresses and enables the use of a multi-AZ RDS MySQL instance for the database.\nOption B suggests implementing Cross-Zone support in the application. This approach is used to distribute traffic evenly across all instances in all AZs. However, it does not address the issue of the application server needing to know the clients' IP address.\nOption C suggests implementing Latency-Based Routing support in the application. This approach is used to route traffic to the AWS region that provides the lowest latency for the client. However, it does not address the issue of the application server needing to know the clients' IP address.\nOption D suggests implementing Alias Resource Support in the application. This approach is used to route traffic to an Amazon S3 bucket or another AWS resource by using a friendly DNS name. However, it does not address the issue of the application server needing to know the clients' IP address.\nIn conclusion, option A is the best answer as it addresses the main issue of the application server needing to know the clients' IP address and enables the use of a multi-AZ RDS MySQL instance for the database.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "File a change request to implement Proxy Protocol Support in the application. Use an ELB with a TCP Listener and Proxy Protocol enabled to distribute the load on two application servers in different AZs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "File a change request to Implement Cross-Zone support in the application. Use an ELB with a TCP Listener and Cross-Zone Load Balancing enabled to distribute the load on two application servers in different AZs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "File a change request to implement Latency Based Routing support in the application. Use Route 53 with Latency Based Routing enabled to distribute the load on two application servers in different AZs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "File a change request to implement Alias Resource Support in the application. Use a Route 53 Alias Resource Record to distribute the load on two application servers in different AZs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 117,
  "query" : "You have an application that analyzes images within files.\nFor each of the files in the input stream, the application writes to some number of output files.\nThe number of input files processed each day is high and concentrated within a few hours of the day.",
  "answer" : "Answer - A.\nThe scenario in this question is that (a) there are EC2 instances that need to process a high number of input files, (b) currently the processing takes 20 hrs a day, which needs to be reduced, (c) the availability needs to be improved.\nFirst, let's see whether we should choose S3 or EBS with PIOPS.\nIt appears that all the options have auto-scaling in common.\nThere will be multiple EC2 instances working in parallel on the input data.\nThis should reduce the overall elaboration time, satisfying one of the requirements.\nYou can enable Multi-Attach on Amazon EBS Provisioned IOPS io1 volumes to allow a single volume to be concurrently attached to up to sixteen AWS Nitro System-based Amazon Elastic Compute Cloud (Amazon EC2) instances within the same Availability Zone which does not guarantee the availability.\nWhereas S3 provides high availability, which satisfies the other requirements.\nSecond, SQS is a great option to do autonomous tasks and can queue service requests, and can be scaled to meet the high demand.\nSNS is a mere notification service and would not hold the tasks.\nHence, SQS is certainly the correct choice.\nOption A is CORRECT because, as mentioned above, it provides high availability and can store a massive amount of data.\nAuto-scaling of EC2 instances reduces the overall processing time and SQS helps distribute the commands/tasks to EC2 instances.\nOption B is incorrect because, as mentioned above, neither EBS nor SNS is a valid choice in this scenario.\nOption C is incorrect because, as mentioned above, SNS is not a valid choice in this scenario.\nOption D is incorrect because, as mentioned above, EBS is not a valid choice in this scenario.\nThe best answer for this scenario is option A. Here is a detailed explanation of why:\nOption A:\nUse S3 to store the files: S3 is a highly scalable and durable object storage service that can handle large volumes of data, making it a good choice for storing files.\nUse SQS to store the elaboration commands: SQS is a fully managed message queuing service that decouples the components of a cloud application, allowing them to run independently and scale quickly. It is a good choice for storing the commands needed for processing the image files.\nConfigure an Auto Scaling group to process the messages in the queue and dynamically adjust the size of the group depending on the length of the SQS queue: Auto Scaling enables you to automatically scale out or in the number of EC2 instances based on the demand. By configuring an Auto Scaling group to process the messages in the SQS queue, the number of EC2 instances will scale dynamically based on the length of the queue, ensuring that the processing of the image files is efficient and cost-effective.\nOption B:\nUse EBS with Provisioned IOPS (PIOPS) to store I/O files: EBS is a block-level storage service that provides high-performance storage volumes for EC2 instances. Provisioned IOPS is a type of EBS volume that delivers predictable and consistent performance. However, EBS volumes have a limit on the amount of IOPS they can deliver, and they can become a bottleneck if the I/O workload is too high.\nUse SNS to distribute elaboration commands to a group of hosts working in parallel: SNS is a fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. It is a good choice for distributing the commands needed for processing the image files.\nAuto Scaling to dynamically size the group of hosts depending on the number of SNS notifications: By configuring an Auto Scaling group to process the SNS notifications, the number of EC2 instances will scale dynamically based on the number of notifications.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use S3 to store the files. Use SQS to store the elaboration commands. Configure an Auto Scaling group to process the messages in the queue and dynamically adjust the size of the group depending on the length of the SQS queue.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use EBS with Provisioned IOPS (PIOPS) to store I/O files. Use SNS to distribute elaboration commands to a group of hosts working in parallel and Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use S3 to store I/O files and SNS to distribute elaboration commands to a group of hosts working in parallel. Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use EBS with Provisioned IOPS (PIOPS) to store I/O files. Use SQS to distribute elaboration commands to a group of hosts working in parallel. Use Auto Scaling to dynamically size the group of hosts depending on the length of the SQS queue.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 118,
  "query" : "You require the ability to analyze a customer's clickstream data on a website to do the behavioral analysis.\nYour customer needs to know what sequence of pages and ads their customer clicked on.\nThis data will be used in real-time to modify the page layouts as customers click through the site to increase stickiness and advertising click-through.\nWhich option meets the requirements for capturing and analyzing this data?",
  "answer" : "Answer - B.\nWhenever the question presents a scenario where the application needs to analyze real-time data such as clickstream (i.e.massive real-time data analysis), most of the time the best option is Amazon Kinesis.\nIt is used to collect and process large streams of data records in real-time.\nYou'll create data-processing applications, known as Amazon Kinesis Streams applications.\nA typical Amazon Kinesis Streams application reads data from an Amazon Kinesis stream as data records.\nThese applications can use the Amazon Kinesis Client Library, and they can run on Amazon EC2 instances.\nThe processed records can be sent to dashboards, used to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services.\nThe below diagrams from the AWS documentation shows how you can create custom streams in Amazon Kinesis.\nRefer to page 129 on the below link.\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-dg.pdf\nApplication Name.\nThe KCL requires an application name that is unique across your applications and across Amazon DynamoDB tables in the same Region.\nIt uses the application name configuration value in the following ways: • All workers associated with this application name are assumed to be working together on the same stream.\nThese workers may be distributed on multiple instances.\nIf you run an additional instance of the same application code, but with a different application name, the KCL treats the second instance as an entirely separate application that is also operating on the same stream.\n• The KCL creates a DynamoDB table with the application name and uses the table to maintain state information (such as checkpoints and worker-shard mapping) for the application.\nEach application has its own DynamoDB table.\nFor more information on Kinesis, please visit the below link-\nhttp://docs.aws.amazon.com/streams/latest/dev/introduction.html\nThe scenario described in the question involves analyzing clickstream data in real-time to understand the behavior of website visitors and modify page layouts and ads to increase user engagement and click-through rates. To meet these requirements, we need a solution that can capture and process the clickstream data in real-time, store it securely, and enable analysis using appropriate tools.\nLet's review each of the answer options in detail:\nOption A: Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce.\nThis option involves capturing clickstream data in weblogs and storing it in Amazon S3, a scalable and durable object storage service offered by AWS. Then, the data is analyzed using Elastic MapReduce (EMR), which is a fully managed big data processing service that uses Hadoop and Spark to process large amounts of data. This option requires batch processing of the data, which means that the analysis may not be real-time. Moreover, weblogs typically capture a large amount of data, including all HTTP requests, which can be challenging to process and analyze. Hence, this option may not be suitable for real-time analysis of clickstream data.\nOption B: Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers.\nThis option involves capturing clickstream data in real-time and processing it using Amazon Kinesis, a fully managed streaming data service offered by AWS. Kinesis enables real-time processing of streaming data and provides various tools for analysis, including Kinesis Data Analytics and Kinesis Data Firehose. The data is pushed to Kinesis by session, which means that it can be analyzed in near-real-time, providing insights into customer behavior and enabling real-time modification of page layouts and ads. Kinesis also allows for scaling up or down based on the volume of incoming data, ensuring that the solution can handle spikes in traffic. This option is suitable for real-time analysis of clickstream data and meets the requirements specified in the scenario.\nOption C: Write the click events directly to Amazon Redshift and then analyze with SQL.\nThis option involves capturing clickstream data and storing it directly in Amazon Redshift, a fully managed data warehouse service offered by AWS. Redshift is optimized for querying large amounts of data using SQL and provides various tools for analysis, including Amazon QuickSight and third-party BI tools. However, this option may not be suitable for real-time analysis of clickstream data as it involves writing data directly to the data warehouse, which can be slow and may not provide real-time insights into customer behavior. Moreover, Redshift is optimized for batch processing, which means that it may not be suitable for real-time analysis of streaming data.\nOption D: Publish web clicks by session to an Amazon SQS queue, periodically drain these events to Amazon RDS, and analyze with SQL.\nThis option involves capturing clickstream data and publishing it to an Amazon SQS (Simple Queue Service) queue, which is a fully managed message queuing service offered by AWS. Then, the data is periodically drained to Amazon RDS (Relational Database Service), a fully managed relational database service offered by AWS, and analyzed using SQL. This option requires batch processing of the data, which means that it may not provide real-time insights into customer behavior. Moreover, SQS is designed for asynchronous messaging, which may not be suitable for real-time analysis of streaming data.\nIn summary, option B is the most suitable option for capturing and analyzing clickstream data in real-time. It involves using Amazon Kinesis to capture and process streaming data, which enables real-time analysis of customer behavior and modification of page layouts and ads in real-time. Options A, C, and D may be suitable for batch processing of clickstream data but may not provide real-time insights into customer behavior.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Write the click events directly to Amazon Redshift and then analyze with SQL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Publish web clicks by session to an Amazon SQS queue, periodically drain these events to Amazon RDS, and analyze with SQL.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 119,
  "query" : "An AWS customer runs a public blogging website.\nThe site users upload two million blog entries a month.\nThe average blog entry size is 200 KB.\nThe access rate to blog entries drops to negligible 6 months after publication, and users rarely access a blog entry 1 year after publication.\nAdditionally, blog entries have a high update rate during the first 3 months following publication.\nThis drops to no updates after 6 months.\nThe customer wants to use CloudFront to improve his user's load times.\nWhich of the following recommendations would you make to the customer?",
  "answer" : "Answer - C.\nCloudFront allows you to configure caching based on a URL path pattern when you create a new distribution.\nBy partitioning the S3 bucket by the month, the blog post was created.\nYou can control the CloudFront caching behavior of the distribution to optimize cost.\nRefer to page 46 on the below link under the section \"Cache Behaviour Settings.\"\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AmazonCloudFront_DevGuide.pdf\nThe scenario here is that (a) blogs have high access/updates rate in the first 3 months of their creation, (b) this rate drops after 6 months.\nThe main architectural consideration is that the user's load time of the blog needs to be improved.\nThis question is based on making the best use of CloudFront's Cache Behavior.\nYou need to understand two things about CloudFront for such scenario: (1) CloudFront is a service that is designed to give geographically distributed users fast access to the content by maintaining the content in the cache that is maintained at multiple edge locations, and (2) using the cache-behavior of CloudFront, you can control the origin and path of the content, time to live (TTL), and control the user access using trusted signers.\nIn this scenario, you need to control the content based on the time period at which the blog is published.\ni.e., when a blog is published, you need to cache the update for the first 3 months so that the users can quickly access it.\nAfter six months from the update, the content can be removed from the cache, as it is rarely accessed.\nAlso, you need to make sure that the CloudFront only accesses the content.\nOption A is incorrect because maintaining two separate buckets will not improve the load time for the users.\nOption B is incorrect as the location-wise distribution will not improve the load time for the users.\nOption C is CORRECT because if (a) the content is only accessed by CloudFront, and (b) if the content is partitioned at the origin based on the month it was uploaded.\nYou can control the cache behavior accordingly and keep only the latest updated content in the CloudFront cache so that it can be accessed with fast load-time, hence, improving the performance.\nFor the contents that are over 6 months, you do not need to put them in the cache.\nOption D is incorrect.\nThe scenario states that the customer is running a public access blogging website.\nSo there is no need to restrict viewer access.\nFor more information on Cloudfront identity, please visit the below links-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesCacheBehavior http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\nOption C is the recommended solution in this scenario.\nExplanation:\nOption A: Duplicating entries into two different buckets and creating two separate CloudFront distributions may increase complexity and cost. This approach is not necessary in this scenario and may lead to higher costs for storage and data transfer.\nOption B: Creating two separate CloudFront distributions for different users is not necessary in this scenario. The access rate to blog entries drops to negligible 6 months after publication, and users rarely access a blog entry 1 year after publication, which means the distribution should be able to handle traffic from all regions.\nOption C: Creating a CloudFront distribution with S3 access restricted only to the CloudFront identity and partitioning the blog entry's location in S3 according to the month it was uploaded is the recommended solution in this scenario. By partitioning the blog entries, CloudFront behaviors can be used to set different caching and TTL rules based on the upload month, allowing CloudFront to cache frequently accessed objects and reduce the number of requests to S3. Restricting access to S3 only through CloudFront ensures that objects are accessed only through CloudFront and not directly through S3, which provides better security and control.\nOption D: Creating a CloudFront distribution with \"Restrict Viewer Access Forward Query string set to true and minimum TTL of 0\" is not necessary in this scenario. This option is used when there is a need to restrict access to specific objects or when there is a need for real-time updates to the content. In this scenario, the access rate drops after six months, and there is no need for real-time updates, so this option is not recommended.\nTherefore, the best solution for this scenario is to create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry's location in S3 according to the month it was uploaded to be used with CloudFront behaviors.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Duplicate entries into two different buckets and create two separate CloudFront distributions where S3 access is restricted only to Cloud Front identity.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution with “US’Europe price class for US/Europe users and a different CloudFront distribution with All Edge Locations’ for the remaining users.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry’s location in S3 according to the month it was uploaded to be used with CloudFront behaviors.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution with Restrict Viewer Access Forward Query string set to true and minimum TTL of 0.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 120,
  "query" : "Your company is getting ready to make a major public announcement of a social media site on AWS.\nThe website runs on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL Extra Large DB Instance.\nThe site performs a high number of small reads and writes per second and relies on an eventual consistency model.",
  "answer" : "Answer - A and D.\nOne of AWS pillars of a well-architected framework is cost optimization which incorporates \"Right Sizing\"\nAWS defines right Sizing using the lowest cost resource that still meets the technical specifications of the specific workload.\nOptions B and C do not meet those AWS standards.\nOption A is CORRECT because ElastiCache is an in-memory caching solution that reduces the load on the database and improves the read performance.\nOption B is INCORRECT because splitting the RDS into multiple databases increases both the read and write.\nTherefore wasting money on increased write capacity is not required.\nAlso, Sharding increases the maintenance overhead as now multiple databases must be maintained and the application must be refactored to incorporate the additional connection strings.\nOption C is INCORRECT because this increases both read and write.\nTherefore wasting money on increased write capacity which is not required.\nOption D is CORRECT because Read Replicas are used to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\nHence, improving the read performance.\nSee more information on Read Replicas and ElastiCache below.\nRead Replicas.\nAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis replication feature makes it easy to elastically scale out beyond a single DB Instance's capacity constraints for read-heavy database workloads.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.\nFor more information on Read Replica's, please visit the below link:\nhttps://aws.amazon.com/rds/details/read-replicas/\nElastiCache.\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.\nThe service improves web applications' performance by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.\nFor more information on Amazon ElastiCache, please visit the below link-\nhttps://aws.amazon.com/elasticache/\nThe scenario described above involves a social media site running on EC2 instances in multiple availability zones with a Multi-AZ RDS MySQL Extra Large DB Instance. The site performs a high number of small reads and writes per second and relies on an eventual consistency model. The question asks which of the following options would be the best solution to improve the performance and scalability of the site.\nA. Deploy ElasticCache in-memory cache running in each availability zone. ElasticCache is an in-memory caching service that can be used to improve the performance and scalability of web applications. By deploying ElasticCache in each availability zone, the application can cache frequently accessed data closer to the application, reducing latency and improving performance. However, since ElasticCache is eventually consistent, it may not be the best solution for applications that require strong consistency.\nB. Implement sharding to distribute the load to multiple RDS MySQL instances. Sharding involves dividing the data into multiple smaller data sets, each of which is stored on a separate RDS MySQL instance. This can help distribute the load and improve performance. However, sharding can be complex and requires careful planning and management. In addition, it may not be the best solution for applications that require strong consistency.\nC. Increase the RDS MySQL Instance size and implement provisioned IOPS. Increasing the RDS MySQL instance size and implementing provisioned IOPS can improve the performance of the database by providing more processing power and storage resources. Provisioned IOPS ensures that the database can handle the high number of reads and writes by providing a predictable level of performance. This option is simple and can be implemented quickly, but it may not be the best solution for applications that require strong consistency.\nD. Add an RDS MySQL read replica in each availability zone. Adding RDS MySQL read replicas in each availability zone can improve performance by allowing reads to be distributed across multiple database instances. This can reduce latency and improve response times for read-intensive workloads. However, it may not be the best solution for applications that require strong consistency, as the replicas may lag behind the primary database in terms of consistency.\nOverall, the best option will depend on the specific requirements of the application. If eventual consistency is acceptable, option A may be the best solution. If strong consistency is required, options C or D may be more appropriate. Option B may also be a viable solution, but it requires careful planning and management.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy ElasticCache in-memory cache running in each availability zone.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement sharding to distribute the load to multiple RDS MySQL instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Increase the RDS MySQL Instance size and implement provisioned IOPS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add an RDS MySQL read replica in each availability zone.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 121,
  "query" : "A company runs a batch analysis with transactional reporting every hour on its main transactional DB running on an RDS MySQL instance to populate its central Data Warehouse running on Redshift.\nDuring the execution of the batch, their transactional applications are very slow.\nWhen the batch completes, they need to update the top management dashboard with the new data.\nThe dashboard is produced by another system running on-premises that is currently started when a manually-sent email notifies that an update is required.\nThe on-premises system cannot be modified because it is managed by another team.",
  "answer" : "Answer - C.\nThere are two architectural considerations here.\n(1) you need to improve read performance by reducing the load on the RDS MySQL instance, and (2) automate the process of notifying to the on-premise system.\nWhen the scenario asks you to improve a DB instance's read performance, always look for options such as ElastiCache or Read Replicas.\nAnd when the question asks you to automate the notification process, always think of using SNS.\nOptions A and B are incorrect because replacing RDS with Redshift may need many changes on the applications and are not required in this scenario.\nOption C is CORRECT because (a) it uses Read Replicas which improves the read performance, and (b) it uses SNS which automates the process of notifying the on-premise system to update the dashboard.\nOption D is incorrect because SQS is not a service to be used for sending the notification.\nFor more information on Read Replica's, please visit the below link-\nhttps://aws.amazon.com/rds/details/read-replicas/\nThe problem statement describes that the batch analysis with transactional reporting every hour on the main transactional DB running on an RDS MySQL instance is causing slow performance on the transactional applications. After completion of the batch, the central Data Warehouse running on Redshift needs to be updated, and the top management dashboard needs to be notified.\nTo address the issue, we need to ensure that the batch analysis doesn't affect the performance of the transactional applications, and the top management dashboard is updated with the new data without modifying the on-premises system.\nOption A suggests replacing RDS with Redshift for the batch analysis and using SNS to notify the on-premises system to update the dashboard. This option is not recommended because Redshift is designed for analytics workloads, not transactional workloads. Replacing RDS with Redshift could lead to poor transactional performance and may not address the slow performance issue.\nOption B suggests replacing RDS with Redshift for the batch analysis and using SQS to send a message to the on-premises system to update the dashboard. This option is better than option A, but it still has the same issues as option A, which is using Redshift for a transactional workload.\nOption C suggests creating an RDS Read Replica for the batch analysis and using SNS to notify the on-premises system to update the dashboard. This option is better than options A and B because RDS Read Replicas are designed for read-heavy workloads and will not impact the performance of the transactional applications. SNS can be used to notify the on-premises system to update the dashboard once the batch analysis is completed.\nOption D suggests creating an RDS Read Replica for the batch analysis and using SQS to send a message to the on-premises system to update the dashboard. This option is similar to option C, but instead of using SNS to notify the on-premises system, SQS is used to send a message. This option is also valid, but it requires more configuration and maintenance than option C.\nIn summary, the recommended option is either option C or D. Creating an RDS Read Replica for the batch analysis is the best approach to avoid impacting the performance of the transactional applications. SNS or SQS can be used to notify the on-premises system to update the dashboard.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Replace RDS with Redshift for the batch analysis and SNS to notify the on-premises system to update the dashboard.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Replace RDS with Redshift for the batch analysis and SQS to send a message to the on-premises system to update the dashboard.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an RDS Read Replica for the batch analysis and SNS to notify the on-premises system to update the dashboard.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an RDS Read Replica for the batch analysis and SQS to send a message to the on-premises system to update the dashboard.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 122,
  "query" : "You are implementing a URL whitelisting system for a company that wants to restrict outbound HTTPS connections to specific domains from their EC2-hosted applications.\nYou deploy a single t2.micro EC2 instance running proxy software and configure it to accept traffic from all subnets and EC2 instances in the VPC.\nYou configure the proxy to only pass through traffic to domains that you define in its whitelist configuration.",
  "answer" : "Answer: A\nThis scenario contains the following main points: (1) there is a single EC2 instance running proxy software that either acts as or connects to a NAT instance.\nThe NAT instances are not AWS managed; they are user-managed; so, it may become the bottleneck, (2) there is a whitelist maintained so that limited outside access is given to the instances inside VPC, (3) the URLs in the whitelist are correctly maintained.\nHence, whitelist is not an issue, (4) only some machines are having download problems with some updates.\ni.e.\nsome updates are successful on some machines.\nThis indicates that there is no setup issue, but most likely, it is the proxy instance that is a bottleneck and under-performing or inconsistently performing.\nAs the proxy instance is not part of an auto-scaling group, its size must be definitely the issue.\nOption A is CORRECT because due to the limited size of the proxy instance, its network throughput might not be sufficient to provide service to all the VPC instances (as only some of the instances are not able to download the updates).\nOption B is incorrect because limited storage on the proxy instance should not cause other instances of any problems in downloading the updates.\nOption C is incorrect because proxy instances are supposed to be in a public subnet, but the allocation of EIPs should not cause any issues for other instances in the VPC.Option D is incorrect because none of the instances would get the updates if this were the case.\nHowever, some of the instances could get the updates.\nSo this cannot be the case.\nThe correct answer to this question is D. The route table for the subnets containing the affected EC2 instances is not configured to direct network traffic for the software update locations to the proxy.\nThe scenario presented in the question is about implementing a URL whitelisting system to restrict outbound HTTPS connections to specific domains from EC2-hosted applications. To achieve this, a proxy software is deployed on a t2.micro EC2 instance and configured to only pass through traffic to domains defined in its whitelist configuration.\nOne possible issue with this setup is that the software updates for the EC2 instances might fail. To understand why, we need to look at the network traffic flow.\nWhen an EC2 instance needs to download a software update, it sends a request to the Internet Gateway (IGW) to reach the update server. The IGW routes the request to the appropriate update server and sends back the response to the EC2 instance. In the current setup, the proxy instance is the only EC2 instance that is allowed to connect to the update server, so the request from the affected EC2 instance needs to be routed through the proxy.\nHowever, if the route table for the subnets containing the affected EC2 instances is not configured to direct network traffic for the software update locations to the proxy, the requests will bypass the proxy and fail. This is because the EC2 instances will send the requests directly to the IGW instead of going through the proxy.\nTherefore, the correct answer is D, which suggests that the route table for the subnets containing the affected EC2 instances needs to be configured to direct network traffic for the software update locations to the proxy. This will ensure that all requests from the EC2 instances to the update server go through the proxy and are allowed only if the domain is in the whitelist configuration.\nAnswers A, B, and C are not relevant to the issue presented in the scenario. Answer A talks about network throughput issues, which are not related to the problem of software updates failing. Answer B mentions storage allocation, which is not relevant to network traffic routing. Answer C talks about EIP allocation, which is also not relevant to network traffic routing.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You are running the proxy on an undersized EC2 instance type. So network throughput is not sufficient for all instances to download their updates in time.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You have not allocated enough storage to the EC2 instance running the proxy. So the network buffer is filling up causing some requests to fail.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You are running the proxy in a private subnet but have not allocated enough EIP’s to support the needed network throughput through the Internet Gateway (IGW).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The route table for the subnets containing the affected EC2 instances is not configured to direct network traffic for the software update locations to the proxy.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 123,
  "query" : "To serve Web traffic for a popular product, your chief financial officer and the IT director have purchased 10 large Reserved Instances (RIs), evenly spread across two availability zones.\nRoute 53 is used to deliver the traffic to an Elastic Load Balancer (ELB)\nAfter several months, the product grows even more popular, and you need additional capacity.\nAs a result, your company purchases two c5.2xlarge RI.\nYou register the two c5.2xlarge instances with your ELB and quickly find while all the instances are at 100% of their capacity, the c5.2xlarge instances have a significant capacity that is unused.\nWhich of the following is the most cost-effective solution that uses EC2 capacity most effectively?",
  "answer" : "Correct Answer - A.\nIn this question, the problem is that the newly added c5.2xlarge instances are not fully utilized.\nThis is happening because the load is spread evenly across all the instances.\nThere is no logic to determine how much traffic is to be routed to which instance types.\nHence, there is a need to add some logic where higher (more-weighted) traffic should be routed to c5.2xlarge instances and light-weighted to the other instances.\nRoute 53's weighted routing policy does exactly like this, so you should look for this option.\nOption A is Correct because it first creates separate ELBs, one each for the set of different instance types and uses Route 53's weighted routing policy such that a higher proportion of the load is routed to the ELB that has c5.2xlarge instances and a smaller proportion to the one with smaller instances.\nOption B is incorrect because shutting down c5.2xlarge instances will not effectively use the EC2 capacity.\nYou have already paid for the instance so that you would lose money here.\nOption C is incorrect because latency-based routing may not always distribute heavy traffic to a large instance.\nYou must use a weighted routing policy.\nOption D is incorrect because this option is not a good use of the existing capacity and would add to the cost.\nFor more information on Route 53 weighted routing policy, please visit the URL below.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\nThe most cost-effective solution that uses EC2 capacity most effectively would be option D. Here's why:\nOption A: Use a separate ELB for each instance type and distribute the load to ELBs with Route53 Weighted Routing. This option would require creating a separate ELB for each instance type, which would add complexity to the architecture. Additionally, the Weighted Routing policy only allows traffic to be distributed based on a percentage, so it wouldn't allow for fine-grained control over which instances receive which traffic. This solution could lead to underutilization of instances and potentially higher costs.\nOption B: Configure AutoScaling group and Launch Configuration with ELB to add up to 10 more on-demand large instances when triggered by Cloudwatch shut off c5.2xlarge instances. This option involves using an Auto Scaling group to add more instances when needed. However, the group is set to only launch large instances, which means that the c5.2xlarge instances would not be utilized. Additionally, shutting off the c5.2xlarge instances may not be the most efficient way to manage capacity, as it could lead to uneven distribution of traffic across the remaining instances.\nOption C: Route traffic to EC2 large and c5.2xlarge instances directly. Using Route 53 latency-based routing and health checks shut off EL. This option would require routing traffic directly to each instance type, which would add complexity to the architecture. Additionally, shutting off the ELB may not be the most efficient way to manage capacity, as it could lead to uneven distribution of traffic across the remaining instances.\nOption D: Configure ELB with two c5.2xlarge Instances and use the on-demand AutoScaling group for up to two additional c5.2xlarge instances. This option involves configuring the ELB to use the c5.2xlarge instances and using an Auto Scaling group to add more c5.2xlarge instances when needed. This solution would allow for fine-grained control over which instances receive which traffic and would ensure that all instances are being utilized. Additionally, using an Auto Scaling group with on-demand instances would provide flexibility to handle fluctuations in traffic without over-provisioning.\nIn conclusion, option D is the most cost-effective solution that uses EC2 capacity most effectively, as it allows for fine-grained control over which instances receive which traffic, ensures that all instances are being utilized, and provides flexibility to handle fluctuations in traffic without over-provisioning.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a separate ELB for each instance type and distribute the load to ELBs with Route53 Weighted Routing.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure AutoScaling group and Launch Configuration with ELB to add up to 10 more on-demand large instances when triggered by Cloudwatch shut off c5.2xiarge instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Route traffic to EC2 large and c5.2xlarge instances directly. Using Route 53 latency based routing and health checks shut off EL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure ELB with two c5.2xlarge Instances and use the on-demand AutoScaling group for up to two additional c5.2xlarge instances.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 124,
  "query" : "A read-only news reporting site has a web application tier in EC2 and a database tier in RDS.\nThe website receives large and unpredictable read traffic and automatically responds to these traffic fluctuations.\nWhich of the following AWS services are the most suitable to meet the requirements?",
  "answer" : "Answer - A.\nThe scenario asks for 2 things: (1) a performance improvement solution for read-heavy web tier and database tier.\nHint: Always see if any of the options contain caching solutions such as ElastiCache, CloudFront, or Read Replicas, and (2) whether to use stateless or stateful instances.\nStateful instances are not suitable for distributed systems, as they retain the state of connection between client and web server.\nAnd the database remains engaged as long as the session is active.\nHence, it increases the load on the server as well as the database.\nStateless instances, however, are distributed and easy to scale in/scale-out.\nHence, the stateless application tends to improve the performance of a distributed application.\nOption A is CORRECT because (a) it uses stateless instances, (b) the webserver uses ElastiCache for read operations, (c) it uses CloudWatch which monitors the fluctuations in the traffic and notifies to the auto-scaling group to scale in/scale-out accordingly, and (d) it uses read replicas for RDS to handle the read-heavy workload.\nOption B is incorrect because (a) it uses stateful instances, and (b) it does not use any caching mechanism for web and application tier.\nOption C is incorrect because (a) it uses stateful instances, (b) it does not use any caching mechanism for web and application tier, and (c) multi-AZ RDS does not improve read performance.\nOption D is incorrect because multi-AZ RDS does not improve read performance.\nFor more information on ElastiCache and Read Replicas, please refer to the following links-\nhttps://aws.amazon.com/elasticache/ https://aws.amazon.com/rds/details/read-replicas/\nThe most suitable solution for a read-only news reporting site that receives large and unpredictable read traffic with an EC2 web application tier and an RDS database tier is A. Stateless instances for the web application tier in an auto-scaling group monitored with CloudWatch, RDS configured with read replicas, and adding Amazon ElastiCache for RDS.\nExplanation:\nA read-only news reporting site that receives large and unpredictable read traffic requires a scalable and highly available architecture to meet these traffic fluctuations. Therefore, the solution must have a load-balancing mechanism to distribute traffic to multiple instances and handle traffic spikes automatically. The site is read-only, so write traffic is not a concern. The proposed solution includes:\n1. Stateless instances for the web application tier in an auto-scaling group monitored with CloudWatch:\nStateless instances are ideal for web applications as they can handle traffic from multiple sources and scale horizontally to handle traffic spikes automatically. Auto-scaling group automatically scales the number of instances based on the traffic to ensure that there is always enough capacity to handle the incoming traffic. CloudWatch monitors the instances and triggers the auto-scaling group to add or remove instances based on the pre-defined metrics, such as CPU utilization or network traffic.\n1. RDS configured with read replicas:\nRDS configured with read replicas provides high availability and read scalability to the database tier. Read replicas are copies of the primary database instance that can handle read traffic, offloading the primary database instance and reducing the load on it. This architecture also ensures that read traffic is handled by the read replicas, and write traffic is handled by the primary database instance, improving the overall performance of the system.\n1. Amazon ElastiCache for RDS:\nAmazon ElastiCache for RDS provides a caching layer that can improve the performance of read-heavy workloads. The caching layer stores frequently accessed data in memory, reducing the need to access the database tier for every read request. This architecture improves the overall performance of the system, reduces the load on the database tier, and improves the user experience.\nOption B is incorrect because using stateful instances for the web application tier can cause issues with scaling and availability. Stateful instances store session data, which can make it difficult to scale the application horizontally, and it can also result in data loss if the instance fails.\nOption C is incorrect because RDS with Multi-AZ does not provide read scalability to the database tier. Multi-AZ provides high availability and failover protection, but it does not distribute read traffic to multiple instances.\nOption D is incorrect because using stateless instances for the web application tier with Multi-AZ RDS can cause issues with availability. Multi-AZ provides high availability and failover protection, but it does not distribute read traffic to multiple instances, which can cause performance issues during traffic spikes.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Stateless instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS configured with read replicas. Add Amazon ElastiCache for RDS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Stateful instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS with read replicas.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Stateful instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS with Multi-AZ.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Stateless instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS with Multi-AZ.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 125,
  "query" : "You are running a news website in the EU-west-1 region that updates every 15 minutes.\nThe website has a worldwide audience.\nIt uses an Auto Scaling group behind an Elastic Load Balancer and an Amazon RDS database.\nStatic content resides on Amazon S3 and is distributed through Amazon CloudFront.\nYour Auto Scaling group is set to trigger a scale-up event at 60% CPU utilization.\nYou use an Amazon RDS extra large DB instance with 10,000 Provisioned IOPS.\nIts CPU utilization is around 80%\nFreeable memory is in the 2 GB range.\nWeb analytics reports show that the average load time of your web pages is around 1.5 to 2 seconds, but your SEO consultant wants to bring down the average load time to under 0.5 seconds.\nWhich of the following option would NOT help to improve page load times for your users?",
  "answer" : "Answer - A.\nIn this scenario, there are three major points of consideration.\n(1) news website updates every 15 minutes,\n(2) current average load time is high, and.\n(3) the performance of the website should be improved (i.e., read performance needs improvement).\nWhen the questions ask for performance improvement solutions for read-heavy applications, always see if any of the options contain caching solutions such as ElastiCache, CloudFront, or Read Replicas.\nOption A (why option A is correct): Lowering scale up trigger of ASG will not help to improve the page load time of users.\nOption B is INCORRECT because using an ElastiCache for storing sessions as well as for frequent DB queries would reduce the load on the database.\nThis would help to increase the read performance.\nSince the question is asking for a NOT recommended option, this option is INCORRECT.\nOption C is incorrect because it uses CloudFront, a network of globally distributed \"edge-locations\" that caches the content and improves user experience.\nOption D is incorrect because scaling up the RDS read replicas will help improve read performance.\nThe correct answer is A. Lower the scale-up trigger of your Auto Scaling group to 30%, so it scales more aggressively.\nExplanation:\nTo improve page load times for your users, you need to focus on reducing the latency of your application. Latency can be reduced by reducing the round-trip time (RTT) between the user's browser and the web server.\nOption A: Lower the scale-up trigger of your Auto Scaling group to 30%, so it scales more aggressively. This option would not help to improve page load times for your users because it would cause the Auto Scaling group to spin up additional instances even when they are not needed. This could result in unnecessary cost and complexity without any significant improvement in latency.\nOption B: Add an Amazon ElastiCache caching layer to your application for storing sessions and frequent DB queries. ElastiCache can be used to cache frequently accessed data and reduce the number of round-trips to the RDS database. By reducing the RTT, ElastiCache can help to improve the page load times for your users.\nOption C: Configure Amazon CloudFront dynamic content support to enable caching of re-usable content from your site. CloudFront can cache frequently accessed content and serve it from edge locations around the world, reducing the RTT and improving page load times for users. This option can also help to reduce the load on the origin server.\nOption D: Configure read replicas for RDS Database Instance. Read replicas can be used to offload read traffic from the primary RDS instance and reduce the load on it. By reducing the load on the primary instance, read replicas can help to improve the page load times for your users.\nIn conclusion, option A would not help to improve page load times for your users, while options B, C, and D can all help to reduce the latency and improve page load times.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Lower the scale-up trigger of your Auto Scaling group to 30%, so it scales more aggressively.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Add an Amazon ElastiCache caching layer to your application for storing sessions and frequent DB queries.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure Amazon CloudFront dynamic content support to enable caching of re-usable content from your site.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure read replicas for RDS Database Instance.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 126,
  "query" : "A large real-estate brokerage is exploring the option of adding a cost-effective location-based alert to their existing mobile application.\nThe application backend infrastructure currently runs on AWS.\nUsers who opt for this service will receive alerts on their mobile devices regarding real-estate offers in proximity to their location.\nFor the alerts to be relevant, delivery time needs to be in the low minute count.\nThe existing mobile app has 5 million users across the US.\nWhich one of the following architectural suggestions would you make to the customer?",
  "answer" : "E.\nF.\nAnswer - C.\nThe scenario has the following architectural considerations: (1) the users should get notifications about the real estate in the area near to their location, (2) only subscribed users should get the notification, (3) the notification delivery should be fast, (4) the architecture should be scalable, and (5) it should be cost-effective.\nWhen the question has considerations for scalability, always think about DynamoDB as it is the most recommended database solution to handle the huge amount of data/records.\nFor automated notifications, always think about SNS.\nOption A is incorrect because sending notifications via mobile earners/device providers as alerts is neither feasible nor cost-effective.\nOption B is incorrect because receiving location via Direct Connect and carrier connection is not cost-effective.\nIt also does not deal with subscriptions.\nSending notifications via mobile carriers as the alert is not cost-effective as well.\nOption C is CORRECT because (a) SQS is a highly scalable, cost-effective solution for carrying out utility tasks such as holding the location of millions of users, (b) it uses highly scalable DynamoDB, and (c) it uses the cost-effective AWS SNS Mobile Push service to send push notification messages directly to apps on mobile devices.\nOption D is incorrect because the AWS SNS Mobile Push service is used to send push notification messages to the mobile devices, not to get the mobile devices' location.\nFor more information on AWS SNS Mobile Push, please see the diagram and link given below:\nhttps://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html\nNote:\nOption C says that the mobile application will send the device location to the processing EC2 instances using SQS.\nThen the instances would look at the DynamoDB database for offers relevant to the location.\nThen finally, SNS Mobile Push, which is part of SNS, will be used to send offers to the mobile application.\nSo it leverages both SQS as well as SNS functionality for different parts of the architecture.\nThis is the correct solution to this problem.\nBased on the scenario described, we need to design an architecture that enables the mobile application to receive alerts in real-time regarding real-estate offers in proximity to their location. The solution should be cost-effective and able to handle a large number of users (5 million) across the US.\nLet's analyze each of the architectural suggestions provided and their suitability for the scenario:\nA. The mobile application will submit its location details to a web service endpoint using ELB and EC2 instances. DynamoDB will be used to store and retrieve the relevant offers from EC2 instances, which would then communicate with mobile carriers or device providers to push alerts back to the mobile application.\nThis solution involves using ELB and EC2 instances to receive location details from the mobile app and storing and retrieving relevant offers using DynamoDB. While this solution can handle a large number of users, it involves additional communication with mobile carriers or device providers to push alerts back to the mobile app, which could be complex and costly to set up.\nB. Use AWS DirectConnect or VPN to establish connectivity with the mobile carrier. EC2 instances will receive the mobile application's location details through the mobile carrier. RDS will be used to store and retrieve the relevant offers. EC2 instances will communicate with the mobile carrier to push alerts back to the mobile application.\nThis solution involves using AWS DirectConnect or VPN to establish connectivity with the mobile carrier and using EC2 instances to receive location details from the mobile app. It also involves using RDS to store and retrieve relevant offers. While this solution may provide more secure communication with the mobile carrier, it could be complex and costly to set up.\nC. SQS ( buffer storage ) would be used to capture the device location details sent from the Mobile application. EC2 instances will process the messages from the SQS queue and retrieve the relevant offers from DynamoDB.\nThis solution involves using SQS to capture location details from the mobile app and using EC2 instances to process messages from the SQS queue and retrieve relevant offers from DynamoDB. This solution is suitable for handling a large number of users and provides a scalable and cost-effective solution.\nD. SNS Mobile Push will be used to send offers to the mobile application.\nThis solution involves using SNS Mobile Push to send offers to the mobile app. While this solution can handle a large number of users and is scalable, it does not address the need for real-time alerts based on the user's location.\nE. The mobile application will send the device location details using the SNS Mobile Push. EC2 instances will retrieve the relevant offers from DynamoDB.\nThis solution involves using SNS Mobile Push to receive location details from the mobile app and using EC2 instances to retrieve relevant offers from DynamoDB. While this solution can handle a large number of users and is scalable, it does not address the need for real-time alerts based on the user's location.\nF. EC2 instances will communicate with the mobile carrier or the device provider to push alerts back to the mobile application.\nThis solution involves using EC2 instances to communicate with the mobile carrier or device provider to push alerts back to the mobile app. While this solution may provide more control over the communication with the mobile carrier, it could be complex and costly to set up.\nBased on the analysis, the most suitable architectural suggestion for this scenario would be option C. It provides a scalable and cost-effective solution for handling a large number of users and ensures real-time alerts based on the user's location.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The mobile application will submit its location details to a web service endpoint using ELB and EC2 instances. DynamoDB will be used to store and retrieve the relevant offers from EC2 instances, which would then communicate with mobile carriers or device providers to push alerts back to the mobile application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS DirectConnect or VPN to establish connectivity with the mobile carrier. EC2 instances will receive the mobile application`s location details through the mobile carrier. RDS will be used to store and retrieve the relevant offers. EC2 instances will communicate with the mobile carrier to push alerts back to the mobile application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "SQS ( buffer storage ) would be used to capture the device location details sent from the Mobile application. EC2 instances will process the messages from the SQS queue and retrieve the relevant offers from DynamoD.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "SNS Mobile Push will be used to send offers to the mobile application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The mobile application will send the device location`s details using the SNS Mobile Push. EC2 instances will retrieve the relevant offers from DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "EC2 instances will communicate with the mobile carrier or the device provider to push alerts back to the mobile application.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 127,
  "query" : "A newspaper organization has an on-premises application that allows the public to search its back catalog and retrieve individual newspaper pages via a website written in Java.\nIt also has a commercial search application nearing its end of life.\nThey have scanned the old newspapers into JPEGs which is of a total size of 17TB and used Optical Character Recognition (OCR) to populate a commercial search product.\nThe organization wants to migrate its archive to AWS and produce a cost-efficient, highly-available and durable architecture.\nWhich of the below options is the most appropriate?",
  "answer" : "E.\nAnswer - C.\nThis question presents the following scenarios: (1) type of storage that can store a large amount of data (17TB), (2) the architecture should be cost-effective, highly available, and durable.\nTip: Whenever a storage service stores a large amount of data with low cost, high availability, and high durability, always think about using S3\nOption A is incorrect because even though it uses S3, it uses the commercial search software at the end of its life.\nOption B is incorrect because striped EBS is not as durable of a solution as S3 and certainly not as cost-effective as S3\nAlso, it has maintenance overhead.\nOption C is CORRECT because (a) it uses S3 to store the cost-effective images, (b) for more efficiency, it uses CloudSearch for query processing, and (c) with an Auto Scaling group in multi-AZs, it achieves high availability.\nOption D is incorrect because it does not have high availability with a single AZ RDS instance.\nOption E is incorrect because (a) this configuration is not scalable, and (b) it does not mention any origin for the CloudFront distribution.\nAmazon CloudSearch.\nWith Amazon CloudSearch, you can quickly add rich search capabilities to your website or application.\nYou don't need to become a search expert or worry about hardware provisioning, setup, and maintenance.\nWith a few clicks in the AWS Management Console, you can create a search domain and upload the data that you want to make searchable.\nAmazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.\nYou can easily change your search parameters, fine-tune search relevance, and apply new settings at any time.\nAs your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs.\nFor more information on AWS CloudSearch, please visit the below link-\nhttps://aws.amazon.com/cloudsearch/\nThe most appropriate option for the newspaper organization to migrate its archive to AWS is Option C, which involves using S3 to store and serve the scanned files, CloudSearch for query processing, and an Auto Scaling group to host the website across multiple availability zones.\nOption A is not the best choice because it requires the organization to manage its own search application on EC2 instances, which can be complex and time-consuming. Additionally, auto-scaling and load balancing would increase the cost and complexity of the solution.\nOption B is not the best choice because it involves using an EC2 instance running Apache webserver and an open-source search application, which would require significant management and maintenance. Additionally, striping multiple EBS volumes together would increase the complexity of the storage solution.\nOption D is not the best choice because it involves using a single-AZ RDS MySQL instance, which is not highly available. Additionally, using an EC2 instance to serve the website and translate user queries into SQL would increase the complexity of the solution.\nOption E is not the best choice because it involves using a CloudFront download distribution, which is primarily intended for serving static content, such as images, to end users. Additionally, installing the current commercial search product on EC2 instances would require significant management and maintenance.\nOption C is the most appropriate because it leverages S3 for storage and CloudSearch for query processing, which are both fully managed services that reduce the management and maintenance burden for the organization. Additionally, using an Auto Scaling group to host the website across multiple availability zones ensures high availability and durability. Overall, this option provides a cost-efficient, highly-available, and durable architecture that meets the organization's requirements.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use S3 to store and serve the scanned files. Install the commercial search application on EC2 Instances and configure it with auto-scaling and an Elastic Load Balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Model the environment using CloudFormation. Use an EC2 instance running Apache webserver and an open-source search application, stripe multiple standard EBS volumes together to store the JPEGs and search index.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use S3 to store and serve the scanned files. Use CloudSearch for query processing, and use an Auto Scaling group to host the website across multiple availability zones.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a single-AZ RDS MySQL instance to store the search index for the JPEG images and use an EC2 instance to serve the website and translate user queries into SQL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a CloudFront download distribution to serve the JPEGs to the end users. Install the current commercial search product, along with a Java Container for the website on EC2 instances and use Route53 with DNS round-robin.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 128,
  "query" : "A company is building a voting system for a popular TV show.\nViewers watch the performances then visit the show's website to vote for their favorite performer.\nIt is expected that the site will receive millions of visitors in a short period of time after the show is finished.\nThe visitors will first login to the site using their Amazon.com credentials and then submit their votes.\nAfter the voting is completed, the page will display the vote totals.\nThe company needs to build a site to handle the rapid influx of traffic while maintaining good performance.",
  "answer" : "Answer - D.\nThis scenario has the following architectural considerations: (1) the application needs to be scalable to handle traffic coming from millions of users, (2) the application should handle the rapid influx of traffic, maintaining good performance.\nWhen the application needs to handle the data coming from millions of users, always think about using DynamoDB.\nAlso, to provide the global users with high-performance content access, you need to consider CloudFront.\nYou need to set the appropriate IAM Role for the front end/web servers to access DynamoDB tables.\nOption A is incorrect because multi-AZ RDS is not a preferable solution compared to DynamoDB.Option B is incorrect because IAM roles should be used to provide permissions to write the DynamoDB table.\nOption C is incorrect because DynamoDB needs a high write capacity unit.\nWith this option, DynamoDB may have a performance issue.\nOption D is CORRECT because (a) it is highly scalable, (b) creates appropriate IAM Role to access the DynamoDB database, and (c) more importantly uses SQS to hold the user data/votes such that the application does not consume read and write provisioned capacity of DynamoDB.\nAs per the scenario, once the user completes the voting, the web page should display the total number of votes submitted online.\nOption D also includes an Autoscaling group of EC2 instances to handle the traffic.\nHence option D is optimal.\nThe best answer for the company to build a site to handle the rapid influx of traffic while maintaining good performance is:\nC. Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers. The web servers will first login with the Amazon service to authenticate the user. The web servers will process the users' votes and store the votes into a DynamoDB table that has a large read capacity unit.\nExplanation: The key requirements for this system are to handle millions of visitors and maintain good performance while ensuring security and reliability. The proposed architecture should be able to handle the expected traffic, provide a secure login process, and store user votes in a reliable and scalable way.\nOption A suggests using CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers. This approach can handle high traffic, but storing the votes in a multi-AZ Relational Database Service instance may not be the best choice since it could become a bottleneck in the system. Furthermore, this option doesn't address how to authenticate users before processing their votes.\nOption B suggests using CloudFront and the static website hosting feature of S3 to serve the website and calling the login with Amazon service to authenticate users. While this approach can also handle high traffic, storing user votes in DynamoDB would be a better choice since DynamoDB is a highly scalable, NoSQL database service. However, using IAM users to gain permissions to a DynamoDB table can be complex to manage and maintain, especially when dealing with millions of visitors.\nOption D proposes using CloudFront and an Elastic Load Balancer in front of auto-scaled web servers, and storing the votes in an SQS queue. While this approach can handle high traffic, using an SQS queue to store votes can introduce a delay in updating the vote totals. This option also requires additional application servers to retrieve the items from the queue and update the vote totals into a DynamoDB table.\nOption C is the best choice since it uses CloudFront and an Elastic Load Balancer to handle high traffic, and stores votes directly in a DynamoDB table. DynamoDB provides high availability, scalability, and performance, and a DynamoDB table with a large read capacity unit can handle millions of reads per second. Using Amazon's login service to authenticate users before processing their votes also ensures security and reliability.\nIn summary, Option C provides a scalable, reliable, and secure architecture to handle millions of visitors and maintain good performance for the TV voting system.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use CloudFront and an Elastic Load balancer in front of an auto-scaled set of web servers. The web servers will first login with the Amazon service to authenticate the users, then process the users` votes and store the votes into a multi-AZ Relational Database Service instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudFront and the static website hosting feature of S3 with the Javascript SDK to call the login with Amazon service to authenticate the user. Configure IAM users to gain permissions to a DynamoDB table to store the users` votes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers. The web servers will first login with Amazon service to authenticate the user. The web servers will process the users` votes and store the votes into a DynamoDB table that has a large read capacity unit.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers. The web servers will first login with Amazon service to authenticate the user. The web servers will process the users` votes and store the votes into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and update the `vote totals` into a DynamoDB table.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 129,
  "query" : "You are developing a new mobile application and are considering storing user preferences in AWS.\nThis would provide a more uniform cross-device experience to users using multiple mobile devices to access the application.\nThe preference data for each user is estimated to be 50KB in size.\nAdditionally, 5 million customers are expected to use the application regularly.\nThe solution needs to be quick, highly available, scalable, and secure.\nHow would you design a solution to meet the above requirements?",
  "answer" : "Answer - B.\nThis scenario has the following architectural considerations:(1) the application should support millions of customers, so it should be scalable, (2) multiple mobile devices should be able to access the application, and (3) it should be cost-effective, highly available and secure.\nTip: Whenever the application needs to (a) support millions of users and scalability is most important, always think about DynamoDB, and (b) give mobile apps access to AWS resource/service, always think about federated access using Web Identity Provider and \"AssumeRoleWithWebIdentity\" API.\nOption A is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB.Option B is CORRECT because (a) it uses DynamoDB for scalability.\n(b) It uses federated access using Web Identity Provider, and (c) uses Fine-Grained Access privileges for authenticating the access.\nOption C is incorrect because (a) RDS MySQL is not as scalable and cost-effective as DynamoDB, and (b) user management and access privilege system cannot be used for controlling access.\nOption D is incorrect because accessing the data via S3 would be slower compared to DynamoDB.For more information on DynamoDB, please visit the below URL-\nhttps://aws.amazon.com/dynamodb/developer-resources/\nTo meet the requirements of quick, highly available, scalable, and secure storage of user preferences for a new mobile application, we need to design a suitable AWS solution. The size of preference data for each user is estimated to be 50KB, and there will be 5 million customers using the application regularly.\nOut of the given options, the most suitable solution for this scenario is option B: Set up a DynamoDB table with each user's item having the necessary attributes to hold the user preferences. The mobile application will query the user preferences directly from the DynamoDB table. Utilize STS, Web Identity Federation, and DynamoDB Fine-Grained Access Control to authenticate and authorize access.\nDynamoDB is a managed NoSQL database service that provides fast, scalable, and highly available performance with seamless integration with other AWS services. DynamoDB can handle large volumes of data with low latency, and it can scale automatically to meet the needs of the application. Additionally, DynamoDB provides strong security features, including encryption at rest and in transit, fine-grained access control, and monitoring through Amazon CloudWatch.\nTo implement this solution, we can create a DynamoDB table with the necessary attributes to store user preferences for each customer. Each user's preference data can be stored in a single item in the DynamoDB table, and the attributes of the item can be customized to fit the data schema. DynamoDB supports JSON document data types, which can be used to store structured preference data.\nThe mobile application can query the user preference data directly from the DynamoDB table using the DynamoDB API. This approach eliminates the need for an intermediary server, reducing the complexity of the system and improving performance. To authenticate and authorize access to the DynamoDB table, we can utilize AWS Security Token Service (STS) and Web Identity Federation to provide temporary credentials for mobile application users.\nTo provide fine-grained access control for the DynamoDB table, we can use DynamoDB Fine-Grained Access Control. This allows us to define fine-grained access policies for individual items in the table based on user roles or attributes. This approach provides a secure and flexible way to manage access to the user preference data.\nOverall, the solution of using DynamoDB with STS, Web Identity Federation, and DynamoDB Fine-Grained Access Control provides a highly available, scalable, and secure solution for storing user preferences for a mobile application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Setup an RDS MySQL instance in 2 availability zones to store the user preference data. Deploy a public-facing application on a server in front of the database to manage security and access credentials.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up a DynamoDB table with each user`s item having the necessary attributes to hold the user preferences. The mobile application will query the user preferences directly from the DynamoDB table. Utilize STS. Web Identity Federation, and DynamoDB Fine-Grained Access Control to authenticate and authorize access.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Setup an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data. The mobile application will query the user preferences from the read replicas. Leverage the MySQL user management and access privilege system to manage security and access credentials.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the user preference data in S3. Setup a DynamoDB table with an item for each user and an item attribute pointing to the user’s S3 object. The mobile application will retrieve the S3 URL from DynamoDB and then access the S3 object directly by utilizing STS, Web identity Federation, and S3 ACLs to authenticate and authorize access.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 130,
  "query" : "Your team has a Tomcat-based Java application you need to deploy into development, test, and production environments.\nAfter some research, you opt to use Elastic Beanstalk due to its tight integration with your developer tools and RDS due to its ease of management.\nYour QA team lead points out that you need to roll a sanitized set of production data into your environment on a nightly basis.\nSimilarly, other software teams in your organization want access to that same restored data via their EC2 instances in your VPC.\nWhat of the following would be the optimal setup for persistence and security that meets the above requirements?",
  "answer" : "Answer - C.\nThe main consideration in this question is only the EC2 instances in your VPC.\nYou should be able to access RDS instances, and the setup should support persistence.\nOption A is incorrect because the RDS instance will be part of the Elastic Beanstalk environment and would not be optimal for persistence.\nOption B is incorrect because you should always use the DNS endpoint of the RDS instance, not the IP address, as this option suggests.\nOption C is CORRECT because (a) it uses a separate RDS instance (not part of Beanstalk), (b) it uses the DNS name of the RDS instance to the app's DB connection string for accessing the same and (c) it correctly configures the security group such that only the valid client machines have access to RDS instance.\n( need to create security group on the client machines first )\nOption D is incorrect because the security group is not configured correctly as it gives access to all the hosts in the application subnets.\nThe optimal setup for persistence and security that meets the given requirements is Option C: Use the ElasticBeanstalk to deploy your application in various environments. Create your RDS instance separately, controlled by automation, and pass its DNS name to your app's DB connection string as an environment variable. After this, restore the sanitized copy of production data to the RDS instance. Create a security group for client machines and add it as a valid source for DB traffic to the security group of the RDS instance itself.\nOption A is not the best approach because it creates the RDS instance as part of the Elastic Beanstalk definition, which means that it will be deleted along with the Elastic Beanstalk environment. Also, it would require altering the security group of the RDS instance to allow access from hosts in the application subnets, which could potentially create security vulnerabilities.\nOption B is not the best approach either, because it involves hard-coding the IP address of the RDS instance in the application's DB connection strings in the code. This makes it difficult to scale the application and manage the IP addresses of the RDS instances.\nOption D is not the best approach because it only allows access to the RDS instance from hosts in the application subnets. This would make it difficult for other software teams in the organization to access the same restored data via their EC2 instances in the VPC.\nOption C is the best approach because it separates the creation of the RDS instance from the Elastic Beanstalk environment, which allows for greater flexibility and scalability. The RDS instance can be controlled by automation, and its DNS name can be passed to the application's DB connection string as an environment variable. This allows for easy management of the RDS instances and their IP addresses. Also, a security group can be created for client machines, and it can be added as a valid source for DB traffic to the security group of the RDS instance itself. This ensures that only authorized clients can access the RDS instance, and it provides an additional layer of security for the data. Finally, after creating the RDS instance and passing the DNS name to the application's DB connection string, the sanitized copy of production data can be restored to the RDS instance, providing the required functionality.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow access to it from hosts in your application subnets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create your RDS instance separately and add its IP address to your application’s DB connection strings in your code. Alter its security group to allow access to it from hosts within your VPC’s IP address block.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the ElasticBeanstalk to deploy your application in various environments. Create your RDS instance separately, controlled by automation, and pass it`s DNS name to your app`s DB connection string as an environment variable. After this, restore the sanitized copy of production data to the RDS instance. Create a security group for client machines and add it as a valid source for DB traffic to the security group of the RDS instance itself.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create your RDS instance separately and pass its DNS name to your DB connection string as an environment variable. Alter its security group to allow access to it from hosts in your application subnets.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 131,
  "query" : "You are looking to migrate your Development and Test environments to AWS.\nYou have decided to use separate AWS accounts to host each environment.\nYou plan to link each account bill to a Management AWS account using Consolidated Billing.\nTo make sure that you keep within the budget, you would like to implement a way for administrators in the Management account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts.\nIdentify which of the options will allow you to achieve this goal.",
  "answer" : "Answer - C.\nThe scenario here is asking you to give permissions to administrators in the Management account such that they can have access to stop, delete, and terminate the resources in two accounts: Dev and Test.\nTip: Remember that you always create roles in the account whose resources are to be accessed.\nIn this example, that would be Dev and Test.\nThen you create the users in the account who will be accessing the resources and give them that particular role.\nIn this example, the Management account should create the users.\nOption A is incorrect because the Management account IAM user needs to assume roles from the Dev and Test accounts.\nThe roles should have suitable permissions so that the Management account IAM user can access resources.\nOption B is incorrect because the cross-account role should be created in Dev and Test accounts, not in the Management account.\nOption C is CORRECT because (a) the cross-account role is created in Dev and Test accounts, and the users are created in the Management account given that role.\nOption D is incorrect because consolidated billing does not give access to resources in this fashion.\nFor more information on cross-account access, please visit the below URL-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nOption A is the correct answer.\nIn this option, we create IAM users in the Management account with full Admin permissions. We also create cross-account roles in the Dev and Test accounts that grant the Management account access to the resources in those accounts by inheriting permissions from the Management account.\nThis approach is secure and flexible, as it allows the Management account to access the resources in the Dev and Test accounts without requiring IAM users to be created in each of those accounts. This also means that access to the resources in the Dev and Test accounts can be easily managed and revoked by updating the cross-account roles in those accounts.\nOption B is not the recommended approach because granting full Admin permissions to the Dev and Test accounts from the Management account could create security risks. It is generally not advisable to grant full Admin permissions to accounts that do not require them.\nOption C is also not the recommended approach because it grants full Admin permissions to the cross-account roles in the Dev and Test accounts. This could create a security risk as the Management account may not need full Admin permissions to manage the resources in the Dev and Test accounts.\nOption D is not a valid approach because linking accounts using Consolidated Billing does not automatically grant access to resources in other accounts. Consolidated Billing only consolidates billing information and does not provide any additional access to resources.\nIn summary, Option A is the best approach as it provides a secure and flexible way to grant Management account access to resources in the Dev and Test accounts without creating security risks or requiring additional permissions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create IAM users in the Management account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant Management account access to the resources in the account by inheriting permissions from the Management account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create IAM users and a cross-account role in the Management account that grants full Admin permissions to the Dev and Test accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create IAM users in the Management account with the \"AssumeRole\" permissions. Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant Management account access.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Link the accounts using Consolidated Billing. This will give IAM users in the Management account access to the resources in Dev and Test accounts.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 132,
  "query" : "Your customer is willing to consolidate their log streams, access logs, application logs, security logs, etc.\nin one single system.\nOnce consolidated, the customer wants to analyze these logs in real-time based on heuristics.\nFrom time to time, the customer needs to validate heuristics, which requires going back to data samples extracted from the last 12 hours? What is the best approach to meet your customer's requirements?",
  "answer" : "Answer - B.\nWhenever the scenario - just like this one - wants to do real-time processing of a stream of data, always think about Amazon Kinesis.\nAlso, remember that the records of the stream are available for 24 hours.\nOption A is incorrect because SQS is not used for real-time processing of the stream of data.\nOption B is CORRECT because Amazon Kinesis is best suited for applying the real-time processing of the stream of data.\nAlso, the records of the stream are available for 24 hours in Kinesis.\nOption C is incorrect because CloudTrail is not used to process the real-time data processing, and EMR is used for batch-processing.\nOption D is incorrect because setting autoscaling of EC2 instances is not cost-effective, and EMR is used for batch-processing.\nMore information on Amazon Kinesis:\nAmazon Kinesis is a platform for streaming data on AWS, making it easy to load and analyze streaming data and provide the ability to build custom streaming data applications for specialized needs.\nUse Amazon Kinesis Streams to collect and process large streams of data records in real-time.\nUse Amazon Kinesis Firehose to deliver real-time streaming data to destinations such as Amazon S3 and Amazon Redshift.\nUse Amazon Kinesis Analytics to process and analyze streaming data with standard SQL.\nFor more information on Kinesis, please visit the below URL-\nhttps://aws.amazon.com/documentation/kinesis/\nThe best approach to meet the customer's requirements is option B, which is to send all the log events to Amazon Kinesis and develop a client process to apply heuristics to the logs.\nAmazon Kinesis is a fully managed service that can handle large volumes of streaming data in real-time. It can receive and process data continuously from various sources, such as log files, IoT devices, social media feeds, and more. Kinesis can also store the data for up to 7 days and provide real-time analytics for the incoming data streams.\nSending all the log events to Amazon Kinesis will allow the customer to consolidate their log streams into one single system. The Kinesis client process can apply heuristics to the logs in real-time, allowing for immediate analysis and detection of any issues or anomalies. Kinesis can also scale automatically based on the incoming data volume, ensuring that the customer can handle any spikes in data traffic.\nIn addition, Kinesis can be used to store the logs for up to 7 days, which can be accessed for further analysis or validation of heuristics. This meets the customer's requirement of going back to data samples extracted from the last 12 hours, as they can retrieve the necessary logs from Kinesis.\nOption A, sending all the log events to Amazon SQS and setting up an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics, is not the best approach because SQS is a message queuing service, and is not designed for real-time processing of streaming data. Using EC2 servers to consume the logs can also be inefficient and costly.\nOption C, configuring Amazon Cloud Trail to receive custom logs and using EMR to apply heuristics to the logs, is not the best approach because Cloud Trail is primarily used for auditing and monitoring API calls made in AWS, and not for collecting and processing application logs or other types of logs.\nOption D, setting up an Auto Scaling group of EC2 Syslog servers and storing the logs in S3, and using EMR to apply heuristics on the logs, is not the best approach because it requires managing and scaling the EC2 servers manually, which can be time-consuming and costly. In addition, S3 is not designed for real-time processing of streaming data, and EMR may not be the best tool for real-time analysis of logs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Send all the log events to Amazon SQS. Setup an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Send all the log events to Amazon Kinesis. Develop a client process to apply heuristics to the logs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure Amazon Cloud Trail to receive custom logs and use EMR to apply heuristics to the logs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Setup Auto Scaling group of EC2 Syslog servers and store the logs S3 use EMR to apply heuristics on the logs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 133,
  "query" : "A team is developing a feature that needs to recognize Celebrities.\nBy using the App, clients can upload photos and search celebrities among the photos by clicking a button.\nOr they can upload a bunch of photos and search the times that a given celebrity has appeared.\nThe team wants to run the App in AWS at a lower cost.\nWhich option is the most efficient one to implement while still ensuring availability and stability?",
  "answer" : "Correct Answer - C.\nAs the question asks for a lower cost while still ensuring availability and stability.\nLambda should be considered first.\nLambda can autoscale without manual operations.\nMore importantly, it is very efficient to develop a web app without considering a lot about OS, patching, etc.\nAmazon Rekognition can recognize thousands of celebrities in a wide range of categories, such as entertainment and media, sports, business, and politics.\nWith Amazon Rekognition, you can recognize celebrities in images and stored videos.\nYou can also get additional information for recognized celebrities.\nOption A is incorrect: Because EC2 does not cost-efficient and brings in extra cost if the load is not stable.\nLambda should be considered instead.\nOption B is incorrect: Because Rekognition API “RecognizeCelebrities” should be used in this case.\nFor each celebrity recognized, “RecognizeCelebrities” returns a Celebrity object.\nThe Celebrity object contains the celebrity name, ID, URL links to additional information, match confidence, and a ComparedFace object that you can use to locate the celebrity's face on the image.\n“SearchFaces” is incorrect as it is not used to search Celebrities.\nOption C is CORRECT: Because Lambda together with Rekognition can meet the need for low cost, availability, and stability.\nRefer to https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html for this feature.\nOption D is incorrect: Because firstly, it has used EC2\nEven if Autoscaling and ELB are used, it is not as efficient as Lambda.\nSecondly, API “SearchFaces” should not be used for the function of Celebrities Recognition.\nSure, I'd be happy to explain the options in detail.\nOption A: This option suggests creating the frontend and backend of the application in a T2 medium EC2 instance. The Rekognition API \"RecognizeCelebrities\" will be used to fetch information in a JSON format. The JSON result will be processed in the backend service, and the result will be returned to the frontend UI. The T2 instance is a good choice since it has burstable capabilities that can help handle spikes in traffic. However, this option requires that you manage the EC2 instance and the application running on it, which can be time-consuming and complex. It also involves direct calls to the Rekognition API, which could lead to higher costs.\nOption B: This option suggests developing the application in a serverless Lambda function to use the Rekognition API \"SearchFaces\" to search for celebrities. The input image can be base64-encoded bytes or an S3 object. After the API has returned, the result will be presented to clients. This option is very cost-effective because Lambda is a serverless service and automatically scales based on the incoming requests. You only pay for the compute time used, making it much cheaper than maintaining an EC2 instance. Additionally, using the \"SearchFaces\" API is an efficient way to search for celebrities.\nOption C: This option suggests using the AWS Rekognition service and implementing the application in a Lambda function. The \"RecognizeCelebrities\" API will be used to fetch information required in a JSON format. The information will be processed in Lambda, and the result will be returned to end-users. S3 will be used for clients to upload photos. This option is similar to option B, but it uses the \"RecognizeCelebrities\" API instead of \"SearchFaces.\" This option is also very cost-effective, but it might not be as efficient as option B, especially if there are many photos to analyze.\nOption D: This option suggests implementing the application in an M4 large EC2 instance with Autoscaling and Elastic Load Balancer. The application will be built via a CloudFormation template. The App will call the Rekognition API \"SearchFaces\" to get the required information. The JSON result will be processed in the backend service and returned to the frontend with a Cloudfront CDN. This option provides a highly available and scalable architecture, but it's the most expensive option since you have to manage the EC2 instance, autoscaling, and load balancing.\nOverall, Options B and C are the most cost-effective and efficient options since they utilize Lambda and the Rekognition API. They are also highly scalable and easy to maintain, making them the best options for this use case.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create the frontend and backend in a T2 medium EC2 instance to use its burstable capability. Call Rekognition API “RecognizeCelebrities” to fetch the information in a JSON format. Process the JSON result in the backend service and return the result to the frontend UI.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Develop the App in a serverless lambda to use Rekognition API “SearchFaces” to search a Celebrity. The input image can be base64-encoded bytes or an S3 object. After the API has been returned, present the result to clients.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the AWS Rekognition service. Implement the App in a lambda to call Rekognition API “RecognizeCelebrities” to fetch the information required in a JSON format. Process the information in Lambda and return the result to end-users. Use S3 for clients to upload photos.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement the App in an M4 large EC2 instance with Autoscaling and Elastic Load Balancer. Build the application via a Cloudformation template. Use the App to call Rekognition API “SearchFaces” to get the information. Process the JSON result in the backend service and return the result to the frontend with a Cloudfront CDN.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 134,
  "query" : "You are running a successful multitier web application on AWS.\nYour marketing department has asked you to add a reporting tier to the application.\nThe reporting tier will aggregate and publish status reports every 30 minutes from user-generated information that is being stored in your web application's database.\nYou are currently running a Multi-AZ RDS MySQL instance for the database tier.\nYou also have implemented ElastiCache as a database caching layer between the application tier and database tier.\nSelect the answer that will allow you to successfully implement the reporting tier with as little impact as possible to your database.",
  "answer" : "Answer - C.\nIn question is asking you to design a reporting layer with the least impact on the database.\nIf the reporting queries are fired on the database instance, the database instance's performance would surely get impacted.\nSince querying for the report would be a read-heavy operation, the best solution is to create the read replicas of the database instance and query on them rather than on the database instance.\nThis way, the existing database instance will have the least impact.\nOption A is incorrect because sending the logs to S3 would add to the overhead on the database instance.\nOption B is incorrect because you cannot access the standby instance.\nOption C is CORRECT because it uses the Read Replicas of the database for the querying of reports.\nOption D is incorrect because the querying on ElastiCache may not always give you the latest and entire data, as the cache may not always be up-to-date.\nFor more information on Read Replica's, please visit the below URL-\nhttps://aws.amazon.com/rds/details/read-replicas/ https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\nThe best option for adding a reporting tier to a multitier web application on AWS with as little impact as possible to the database is to use an RDS Read Replica.\nOption A, sending transaction logs to an S3 bucket and generating reports from there, may not be the best option since it involves a lot of manual work to create and maintain reports from S3 data, and S3 byte-range requests may not be efficient or cost-effective.\nOption B, generating reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ, is also not the best option because the standby instance is meant for disaster recovery and not for reporting, so it may not be able to handle reporting workload.\nOption D, generating reports by querying the ElastiCache database caching tier, is not optimal since the ElastiCache layer is designed for caching and not for reporting. This means that the reports generated from the cache layer may not be up-to-date or accurate, since the cache layer may not always contain the latest data.\nTherefore, the best option is to launch an RDS Read Replica connected to the Multi-AZ master database and generate reports by querying the Read Replica. This allows for reporting to be performed without impacting the performance of the Multi-AZ master database, as the Read Replica is separate from the master database and can be scaled and managed independently. Additionally, the Read Replica can be located in a different Availability Zone or region, which provides additional redundancy and resiliency for reporting purposes.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Continually send transaction logs from your master database to an S3 bucket and generate the reports from the S3 bucket using S3 byte-range requests.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Generate the reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Launch an RDS Read Replica connected to your Multi AZ master database and generate reports by querying the Read Replica.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Generate the reports by querying the ElasliCache database caching tier.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 135,
  "query" : "A web company is looking to implement an intrusion detection and prevention system for their deployed VPC.\nThis platform should have the ability to scale to thousands of instances running inside of the VPC.\nHow should they architect their solution to achieve these goals?",
  "answer" : "Answer - B.\nThis question asks you to design a scalable IDS/IPS solution (easily applicable to thousands of instances)\nUsers can set up third party IDS/IPS solutions offered in AWS Marketplace.\nOption A is incorrect because AWS does not support promiscuous mode.\nOption B is CORRECT because there are various IDS/IPS solutions offered in AWS Marketplace that monitor networks and systems for malicious activity and provide layer of security for potential threats.\nOption C is incorrect.\nThe incoming traffic should be passed through IDS/IPS before sending it to the servers.\nOption D is plausible, but (a) it is not a scalable solution, (b) it is only an IDS solution, not an IPS solution.\nPlease check the following references:\nhttps://aws.amazon.com/mp/scenarios/security/ids/ https://aws.amazon.com/marketplace/solutions/infrastructure-software/ids-ips\nThe best option for the web company to implement an intrusion detection and prevention system that can scale to thousands of instances running inside of their VPC would be to use a cloud-native IDS/IPS solution that is designed to integrate with AWS services.\nOption B: Create an IDS/IPS system from AWS Marketplace to monitor security events in the VPC network and stop threats once detected.\nExplanation: AWS Marketplace offers a wide range of IDS/IPS solutions from various vendors that are designed to work seamlessly with AWS services. This option allows the web company to select the most suitable solution for their needs and scale the solution as their VPC grows.\nThe cloud-native IDS/IPS solution can provide automated security incident response and can integrate with AWS services, such as Amazon CloudWatch, AWS Lambda, and Amazon SNS, to deliver real-time alerts on detected security threats.\nThe solution can be easily deployed and managed from the AWS Management Console or via APIs, and the web company can leverage the AWS security groups and network ACLs to enforce security policies and control access to their VPC.\nOption A: Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see all traffic across the VPC.\nThis option involves setting up a single instance with monitoring software to capture all network traffic in the VPC. While this solution may work for smaller VPCs, it can be challenging to scale and manage for VPCs with thousands of instances. Additionally, setting the ENI to promiscuous mode may lead to performance issues and increase the risk of false positives.\nOption C: Configure servers running in the VPC using the host-based ‘route' commands to send all traffic through the platform to a scalable virtualized IDS/IPS.\nThis option requires configuring each server in the VPC to route all traffic through the IDS/IPS platform, which can be challenging to manage for VPCs with thousands of instances. Additionally, this option may impact the performance of the servers and increase the risk of false positives.\nOption D: Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection.\nThis option involves installing an agent on each instance in the VPC to collect network traffic and send it to the IDS/IPS platform for inspection. While this option may work for smaller VPCs, it can be challenging to manage and scale for VPCs with thousands of instances. Additionally, the agent may impact the performance of the instances and increase the risk of false positives.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see all traffic across the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a IDS/IPS system from AWS Marketplace to monitor security events in the VPC network and stop threats once detected.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure servers running in the VPC using the host-based ‘route’ commands to send all traffic through the platform to a scalable virtualized IDS/IPS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 136,
  "query" : "A web-startup runs its very successful social news application on Amazon EC2 with an Elastic Load Balancer, an Auto-Scaling group of Java/Tomcat application-servers, and DynamoDB as a data store.\nThe main web application best runs on m2 x large instances since it is highly memory-bound.\nEach new deployment requires the semi-automated creation and testing of a new AMI for the application servers which takes quite a while and is therefore only done once per week.\nRecently, a new chat feature has been implemented in Node.js and waits to be integrated into the architecture.\nThe first tests show that the new component is CPU bound.\nSince the company has some experience with using Chef, they decided to streamline the deployment process and use AWS OpsWorks as an application lifecycle tool to simplify the application's management and reduce the deployment cycles.\nWhat configuration in AWS OpsWorks is required to manage the application integrated with the new chat module in the most cost-efficient and flexible way?",
  "answer" : "Answer - B.\nThe scenario here requires that you need to manage the application that is created with java, node.js, and DynamoDB using AWS OpsWorks.\nThe deployment process should be streamlined, and the deployment cycles should be reduced.\nAs the java and node.js have different resource requirements, it makes sense to deploy them on different layers.\nIt would make maintenance easier as well.\nOption A is incorrect because it would be a better solution to create two separate layers: one for Java and one for node.js.\nOption B is CORRECT because only one stack would be sufficient, and two layers (one for Java and one for node.js) would be required for handling separate requirements.\nOne custom recipe for DynamoDB would be required.\nOption C is incorrect because two OpsWork stacks are unnecessary.\nOption D is incorrect because two OpsWork stacks are unnecessary.\nMore information on AWS OpsWork Stack.\nAn AWS OpsWorks Stack defines your entire application's configuration: the load balancers, server software, database, etc.\nYou control every part of the stack by building layers that define the software packages deployed to your instances and other configuration details such as Elastic IPs and security groups.\nYou can also deploy your software onto layers by identifying the repository and optionally using Chef Recipes to automate everything Chef can do, such as creating directories and users, configuring databases, etc.\nYou can use OpsWorks Stacks' built-in automation to scale your application and automatically recover from instance failures.\nYou can control who can view and manage the resources used by your application, including ssh access to the instances that your application uses.\nFor more information on Ops work, please visit the below URL-\nhttps://aws.amazon.com/opsworks/stacks/faqs/\nThe best answer for this question would be B. Create one AWS OpsWorks stack, create two AWS OpsWorks layers, and create one custom recipe.\nExplanation: AWS OpsWorks is an application lifecycle management tool that provides many benefits, including managing AWS resources, creating and managing instances, and deploying applications. To manage the application integrated with the new chat module in the most cost-efficient and flexible way, the following configuration in AWS OpsWorks is required:\n1.\nCreate one AWS OpsWorks stack: An AWS OpsWorks stack is a container for AWS resources that are used to manage the application's lifecycle. Creating one stack for the entire application is the best choice since it allows for centralized management and better organization of resources.\n2.\nCreate two AWS OpsWorks layers: An AWS OpsWorks layer is a set of resources that share the same configuration, security settings, and lifecycle. Creating two layers for the application allows for more control over the resources and better scalability. One layer should be created for the Java/Tomcat application servers and another for the Node.js chat feature.\n3.\nCreate one custom recipe: A custom recipe in AWS OpsWorks is a set of instructions that automate the configuration of instances in a layer. In this case, one custom recipe should be created for the Node.js chat feature to optimize the CPU usage and ensure efficient deployment. The existing recipe for the Java/Tomcat application servers can be reused.\nTherefore, option B is the best answer as it allows for centralized management of the entire application, better scalability, and more control over resources while minimizing deployment cycles and reducing costs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create one AWS OpsWorks stack, create one AWS OpsWorks layer, and create one custom recipe.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create one AWS OpsWorks stack, create two AWS OpsWorks layers, and create one custom recipe.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create two AWS OpsWorks stacks, create two AWS OpsWorks layers, and create one custom recipe.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create two AWS OpsWorks stacks, create two AWS OpsWorks layers, and create two custom recipes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 137,
  "query" : "Your firm has uploaded a large amount of aerial image data to S3\nIn the past, in your on-premises environment, you used a dedicated group of servers to process this data.\nYou used Rabbit MQ - An open-source messaging system to get job information to the servers.\nOnce processed, the data would go to the tape and be shipped offsite.\nYour manager told you to stay with the current design and leverage AWS archival storage and messaging services to minimize cost.\nWhich of the following options is correct?",
  "answer" : "Answer - B.\nThe most suggestive hint in this question is that it asks you to leverage AWS archival storage and messaging services.\nHence, you should look for options Glacier and SQS.\nOption A is incorrect because (a) RRS is not an archival storage option, and (b) since auto-scaling is not mentioned, you cannot use CloudWatch alarms to terminate the idle EC2 instances.\nOption B is CORRECT because (a) it uses SQS to process the messages, (b) it uses Glacier as the archival storage solution - which is cost-optimized.\nOption C is incorrect because RRS is not an archival storage option; instead, use Glacier as it is a low-cost archival solution (cost lower than RRS).\nOption D is incorrect as SNS cannot be used to process the messages.\nIt cannot replace the functionality that was getting provided by RabbitMQ.\nThe best answer is B.\nExplanation:\nOption A suggests using SQS for job messages, CloudWatch alarms for terminating idle EC2 instances, and changing the storage class of the S3 objects to Reduced Redundancy Storage once the data is processed. Although this option utilizes messaging services and archival storage, it does not provide any scaling capability to handle large amounts of data. Additionally, changing the storage class to Reduced Redundancy Storage may reduce costs, but it also means that the data is less durable, which could be a problem for aerial images.\nOption B is the correct answer because it suggests using Auto-Scaled workers triggered by queue depth that use instances to process messages in SQS. This approach enables automatic scaling to handle large amounts of data, and using Glacier for storage ensures that the data is secure and durable. Glacier also provides cost savings over other storage options.\nOption C is similar to Option A, but instead of using CloudWatch alarms to terminate idle EC2 instances, it suggests using Auto-Scaled workers. However, like Option A, changing the storage class to Reduced Redundancy Storage may not be suitable for aerial images.\nOption D suggests using SNS for job messages, CloudWatch alarms for terminating idle worker instances, and changing the storage class of the S3 object to Glacier once the data is processed. This approach does not provide scaling capability, which may be necessary for processing large amounts of aerial image data. Additionally, SNS is a pub/sub messaging system, which means that messages may be lost if a worker instance is not available to receive them.\nIn summary, Option B is the best answer as it suggests using Auto-Scaled workers triggered by queue depth that use instances to process messages in SQS, and storing the processed data in Glacier for secure, durable, and cost-effective archival storage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use SQS for passing job messages. Use Cloud Watch alarms to terminate EC2 worker instances when they become idle. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Setup Auto-Scaled workers triggered by queue depth that use instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Glacier.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Setup Auto-Scaled workers triggered by queue depth that use instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage (RRS).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use SNS to pass the job messages. Use Cloud Watch alarms to terminate worker instances when they become idle. Once data is processed, change the storage class of the S3 object to Glacier.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 138,
  "query" : "A corporate web application is deployed within an Amazon Virtual Private Cloud (VPC) and is connected to the corporate data center via an IPsec VPN.\nThe application must authenticate against the on-premises LDAP server.\nAfter authentication, each logged-in user can only access an Amazon Simple Storage Service(S3) keyspace specific to that user.\nWhich two approaches can satisfy these objectives?",
  "answer" : "Answer - B and C.\nThere are two architectural considerations here: (1) The users must be authenticated via the on-premise LDAP server, and (2) each user should have access to S3 only.\nWith this information, it is important to authenticate the users using LDAP, get the IAM Role name, get the temporary credentials from AWS STS, and access the S3 bucket using those credentials.\nAnd second, create an IAM Role that provides access to S3.\nOption A is incorrect because the users need to be authenticated using LDAP first, not AWS STS.\nAlso, the temporary credentials to log into AWS are provided by AWS STS, not an identity broker.\nOption B is CORRECT because it follows the correct sequence.\nIt authenticates users using LDAP, gets the security token from AWS STS, and then accesses the S3 bucket using the temporary credentials.\nOption C is CORRECT because it follows the correct sequence.\nIt develops an identity broker that authenticates users against LDAP, gets the security token from AWS STS, and then accesses the S3 bucket using the IAM federated user credentials.\nOption D is incorrect because you cannot use the LDAP credentials to log in to IAM.\nAn example diagram of how this works from the AWS documentation is given below.\nFor more information on federated access, please visit the below link-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\nThe scenario presented involves a corporate web application that is deployed within an Amazon Virtual Private Cloud (VPC) and is connected to the corporate data center via an IPsec VPN. The application must authenticate against the on-premises LDAP server, and each logged-in user can only access an Amazon S3 keyspace specific to that user. Two approaches can satisfy these objectives:\nA. Develop an identity broker that authenticates against AWS Security Token Service (STS) to assume an IAM role to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials to access the appropriate S3 bucket.\nThis approach involves developing an identity broker that authenticates against AWS Security Token Service (STS) to assume an IAM role to obtain temporary AWS security credentials. The application then calls the identity broker to obtain the temporary credentials required to access the appropriate S3 bucket.\nIn this approach, the identity broker authenticates against AWS Security Token Service (STS) by assuming an IAM role. The IAM role is configured with the necessary permissions to access the appropriate S3 bucket. Once the identity broker has assumed the IAM role, it obtains temporary AWS security credentials from STS. These temporary credentials are then passed to the application, which uses them to access the appropriate S3 bucket.\nB. The application authenticates against LDAP and retrieves the name of an IAM role associated with the user. The application then calls the AWS Security Token Service (STS) to assume that IAM role ( including the ARN ). The application then uses the temporary credentials to access the appropriate S3 bucket.\nThis approach involves the application authenticating against LDAP and retrieving the name of an IAM role associated with the user. The application then calls the AWS Security Token Service (STS) to assume that IAM role, including the ARN. The application then uses the temporary credentials obtained from STS to access the appropriate S3 bucket.\nIn this approach, the application authenticates against LDAP and retrieves the name of the IAM role associated with the user. The application then calls STS to assume that IAM role, which is configured with the necessary permissions to access the appropriate S3 bucket. The application then uses the temporary credentials obtained from STS to access the S3 bucket.\nC. Develop an identity broker that authenticates against LDAP and then calls AWS Security Token Service (STS) to get IAM federated user credentials. The application then uses the temporary credentials to access the appropriate S3 bucket.\nThis approach involves developing an identity broker that authenticates against LDAP and then calls AWS Security Token Service (STS) to obtain IAM federated user credentials. The application then uses the temporary credentials obtained from STS to access the appropriate S3 bucket.\nIn this approach, the identity broker authenticates against LDAP and obtains IAM federated user credentials from STS. These temporary credentials are then passed to the application, which uses them to access the appropriate S3 bucket.\nD. The application authenticates against LDAP and then calls the AWS Identity and Access Management (IAM) Security service to log in to IAM using the LDAP credentials. The application can then access the appropriate S3 bucket.\nThis approach involves the application authenticating against LDAP and then calling the AWS Identity and Access Management (IAM) Security service to log in to IAM using the LDAP credentials. The application can then access the appropriate S3 bucket.\nIn this approach, the application authenticates against LDAP and uses the LDAP credentials to log in to IAM. Once logged in, the application can access the appropriate S3 bucket.\nIn summary, options A and B are both viable solutions to the given scenario. Option A involves developing an identity broker that authenticates against AWS STS to assume an IAM role and obtain temporary AWS security credentials, while option B involves the application authenticating against LDAP and retrieving the name of an IAM role associated with",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Develop an identity broker that authenticates against AWS Security Token Service (STS) to assume an IAM role to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials to access the appropriate S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The application authenticates against LDAP and retrieves the name of an IAM role associated with the user. The application then calls the AWS Security Token Service (STS) to assume that IAM role ( including the ARN ). The application then uses the temporary credentials to access the appropriate S3 bucket.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Develop an identity broker that authenticates against LDAP and then calls AWS Security Token Service (STS) to get IAM federated user credentials. The application then uses the temporary credentials to access the appropriate S3 bucket..",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The application authenticates against LDAP and then calls the AWS Identity and Access Management (IAM) Security service to log in to IAM using the LDAP credentials. The application can then access the appropriate S3 bucket.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 139,
  "query" : "Your company is planning to develop an application in which the front end is in .Net and the backend is in DynamoDB.\nIt is expected that there would be an intermittent high load on the application.\nHow could you ensure the application's scalability and cost-effectiveness to reduce the load on the DynamoDB database? Choose an answer from the below options.",
  "answer" : "Answer - C.\nThis question asks for an option that can be used to reduce the load on the DynamoDB database.\nThe option has to be scalable.\nIn such a scenario, the best option to use is SQS, because it is scalable and cost-efficient as well.\nOption A is incorrect because adding more databases will not reduce the load on the existing DynamoDB database.\nAlso, this is not a cost-efficient solution.\nOption B is incorrect because increasing the write capacity is an expensive option.\nOption C is CORRECT because it uses SQS to assist in taking over the load from storing the data in DynamoDB, and it is scalable and cost-efficient.\nOption D is incorrect because the MultiAZ configuration is not going to help reduce the load.\nIn fact, it will affect the performance as the records in DynamoDB would get replicated in multiple availability zones.\nMore information on SQS:\nWhen the idea comes for scalability, then SQS is the best option.\nNormally DynamoDB is scalable, but since one is looking for a cost-effective solution, the messaging in SQS can help manage the situation mentioned in the question.\nAmazon Simple Queue Service (SQS) is a fully-managed message queuing service for reliably communicating among distributed software components and microservices - at any scale.\nBuilding applications from individual components that perform a discrete function improves scalability and reliability and is the best practice design for modern applications.\nSQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application.\nUsing SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available.\nFor more information on SQS, please refer to the below URL-\nhttps://aws.amazon.com/sqs/\nOption A: Adding more DynamoDB databases to handle the load: Adding more databases might increase the overall capacity of the system, but it will not ensure scalability and cost-effectiveness. The application would still need to handle the complexity of managing multiple databases and ensure data consistency across them. Additionally, this approach might result in higher costs due to the added operational overhead and duplication of data.\nOption B: Increasing the write capacity of DynamoDB to meet the peak loads: Increasing the write capacity of DynamoDB might help in handling the peak loads, but it might not be a scalable and cost-effective solution in the long term. Provisioned throughput in DynamoDB is expensive, and it might not be possible to predict the exact amount of capacity needed to handle the load. Additionally, increasing the capacity might not be feasible when the application requires consistency in reads.\nOption C: Using SQS to hold the database requests instead of overloading the DynamoDB database: Using SQS to buffer the requests is a scalable and cost-effective approach. SQS can handle a large number of requests, and the application can asynchronously process the messages in a controlled manner to ensure that DynamoDB is not overwhelmed with requests. The approach also decouples the frontend from the backend, which helps in improving the application's overall performance and reliability.\nOption D: Launching DynamoDB in Multi-AZ configuration with a global index to balance writes: Launching DynamoDB in Multi-AZ configuration with a global index can help in distributing the load across multiple availability zones and improve the application's overall availability. However, it might not be a scalable and cost-effective solution for handling high write loads. The approach might result in higher costs due to the added operational overhead and the cost of maintaining the global index.\nOverall, Option C, i.e., using SQS to buffer the requests, is the most suitable and cost-effective approach for handling intermittent high loads in the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Add more DynamoDB databases to handle the load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Increase the write capacity of Dynamo DB to meet the peak loads.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use SQS to hold the database requests instead of overloading the DynamoDB database. Then have a service that asynchronously pulls the messages and writes them to DynamoD.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Launch DynamoDB in Multi-AZ configuration with a global index to balance writes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 140,
  "query" : "A team has just received a task to build an application that needs to recognize faces in streaming videos.\nThey will get the source videos from a third party that uses a container format (MKV).",
  "answer" : "E.\nF.\nCorrect Answer - B, C and F.\nFor facial recognition in live videos, it is different from that in photos.\nKinesis is required to meet the needs of the real-time process.\nAmazon Rekognition Video uses Amazon Kinesis Video Streams to receive and process a video stream.\nThe analysis results are output from Amazon Rekognition Video to a Kinesis data stream and then read by your client application.\nAmazon Rekognition Video provides a stream processor (CreateStreamProcessor) that you can use to start and manage the analysis of streaming video.\nAs a summary, the below 3 items are needed for Amazon Rekognition Video with streaming video.\nA Kinesis video stream for sending streaming video to Amazon Rekognition Video.\nFor more information, see the Kinesis video stream.\nAn Amazon Rekognition Video stream processor to manage the analysis of the streaming video.\nFor more information, see Starting Streaming Video Analysis.\nA Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream.\nFor more information, see Consumers for Amazon Kinesis Streams.\nOption A is incorrect because the source videos should be put into the Kinesis video stream instead of S3\nAfterwards, the Rekognition processor will pick up records in the Kinesis stream to process.\nOption B is CORRECT because it is the step to convert source data into the Kinesis video stream.\nOption C is CORRECT.\nA stream processor can be created by calling CreateStreamProcessor.\nThe request parameters include the Amazon Resource Names (ARNs) for the Kinesis video stream, the Kinesis data stream, and the identifier for the collection that's used to recognize faces in the streaming video.\nIt also includes the name that you specify for the stream processor.\nBelow is an example:\nOption D is incorrect because, for video processing, Rekognition API “DetectFaces” should not be used.\n“DetectFaces” is used to detects faces within an image that is provided as input.\nInstead, stream processor relevant APIs should be used.\nOption E is incorrect because the output from Rekognition should be stored in the Kinesis data stream.\nWhen the Rekognition stream processor is created, the Rekognition output (Kinesis Data Stream) is defined.\n\"Output\": {\n\"KinesisDataStream\": {\n\"Arn\": \"arn:aws:kinesis:us-east-1:nnnnnnnnnnnn:stream/outputData\"\n}\nOption F is CORRECT because it describes correctly how to consume the Kinesis data stream.\nYou can use the Amazon Kinesis Data Streams Client Library to consume analysis results that are sent to the Amazon Kinesis Data Streams output stream.\nDetails can be found in https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video-kinesis-output.html.\nTo build an application that can recognize faces in streaming videos, several AWS services can be utilized. Here are the possible options:\nA. S3 buckets to store the source MKV videos for AWS Rekognition to process: In this approach, the source videos are stored in S3 buckets, and Rekognition is used to process them. S3 buckets provide unlimited, highly available, and durable storing space, making it a suitable choice for storing videos. However, it is important to ensure that the third party has the write access to the S3 buckets to be able to upload the videos.\nB. A Kinesis video stream for sending streaming video to Amazon Rekognition Video: This approach involves using a Kinesis video stream to send streaming video to Amazon Rekognition Video. This can be done by using Kinesis \"PutMedia\" API in Java SDK. The PutMedia operation writes video data fragments into a Kinesis video stream that Amazon Rekognition Video consumes. This approach is suitable for real-time video processing.\nC. An Amazon Rekognition Video stream processor to manage the analysis of the streaming video: This approach involves using an Amazon Rekognition Video stream processor to manage the analysis of the streaming video. The stream processor can be used to start, stop, and manage stream processors according to needs. This approach is suitable for real-time video processing.\nD. Use EC2 or Lambda to call Rekognition API \"DetectFaces\" with the source videos saved in the S3 bucket: This approach involves using EC2 or Lambda to call the Rekognition API \"DetectFaces\" with the source videos saved in the S3 bucket. For each face detected, the operation returns face details such as a bounding box of the face, a confidence value, and a fixed set of attributes such as facial landmarks, etc. This approach is suitable for batch processing.\nE. After the APP has utilized Rekognition API to fetch the recognized faces from live videos, use S3 or RDS database to store the output from Rekognition: This approach involves storing the output from Rekognition in either S3 or RDS database. Another Lambda can be used to post-process the result and present it to UI.\nF. A Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream: This approach involves using a Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream. The consumer can be autoscaled by running it on multiple EC2 instances under an Auto Scaling group.\nOverall, the best approach depends on the specific needs of the application. If real-time video processing is required, options B and C are suitable. If batch processing is sufficient, option D can be used. Options A and E are suitable for storing the source videos and output, respectively. Option F can be used to read the analysis results in a scalable manner.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "S3 buckets to store the source MKV videos for AWS Rekognition to process. S3 should be used in this case as it has provided an unlimited, highly available, and durable storing space. Make sure that the third party has the write access to S3 buckets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A Kinesis video stream for sending streaming video to Amazon Rekognition Video. This can be done by using Kinesis “PutMedia” API in Java SDK. The PutMedia operation writes video data fragments into a Kinesis video stream that Amazon Rekognition Video consumes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "An Amazon Rekognition Video stream processor to manage the analysis of the streaming video. It can be used to start, stop, and manage stream processors according to needs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use EC2 or Lambda to call Rekognition API “DetectFaces” with the source videos saved in the S3 bucket. For each face detected, the operation returns face details. These details include a bounding box of the face, a confidence value, and a fixed set of attributes such as facial landmarks, etc.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "After the APP has utilized Rekognition API to fetch the recognized faces from live videos, use S3 or RDS database to store the output from Rekognition. Another Lambda can be used to post-process the result and present it to UI.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream. The consumer can be autoscaled by running it on multiple EC2 instances under an Auto Scaling group.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 141,
  "query" : "A company has maintained a MySQL database on-premises and is considering migrating the database to AWS.\nThey require the database instance in AWS to keep synchronizing with the on-premises one.\nThen after a while, the on-premises database will be decommissioned after the AWS RDS instance has been tested thoroughly by the QA team.\nAmazon Database Migration Service has been chosen to implement this migration.\nFor this data migration task, which below options are necessary that you must perform? Select 3.",
  "answer" : "E.\nCorrect Answer - A, D, E.\nTo use the wizard to begin a database migration, select “Getting started” from the navigation pane on the AWS DMS console.\nFollowing the wizard process, you need to do the following.\nAllocate a replication instance that performs all the processes for the migration.\nspecify a source and a target database.\ncreate a task or set of tasks to define what tables and replication processes you want to use.\nAWS DMS then creates your replication instance and performs the tasks on the data being migrated.\nAlternatively, you can create each of the components of an AWS DMS database migration by selecting the items from the navigation pane.\nDetails on how to get started with AWS Database Migration Service can be found in.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_GettingStarted.html\nOption A is CORRECT: Because the first task in migrating a database is to create a replication instance that has sufficient storage and processing power to perform the tasks you assign and migrate data from your source database to the target database.\nOption B is incorrect: For DMS, the replication instance should be created in the DMS console instead of the EC2 instance.\nAn example is as below.\nOption C is incorrect: Because for DMS, The source and target data stores can be on an Amazon Elastic Compute Cloud (Amazon EC2) instance, an Amazon Relational Database Service (Amazon RDS) DB instance, or an on-premises database.\nOption D is CORRECT: Because it is a required step to choose the source and target endpoints.\nThe database engine should be “MySQL”\nFor example.\nOption E is CORRECT: Because the task is needed to specify what tables to migrate, how to map data using a target schema, and how to create new tables on the target database.\nIn this case, the type of migration should be “migrate existing data and replicate ongoing changes”.\nSure, I can provide a detailed explanation for the necessary steps to perform the migration task using the Amazon Database Migration Service (DMS) to keep synchronizing an on-premises MySQL database instance with an AWS RDS instance.\nHere are the three options that must be performed:\nA. Allocate a replication instance in the Database Migration Service console that handles the migration processing. Ensure that the instance has a proper instance type with enough CPU and Memory resources to perform the migration task.\nThe first step is to allocate a replication instance in the Database Migration Service console, which handles the migration processing. The replication instance should have a proper instance type with enough CPU and memory resources to perform the migration task efficiently. The replication instance acts as a buffer between the source and the target databases during the migration process. It also performs the necessary transformations and mappings of the data.\nB. Create an EC2 instance that is used as the processing instance for DMS with a proper instance type. Ensure the EC2 instance has a proper role to access RDS MySQL instance. In the Database Migration Service console, configure the EC2 instance as the Replication instance.\nThe next step is to create an EC2 instance that will be used as the processing instance for DMS. The EC2 instance should have a proper instance type and role to access the RDS MySQL instance. In the Database Migration Service console, the EC2 instance should be configured as the replication instance. This will allow DMS to use the EC2 instance as a processing engine for data replication.\nC. Specify a source and a target database endpoint. Use the on-premises database as the source endpoint and an Amazon Relational Database Service (Amazon RDS) DB instance as the target endpoint. Select the database engine for both source and target as “MySQL”.\nThe final step is to specify a source and a target database endpoint for the migration task. In this case, the on-premises database will be the source endpoint, and the Amazon RDS instance will be the target endpoint. Both the source and target endpoints should be specified as MySQL database engines. This will ensure that the migration task is properly configured for the MySQL database engine.\nAdditionally, it is recommended to create a task or set of tasks to define what tables and replication processes to be used. For migration type, select “migrate existing data and replicate ongoing changes”. This will allow for the migration of existing data and ongoing changes to be replicated between the on-premises and AWS RDS instances during the migration process.\nIn summary, to perform the migration task using Amazon DMS to keep synchronizing an on-premises MySQL database instance with an AWS RDS instance, the following three steps are necessary: A) Allocate a replication instance B) Create an EC2 instance as a processing instance for DMS and configure it as the replication instance in the console C) Specify the source and target database endpoints, selecting the MySQL engine for both, and define tasks to migrate existing data and replicate ongoing changes.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Allocate a replication instance in the Database Migration Service console that handles the migration processing. Ensure that the instance has a proper instance type with enough CPU and Memory resources to perform the migration task.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a EC2 instance that is used as the processing instance for DMS with a proper instance type. Ensure the EC2 instance has a proper role to access RDS MySQL instance. In Database Migration Service console, configure the EC2 instance as the Replication instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Specify the source as the on-premises database, Select the database engine as \"MySQL\", and choose the destination subnet. Database Migration Service will automatically create the destination endpoint.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Specify a source and a target database endpoint. Use the on-premises database as the source endpoint and an Amazon Relational Database Service (Amazon RDS) DB instance as the target endpoint. Select the database engine for both source and target as “MySQL”.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a task or set of tasks to define what tables and replication processes to be used. For migration type, select “migrate existing data and replicate ongoing changes”.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 142,
  "query" : "A team has a website that provides the latest real estate news to subscribers.\nIt has included a Java frontend, backend, and a MySQL database to store user information.\nThey plan to migrate the MySQL database to PostgreSQL database to use some features that MySQL does not have.\nAt the same time, the development lead has raised extra requirements.",
  "answer" : "Correct Answer - D.\nDMS can handle the task of migration from MySQL to PostgreSQL, as shown below.\nThe first task can be achieved by using a \"Selection Rule\"\nThe below diagram depicts using a filter for the \"Customers\" table on the column \"AgencyID\" where the values are between \"01\" and \"85\"\nThe second task can be achieved by using the JSON template for \"Transformation Rules,\" as shown below.\n{\n\"rules\": [{\n\"rule-type\": \"selection\",\n\"rule-id\": \"1\",\n\"rule-name\": \"1\",\n\"object-locator\": {\n\"schema-name\": \"test\",\n\"table-name\": \"%\"\n},\n\"rule-action\": \"include\"\n}, {\n\"rule-type\": \"transformation\",\n\"rule-id\": \"2\",\n\"rule-name\": \"2\",\n\"rule-action\": \"remove-column\",\n\"rule-target\": \"column\",\n\"object-locator\": {\n\"schema-name\": \"test\",\n\"table-name\": \"latestnews\",\n\"column-name\": \"test%\"\n}\n}]\n}\nThe third task can also be achieved by using the JSON template for \"Transformation Rules\" as shown below.\n{\n\"rules\": [{\n\"rule-type\": \"selection\",\n\"rule-id\": \"1\",\n\"rule-name\": \"1\",\n\"object-locator\": {\n\"schema-name\": \"test\",\n\"table-name\": \"%\"\n},\n\"rule-action\": \"include\"\n}, {\n\"rule-type\": \"transformation\",\n\"rule-id\": \"2\",\n\"rule-name\": \"2\",\n\"rule-action\": \"add-prefix\",\n\"rule-target\": \"table\",\n\"object-locator\": {\n\"schema-name\": \"test\",\n\"table-name\": \"%\"\n},\n\"value\": \"DMS_\"\n}]\n}\nOption A is incorrect.\nThis option says DMS cannot handle migration.\nIndeed migration from MySQL to PostgreSQL is supported by DMS.\nOption B is incorrect.\nThis option says tasks 2 and 3 cannot be done.\nIn fact, tasks 2 and 3 can be achieved, as shown above.\nOption C is incorrect: Because task 3 can be achieved via “Transformation Rules and Actions”\nA reference is in.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.html.\nOption D is CORRECT: Because DMS can perform this migration with all three tasks being achievable.\nAWS Database Migration Service (DMS) is a service that helps migrate databases to AWS. DMS can migrate databases to or from the cloud, including between different database engines. In this scenario, the team plans to migrate their MySQL database to PostgreSQL. Additionally, the development lead has raised extra requirements. Let's examine each answer option in detail to determine the most accurate answer:\nOption A states that DMS cannot handle the task of migration from MySQL to PostgreSQL. Instead, the AWS Schema Conversion Tool should be used to convert the existing database schema, including tables, indexes, and most application code, to the target platform. While the AWS Schema Conversion Tool is a useful tool for database migrations, this answer is not entirely accurate. DMS can handle the task of migration from MySQL to PostgreSQL, as it supports both source and target databases. Therefore, option A is incorrect.\nOption B states that DMS can handle the task of migration from MySQL to PostgreSQL. It can achieve task 1 by using the Selection Rule for column “user_activity” in table “user_info”. However, it cannot do tasks 2 and 3. This option is partially correct. DMS can handle the task of migration from MySQL to PostgreSQL, including task 1, which is migrating the user_activity column from the user_info table. However, DMS cannot handle tasks 2 and 3, which are removing columns starting with \"test\" and renaming a column. Therefore, option B is not the best answer.\nOption C states that DMS can handle the task of migration from MySQL to PostgreSQL. It can achieve task 1 by using the Selection Rule for column “user_activity” in table “user_info”. Task 2 can also be completed by using \"Transformation Rules and Actions\" to remove the columns starting with \"test\". However, for task 3, DMS cannot do that. This option is also partially correct. DMS can handle task 1 and task 2. However, it cannot handle task 3, which is renaming a column. Therefore, option C is not the best answer.\nOption D states that DMS can handle the task of migration from MySQL to PostgreSQL. Furthermore, all tasks can be achieved by specifying Table Selection and Transformations using JSON. This option is the most accurate. DMS can handle the task of migration from MySQL to PostgreSQL, including all three tasks, by specifying Table Selection and Transformations using JSON. JSON allows users to specify how data should be migrated and transformed. Therefore, option D is the best answer.\nIn conclusion, the most accurate answer is option D: DMS can handle the task of migration from MySQL to PostgreSQL, and all tasks can be achieved by specifying Table Selection and Transformations using JSON.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "DMS cannot handle the task of migration from MySQL to PostgreSQL. When database engines need to be changed, use the AWS Schema Conversion Tool to convert existing database schema, including tables, indexes, and most application code, to the target platform.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "DMS can handle the task of migration from MySQL to PostgreSQL. It can achieve task 1 by using the Selection Rule for column “user_activity” in table “user_info”. However, it cannot do tasks 2 and 3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "DMS can handle the task of migration from MySQL to PostgreSQL. It can achieve task 1 by using the Selection Rule for column “user_activity” in table “user_info”. Task 2, can also be completed by using a “Transformation Rules and Actions” to remove the columns starting with “test”. However, for task 3, DMS cannot do that.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "DMS can handle the task of migration from MySQL to PostgreSQL. And all tasks can be achieved by specifying Table Selection and Transformations using JSON.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 143,
  "query" : "A data analysis engineer has an old on-premise database for his meteorology analysis for years.\nThis database is growing too big and becoming less responsive.\nHe prefers to migrate it to AWS DynamoDB, and he already has the mapping rules in place.\nHowever, he has been told that the database type is unsupported by AWS Database Migration Service.\nHe can export the data to CSV format files from the old database.\nHow can the data analysis engineer migrate the data to AWS DynamoDB successfully?",
  "answer" : "Correct Answer - C.\nIn most cases, when someone is migrating to a new database, he has access to the source database and can use the database directly as a source.\nSometimes, however, he might not have access to the source directly.\nIn other cases, the source is really old or possibly unsupported.\nIn these cases, if he can export the data in CSV format, he can still migrate or platform, the data.\nIn this question, DMS does not support this database type.\nHowever, the CSV files can be used after being uploaded to S3\nAWS Database Migration Services (AWS DMS) added support for using Amazon S3 as a source for database migration.\nIf S3 is the source endpoint, an external table definition is required.\nAn external table definition is a JSON document that describes how AWS DMS should interpret the data from Amazon S3\nThe maximum size of this document is 2 MB.\nIf a source endpoint is created using the AWS DMS Management Console, a JSON file can be entered directly into the table mapping box such as:\nOption A is incorrect: Because the mapping rule should be put in the source endpoint configuration rather than the task settings if S3 is the source for DMS.\nOption B is incorrect: Because AWS Database Migration Service is suitable.\nMoreover, Data Pipeline does not deal with the table mappings.\nData Pipeline is suitable for data backup instead of database migration.\nOption C is CORRECT: It correctly describes how to use S3 as the source endpoint and defines the mapping rules.\nBelow is a piece of JSON mapping rule:\nA reference is in https://aws.amazon.com/blogs/database/migrate-delimited-files-from-amazon-s3-to-an-amazon-dynamodb-nosql-table-using-aws-database-migration-service-and-aws-cloudformation/.\nOption D is incorrect: Because DynamoDB cannot resolve the table mapping by uploading CSV files exported from databases.\nData Migration Service can deal with this scenario properly.\nThe correct answer is A. Firstly, upload the CSV files to S3. Create an S3 source endpoint and DynamoDB target endpoint in AWS DMS. Create a migration task by referring to the source and target endpoints. Add the mapping rule in the task using a JSON format.\nExplanation:\nThe given scenario describes a use case where an on-premise database needs to be migrated to AWS DynamoDB. The database type is unsupported by AWS Database Migration Service, and the data analysis engineer has to export the data to CSV format files from the old database.\nTo migrate the data to AWS DynamoDB successfully, the following steps can be performed:\n1.\nUpload CSV files to S3: The first step is to upload the CSV files to an S3 bucket. This can be done by creating a bucket in the AWS Management Console and using the Upload feature to upload the CSV files.\n2.\nCreate S3 source endpoint and DynamoDB target endpoint: In the AWS DMS console, create a source endpoint using the S3 bucket as the source. Then, create a target endpoint using DynamoDB as the target.\n3.\nCreate a migration task: Create a migration task in AWS DMS by referring to the source and target endpoints. This task will be responsible for moving the data from the source endpoint to the target endpoint.\n4.\nAdd mapping rule using JSON format: In the migration task, add a mapping rule to map the data from the CSV files to the DynamoDB table using a JSON format. This mapping rule will ensure that the data is correctly transformed and loaded into the DynamoDB table.\nBy following the above steps, the data analysis engineer can successfully migrate the data from the on-premise database to AWS DynamoDB.\nOption B is incorrect because while AWS Data Pipeline can be used to import data from S3 to DynamoDB, it is not the most appropriate solution for this scenario.\nOption C is incorrect because while it mentions using AWS DMS, it incorrectly suggests adding a table mapping rule with a JSON table structure instead of a JSON format.\nOption D is incorrect because while it correctly mentions that DynamoDB supports the import of CSV files directly, it does not mention the need to upload the files to S3 first. Additionally, it does not provide the steps for creating a proper schema for the DynamoDB table.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Firstly, upload the CSV files to S3. Create an S3 source endpoint and DynamoDB target endpoint in AWS DMS. Create a migration task by referring to the source and target endpoints. Add the mapping rule in the task using a JSON format.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Database Migration Service is inappropriate for this task. Upload the CSV files to S3 and then use an AWS Data Pipeline to import the data from S3 to DynamoDB by an S3-to-DynamoDB template.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Upload the exported CSV files to S3 at first. Then create S3 source endpoint and DynamoDB target endpoint in AWS DMS console. When the S3 source endpoint is configured, add the table mapping rule with a JSON table structure. Create a Replication Task to move the data from the source endpoint to the target endpoint.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Database Migration Service is not needed for this scenario. The AWS DynamoDB supports the import of CSV Excel files directly by console and CLI. Create the DynamoDB table firstly with a proper schema and then import the CSV database data.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 144,
  "query" : "A large company starts to use AWS organizations with the consolidated billing feature to manage its separate departments.\nThe AWS operation team has just created 3 OUs (organization units) with 2 AWS accounts each.\nTo be compliant with company-wide security policy, CloudTrail is required for all AWS accounts which is already been set up.\nHowever, after some time, there are cases that users in certain OU have turned off the CloudTrail of their accounts.\nWhat is the best way for the AWS operation team to prevent this from happening again?",
  "answer" : "Correct Answer - AAWS Organizations has provided two feature sets.\nConsolidated billing - This feature set provides shared billing functionality but does not include the more advanced features of AWS Organizations.\nAll features - The complete feature set that is available to AWS Organizations.\nIt includes all the functionality of consolidated billing and advanced features that give you more control over your organization's accounts.\nFor example, when all features are enabled, the master account of the organization has full control over what member accounts can do.\nThe master account can apply SCPs to restrict the services and actions that users (including the root user) and roles in an account can access.\nIt can prevent member accounts from leaving the organization.\nIn this case, we should use “All features”\nOne thing to note is that the feature sets can be upgraded in flight.\nIt does not need to delete/recreate the AWS Organizations.\nOption A is CORRECT: Because SCP is suitable for limiting actions that AWS accounts in an Organization can do.\nBelow is an example of a deny policy:\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Deny\",\n\"Action\": \"cloudtrail:StopLogging\",\n\"Resource\": \"*\"\n}\n]\n}\nOption B is incorrect: Because it does not need to delete/recreate the AWS Organizations to upgrade feature sets.\nOption C is incorrect: Because although it can potentially work, it has lots of repeatable work and is not straightforward if compared with.\nOption A.Option D is incorrect: Because it does not mention the upgrade of feature sets.\nSecondly, the allow policy is incorrect as this case only requires limiting CloudTrail deletion.\nAllow policy implicitly prevents everything except for several allow items.\nOption A is the best solution for preventing users from turning off CloudTrail in their AWS accounts.\nHere's why:\n1.\nUpdate the AWS Organizations feature sets to “All features” - This will enable the use of Service Control Policies (SCP), which are a powerful tool for managing permissions and access across AWS accounts within an organization. By enabling all features, the organization will have access to a broader range of policy options for managing its accounts.\n2.\nCreate a Service Control Policy (SCP) - SCPs enable organizations to establish granular controls over the actions that can be taken in their AWS accounts. In this case, the organization can create an SCP that denies users from disabling CloudTrail logging in their accounts. This can be achieved by adding a \"deny\" policy statement that explicitly denies the cloudtrail:StopLogging action.\n3.\nApply the SCP to the relevant OUs - Once the SCP is created, it can be applied to the OUs where users have been turning off CloudTrail logging. This will ensure that any future attempts to disable CloudTrail logging in those accounts will be denied.\n4.\nMonitor compliance - The AWS operation team should regularly monitor compliance with the SCP to ensure that users are not attempting to disable CloudTrail logging in their accounts. This can be done by reviewing CloudTrail logs and setting up alerts for any suspicious activity.\nOption B is not the best solution because it involves deleting and recreating the AWS Organizations with \"All features\" enabled. This can be a time-consuming and disruptive process, and may not be necessary if the organization only needs to enable a specific SCP.\nOption C is not the best solution because it involves creating an IAM policy for each AWS account, which can be difficult to manage and enforce across a large number of accounts. Additionally, this approach does not take advantage of the SCP feature provided by AWS Organizations.\nOption D is not the best solution because it suggests using an \"allow\" policy to deny the cloudtrail:StopLogging action. This is not a recommended approach because \"allow\" policies can be difficult to manage and can lead to unintended consequences if not configured correctly. Additionally, it is generally better to use \"deny\" policies to explicitly prohibit certain actions, rather than relying on \"allow\" policies to implicitly grant access.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Update the AWS Organizations feature sets to “All features” and then create a Service Control Policies (SCP) to Prevent Users from Disabling AWS CloudTrail. This can be achieved by a deny policy with cloudtrail:StopLogging denied.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "This can be achieved by Service Control Policies (SCP) in the “All features” set. The team needs to delete and recreate the AWS Organizations with “All features” enabled and then use a proper control policy to limit the operation of cloudtrail:StopLogging.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In each AWS account in this organization, create an IAM policy to deny cloudtrail:StopLogging for all users including administrators.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Service Control Policies (SCP) to prevent users from disabling AWS CloudTrail. This can be done by a allow policy that denies cloudtrail:StopLogging.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 145,
  "query" : "A big company has used AWS Organizations to manage its various AWS accounts by using several organization units.\nThe organization master account is in charge of running the whole organization.\nOne child AWS account belongs to the data analysis department.\nThe company has recently made some organizational adjustments and needs to remove the data analysis department from the existing AWS Organizations.\nHowever, an error happened when the data analysis AWS administrator tried to leave the organization as a member account in the AWS console.\nWhich below options are possible reasons for the failure? Select 2.",
  "answer" : "Correct Answer - A, C.\nAccording to https://docs.aws.amazon.com/organizations/latest/userguide/orgs_troubleshoot_general.html#troubleshoot_general_error-leaving-org, if an error happens when a member leaves an organization, two things need to be checked:\nYou can remove a member account only after enabling IAM user access to billing in the member account.\nYou can remove an account from your organization only if the account has the information required to operate as a standalone account.\nFor the IAM user access to billing settings, log in to the AWS console and modify that in “My account” -&gt; “IAM User and Role Access to Billing Information”:\nOther than this, there are some minimum IAM policy requirements for Leaving Organizations as a Member Account (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html#orgs_manage_accounts_leave-as-member):\norganizations:DescribeOrganization (console only).\norganizations:LeaveOrganization - Note that the organization administrator can apply a policy to your account that removes this permission, preventing you from removing your account from the organization.\nIf you sign in as an IAM user and the account is missing payment information, the IAM user must have the permissions aws-portal:ModifyBilling and aws-portal:ModifyPaymentMethods.\nOption A is CORRECT: Because this is a necessary step before leaving AWS Organizations.\nOption B is incorrect: Because there is no such limitation for overdue bills.\nOption C is CORRECT: Because in order to leave an AWS Organizations in console, organizations:DescribeOrganization and organizations:LeaveOrganization are required.\nOption D is incorrect: Because the member account can leave AWS Organizations as long as it meets the above requirements.\nAWS Organizations is a service that helps to centrally manage and govern multiple AWS accounts. It allows an organization to create separate accounts for different teams, projects, or applications, and manage them under a single master account. AWS Organizations provides various features such as consolidated billing, access control, and policy enforcement across member accounts.\nIn this scenario, the company has used AWS Organizations to manage its AWS accounts, and the data analysis department has a child account. Due to some organizational adjustments, the company wants to remove the data analysis department from AWS Organizations.\nWhen a member account tries to leave an AWS organization, several checks are performed before the account is removed from the organization. If any of these checks fail, the account cannot leave the organization, and an error occurs.\nThe possible reasons for the failure of the data analysis department's AWS account to leave the organization are:\nA. The member account was removed before the IAM user access to billing in the member account was enabled. This setting controls the access to Account Settings, Payment Methods, and Report pages.\nBefore an AWS account can leave an organization, the IAM user of that account needs to have access to the billing information of the account. This access can be granted by enabling the \"IAM user access to billing information\" option in the billing and cost management console. If this option is not enabled, the member account cannot leave the organization, and an error occurs.\nB. The member account has bills that are already overdue for several days. All overdue bills need to be paid before the account is removed from the AWS Organizations.\nIf a member account has overdue bills, it cannot leave the organization until all the bills are paid. This is because the master account is responsible for paying the bills of all member accounts under the organization. If any account has overdue bills, the master account cannot remove it from the organization until the bills are paid.\nC. The IAM user of the member account does not have the permission of “organizations:DescribeOrganization” or “organizations:LeaveOrganization” so that it is blocked by IAM policy.\nThe IAM user of the member account needs to have the necessary permissions to leave the organization. If the IAM policy attached to the user does not allow the \"organizations:DescribeOrganization\" or \"organizations:LeaveOrganization\" actions, the member account cannot leave the organization.\nD. Member account cannot leave AWS Organizations by itself. Instead, the root account can remove member account if it has “organizations:RemoveAccountFromOrganization” permission.\nA member account cannot leave the organization by itself. Instead, the root account of the organization needs to remove the member account using the \"organizations:RemoveAccountFromOrganization\" permission. If the IAM user of the member account does not have this permission, the member account cannot leave the organization.\nIn conclusion, the possible reasons for the failure of the data analysis department's AWS account to leave the organization are related to access to billing information, overdue bills, IAM permissions, and the need for the root account to remove the member account. The appropriate steps need to be taken to address these issues before the member account can leave the organization.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The member account was removed before the IAM user access to billing in the member account was enabled. This setting controls the access to Account Settings, Payment Methods, and Report pages.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The member account has bills that are already overdue for several days. All overdue bills need to be paid before the account is removed from the AWS Organizations.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The IAM user of the member account does not have the permission of “organizations:DescribeOrganization” or “organizations:LeaveOrganization” so that it is blocked by IAM policy.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Member account cannot leave AWS Organizations by itself. Instead, the root account can remove member account if it has “organizations:RemoveAccountFromOrganization” permission.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 146,
  "query" : "As an AWS Administrator, you have set up an ELB within a couple of Availability Zones.\nThe ELB distributes application traffic to an Auto Scaling group with session affinity enabled.\nYou notice that the traffic is not being evenly distributed across the AZ's.\nWhich option may help to alleviate this issue? Choose an answer from the below options.",
  "answer" : "Answer - A.\nThe traffic is not evenly distributed across the instances in multiple AZs.\nThat means the traffic is going to only specific EC2 instances.\nThis happens when either the instances that are not receiving the traffic are unhealthy or the instances receiving the traffic are holding onto the session.\nThis scenario does not mention any unhealthy instances.\nSo, it is most likely related to instances holding onto sessions.\nThis means the ELB has sticky sessions enabled.\nOption A is CORRECT because this situation occurs when ELB has sticky sessions or session affinity enabled.\nOption B is incorrect because reducing the frequency of health checks will not force the even distribution of the traffic.\nOption C is incorrect because if sticky sessions are enabled, increasing the number of instances in each AZ will not help receive the traffic.\nIn fact, more instances will remain idle now.\nOption D is incorrect because recreating ELB again will not resolve this issue.\nMore information on ELB Sticky Sessions:\nThe load balancer uses a special cookie to track the instance for each request to each listener.\nWhen the load balancer receives a request, it first checks to see if this cookie is present in the request.\nIf so, the request is sent to the instance specified in the cookie.\nIf there is no cookie, the load balancer chooses an instance based on the existing load balancing algorithm.\nA cookie is inserted into the response for binding subsequent requests from the same user to that instance.\nThe stickiness policy configuration defines a cookie expiration, which establishes the duration of validity for each cookie.\nThis could be a reason as to why the sessions are going to a certain AZ.\nFor more information on ELB sticky sessions, please refer to the below URL-\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html\nThe issue here is that the traffic is not being evenly distributed across the Availability Zones (AZs) despite having session affinity enabled. Session affinity, also known as sticky sessions, ensures that a client's subsequent requests are sent to the same instance that handled their initial request. This feature can lead to an imbalance in traffic distribution if the same AZ has a large number of active sessions.\nOut of the options given, the best solution to alleviate this issue is to increase the number of instances hosting the web application in each AZ (Option C). This will increase the number of available instances to handle requests, leading to better load distribution. When the number of available instances is higher, the ELB can distribute incoming traffic more evenly among them, reducing the load on any single instance and ensuring that the overall load is balanced across all AZs.\nDisabling sticky sessions (Option A) is not a good solution because it will cause the loss of session state for clients, leading to potential data loss and a poor user experience. Sticky sessions are essential for maintaining user state and providing a consistent user experience.\nReducing the frequency of health checks (Option B) may cause the ELB to route traffic to unhealthy instances, resulting in degraded performance or unavailability of the application. Health checks are essential for ensuring that the instances are available and healthy to serve traffic.\nRecreating the ELB (Option D) will not solve the issue because the problem is not with the ELB itself but with the uneven distribution of traffic across the AZs.\nIn conclusion, the best solution to alleviate the issue of uneven traffic distribution is to increase the number of instances hosting the web application in each AZ.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Disable sticky sessions on the EL.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Reduce the frequency of the health checks.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Increase the number of instances hosting the web application in each AZ.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Recreate the ELB again.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 147,
  "query" : "In Cloudfront, what is the Origin Protocol policy that must be chosen to ensure that the communication with the origin is done either via HTTP or HTTPS? Choose an answer from the options below.",
  "answer" : "Answer - C.\nIt is clearly given in the AWS documentation that the Origin Protocol Policy should be set accordingly.\nOptions A, B, and D are all incorrect because the answer is Match Viewer.\nOption C is CORRECT because if the Origin Protocol Policy is set to Match Viewer, the CloudFront communicates with the origin using HTTP or HTTPS, depending on the viewer's protocol.\nFor more information on Cloudfront CDN, please see the below link-\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html\nThe correct answer is C. Match Viewer.\nWhen using Amazon CloudFront as a content delivery network, the Origin Protocol Policy determines the protocol that CloudFront uses to communicate with the origin server.\nThere are three options for the Origin Protocol Policy:\n1. HTTP Only: With this policy, CloudFront always communicates with the origin server using HTTP.\n2. HTTPS Only: With this policy, CloudFront always communicates with the origin server using HTTPS.\n3. Match Viewer: With this policy, CloudFront communicates with the origin server using the same protocol that the viewer used to request the content. If the viewer used HTTP, then CloudFront will communicate with the origin server using HTTP. If the viewer used HTTPS, then CloudFront will communicate with the origin server using HTTPS.\nThe Match Viewer option is useful when you want to ensure that communication with the origin is done either via HTTP or HTTPS, depending on the viewer's request. This is important because HTTPS is a more secure protocol, but there are some situations where using HTTP is necessary, such as when the origin server doesn't support HTTPS.\nTherefore, the correct option to choose when you want to ensure that the communication with the origin is done either via HTTP or HTTPS is the Match Viewer option.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "HTTP",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "HTTPS",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Match Viewer",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "None of the above.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 148,
  "query" : "An IoT company needs to develop a product that can quickly count the number of persons in a given area.\nThey have used wireless sensors and a Node.js backend in AWS EC2 (ap-southeast-2 region with 3 availability zones)\nAs the data is very sensitive which will be analyzed by a third-party company, they need the backend to be highly available.\nThe backend EC2 needs to connect to the internet to download patches.\nOther than that, for security reasons, EC2 should only open SSH port to a jump host.\nFor the below descriptions, which one is the best?",
  "answer" : "Correct Answer - C.\nAs in the scenario, high availability is a must.\nWe should consider using public/private pairs in all three availability zones ap-southeast-2a, ap-southeast-2b, and ap-southeast-2c.\nThen create a NAT gateway in each public subnet as NAT gateway is within the scope of an availability zone.\nThis can ensure that other NAT gateways can still server internet traffic even when an availability zone is out of service.\nIn terms of the Bastion host, it should be put in a public subnet, and the EC2 security group should allow the SSH access (port 22) for the inbound traffic from the Bastion host security group.\nOption A is incorrect: Because NAT gateway should be created in each availability zone instead of meeting the need for high availability.\nAnd bastion host ARN id is incorrect as well.\nBastion host security group should be used.\nOption B is incorrect: Because NAT gateway is better than NAT instance as NAT gateway is more reliable and easier to maintain.\nMoreover, the route should connect to the NAT gateway.\nOption C is CORRECT: Because it has used NAT gateway in three availability zones and Bastion host in a public subnet, which is the best way.\nOption D is incorrect: Because it should create three private subnets to make three public/private pairs.\nIt can ensure that the service is not interrupted even when one availability zone is out of service.\nMoreover, NAT gateway is highly managed by AWS, and autoscaling is impossible to be added for that.\nThe best option for the IoT company would be Option A.\nOption A suggests creating a VPC with three availability zones in the ap-southeast-2 region. For each availability zone, a public subnet and a private subnet should be created. A NAT gateway should be created in a single public subnet, and for the route table in the private subnets of all three availability zones, a route from 0.0.0.0/0 to the NAT gateway should be added.\nA bastion host should be created in one public subnet, and for EC2 instances, only port 22 should be opened for inbound traffic from the bastion host ARN id. This configuration will provide highly available connectivity to the internet, secure SSH access to the EC2 instances, and keep sensitive data within the VPC's private subnet.\nOption B suggests creating a VPC with three availability zones in the ap-southeast-2 region. For each availability zone, a public subnet and a private subnet should be created. A NAT instance should be created in each public subnet, and for the route table in the private subnets of all three availability zones, a route from 0.0.0.0/0 to the public subnet should be added.\nA bastion host should be created in one public subnet, and for EC2 instances, only port 22 should be opened for inbound traffic from the bastion host ARN id. This option is less optimal than Option A because NAT instances are less scalable and less available than NAT gateways.\nOption C suggests creating a VPC with three availability zones in the ap-southeast-2 region. For each availability zone, a public subnet and a private subnet should be created. A NAT gateway should be created in each public subnet, and for the route table in the private subnets of all three availability zones, a route from 0.0.0.0/0 to the NAT gateway should be added.\nA bastion host should be created in one public subnet, and for EC2 instances, only port 22 should be opened for inbound traffic from the security group of the bastion host. This option is less optimal than Option A because it creates three NAT gateways instead of one, which is unnecessary and more expensive.\nOption D suggests creating a private subnet in ap-southeast-2a and three public subnets in ap-southeast-2a, ap-southeast-2b, and ap-southeast-2c. A NAT gateway should be created using an autoscaling group in all three availability zones, and for the route table in the private subnet of ap-southeast-2a, a route from 0.0.0.0/0 to the NAT gateway should be added.\nA bastion host should be created in one public subnet, and for EC2 instances, only port 22 should be opened for inbound traffic from the security group of the bastion host. This option is less optimal than Option A because it creates unnecessary complexity by using an autoscaling group for the NAT gateway and only provides a NAT gateway in one availability zone instead of three.\nIn summary, Option A is the best option because it creates a VPC with three availability zones and a single NAT gateway for all three private subnets. It also provides secure SSH access to the EC2 instances through a bastion host and keeps sensitive data within the private subnet.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "For each availability zone, create a public subnet and a private subnet. Create a NAT gateway in a single public subnet and for the route table in three private subnets, add a route from 0.0.0.0/0 to the NAT gateway. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the bastion host ARN id.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a public subnet and a private subnet in all three availability zones in the ap-southeast-2 region. Create a NAT instance in each public subnet and for the route table in the private subnet, add a route from 0.0.0.0/0 to the public subnet. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the bastion host ARN id.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For each availability zone, create a public subnet and a private subnet. Create a NAT gateway in each public subnet and for the route table in the private subnet, add a route from 0.0.0.0/0 to the NAT gateway. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the security group of the bastion host.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a private subnet in ap-southeast-2a and three public subnets in ap-southeast-2a, ap-southeast-2b, and ap-southeast-2c. Create a NAT gateway using an autoscaling group in all three availability zones and for the route table in the private subnet, add a route from 0.0.0.0/0 to the NAT gateway. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the security group of the bastion host.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 149,
  "query" : "You are designing a multi-platform,multi-port web application for AWS.\nThe application will run on EC2 instances and will be accessed from PCs, tablets, and smartphones.\nSupported accessing platforms are Windows, MAC OS, IOS, and Android.\nSeparate SSL certificate setups are required for different platform types.\nWhich of the following describes the most cost-effective and performance efficient architecture setup?",
  "answer" : "Answer - B.\nIn this scenario, the main architectural considerations are (1) web application has EC2 instances running multiple platforms such as Android, iOS, etc., and (2) separate SSL certificate setups are required for different platforms.\nOptions A is incorrect because it is not cost-effective to handle such hybrid architecture.\nOption B is correct.\nOriginally, Application Load Balancers used to support only one certificate for a standard HTTPS listener (port 443)\nYou had to use Wildcard or Multi-Domain (SAN) certificates to host multiple secure applications behind the same load balancer.\nThe potential security risks with Wildcard certificates and the operational overhead of managing Multi-Domain certificates presented challenges.\nWith SNI support you can associate multiple certificates with a listener and each secure application behind a load balancer can use its own certificate.You can use host conditions to define rules that forward requests to different target groups based on the hostname in the host header (also known as host-based routing)\nThis enables you to support multiple domains using a single load balancer.\nOption C is incorrect because Classic Load Balancer cannot handle multiple SSL certificates.\nOption D is incorrect as it is not required since there is support for multiple TLS/SSL certificates on Application Load Balancers.\nIt is not cost-efficient as one ALB can achieve the requirement.\nFor more information on ELB, please visit the below URL-\nhttps://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/\nAmong the given options, the most cost-effective and performance efficient architecture setup for the multi-platform, multi-port web application would be option B, which is to set up an Application Load Balancer (ALB) with Server Name Indicator (SNI) support to handle separate SSL certificates for each device platform.\nExplanation of each option:\nOption A, which suggests setting up a hybrid architecture to handle session state, SSL certificates, on-premise resources, and EC2 instances that run the web applications for different platform types in a VPC, can be a complex setup that requires additional resources and management overhead. This option does not necessarily provide the most cost-effective and performance-efficient solution.\nOption C, which suggests setting up two Classic Load Balancers (CLBs) for SSL certificates and session stickiness respectively, each running separate EC2 instance groups for each platform type, can be a costly setup. CLBs are considered legacy load balancers and do not offer the advanced features of ALBs, such as SNI support, which is essential for handling separate SSL certificates for each device platform.\nOption D, which suggests assigning multiple ALBs to a group of EC2 instances running the common components of the web application, with one ALB for each platform type, session stickiness, and SSL termination, can also be a complex setup that requires additional resources and management overhead. This option may result in increased costs and reduced performance due to the increased number of load balancers.\nTherefore, option B is the most suitable option for handling the multi-platform, multi-port web application. ALBs with SNI support allow the use of multiple SSL certificates on a single IP address, enabling the handling of separate SSL certificates for each device platform. This setup is cost-effective and performance-efficient, as it does not require additional resources and management overhead, and can handle the application's needs effectively.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Setup a hybrid architecture to handle session state, SSL certificates, on-premise resources, and EC2 Instances that run the web applications for different platform types in a VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up an Application Load Balancer with Server Name Indicator (SNI) support, for handling the separate SSL certificates for each of the device platforms.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up two Classic Load Balancers. The first ELB handles SSL certificates for all platforms and the second ELB handles session stickiness for all platforms. For each ELB run separate EC2 instance groups to handle the web application for each the platforms.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Assign multiple ALBs to a group of EC2 instances running the common components of the web application. One ALB for each platform type, Session stickiness, and SSL termination.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 150,
  "query" : "A software development team just finished phase 1 of a web service that provides NBA news to subscribers.\nThe web service has used a dedicated VPC which has only IPv4 CIDR (10.0.0.0/16) with two public subnets and two private subnets.\nA NAT gateway is put into each public subnet for outbound internet traffic.\nThe EC2 instances are put into private subnets with a route that connects all Internet-bound IPv4 traffic to the relevant NAT gateway.\nThe product is getting more and more popular and needs IPv6 to support some new features.\nWhich below options are required for the new support for IPv6? Select 3.",
  "answer" : "E.\nF.\nCorrect Answer - B, C, F.\nRefer to https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html for the details on how to migrate to IPv6 traffic.\nPlease also note that this question asks for the necessary options.\nThe below steps are required.\nStep 1: Associate an IPv6 CIDR Block with Your VPC and Subnets.\nStep 2: Update Your Route Tables.\nStep 3: Update Your Security Group Rules.\nStep 4: Change Your Instance Type.\nStep 5: Assign IPv6 Addresses to Your Instances.\nOption A is incorrect: Because you do not need to delete and recreate VPC to support IPv6.\nOption B is CORRECT: Because that is a required step to add IPv6 CIDR in both VPC and subnets.\nOption C is CORRECT: Because the routing table needs to be modified to route the IPv6 traffic properly.\nOption D is incorrect: Because the NAT gateway is IPv4 only.\nFor IPv6, an egress-only internet gateway should be used.\nRefer to https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html.\nOption E is incorrect: Same reason as option.\nD.\nAlso, you cannot add IPv6 IP range to the NAT gateway.\nOption F is CORRECT: Because EC2 instances need IPv6 addresses to route IPv6 traffic.\nThis can be done in “Actions, Networking, Manage IP Addresses” and choose “Assign new IP” Under “IPv6 Addresses”.\nSure, I can provide a detailed explanation for each option and why it is required for the new support for IPv6.\nOption A: Delete existing VPC and recreate a new VPC with both IPv4 and IPv6 CIDR. Create new public and private subnets with both IPv4 and IPv6 address ranges.\nThis option involves creating a new VPC that supports both IPv4 and IPv6 CIDR blocks, as well as creating new public and private subnets with both IPv4 and IPv6 address ranges. This option may be necessary if the existing VPC and subnets were not designed to support IPv6.\nOption B: Associate an Amazon-provided IPv6 CIDR block with existing VPC and subnets. In the VPC and subnets console, choose “Add IPv6 CIDR”.\nThis option involves associating an Amazon-provided IPv6 CIDR block with the existing VPC and subnets. This option is recommended if the existing VPC and subnets were designed to support IPv6 but were not configured with an IPv6 CIDR block.\nOption C: For public subnets, create a route that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, create a route that routes all Internet-bound IPv6 traffic to an egress-only internet gateway.\nThis option involves creating a route in the route table that routes all IPv6 traffic from the public subnet to the internet gateway, and creating a route in the route table that routes all Internet-bound IPv6 traffic from the private subnet to an egress-only internet gateway. This option is required to ensure that IPv6 traffic can flow correctly between the VPC and the internet.\nOption D: Update the route tables to route the IPv6 traffic. For public subnets, create a route that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, create a route that routes all Internet-bound IPv6 traffic to NAT gateway.\nThis option involves updating the route tables to route IPv6 traffic correctly. For public subnets, a route is created that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, a route is created that routes all Internet-bound IPv6 traffic to the NAT gateway. This option is required to ensure that IPv6 traffic can flow correctly between the VPC and the internet.\nOption E: Assign IPv6 addresses to NAT gateway which will be used to route the internet-bound IPv6 traffic from EC2 instances.\nThis option involves assigning IPv6 addresses to the NAT gateway so that it can route Internet-bound IPv6 traffic from the EC2 instances. This option is required to ensure that IPv6 traffic can flow correctly from the private subnets to the internet.\nOption F: Assign IPv6 addresses to EC2 instances from the IPv6 address range that is allocated to the subnet.\nThis option involves assigning IPv6 addresses to the EC2 instances from the IPv6 address range that is allocated to the subnet. This option is required to ensure that the EC2 instances can communicate with each other over IPv6 and that they can communicate with other IPv6-enabled resources in the VPC.\nIn summary, the three options that are required for the new support for IPv6 are:\nAssociate an Amazon-provided IPv6 CIDR block with existing VPC and subnets.\nCreate routes in the route table that route IPv6 traffic correctly.\nAssign IPv6 addresses to the EC2 instances from the IPv6 address range that is allocated to the subnet.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Delete existing VPC and recreate a new VPC with both IPv4 and IPv6 CIDR. Create new public and private subnets with both IPv4 and IPv6 address ranges.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Associate an Amazon-provided IPv6 CIDR block with existing VPC and subnets. In the VPC and subnets console, choose “Add IPv6 CIDR”.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "For public subnets, create a route that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, create a route that routes all Internet-bound IPv6 traffic to an egress-only internet gateway.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Update the route tables to route the IPv6 traffic. For public subnets, create a route that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, create a route that routes all Internet-bound IPv6 traffic to NAT gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Assign IPv6 addresses to NAT gateway which will be used to route the internet-bound IPv6 traffic from EC2 instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Assign IPv6 addresses to EC2 instances from the IPv6 address range that is allocated to the subnet.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 151,
  "query" : "You are responsible for a legacy web application whose server environment is approaching the end of life.\nYou would like to migrate this application to AWS as quickly as possible since the application environment currently has the following limitations.",
  "answer" : "Answer - B.\nOption A is Incorrect because EC2 VM Import Connector is a facility that builds on top of VM Import.\nWe need the VM import in place for the connector to work.\nhttps://aws.amazon.com/blogs/aws/ec2-vm-import-connector/\nOption B is correct because VM Import/Export offers several ways to import your virtual machine into Amazon EC2\nVM Import/Export enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances and export them back to your on-premises environment.\nOption C is incorrect because the backup that is taken and stored on S3 may not be directly restored as an EC2 instance, and (b) it may not meet the RPO of 1 hour as this process will be slow for a large number of servers.\nOption D is incorrect because (a) it is applicable to only instance store-backed Windows instance and the data on the volumes other than the root device volume does not get preserved, and (b) this API is not applicable to the Windows instances that are backed by EBS volumes.\nFor more information on EC2 VM Import/Export, please see the URL below.\nhttps://aws.amazon.com/ec2/vm-import/\nSure, I can provide you with a detailed explanation of the options provided in the question.\nA. Use the EC2 VM Import Connector for vCenter to import the VM into EC2: The EC2 VM Import Connector for vCenter is a tool that enables you to import virtual machines (VMs) from your VMware vCenter environment to Amazon EC2 as Amazon Machine Images (AMIs). This option allows you to migrate your legacy web application to AWS by importing the VM directly into EC2. This approach can be a quick and efficient way to migrate your application to AWS, as it eliminates the need to manually configure your new environment in EC2. However, you will need to ensure that the VM meets the prerequisites for the import process, such as the correct VM format, required permissions, and appropriate network settings.\nB. Use VM Import/Export to import the VM as an Amazon Machine Image (AMI). Launch EC2 instances from the AMI: VM Import/Export is a service that enables you to import virtual machine images from your existing environment to Amazon EC2, and export them back to your on-premises environment. This option allows you to migrate your legacy web application to AWS by importing the VM as an AMI and launching EC2 instances from the AMI. This approach can be a good option if you need to modify the VM before importing it into AWS, such as updating the operating system or software, as it allows you to create a customized AMI. However, the import process can take longer than using the VM Import Connector for vCenter, as it requires additional steps.\nC. Use S3 to create a backup of the VM and restore the data into EC2: Using Amazon S3 to create a backup of the VM and restoring the data into EC2 can be a viable option if you want to migrate your legacy web application to AWS incrementally. This approach involves creating a backup of the VM using a third-party tool or utility and storing it in Amazon S3, then restoring the data into EC2. This option can be useful if you need to migrate large amounts of data gradually or have complex data sets that require extensive testing. However, this approach can be time-consuming and require additional steps to complete the migration process.\nD. Use the EC2's bundle-instance API to import an image of the VM into EC2: The EC2 bundle-instance API enables you to create an Amazon Machine Image (AMI) from a running or stopped instance in Amazon EC2. This option allows you to migrate your legacy web application to AWS by creating an image of the VM and importing it into EC2. This approach can be useful if you want to customize the image before importing it, such as updating the operating system or software. However, it may require additional steps to configure the new environment in EC2, and the import process can take longer than using the VM Import Connector for vCenter.\nIn summary, the most appropriate option for migrating your legacy web application to AWS will depend on your specific requirements and constraints. If you need a quick and efficient migration, using the EC2 VM Import Connector for vCenter or VM Import/Export may be the best option. If you need to migrate incrementally or have complex data sets, using S3 to create a backup and restore the data may be a better fit. And if you need to customize the VM image before importing it, using the EC2 bundle-instance API may be the way to go.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the EC2 VM Import Connector for vCenter to import the VM into EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use VM Import/Export to import the VM as an Amazon Machine Image (AMI). Launch EC2 instances from the AMI.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use S3 to create a backup of the VM and restore the data into EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the EC2’s bundle-instance API to import an image of the VM into EC2.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 152,
  "query" : "An administrator in your company has created a VPC with an IPv4 CIDR block 10.0.0.0/24\nNow they want to add additional address space outside of the current VPC CIDR.\nBecause there is a requirement to host more resources in that VPC.\nWhich of the below requirement can be used to accomplish this? Choose an answer from the below options.",
  "answer" : "Answer - B.\nAn existing CIDR for a VPC is not modifiable.\nHowever, you can add additional CIDR blocks, i.e., up to four secondary IPv4 CIDR blocks to an already existing VPC.Option A is incorrect because you can change the CIDR of VPC by adding up to 4 secondary IPv4 IP CIDRs to your VPC.Option B is CORRECT because you can expand your existing VPC by adding up to four secondary IPv4 IP ranges (CIDRs) to your VPC.Option C is incorrect because deleting the subnets is unnecessary.\nOption D is incorrect because this configuration would peer the VPC.\nIt will not alter the existing VPC's CIDR.\nFor more information on VPC and its FAQs, please refer to the following links-\nhttps://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-now-allows-customers-to-expand-their-existing-vpcs/ https://aws.amazon.com/vpc/faqs/\nThe correct answer to the question is B. Expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VPC.\nExplanation: A Virtual Private Cloud (VPC) in AWS is a virtual network that allows you to launch AWS resources such as Amazon EC2 instances, RDS databases, and Elastic Load Balancers, among others. When you create a VPC, you specify an IPv4 CIDR block for the VPC, which is the IP address range for the VPC. This IP address range can be expanded or decreased based on your requirements.\nIn this scenario, the administrator has created a VPC with an IPv4 CIDR block of 10.0.0.0/24, which means that the VPC can have up to 256 IP addresses. However, now they want to add more resources to the VPC, which would require more IP addresses. To accomplish this, the administrator needs to expand the IP address range of the VPC.\nOption A, \"You cannot change a VPC's size. Currently, to change the size of a VPC, you must terminate your existing VPC and create a new one,\" is incorrect because you can change the size of a VPC by expanding or decreasing the IPv4 CIDR block.\nOption C, \"Delete all the subnets in the VPC and expand the VPC,\" is also incorrect because deleting all the subnets in the VPC would not expand the IP address range of the VPC.\nOption D, \"Create a new VPC with a greater range and then connect the older VPC to the newer one,\" is technically possible, but it is not the most efficient solution. Connecting two VPCs requires setting up a VPC peering connection, which adds complexity and can result in higher network latency.\nOption B, \"Expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VPC,\" is the correct answer. You can add secondary IPv4 CIDR blocks to a VPC, which increases the IP address range of the VPC. This can be done by going to the VPC console, selecting the VPC, and then choosing \"Edit CIDRs\" from the \"Actions\" menu. You can then add one or more CIDR blocks to the VPC. Once you have added the CIDR blocks, you can create new subnets in the VPC using those CIDR blocks, which will provide more IP addresses for your resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You cannot change a VPC`s size. Currently, to change the size of a VPC, you must terminate your existing VPC and create a new one.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VP.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Delete all the subnets in the VPC and expand the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new VPC with a greater range and then connect the older VPC to the newer one.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 153,
  "query" : "A company has a legacy-based software that needs to be transferred to the AWS cloud.\nThe legacy-based software has a dependency on the license which is based on the MAC Address.\nWhat would be a possible solution to ensure that the legacy-based software will always work properly and not lose the MAC address at any point in time? Choose an answer from the below options.",
  "answer" : "Answer - D.\nOption A is incorrect because you cannot map a static IP address to a MAC address.\nOption B is incorrect because putting a license server in a private subnet would not resolve the dependency on the license based on a MAC address.\nOption C is incorrect because MAC addresses cannot be tied to subnets.\nOption D is CORRECT because you should use the Elastic Network Interface associated with a fixed MAC address.\nThis will ensure that the legacy license-based software would always work and not lose the MAC address at any point in the future.\nFor more information on Elastic Network Interfaces, please refer to the URL below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\nThe correct answer is D: Use a VPC with instances having an elastic network interface attached that has a fixed MAC Address.\nExplanation: When transferring a legacy-based software to the AWS cloud, it is important to consider the licensing requirements. If the license is based on the MAC address, it is essential to ensure that the software continues to function properly without losing the MAC address at any point in time.\nIn this scenario, using a VPC (Virtual Private Cloud) with instances having an elastic network interface (ENI) attached that has a fixed MAC address would be the most appropriate solution.\nAn elastic network interface is a virtual network interface that can be attached to an EC2 instance in a VPC. It provides additional network interfaces for an instance, including one or more IPv4 or IPv6 addresses, and a MAC address.\nBy attaching an ENI with a fixed MAC address to the EC2 instance running the legacy-based software, the software can maintain its licensing requirement as the MAC address will remain constant even if the EC2 instance is stopped and restarted.\nOption A (Ensure any EC2 Instance that you deploy has a static IP address mapped to the MAC address) is incorrect because a static IP address does not guarantee that the MAC address will remain the same when the instance is stopped and restarted.\nOption B (Use a VPC with a private subnet for the license and a public subnet for the EC2) is also incorrect because it does not address the issue of the licensing requirement based on the MAC address.\nOption C (Use a VPC with a private subnet and configure the MAC address to be tied to that subnet) is incorrect because it is not possible to tie a MAC address to a subnet directly.\nTherefore, option D is the most appropriate solution to ensure that the legacy-based software will always work properly and not lose the MAC address at any point in time.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Ensure any EC2 Instance that you deploy has a static IP address mapped to the MAC address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a VPC with a private subnet for the license and a public subnet for the EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a VPC with a private subnet and configure the MAC address to be tied to that subnet.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a VPC with instances having an elastic network interface attached that has a fixed MAC Address.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 154,
  "query" : "Your customer has recently completed a Proof of Concept (PoC) and now wants you to recommend a database solutions in AWS.\nThe PoC deployed 100 sensors to measure street noise and air quality for 3 months.\nDuring the PoC, your peak IOPS was 10, and the database received 30 GB worth of key-value sensor data per month.\nYou are required to provide an architecture for 100,000 sensors initially and accommodate future scaling needs.\nThe sensor data must be retained for a least two years to compare yearly improvements.\nWhich AWS storage solution requires the least amount of change to the application?",
  "answer" : "Answer - A.\nThe key point here is that the data is being sent key-value pairs from the sensors.\nWhenever you see key-value, you should immediately think of the Amazon no-SQL database DynamoDB.\nDynamoDB has nearly infinite capacity, sub-second latency, and designed to ingest large amounts of data.\nOption A is correct as DynamoDB is best suited to store and analysis key-value data.\nReference - https://aws.amazon.com/nosql/key-value/\nOption B is incorrect as MySQL has a limit of 16 TB, and a no-SQL database is recommended for key-value pairs.\nReference - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.FileSize.\nOption C is incorrect as Kinesis is not a storage solution and only stores data for up to 7 days.\n(24 hours by default) The data from the sensors doesn't need to be analyzed in real-time or converted from a key-value pair.\nSo Kinesis isn't needed.\nReference - https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html.\nOption D is incorrect as it would require a change to the application.\nThe PoC sent the data to a table in a database to use S3\nWe would need to change the application to add a log file to S3, as S3 is object storage.\n(.txt, .jpg, mp4, etc.)\nMore importantly, S3 is not an ideal service for database storage.\nSo it is not suitable for this case.\nReference - https://aws.amazon.com/s3/\nIf you are interested in learning more about how to use DynamoDB for high-volume, time-series data.\nHere is a reference link on the Database Blog from 2019:\nhttps://aws.amazon.com/blogs/database/design-patterns-for-high-volume-time-series-data-in-amazon-dynamodb/\nBased on the given scenario, the customer needs a database solution for their sensor data that will be collected from 100,000 sensors. The data will be retained for at least two years, and the customer wants to compare yearly improvements. Additionally, the solution must require the least amount of change to the application.\nOf the options given, DynamoDB is the most suitable solution for the customer's requirements. Here's why:\nOption A: DynamoDB DynamoDB is a fully managed NoSQL database service that can handle massive amounts of data and scale to accommodate growing needs. It's ideal for applications that require high performance and low latency, such as IoT applications. DynamoDB's flexible data model can handle key-value data, which is what the sensor data is in the given scenario. DynamoDB can also store and retrieve large volumes of data, making it suitable for storing two years' worth of data from 100,000 sensors. Finally, DynamoDB is fully managed, meaning the customer won't need to worry about infrastructure management or scaling the database as the application grows.\nOption B: RDS MySQL Amazon RDS is a managed relational database service that supports various database engines, including MySQL. While RDS MySQL can handle the amount of data being collected, it's not the best solution for the given scenario because it's a relational database, and the data collected is key-value. Moreover, the application may require significant changes to accommodate MySQL's schema and data types. Additionally, as the data grows, it may become difficult to scale RDS MySQL to accommodate the increasing storage needs.\nOption C: Kinesis Amazon Kinesis is a managed service for real-time processing of streaming data at scale. While Kinesis is suitable for real-time data processing and analytics, it's not the best solution for the given scenario. The sensor data is not real-time and is instead collected over three months. Kinesis is also designed for processing data as it streams in, whereas the sensor data in the given scenario is collected and then stored for analysis.\nOption D: S3 Amazon S3 is a scalable object storage service that can store and retrieve large volumes of data. While S3 can store the sensor data, it's not the best solution for the given scenario because it's not a database service and doesn't offer the querying and indexing capabilities required for analyzing sensor data. Additionally, storing the data in S3 may require significant changes to the application to access the data and perform analysis.\nIn conclusion, based on the given scenario, DynamoDB is the best solution for the customer's requirements because it can handle massive amounts of data and scale to accommodate growing needs. Moreover, DynamoDB's flexible data model can handle key-value data, making it suitable for storing sensor data. Finally, DynamoDB is fully managed, meaning the customer won't need to worry about infrastructure management or scaling the database as the application grows.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "DynamoDB",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "RDS MySQL",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Kinesis",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "S3",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 155,
  "query" : "AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)-aware applications in the cloud.\nIt also offers those same choices to developers who need a directory to manage users, groups, devices, and access.\nSimple AD is a Microsoft Active Directory-compatible directory from AWS Directory Service powered by Samba 4\nWhich below scenarios are suitable for Simple AD? Choose 3.",
  "answer" : "E.\nF.\nCorrect Answer - B, D, F.\nSimple AD is a subset of the features offered by AWS Managed Microsoft AD.\nIt has included the ability to manage user accounts and group memberships, create and apply group policies, securely connect to Amazon EC2 instances, and provide Kerberos-based single sign-on (SSO).\nSimple AD offers many advantages according to https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html:\nSimple AD makes it easier to manage Amazon EC2 instances running Linux and Windows and deploy Windows applications in the AWS Cloud.\nMany of the applications and tools that you use today that require Microsoft Active Directory support can be used with Simple AD.User accounts in Simple AD allow access to AWS applications such as Amazon WorkSpaces, Amazon WorkDocs, or Amazon WorkMail.\nYou can manage AWS resources through IAM role-based access to the AWS Management Console.\nDaily automated snapshots enable point-in-time recovery.\nOption A is incorrect: Because Simple AD is not used to manage on-premise instances.\nIt is a directory service in the AWS domain.\nOption B is CORRECT: Because Simple AD can provide directory service with lots of existing Active Directory tools and features supported.\nOption C is incorrect: Because this is suitable for AD Connector instead of Simple AD.\nRefer to https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html.\nOption D is CORRECT: Because Simple AD has the capability to backup with daily automated snapshots.\nOption E is incorrect: Because Simple AD is a basic directory service which is not suitable for large service.\nAlso, note that Simple AD supports a maximum of 5000 users.\nSimple AD is available in two sizes.\nSmall - Supports up to 500 users (approximately 2,000 objects including users, groups, and computers).\nLarge - Supports up to 5,000 users (approximately 20,000 objects including users, groups, and computers).\nOption F is CORRECT: Because Simple AD provides basic directory service and is cheaper than “AWS Directory Service for Microsoft Active Directory”.\nAWS Directory Service is a cloud-based directory service that allows developers and IT teams to use existing Microsoft Active Directory or LDAP (Lightweight Directory Access Protocol) applications in the cloud. AWS Directory Service provides multiple directory choices for customers based on their requirements. One of these options is Simple AD, which is a Microsoft Active Directory-compatible directory from AWS Directory Service powered by Samba 4.\nSimple AD is suitable for specific scenarios where basic Active Directory features are required. Below are the three scenarios where Simple AD can be the best choice:\nA. An operation management team needs to manage its Amazon EC2 instances and on-premises servers running Linux Ubuntu and Windows. Basic Active Directory features such as user accounts, group memberships are required.\nIn this scenario, Simple AD can be the right choice as it supports basic Active Directory features and is compatible with Linux Ubuntu and Windows. The operation management team can manage user accounts and group memberships using Simple AD.\nB. For a small project, a standalone directory in the cloud is needed, where the operators can create and manage user identities and manage access to applications. The operators want to use many familiar Active Directory-aware applications and tools that require basic Active Directory features.\nIn this scenario, Simple AD is suitable as it supports basic Active Directory features and can be used to create and manage user identities and manage access to applications. The operators can also use many familiar Active Directory-aware applications and tools with Simple AD.\nF. With a limited budget, a startup company requires a directory service to be set up with basic Active Directory compatibility that supports Samba 4-compatible applications.\nIn this scenario, Simple AD can be the best choice as it is a cost-effective option that provides basic Active Directory compatibility and supports Samba 4-compatible applications.\nThe other options in the question are not suitable for Simple AD:\nC. The development lead needs to set up a service that allows the on-premises users to log in to AWS applications and services with their Active Directory credentials.\nIn this scenario, AD Connector or AWS Managed Microsoft AD would be a better choice. AD Connector is used to connect on-premises directories with AWS, while AWS Managed Microsoft AD provides a fully managed Microsoft AD in the cloud.\nD. For the directory service in AWS, the security manager requires that it can be backed up via daily automated snapshots with point-in-time recovery enabled.\nThis requirement can be met with AWS Managed Microsoft AD, which provides daily automated snapshots with point-in-time recovery enabled.\nE. A large company is considering to set up a new directory service in AWS that can support its existing 10000 users (approximately 30,000 objects including users, groups, and computers).\nAWS Managed Microsoft AD would be the best choice for this scenario as it provides a fully managed Microsoft AD in the cloud and can support large numbers of users and objects. Simple AD has limitations regarding the number of users and objects it can support.\nIn summary, Simple AD is suitable for scenarios where basic Active Directory features are required, and the number of users and objects is not significant. For scenarios where a fully managed Microsoft AD is required or where there is a need to connect on-premises directories with AWS, AD Connector or AWS Managed Microsoft AD would be a better choice.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "An operation management team needs to manage its Amazon EC2 instances and on-premises servers running Linux Ubuntu and Windows. Basic Active Directory features such as user accounts, group memberships are required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For a small project, a standalone directory in the cloud is needed, where the operators can create and manage user identities and manage access to applications. The operators want to use many familiar Active Directory–aware applications and tools that require basic Active Directory features.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The development lead needs to set up a service that allows the on-premises users to log in to AWS applications and services with their Active Directory credentials.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For the directory service in AWS, the security manager requires that it can be backed up via daily automated snapshots with point-in-time recovery enabled",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A large company is considering to set up a new directory service in AWS that can support its existing 10000 users (approximately 30,000 objects including users, groups, and computers).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "With a limited budget, a startup company requires a directory service to be set up with basic Active Directory compatibility that supports Samba 4–compatible applications.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 156,
  "query" : "Your company has servers located in dozens of VPCs in different availability zones.\nThese VPCs need to connect to each other.\nYou need a service to establish the connections, simplify management and reduce operational costs.\nWhich of the below options is best suited to achieve this requirement?",
  "answer" : "Answer - A.\nOption A is CORRECT because Transit Gateway acts as a hub to connect different VPCs.\nIt is a fully managed service and can achieve all the requirements.\nOption B is incorrect because you need to create lots of VPC Peering connections since there are dozens of VPCs.\nUnlike Transit Gateway, you have to manage all the connections.\nOption C is incorrect because, with Transit VPC, you need to manage and scale EC2 based software appliances.\nPlease check https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html for the comparations between Transit Gateway and Transit VPC.Option D is incorrect because AWS PrivateLink is a service to establish private connections with AWS services.\nIt is not a service to manage the connections among VPCs.\nThe best option to establish connections among VPCs in different availability zones in a scalable, manageable, and cost-effective way is to use AWS Transit Gateway.\nAWS Transit Gateway acts as a hub for multiple VPCs and on-premises networks to connect and share resources. It simplifies network management by allowing centralized control of network routing and security policies for all connected VPCs. AWS Transit Gateway provides high availability and the ability to scale to support the increasing number of VPCs.\nOption B, VPC Peering connections, can be used to connect two VPCs within the same region, but it is not recommended to use VPC Peering connections to connect VPCs across regions or accounts. It requires manual configuration for each peering connection and can become difficult to manage as the number of VPCs grows.\nOption C, Transit VPC, is a similar solution to AWS Transit Gateway, but it requires more setup and configuration. It also lacks the scalability and high availability of AWS Transit Gateway.\nOption D, AWS PrivateLink, is a service that enables secure and private communication between VPCs, but it is not designed for connecting multiple VPCs. It is mainly used to connect VPCs to AWS services over private connections.\nTherefore, the best option to connect VPCs in different availability zones is to set up AWS Transit Gateway as a hub for all VPCs to connect and share resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up AWS Transit Gateway to provide a hub for connecting VPCs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up VPC Peering connections between each VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up Transit VPC as a hub among the VPCs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Establish AWS PrivateLink among different VPCs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 157,
  "query" : "Company ABC has used on-premises AD administrative tools to manage resources.\nThe company wants to enable all employees to use on-premises credentials to sign in to the AWS Management Console to access and manage its various AWS resources (e.g., AWS Workspaces etc..) To achieve this, they have successfully created an AWS Microsoft AD directory connected to their on-premises AD via a forest trust relationship.\nWhich below items are still required? Select 3.",
  "answer" : "E.\nCorrect Answer - C, D, E.\nUsing AWS Managed Microsoft AD directory, you can easily enable access to multiple AWS applications and services such as the AWS Management Console, Amazon WorkSpaces, and Amazon RDS for SQL Server.\nMore importantly, your users can access the application or service with their existing AD credentials.\nRefer to https://aws.amazon.com/blogs/security/how-to-access-the-aws-management-console-using-aws-microsoft-ad-and-your-on-premises-credentials/ on the details on how to setup AWS Managed Microsoft AD directory and manage administrative permissions to AWS resources.\nOption A is INCORRECT because Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and is not used to provide access to AWS instances for on-premise users.\nOption B is incorrect: Because NACL ( Network Access Control List ) belongs to the scope of subnets.\nIn this case, the security group of the AWS Microsoft AD directory should be taken care of.\nOption C is CORRECT because we need to access all services like the AWS Workspaces, AWS WorkMail, AWS Console, and RDS.\nThe access to the management console can be enabled as given below.\nOption D is CORRECT: Because this enables AWS Management Console access and provides a URL that can be used to connect to the console.\nThe URL is generated by appending “/console” to the end of the access URL generated in Option.\nC.Option E is CORRECT: Because this critical step is used to control which AWS resources on-premises users and groups can access from the AWS Management Console.\nThe correct answers are A, B, and E.\nA) Configure AWS System Manager to provide access to AWS instances for on-premise users: This step is required because it enables on-premises users to access AWS resources that are running on EC2 instances. AWS System Manager is a service that allows you to manage resources on EC2 instances and on-premises servers. By configuring AWS System Manager, on-premises users will be able to access EC2 instances and perform actions like patching, executing scripts, and managing resources.\nB) Ensure the NACL of the AWS Microsoft AD directory allows the outbound traffic to on-premise AD tools: This step is necessary to allow on-premises AD tools to communicate with the AWS Microsoft AD directory. Network Access Control Lists (NACLs) are used to control inbound and outbound traffic at the subnet level. By ensuring that the NACL of the AWS Microsoft AD directory allows outbound traffic to on-premise AD tools, the on-premises AD tools can communicate with the AWS Microsoft AD directory and authenticate users.\nC) In AWS apps & services section for the AWS Microsoft AD directory, enable all services: This step is not required to achieve the goal of allowing on-premises users to use their credentials to sign in to the AWS Management Console. However, it is necessary to enable specific AWS services like Amazon WorkSpaces, Amazon WorkMail, RDS SQL Server, and AWS Management Console. By enabling these services, on-premises users can access and manage these AWS resources using their on-premises credentials.\nD) Enable AWS Management Console access for the AWS Microsoft AD directory and get the URL that can be used to connect to the console: This step is necessary to allow on-premises users to access and manage AWS resources using the AWS Management Console. By enabling AWS Management Console access for the AWS Microsoft AD directory and getting the URL that can be used to connect to the console, on-premises users can use their on-premises credentials to sign in to the AWS Management Console and access and manage various AWS resources.\nE) Assign on-premises users and groups to IAM roles: This step is required to grant on-premises users access to AWS resources. IAM (Identity and Access Management) is a service that allows you to manage access to AWS resources. By assigning on-premises users and groups to IAM roles, you can grant them access to specific AWS resources and limit their access based on their job responsibilities.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure AWS System Manager to provide access to AWS instances for on-premise users.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure the NACL of the AWS Microsoft AD directory allows the outbound traffic to on-premise AD tools.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In AWS apps & services section for the AWS Microsoft AD directory, enable all services. This is used to activate the access for services include Amazon WorkSpaces, Amazon WorkMail, RDS SQL Server, and AWS Management Console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Enable AWS Management Console access for the AWS Microsoft AD directory and get the URL that can be used to connect to the console. For example, the AWS Management Console URL is “https://example-corp.awsapps.com/console”.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Assign on-premises users and groups to IAM roles.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 158,
  "query" : "Your company has an on-premises multi-tier PHP web application, which recently experienced downtime due to a large burst In web traffic due to a company announcement.\nOver the coming days, you are expecting similar announcements to drive similar unpredictable bursts.\nYou are looking for ways to quickly improve your infrastructure's ability to handle unexpected traffic increases.",
  "answer" : "Answer - A.\nIn this scenario, the major points of consideration are: (1) your application may get unpredictable bursts of traffic, (b) you need to improve the current infrastructure in the shortest period possible, and (3) your web servers are on-premises.\nSince the time period in hand is short, instead of migrating the app to AWS, you need to consider different ways to improve the performance without modifying the existing infrastructure.\nOption A is CORRECT because (a) CloudFront is AWS's highly scalable, highly available content delivery service, where it can perform excellently even in case of a sudden unpredictable burst of traffic, (b) the only change you need to make is making the on-premises load balancer as the custom origin of the CloudFront distribution.\nOption B is incorrect because you are supposed to improve the current situation in the shortest time possible.\nMigrating to AWS would be more time consuming than simply setting up the CloudFront distribution.\nOption C is incorrect because you cannot host dynamic websites on the S3 bucket.\nAlso, this option provides insufficient infrastructure to set up options.\nOption D is incorrect.\nIt is now possible to use Application Load Balancers through IP Address to On-Premises and AWS Resources.\nHowever, option D is incorrect as it is not covering the database tier which is an essential part of this 2 tier architecture.\nMore information on CloudFront:\nYou can have CloudFront sit in front of your on-premise web environment via a custom origin.\nThis would protect against unexpected bursts in traffic by letting CloudFront handle the traffic from the cache, thus removing some of the load from the on-premise web servers.\nAmazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds.\nLike other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long-term commitments or minimum fees.\nWith CloudFront, your files are delivered to end-users using a global network of edge locations.\nIf you have dynamic content, then it is best to have the TTL set to 0.\nFor more information on CloudFront, please visit the below URL-\nhttps://aws.amazon.com/cloudfront/\nSure, I'd be happy to explain the options and their implications.\nOption A: Offload traffic from an on-premises environment by setting up a CloudFront distribution and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behavior, and select a TTL that objects should exist in the cache.\nThis option involves using AWS CloudFront, a content delivery network (CDN), to cache objects from a custom origin server. By doing so, CloudFront can offload traffic from your on-premises environment, reducing the load on your servers and potentially improving their availability during traffic surges. The TTL (time-to-live) value determines how long CloudFront caches objects before fetching them again from the origin server. This option could be a good fit if your application has static content that can be cached for long periods, or if you can tolerate some delay in serving stale content.\nOption B: Migrate to AWS. Use VM import 'Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic. Create an RDS read replica and set up replication between the RDS instance and on-premises MySQL server to migrate the database.\nThis option involves migrating your entire application to AWS. It starts with using VM Import/Export to create an Amazon Machine Image (AMI) of your on-premises web server, which can then be launched as an EC2 instance. An Auto Scaling group can be set up to automatically launch additional instances as traffic increases, using the imported AMI. An RDS read replica can also be created to migrate your database to AWS, and replication can be set up between the RDS instance and your on-premises MySQL server to ensure data consistency. This option provides a scalable, resilient environment for your application, but requires more effort to set up and migrate your data to AWS.\nOption C: Create an S3 bucket and configure it for website hosting. Migrate your DNS to Route53 using zone import and leverage Route53 DNS failover to failover to the S3 hosted website.\nThis option involves using Amazon S3 to host your website, which can then be accessed using a custom domain name through Amazon Route53. Route53 DNS failover can be configured to automatically route traffic to the S3-hosted website in the event of a failure on your on-premises environment. This option is suitable for simple websites with static content, but may not be suitable for more complex web applications.\nOption D: Create an AMI which can be used to launch web servers in EC2. Create an Auto Scaling group which uses the AMI's to scale the web tier based on incoming traffic. Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS.\nThis option involves creating an AMI of your web server and launching it as an EC2 instance. An Auto Scaling group can be set up to automatically launch additional instances as traffic increases, using the AMI. Elastic Load Balancing can be used to distribute traffic between your on-premises web servers and those hosted in AWS. This option provides a scalable and resilient environment, but may require more complex networking configurations to integrate with your on-premises environment.\nOverall, each option has its pros and cons depending on the specific requirements of your application. Option B provides the most comprehensive solution but requires the most effort to set up and migrate data. Option A and D are more focused on offloading traffic and providing scalable capacity, respectively. Option C is the simplest option but may not be suitable for more complex applications.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Offload traffic from an on-premises environment by setting up a CloudFront distribution and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behavior, and select a TTL that objects should exist in the cache.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Migrate to AWS. Use VM import ‘Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic. Create an RDS read replica and setup replication between the RDS instance and on-premises MySQL server to migrate the database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an S3 bucket and configure it tor website hosting. Migrate your DNS to Route53 using zone import and leverage Route53 DNS failover to failover to the S3 hosted website.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an AMI which can be used to launch web servers in EC2. Create an Auto Scaling group which uses the AMI’s to scale the web tier based on incoming traffic. Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 159,
  "query" : "Your application provides data transformation services.\nFiles containing data to be transformed are first uploaded to Amazon S3 and then transformed by a fleet of spot EC2 instances.\nFiles submitted by your premium customers must be transformed with the highest priority.\nHow should you implement such a system?",
  "answer" : "Answer - C.\nOption A is incorrect because using DynamoDB tables will be a very expensive solution compared to using the SQS queue(s).\nOption B is incorrect because the transformation instances are spot instances that may not be up and running all the time; there are chances that they will be terminated.\nOption C is CORRECT because (a) it decouples the components of a distributed application, so the application is not impacted due to using spot instances, (b) it is a much cheaper option compared to using DynamoDB tables, and more importantly, (b) it maintains a separate queue for the high priority messages which can be processed before the default priority queue.\nOption D is incorrect because the transformation instances cannot poll high-priority messages first.\nThey just poll and can determine priority only after receiving the messages.\nMore information about implementing the priority queue via SQS:\nhttp://awsmedia.s3.amazonaws.com/pdf/queues.pdf\nOption C is the recommended solution for this use case as it provides an efficient way to manage and prioritize tasks using SQS.\nThe solution involves the use of two SQS queues, one for high priority messages and another for default priority. The transformation instances poll the high priority queue first and only poll the default priority queue when there are no high priority messages.\nTo implement this solution, you would first need to create two SQS queues, one for high priority and the other for default priority. When a new file is uploaded to S3, a message containing the file location and the priority level is sent to the appropriate queue.\nNext, you would launch a fleet of spot EC2 instances to perform the data transformation. These instances would poll the high priority queue first and start working on the messages with the highest priority. If there are no high priority messages in the queue, the instances would then poll the default priority queue.\nYou can also configure the number of instances in your fleet based on the expected workload and performance requirements. Additionally, you can use CloudWatch metrics to monitor the number of messages in each queue and adjust the size of your fleet accordingly.\nUsing SQS for task management provides several benefits, including fault tolerance and scalability. If an instance fails or is terminated, messages in the queue are not lost and can be processed by another instance. Additionally, the number of instances in the fleet can be adjusted dynamically based on the workload.\nOption A, using a DynamoDB table, can also work but it would require more implementation effort as you need to implement a scanning process that sorts the messages by priority. Furthermore, scanning a DynamoDB table can be less efficient than polling SQS queues.\nOption B, using Route 53 latency based-routing, is not suitable for this use case as it does not provide the ability to prioritize tasks based on their priority level. Latency based-routing is used to route traffic to the closest endpoint based on network latency.\nOption D, using a single SQS queue, can work but it would be less efficient as the transformation instances would need to scan the queue and sort messages based on their priority level, which can be less efficient than using separate queues for each priority level.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a DynamoDB table with an attribute defining the priority level. Transformation instances will scan the table for tasks, sorting the results by priority level.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Route 53 latency based-routing to send high priority tasks to the closest transformation instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use two SQS queues, one for high priority messages, the other for default priority. Transformation instances first poll the high priority queue; if there is no message, they poll the default priority queue.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a single SQS queue. Each message contains the priority level. Transformation instances poll high-priority messages first.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 160,
  "query" : "There is a requirement to split a VPC with a CIDR block of 10.0.0.0/24 into two subnets, each of which should give 128 IP addresses (including the 5 fixed IPs reserved by AWS)\nCan this be done and if so, how will the allocation of the IP addresses be configured? Choose the correct answer from the below options.",
  "answer" : "Answer - C.\nThis is clearly given in the AWS documentation.\n\" For example, if you create a VPC with CIDR block 10.0.0.0/24, it supports 256 IP addresses.\nYou can break this CIDR block into two subnets, each supporting 128 IP addresses.\nOne subnet uses CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other uses CIDR block 10.0.0.128/25 (for addresses 10.0.0.128 - 10.0.0.255)\".\nFor more information on VPC and subnets, please see the below link-\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\nTo get the IP addresses and subnets, please see the below link-\nhttp://www.subnet-calculator.com/cidr.php\nTo split a VPC with a CIDR block of 10.0.0.0/24 into two subnets, each of which should give 128 IP addresses (including the 5 fixed IPs reserved by AWS), we need to divide the /24 network into two smaller subnets of /25.\nThe /25 network provides 128 IP addresses, out of which 5 IP addresses are reserved by AWS for various purposes such as network address, broadcast address, etc. This means that each of the two subnets would have 123 usable IP addresses.\nOption A suggests that we use CIDR block 10.0.0.0/127 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/255 (for addresses 10.0.0.128 - 10.0.0.255). This option is incorrect because the subnet mask /127 only allows for two IP addresses, which is not sufficient to provide 123 usable IP addresses required by each subnet.\nOption B suggests that we use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.1.0/25 (for addresses 10.0.1.0 - 10.0.1.127). This option is incorrect because the two subnets are not contiguous, and it is not possible to have a contiguous CIDR block that spans two non-contiguous /25 subnets.\nOption C suggests that we use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/25 (for addresses 10.0.0.128 - 10.0.0.255). This option is correct because it provides two contiguous subnets of /25 each, with 123 usable IP addresses in each subnet.\nTherefore, the correct answer is option C.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "One subnet will use CIDR block 10.0.0.0/127 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/255 (for addresses 10.0.0.128 - 10.0.0.255).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One subnet will use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.1.0/25 (for addresses 10.0.1.0 - 10.0.1.127).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One subnet will use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/25 (for addresses 10.0.0.128 - 10.0.0.255).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "This is not possible.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 161,
  "query" : "A mobile App developer just made an App in both IOS and Android with a feature to count step numbers.\nHe has used AWS Cognito to authorize users to provide access to the AWS DynamoDB table.\nThe App uses the DynamoDB table to store user subscriber data and many steps.\nNow the developer also needs Cognito to integrate with Google to provide federated authentication for the mobile application users so that the user does not need to remember extra login access.What should the developer do to authenticate and authorize the users with suitable permissions for the IOS and Android App? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A and C.\nOne common use case for Amazon Cognito is to access AWS Services with an Identity Pool.\nFor the Identity pool itself, it can include:\nUsers in an Amazon Cognito identity pool.\nUsers who authenticate with external identity providers such as Facebook, Google, or a SAML-based identity provider.\nUsers authenticated via your own existing authentication process.\nOption A is CORRECT because Identity pool can be used to set up the federated identities through third-party identity providers such as Google.\nOption B is incorrect: Because Google federated identities work for both Android and IOS.\nRefer to https://docs.aws.amazon.com/cognito/latest/developerguide/google.html on the details.\nOption C is CORRECT because the User Pool is where the federated identity would be set-up and the Identity Pool is where permissions would be granted.\nPlease also check https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/ for the differences between User pools and Identify pools.\nUser pools are for authentication (identify verification), while Identity pools are for authorization (access control).\nOption D is incorrect: Same reason as Option.\nB.\nTo integrate Google authentication into the mobile app for users, the mobile app developer should use Amazon Cognito User Pools or Identity Pools. Both of these services support user authentication through federated identity providers, including Google.\nAnswer A is correct. Amazon Cognito Identity pools (federated identities) support user authorization through federated identity providers, including Amazon, Facebook, Google, and SAML identity providers. The developer just needs to set up the federated identities for Google access. This will allow the user to log in using their Google credentials, without needing to remember any additional login information. The developer can also set up the necessary permissions for users to access the DynamoDB table.\nAnswer C is also correct. Amazon Cognito User pools support user authentication through federated identity providers, including Amazon, Facebook, Google, and SAML identity providers. The developer just needs to set up the federated identities for Google access in the Cognito User Pool. The user can log in using their Google credentials, and the developer can set up the necessary permissions for users to access the DynamoDB table.\nAnswer B and D are incorrect. Both Android and iOS support federated identities through Amazon Cognito. Google access can be configured for Cognito identity pools with a Google Client ID for both Android and iOS.\nAnswer E is incorrect. Google federated access works for both Android and iOS mobile applications with Amazon Cognito.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Amazon Cognito Identity pools (federated identities) support user authorization through federated identity providers—including Amazon, Facebook, Google, and SAML identity providers. The developer just needs to set up the federated identities for Google access.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Only Android works for federated identities if Google access is required for AWS Cognito. This can be done by configuring Cognito identity pools with a Google Client I.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Cognito User pools support user authentication through federated identity providers—including Amazon, Facebook, Google, and SAML identity providers. The developer just needs to set up the federated identities for Google access in the Cognito User Pool.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Only IOS (Objective-C and Swift) works for federated identities if Google access is required for AWS Cognito. This can be done by configuration Cognito identity pools with a Google Client I.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Google federated access does not work for the android app.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 162,
  "query" : "A software engineer is developing a web App using Sinatra, which is a free and open-source software web application library and domain-specific language written in Ruby.\nHe wants to develop that in AWS Elastic Beanstalk.\nHowever, it seems that Sinatra is not a supported built-in platform that can be used.\nHe does not want to use docker either for this.\nCan he create a customized environment for the App in Elastic Beanstalk? Choose 2 options.",
  "answer" : "Correct Answer: B and C.\nElastic Beanstalk supports custom platforms now.\nA custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances.\nThe custom image can be built from one of the supported operating systems of Ubuntu, RHEL, or Amazon Linux.\nIn order to simplify the creation of these specialized Elastic Beanstalk platforms, machine images are now created using the Packer tool.\nRefer to the technical blog https://aws.amazon.com/blogs/aws/launch-aws-elastic-beanstalk-launches-support-for-custom-platforms/ on how to set up a customer platform.\nAfter setting up a customer platform, choose the new platform while creating a new environment.\nOption A is incorrect.\nBecause the customer platform is supported.\nAlso, please note that Docker is another option that can work as well.\nHowever, this case disallows Docker.\nOption B is Correct.\nOption C is CORRECT.\nBecause it is a proper way to create a custom platform for Sinatra.\nOption D is incorrect.\nBecause creating a customer platform uses Packer rather than Docker as a tool to build up the AMI.\nIn order to create the custom platform, the developer should start with a Packer template.\nA sample of a basic folder structure for building a platform definition archive looks as follows.\nSure, I'll provide a detailed explanation of each answer option.\nOption A: No. He cannot create the Sinatra app in Elastic Beanstalk as only the Platforms published and maintained by AWS Elastic Beanstalk are supported. He should use the Docker platform.\nThis option is partially correct. AWS Elastic Beanstalk provides several built-in platforms to choose from, and Sinatra is not one of them. Therefore, creating a Sinatra app directly in Elastic Beanstalk is not possible. However, the suggestion to use the Docker platform is not the only solution, and there are other options available.\nOption B: Yes, He can build a custom AMI in advance with Sinatra installed and configured on it. Create an ElasticBeanStalk environment and then modify the EC2 image ID with the customized AMI.\nThis option is correct. The engineer can create a custom Amazon Machine Image (AMI) with Sinatra installed and configured on it. Once the AMI is created, the engineer can create an Elastic Beanstalk environment and select the option to use a custom AMI. The engineer can then modify the EC2 image ID with the customized AMI to deploy the Sinatra app. This option allows the engineer to use Elastic Beanstalk to deploy and manage the infrastructure while still using Sinatra as the web application library.\nOption C: Yes. He can create his own platforms on Ubuntu, Red Hat Enterprise, or Amazon Linux and customize his instances with Sinatra. A Packer template will be used. When creating an Elastic Beanstalk environment, select the customized platform.\nThis option is also correct. The engineer can create their own platform using Ubuntu, Red Hat Enterprise, or Amazon Linux and customize their instances with Sinatra. This can be done using a tool like Packer to create a custom machine image that includes Sinatra. The engineer can then create an Elastic Beanstalk environment and select the custom platform they created. This option provides more control over the infrastructure than using a custom AMI, but it requires more setup and maintenance.\nOption D: Yes. He can create his own platforms on Amazon Linux and customize his instances using docker. A Dockerfile will be used together with other scripts. When creating an Elastic Beanstalk environment, select the customized docker platform.\nThis option is also correct. The engineer can create their own platform using Amazon Linux and customize their instances using Docker. This can be done using a Dockerfile to define the application and any required dependencies. The engineer can then create an Elastic Beanstalk environment and select the custom Docker platform they created. This option provides more flexibility than using a custom AMI or custom platform but requires more setup and maintenance.\nIn summary, options B, C, and D are all valid solutions for deploying a Sinatra app in Elastic Beanstalk. Option A is partially correct in stating that Sinatra is not a supported platform in Elastic Beanstalk but is not the only solution available.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "No. He cannot create the Sinatra app in Elastic Beanstalk as only the Platforms published and maintained by AWS Elastic Beanstalk are supported. He should use the Docker platform.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Yes, He can build a custom AMI in advance with Sinatra installed and configured on it. Create an ElasticBeanStalk environment and then modify the EC2 image ID with the customized AMI.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Yes. He can create his own platforms on Ubuntu, Red Hat Enterprise, or Amazon Linux and customize his instances with Sinatra. A Packer template will be used. When creating an Elastic Beanstalk environment, select the customized platform.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Yes. He can create his own platforms on Amazon Linux and customize his instances using docker. A Dockerfile will be used together with other scripts. When creating an Elastic Beanstalk environment, select the customized docker platform.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 163,
  "query" : "One full-stack engineer is developing an application to process images and videos from a highly scaled website.\nThe incoming web traffic is very unbalanced and unstable during the day.\nWhen the web traffic rises up, it may require a maximum of 10 m4.xlarge EC2 instances.\nHowever, the requirement may drop down to 1 m4.xlarge instance.\nThe processing for the application takes time, resulting in the delay of web application response.\nHe prefers to deploy the App in Elastic Beanstalk.\nWhich option should the engineer choose?",
  "answer" : "Correct Answer - B.\nOption A is INCORRECT because the traffic is very unbalanced.\nSince it is a \"long-running process\" an Elastic Beanstalk Worker environment is preferred over the Elastic Beanstalk Web environment.\nOption B is CORRECT because it is a \"long-running process\" an Elastic Beanstalk Worker environment is preferred here.\nAlso, SQS has the following capabilities: Separating query from long-running task.\nAlso, we can \"park\" the video and let CPUs process from the queue because we cannot wait on the video to be done before sending a response to the user and for building backlogs (Dead Letter Queues for later processing)\nOption C is INCORRECT because it should be a long-time running task instead of a short-time running task.\nOption D is INCORRECT because since the traffic is very unbalanced, we need to consider using an elastic load balancer for the same and this option does not use it.\nThe most suitable option for the full-stack engineer to deploy the application that processes images and videos from a highly scaled website in Elastic Beanstalk would be Option A: Deploy the application in one Elastic Beanstalk web server environment. Make sure that the environment uses a load-balancing, autoscaling configuration. Therefore, it can auto-scale depending on the traffic level.\nHere's why:\nOption A suggests deploying the application in a web server environment that uses a load-balancing and autoscaling configuration. This configuration will help to auto-scale the application depending on the incoming web traffic, which is unbalanced and unstable during the day. The maximum requirement for EC2 instances is 10 m4.xlarge, while the minimum requirement is one m4.xlarge instance.\nThe autoscaling configuration of Elastic Beanstalk automatically scales up or down the number of EC2 instances running in the environment, based on the demand. This means that when there is a surge in web traffic, Elastic Beanstalk will automatically spin up additional EC2 instances to handle the increased load. Similarly, when the web traffic reduces, Elastic Beanstalk will automatically scale down the number of EC2 instances to save costs.\nMoreover, Elastic Beanstalk also provides the feature of rolling deployments, where the deployment process takes place gradually, so there is no downtime. This helps in avoiding any delay in the web application response while the application is processing images and videos.\nOption B suggests deploying the application in a worker environment, which is not ideal for processing image and video files as it requires high computational resources. This option also suggests creating an SQS queue and an SQS daemon in EC2, which adds complexity to the overall solution.\nOption C suggests creating a worker environment and performing short-running tasks in response to the POST from the SQS daemon. However, this option does not consider the fact that the application processing may take a long time, resulting in a delay in the web application response.\nOption D suggests creating a web server environment with autoscaling configuration, but it does not mention anything about load-balancing, which is essential to distribute incoming traffic across multiple instances.\nIn summary, Option A is the best choice for deploying the application to process images and videos from a highly scaled website, as it provides a load-balancing and autoscaling configuration, which will scale the application depending on the incoming web traffic, without affecting the web application response time.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy the application in one Elastic Beanstalk web server environment. Make sure that the environment uses a load-balancing, autoscaling configuration. Therefore, it can auto-scale depending on the traffic level.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy the application in one Elastic Beanstalk worker environment. Create an SQS queue and an SQS daemon in EC2 to handling with the queue messages. Make sure that the environment uses a load-balancing and autoscaling configuration.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create one Elastic Beanstalk worker environment and deploy the application in the new environment. Make sure that the application performs the short-running task in response to the POST from SQS daemon. Uses an auto-scaling configuration for the environment.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an Elastic Beanstalk web server environment and add the application bundle to it. Make sure that the environment uses an autoscaling configuration with a maximum at 10 and a minimum at 1.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 164,
  "query" : "A big company has a service to process gigantic clickstream data sets which are often the result of holiday shopping traffic on a retail website, or sudden dramatic growth on the data network of a media or social networking site.\nIt is becoming more and more complex to analyze these clickstream datasets for its on-premise infrastructure.\nAs the sample data set keeps growing, fewer applications are available to provide a timely response.\nThe service is using a Hadoop cluster with Cascading.\nHow can they migrate the applications to AWS in the best way?",
  "answer" : "Correct Answer - D.\nThe application needs to process data timely; therefore Kinesis stream should be considered first.\nAfter a click event happens, a message should be put into the Kinesis stream in real-time.\nMoreover, Cascading is a proven, highly extensible application development framework for building massively parallelized data applications on EMR.\nBy using EMR, the application does not need to change a lot for the migration.\nRefer to https://aws.amazon.com/blogs/big-data/integrating-amazon-kinesis-amazon-s3-and-amazon-redshift-with-cascading-on-amazon-emr/.\nOption A is incorrect: Because the RDS database's output is improper as RDS does not scale well when the traffic is high.\nRedshift is much more appropriate.\nOption B is incorrect: AWS Lambda can potentially work for Hadoop.\nHowever, EMR provides native support for Hadoop.\nAlso, RDS is incorrect.\nOption C is incorrect: Because EC2 is not suitable for Hadoop processing if compared with EMR.\nThis question asks for the best option.\nSo EMR should be chosen.\nOption D is CORRECT: Because the EMR cluster with Cascading can process the data from Kinesis stream in real-time, and Redshift is also a proper place to store the output data.\nThe best way to migrate the clickstream data processing service from on-premises infrastructure to AWS depends on various factors, such as the volume of data, processing requirements, budget, and business objectives. However, among the given options, the most appropriate one is:\nA. Put the source data to S3 and migrate the processing service to an AWS EMR Hadoop cluster with Cascading. Enable EMR to read and query data from S3 buckets directly. Write the output to the RDS database.\nThis option offers several advantages for processing large clickstream datasets, such as:\n1.\nScalability: AWS EMR is a managed Hadoop cluster service that allows users to process big data workloads at any scale. EMR automatically provisions and scales the cluster based on the workload and terminates it when the processing is complete, which reduces the cost and complexity of managing the cluster manually.\n2.\nCost-effectiveness: S3 is a highly durable and cost-effective object storage service that allows users to store and retrieve any amount of data from anywhere. By putting the clickstream data in S3, the company can reduce the storage and management costs of the on-premises infrastructure. Moreover, EMR pricing is based on the number and type of instances used, so the company can optimize the cost of processing the clickstream data by choosing the appropriate instance types and scaling policies.\n3.\nFlexibility: EMR supports various Hadoop applications, including Cascading, which is a popular Java-based framework for building complex data processing workflows. By migrating the Cascading applications to EMR, the company can leverage the power of Hadoop and its ecosystem tools, such as Hive, Pig, and Spark, to process and analyze the clickstream data in a more efficient and flexible way.\n4.\nIntegration: EMR integrates with other AWS services, such as RDS, which is a managed relational database service that allows users to store, retrieve, and manage structured data. By writing the output of the Cascading applications to RDS, the company can enable downstream processes to consume and analyze the processed data more easily.\nTherefore, the best approach is to put the clickstream data to S3 and migrate the processing service to an EMR Hadoop cluster with Cascading, enabling EMR to read and query data from S3 buckets directly and writing the output to the RDS database. This approach offers scalability, cost-effectiveness, flexibility, and integration, which are essential for processing large clickstream datasets on AWS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Put the source data to S3 and migrate the processing service to an AWS EMR Hadoop cluster with Cascading. Enable EMR to read and query data from S3 buckets directly. Write the output to the RDS database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Put the source data to a Kinesis stream and migrate the processing service to AWS lambda to utilize its scaling feature. Enable lambda to read and query data from the Kinesis stream directly. Write the output to the RDS database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Put the source data to an S3 bucket and migrate the processing service to AWS EC2 with auto-scaling. Ensure that the auto-scaling configuration has a proper maximum and minimum number of instances. Monitor the performance in the Cloudwatch dashboard. Write the output to the DynamoDB table for downstream to process.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Put the source data to a Kinesis stream and migrate the processing service to an AWS EMR cluster with Cascading. Enable EMR to read and query data from Kinesis streams directly. Write the output to Redshift.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 165,
  "query" : "A development team has got a new assignment to maintain and process data in several DynamoDb tables and S3 buckets.\nThey need to perform sophisticated queries and operations across DynamoDB and S3\nFor example, they would export rarely used data from DynamoDB to Amazon S3 to reduce the storage costs while preserving low latency access required for high-velocity data.\nThe team has rich Hadoop and SQL knowledge.\nWhat is the best way to accomplish the assignment?",
  "answer" : "Correct Answer - B.\nFor the scenarios that Hadoop or Hive is mentioned, the first AWS service to think about should be EMR as it uses a SQL-based engine for Hadoop called Hive.\nOne thing to note is that outside data sources are referenced in the Hive cluster by creating an EXTERNAL TABLE such as:\nDetails on using EMR to efficiently export DynamoDB tables to S3 or import S3 data into DynamoDB are in https://aws.amazon.com/articles/using-dynamodb-with-amazon-elastic-mapreduce/?tag=articles%23keywords%23elastic-mapreduce.\nOption A is incorrect: Because although this option may work, it is more complicated as it needs to maintain the EC2 cluster, install Hadoop, etc.\nOption B is CORRECT: Because by using an EMR cluster, the team can use its Hadoop and SQL experience while EMR takes care of the infrastructure setup.\nOption C is incorrect: Because EMR is better than lambda as EMR provides native Hadoop support.\nOption D is incorrect: Because data pipelines cannot perform complicated SQL commands and unsuitable for this task.\nThe best way to accomplish this assignment would be to use an EMR cluster with Hadoop Hive, as it is a SQL-based engine that supports sophisticated queries and operations across DynamoDB and S3. By creating external tables in EMR for both DynamoDB and S3 buckets, the development team can use SQL-based commands to export and import data. This approach would leverage the team's existing Hadoop and SQL knowledge.\nOption A is not the best choice because Elastic Beanstalk is a Platform-as-a-Service (PaaS) offering that simplifies the deployment and management of web applications. It does not provide the required capabilities for querying tables or processing data.\nOption C is also not the best choice because Lambda is a serverless compute service that is primarily used for event-driven computing. While Lambda can be used to run SQL commands, it may not be the most efficient or cost-effective approach.\nOption D is not the best choice because it involves setting up several data pipelines to move data from DynamoDB to S3. While this approach may work for some use cases, it may not be the most efficient or scalable approach for performing sophisticated queries and operations across multiple DynamoDB tables and S3 buckets.\nIn summary, option B using an EMR cluster with Hadoop Hive and external tables for DynamoDB and S3 is the best way to accomplish the assignment given the team's rich Hadoop and SQL knowledge and the requirements for sophisticated queries and operations across multiple data sources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use an Elastic Beanstalk environment to set up a compute-optimized EC2 instance so that the instance has better performance for SQL commands to query tables or export/import data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use an EMR cluster as it uses Hadoop Hive which is a SQL-based engine. Create external tables in EMR for DynamoDB table and S3 buckets. Use a SQL-based command to export/import data.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a CloudFormation template to utilize a lambda to run SQL commands. Make sure the lambda has enough memory allocated as SQL commands consume high memory resources",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up several data pipelines to automatically move data from DynamoDB to S3 if the data has met user-defined conditions such as the items are older than a year. Modify data pipeline configurations when needed.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 166,
  "query" : "A middle-sized company is planning to migrate its on-premises servers to AWS.\nAt the moment, they have used various licenses, including windows operating system server, SQL Server, IBM Db2, SAP ERP, etc.\nAfter migration, the existing licenses should continue to work in EC2\nThe IT administrators prefer to use a centralized place to control and manage the licenses to prevent potential non-compliant license usages.\nFor example, SQL Server Standard's license only allows 50 vCPUs, which means a rule is needed to limit the number of SQL Servers in EC2\nWhich option is correct for the IT administrators to use?",
  "answer" : "Correct Answer - B.\nAWS License Manager is a central place to manage licenses in AWS EC2 and on-premises instances.\nIt contains 3 parts to use:\nDefine licensing rules.\nEnforce licensing rules.\nTrack usage.\nAWS License Manager currently integrates with Amazon EC2, allowing you to track licenses for default (shared-tenancy) EC2 instances, Dedicated Instances, Dedicated Hosts, Spot Instances, and Spot Fleet, and Auto Scaling groups.\nRefer to https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html.\nOption A is incorrect.\nBecause AWS License Manager manages the BYOL licenses.\nAlthough AWS System Manager can work together with AWS License Manager to manage licenses for on-premises servers and non-AWS public clouds, it is not the central place to provide license management.\nOption B is CORRECT: Because AWS License Manager can define licensing rules, track license usage, and enforce controls on license use to reduce the risk of license overages.\nOption C is incorrect: Because the AWS License manager should be considered first for licensing management.\nOption D is incorrect: Because AWS License Manager can manage non-Microsoft licenses.\nAccording to https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html, license Manager tracks various software products from Microsoft, IBM, SAP, Oracle, and other vendors.\nThe correct option for the IT administrators to use is option B: Define license rules in AWS License Manager for the required licenses. Enforce the license rules in EC2 and track usage in the AWS License Manager console.\nAWS License Manager is a service that allows IT administrators to manage their software licenses in AWS. It simplifies the management of licenses for software that runs on Amazon EC2 instances. By using AWS License Manager, IT administrators can ensure that their software is properly licensed and compliant with the license terms.\nIn this scenario, the IT administrators want to use a centralized place to control and manage licenses to prevent potential non-compliant license usages. They also want to ensure that the existing licenses continue to work in EC2 after migration. AWS License Manager allows them to do both.\nOption A is not the correct answer because while AWS System Manager can be used to manage software licenses, it is not specifically designed for this purpose. Moreover, AWS System Manager does not provide a dedicated console to track license usage, which is important for compliance purposes.\nOption C is not the correct answer because using a license management blueprint to create a dedicated Lambda function to control license usage is a complex and unnecessary solution. AWS License Manager already provides a simple and effective way to define and enforce license rules.\nOption D is not the correct answer because it only defines and enforces license rules for Microsoft licenses such as Windows and SQL Server, but not for other licenses such as IBM Db2. This approach would not be sufficient for compliance purposes.\nIn conclusion, AWS License Manager is the best option for the IT administrators to use because it allows them to define and enforce license rules for all the required licenses, and track usage in a centralized console. This will help them ensure compliance and prevent potential non-compliant license usage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create license rules in AWS System Manager for all BYOL licenses. Use the rules to make sure that there are no non-compliant activities. Link the rules when EC2 AMI is created. System Manager console has provided license usage status.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Define license rules in AWS License Manager for the required licenses. Enforce the license rules in EC2 and track usage in the AWS License Manager console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a license management blueprint to create a dedicated Lambda to control license usage. Lambda outputs the usage status to Cloudwatch Metrics which can be used by the administrators to track the status.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Define and enforce license rules in AWS License Manager for the Microsoft relevant licenses such as windows, SQL Server as only Microsoft licenses are supported. For the other licenses such as IBM Db2, track the license usage in AWS System Manager.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 167,
  "query" : "An outsourcing company is working on a government project.\nSecurity is very important to the success of the application.\nThe application is developed mainly in EC2 with several application load balancers.\nCloudFront and Route53 are also configured.\nThe major concern is that it should be able to be protected against DDoS attacks.\nThe company decides to activate the AWS Shield Advanced feature.\nTo this effect, it has hired an external consultant to 'educate' its employees on the same.\nFor the below options, which ones help the company to understand the AWS Shield Advanced plan? Select 3.",
  "answer" : "E.\nF.\nG.\nH.\nCorrect Answer - A, D, E.\nAWS Shield has two plans - AWS Shield Standard and AWS Shield Advanced.\nAWS Shield Standard:\nAWS Shield Standard activates automatically at no additional charge.\nAWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your applications.\nAWS Shield Advanced:\nFor higher levels of protection against attacks.\nIt has a subscription fee which is $ 3000 per month.\nOption A is CORRECT.\nBecause Elastic Load Balancing (ELB), Amazon CloudFront, Amazon Route 53 are all covered by AWS Shield Advanced.\nOption B is incorrect.\nBecause AWS Shield Advanced has a subscription commitment of 1 year with a base monthly fee of 3000$.\nOption C is incorrect.\nBecause Route 53 is covered by AWS Shield Advanced.\nOption D is CORRECT.\nBecause 24*7 support by the DDoS Response team is a key feature of the advanced plan.\nOption E is CORRECT.\nBecause AWS Shield Advanced integrates with AWS CloudWatch and provides relevant reports.\nOption F is incorrect.\nBecause AWS Shield is not within AWS WAF.\nPlease note that both of them help protect the AWS resources.\nAWS WAF is a web application firewall service, while AWS Shield provides expanded DDoS attack protection for the AWS resources.\nSure, I'd be happy to explain the options and help you understand the AWS Shield Advanced plan in detail.\nA. AWS Shield Advanced plan is able to protect application load balancers, CloudFront and Route53 from DDoS attacks. This option is correct. AWS Shield Advanced plan is a managed DDoS protection service that helps protect AWS resources such as EC2 instances, Elastic Load Balancing, CloudFront, and Route 53 from DDoS attacks. So, it's important to understand that the AWS Shield Advanced plan can protect the application load balancers, CloudFront, and Route53 from DDoS attacks.\nB. AWS Shield Advanced plan does not have a monthly base charge. The company only needs to pay the data transfer fee. Other than that, AWS WAF includes no additional cost. This option is also correct. AWS Shield Advanced plan does not have a monthly base charge. Instead, it charges based on the data transferred out of your AWS resources to the internet. Moreover, the AWS WAF (Web Application Firewall) is also included at no additional cost with AWS Shield Advanced plan. So, it's important to understand that the AWS Shield Advanced plan charges only for data transfer fees and includes AWS WAF at no additional cost.\nC. Route 53 is not covered by AWS Shield Advanced plan. However, Route 53 is able to be protected under AWS WAF. This option is partially correct. While it's true that Route 53 is not covered by the AWS Shield Advanced plan, it can still be protected under AWS WAF. AWS WAF provides protection against web exploits and attacks for the applications that use Amazon Route 53.\nD. A dedicated rule in WAF should be customized. This option is not relevant to understanding the AWS Shield Advanced plan. However, it's important to understand that AWS WAF provides a wide range of pre-configured rules that you can use to protect your web applications. You can also create your own custom rules to meet your specific security requirements.\nE. 24*7 support by the DDoS Response team. Critical and urgent priority cases can be answered quickly by DDoS experts. Custom mitigations during attacks are also available. This option is correct. AWS Shield Advanced plan provides 24/7 support by the DDoS Response team. In case of critical and urgent priority cases, DDoS experts can answer quickly and custom mitigations during attacks are also available.\nF. Real-time notification of attacks is available via Amazon CloudWatch. Historical attack reports are also provided. This option is correct. AWS Shield Advanced plan provides real-time notification of attacks via Amazon CloudWatch. It also provides historical attack reports, which can help you identify patterns and improve your security posture.\nG. AWS Shield is a sub-feature within AWS WA. This option is not correct. AWS Shield is a standalone service that provides DDoS protection for AWS resources. AWS WAF is a web application firewall service that provides protection against web exploits and attacks. While AWS Shield Advanced plan includes AWS WAF, it's important to understand that AWS Shield is not a sub-feature within AWS WAF.\nH. AWS Shield Advanced can be activated in AWS WAF console, which also provides the near real-time metrics and packet captures for attack forensics. This option is partially correct. While it's true that you can activate AWS Shield Advanced plan in the AWS WAF console, it's important to understand that AWS Shield Advanced plan provides near real-time metrics and packet captures for attack forensics, not AWS WAF. AWS WAF provides web application firewall services, whereas AWS Shield Advanced plan provides DDoS protection services.\nIn summary, the correct options to help the company understand the AWS Shield Advanced plan are:\nAWS Shield Advanced plan is able to protect",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Shield Advanced plan is able to protect application load balancers, CloudFront and Route53 from DDoS attacks.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Shield Advanced plan does not have a monthly base charge. The company only needs to pay the data transfer fee. Other than that, AWS WAF includes no additional cost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Route 53 is not covered by AWS Shield Advanced plan. However, Route 53 is able to be protected under AWS WA.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A dedicated rule in WAF should be customized.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "24*7 support by the DDoS Response team. Critical and urgent priority cases can be answered quickly by DDoS experts. Custom mitigations during attacks are also available.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Real-time notification of attacks is available via Amazon CloudWatch. Historical attack reports are also provided.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Shield is a sub-feature within AWS WA.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Shield Advanced can be activated in AWS WAF console, which also provides the near real-time metrics and packet captures for attack forensics.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 168,
  "query" : "An Artificial Intelligence startup company has used lots of EC2 instances.\nSome instances use the SQL Server database, while others use Oracle.\nAs the data needs to be kept secure, regular snapshots are required.\nThey want SQL Server EBS volume to take a snapshot every 12 hours.\nHowever, for Oracle, it only needs a snapshot every day.\nWhich option below is the best one that the company should choose?",
  "answer" : "Correct Answer - D.\nAmazon Data Lifecycle Manager (Amazon DLM) should be considered if automating snapshot management is required (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html).One thing to note is that the DLM policy has used Tags to choose EBS volumes.\nOption A is incorrect.\nBecause although this may work, it is not a straightforward solution as DLM.\nOption B is incorrect.\nBecause prefix in the name is incorrect.\nTags (name and value) are used to choose EBS volumes.\nOption C is incorrect.\nBecause it brings extra cost and is not as easy as DLM.\nThe question asks for the solution without extra cost.\nOption D is CORRECT.\nBecause two management policies in DLM can meet the needs.\nFor example:\nFor SQL Server EBS volume:\nFor Oracle EBS volume:\nThe best option for the company is D. Add different tags for SQL Server and Oracle EBS volumes. In the AWS Data Lifecycle Management console, create two management policies based on the tags. Add a 12 hours schedule to SQL Server lifecycle policy and a 24 hours schedule to Oracle lifecycle policy.\nOption A is incorrect because Clive is not a real tool, and AWS already provides a native solution for managing backups.\nOption B is incorrect because it involves manually adding a prefix to the name of EBS volumes, which is not scalable and prone to human error.\nOption C is incorrect because it requires creating a dedicated Lambda function and setting up Cloudwatch Events Rules, which is more complex and less efficient than using AWS Data Lifecycle Management.\nAWS Data Lifecycle Management (DLM) is a service that helps you automate the creation, retention, and deletion of EBS snapshots. By using DLM, you can create policies that define when and how often to take snapshots of your EBS volumes.\nTo implement the solution, you need to:\n1. Tag your SQL Server and Oracle EBS volumes with distinct tags (e.g., SQL_Server and Oracle).\n2. Create two DLM policies, one for SQL Server and one for Oracle, based on the tags you added in step 1.\n3. Set the frequency of the SQL Server policy to every 12 hours and the Oracle policy to every 24 hours.\n4. Set the retention period for both policies based on your backup requirements.\nBy using tags, you can easily identify and manage your EBS volumes based on their function or usage. The DLM policies ensure that your backups are taken at the required frequency and retained for the desired period.\nIn summary, Option D is the best option for the company as it uses AWS Data Lifecycle Management to automate the creation of EBS snapshots based on distinct tags.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a free third-party tool such as Clive to Manage EC2 instance lifecycle. It can design various backup policies for EC2 EBS volumes. Add a 12 hours backup policy to SQL Server EBS volumes and a 24 hours backup policy to Oracle EBS volumes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a prefix to the name of both SQL Server and Oracle EBS volumes. In the AWS Data Lifecycle Management console, create two management policies based on the name prefix. For example, add a 12 hours backup schedule to EBS volumes with a name starting with “SQL” and add a 24 hours backup schedule to EBS volumes with a name starting with “oracle”.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a dedicated Lambda function to differentiate EC2 EBS volumes and take snapshots. Set up Cloudwatch Events Rules to call the lambda so that the function runs every 12 hours for SQL Server and 24 hours for Oracle.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add different tags for SQL Server and Oracle EBS volumes. In the AWS Data Lifecycle Management console, create two management policies based on the tags. Add a 12 hours schedule to SQL Server lifecycle policy and a 24 hours schedule to Oracle lifecycle policy.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 169,
  "query" : "Your company produces customer commissioned one-of-a-kind high-graphical skiing helmets, combining high-fashion with custom technical enhancements.\nThe current manufacturing process is data-rich and complex.\nIt includes assessments to ensure that the custom electronics and materials used to assemble the helmets are of the highest standards.",
  "answer" : "Answer - B.\nTip: Whenever the scenario in the question mentions high graphical processing servers with low latency networking, always think about using G3 instances.\nAnd, when there are tasks involving human intervention, always think about using SWF.Option A is incorrect because AWS Data Pipeline cannot work in a hybrid approach where some of the tasks involve human actions.\nOption B is CORRECT because (a) it uses G3 instances which are specialized for high graphical processing of data with low latency networking, and (b) SWF supports workflows involving human interactions along with AWS services.\nOption C is incorrect because it uses C3 instances used for situations where compute optimization is required.\nIn this scenario, you should be using G3 instances.\nOption D is incorrect because (a) AWS Data Pipeline cannot work in a hybrid approach where some of the tasks involve human actions, and (b) it uses C3 instances used for situations where compute optimization is required.\nIn this scenario, you should be using G3 instances.\nMore information on G3 instances:\nUsing G3 instances is preferred.\nHence options C and D are wrong.\nFor more information on Instances types, please visit the below URL-\nhttps://aws.amazon.com/ec2/instance-types/\nSince there is an element of human intervention, SWF can be used for this purpose.\nFor more information on SWF, please visit the below URL-\nhttps://aws.amazon.com/swf/\nThe manufacturing process for the customer commissioned high-graphical skiing helmets that combine high-fashion with custom technical enhancements is data-rich and complex. To manage the movement of data, metadata, and assessments, as well as ensure that the custom electronics and materials used to assemble the helmets are of the highest standards, the options provided in the answer choices are:\nA. Use AWS Data Pipeline to manage the movement of data, metadata, and assessments. Use an auto-scaling group of G3 instances in a placement group. B. Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata. Use an auto-scaling group of G3 instances in a placement group. C. Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata. Use an auto-scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization). D. Use AWS Data Pipeline to manage movement of data, metadata, and assessments. Use an auto-scaling group of C3 with SR-IOV (Single Root I/O virtualization).\nOption A suggests using AWS Data Pipeline to manage the movement of data, metadata, and assessments, and an auto-scaling group of G3 instances in a placement group. AWS Data Pipeline is a service that helps to move data between different AWS services and on-premises data sources. G3 instances are instances optimized for graphics-intensive workloads and machine learning applications. However, it is not clear how AWS Data Pipeline would help manage the assessments, and it is not clear why G3 instances in a placement group are the best option for this use case.\nOption B suggests using Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata, and an auto-scaling group of G3 instances in a placement group. Amazon SWF is a service that helps developers build applications with coordinated workflows. It is used to coordinate tasks across multiple distributed components, including on-premises and cloud-based services. However, it is not clear why G3 instances in a placement group are the best option for this use case.\nOption C suggests using Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata, and an auto-scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization). C3 instances are instances optimized for compute-intensive workloads. SR-IOV is a technology that enables direct access to physical hardware from a virtual machine, which can improve network performance. However, it is not clear how Amazon SWF would help manage the assessments.\nOption D suggests using AWS Data Pipeline to manage the movement of data, metadata, and assessments, and an auto-scaling group of C3 instances with SR-IOV. This option combines the benefits of using AWS Data Pipeline for data management and an auto-scaling group of C3 instances with SR-IOV for compute-intensive workloads and improved network performance. However, it is not clear how AWS Data Pipeline would help manage the assessments.\nOverall, Option D appears to be the best option for managing the data-rich and complex manufacturing process of the customer commissioned high-graphical skiing helmets that combine high-fashion with custom technical enhancements. However, additional information is needed to determine the best solution to manage the assessments.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use AWS Data Pipeline to manage the movement of data, metadata, and assessments. Use an auto-scaling group of G3 instances in a placement group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata. Use an auto-scaling group of G3 instances in a placement group.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata. Use an auto-scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Data Pipeline to manage movement of data, metadata, and assessments. Use an auto-scaling group of C3 with SR-IOV (Single Root I/O virtualization).",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 170,
  "query" : "There is a requirement for a company to transfer large amounts of data between AWS and an on-premise location.\nThere is an additional requirement for low latency and high consistency traffic to AWS.\nOut of these given requirements, how would you design a hybrid architecture? Choose the correct answer from the below options.",
  "answer" : "Answer - A.\nTip: Whenever the scenario in the question requires the use of low latency transfer of data between AWS/VPC and on-premise servers/database, always think about provisioning AWS Direct Connect.\nOption A is CORRECT because Direct Connect creates a dedicated connection between AWS and on-premises server for low latency secured data transfer.\nOption B is incorrect because setting up VPN connectivity has higher maintenance overhead compared to Direct Connect.\nAlso, Direct Connect provides a dedicated network connection bypassing the internet.\nHence it is more secure.\nOption C is incorrect because setting up the IPSec tunnel has setup and maintenance overhead.\nAlso, the IPSec tunnel does not guarantee the end-to-end security of the data as it uses the internet.\nOption D is incorrect as Direct Connect is the most suited option for this scenario.\nMore information on AWS Direct Connect:\nAWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS.\nUsing AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.\nFor more information on AWS direct connect, just browse to the below URL-\nhttps://aws.amazon.com/directconnect/\nOption A: Provision a Direct Connect connection to an AWS region using a Direct Connect partner. Direct Connect is a dedicated network connection that enables secure and reliable connectivity between on-premise infrastructure and AWS. Direct Connect provides consistent network performance, low latency, and high bandwidth compared to internet-based connections. Direct Connect can be provisioned through a Direct Connect partner, which allows the customer to connect to AWS in the desired region. This option is a suitable solution for transferring large amounts of data with low latency and high consistency requirements.\nOption B: Create a VPN tunnel for private connectivity which increases network consistency and reduces latency. VPN (Virtual Private Network) is a secure and encrypted connection between two endpoints over the internet. VPN can be used to connect an on-premise data center to AWS for private connectivity. However, VPN may have latency issues and inconsistency in network performance as it relies on the public internet, which can impact the overall performance of the hybrid architecture.\nOption C: Create an IPSec tunnel for private connectivity which increases network consistency and reduces latency. IPSec (Internet Protocol Security) is a protocol that provides secure communication over IP networks. It can be used to establish a private connection between on-premise infrastructure and AWS. However, IPSec may have latency issues and inconsistency in network performance as it relies on the public internet, which can impact the overall performance of the hybrid architecture.\nOption D: This is not possible. This option is incorrect as there are multiple solutions available to design a hybrid architecture for transferring large amounts of data with low latency and high consistency requirements between AWS and on-premise infrastructure.\nBased on the above options, the best solution for designing a hybrid architecture for transferring large amounts of data with low latency and high consistency requirements between AWS and on-premise infrastructure would be to provision a Direct Connect connection to an AWS region using a Direct Connect partner.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Provision a Direct Connect connection to an AWS region using a Direct Connect partner.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a VPN tunnel for private connectivity which increases network consistency and reduces latency.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IPSec tunnel for private connectivity which increases network consistency and reduces latency.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "This is not possible.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 171,
  "query" : "You're running an application on-premises due to its dependency on non-x86 hardware and want to use AWS for data backup.\nYour backup application is only able to write to POSIX-compatible block-based storage.\nYou have 624TB of data and would like to mount it as a single folder on your file server.\nUsers must be able to access portions of this data while the backups are taking place.\nWhat backup solution would be most appropriate for this use case?",
  "answer" : "Answer - A.\nGateway-Cached volumes can support volumes of 1,024TB in size, whereas Gateway-stored volume supports volumes of 512 TB size.\nOption A is CORRECT because (a) it supports volumes of up to 1,024TB in size, and (b) the frequently accessed data is store on the on-premise server while the entire data is backed up over AWS.\nOption B is incorrect because S3 is not ideal for POSIX compliant data.\nOption C is incorrect because the data stored in Amazon Glacier is not available immediately.\nRetrieval jobs typically require 3-5 hours to complete; so, if you need immediate access to your data as mentioned in the question, this may not be the ideal choice.\nOption D is incorrect because gateway stored volumes can only store only 512TB worth of data.\nFor more information on all of the options for storage, please refer to the below link-\nhttp://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#resource-volume-limits\nThe most appropriate backup solution for this use case would be to use Storage Gateway and configure it to use Gateway Stored volumes.\nStorage Gateway is a hybrid cloud storage service that enables on-premises applications to seamlessly use AWS storage services, such as Amazon S3, Amazon EBS, and Amazon Glacier, while still providing low-latency performance and data consistency. It accomplishes this by providing a virtual machine image that can be deployed on-premises and functions as a gateway between the on-premises environment and AWS storage services.\nIn this case, the application is running on-premises due to its dependency on non-x86 hardware, and the backup application is only able to write to POSIX-compatible block-based storage. Therefore, using Storage Gateway and configuring it to use Gateway Stored volumes would be the best backup solution for this use case.\nGateway Stored volumes provide low-latency access to the full data set while still providing durable off-site backups. The data is stored as EBS snapshots in S3, providing a durable and scalable backup solution. Additionally, Gateway Stored volumes can be mounted as iSCSI devices and can be accessed by on-premises applications just like a physical disk.\nUsing Gateway Stored volumes also allows for the ability to use incremental snapshots, which can greatly reduce the amount of data transferred over the network during backups. This is important given the large amount of data in this use case, as it can help to reduce backup windows and network traffic.\nFinally, the requirement to mount the 624TB of data as a single folder on the file server can be achieved through the use of Gateway Stored volumes. By mounting the Gateway Stored volume as an iSCSI device, the volume can be formatted with a file system, such as ext4 or NTFS, and mounted as a single folder on the file server.\nIn summary, the most appropriate backup solution for this use case is to use Storage Gateway and configure it to use Gateway Stored volumes because it provides low-latency access to the full data set while still providing durable off-site backups, allows for incremental snapshots, and can be mounted as a single folder on the file server.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Storage Gateway and configure it to use Gateway Cached volumes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure your backup software to use S3 as the target for your data backups.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure your backup software to use Glacier as the target for your data backups.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Storage Gateway and configure it to use Gateway Stored volumes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 172,
  "query" : "A large financial application generates logs in a comma-separated format and saves them to S3 for later processing.\nAfter each file is generated, a message is sent to an SQS queue.\nEC2 instances monitoring the SQS queue are running behind a load balancer in an Auto Scaling group.\nAn EC2 instance will pick up the message from SQS and start processing it.\nOnce the processing is completed, the processed files are stored into another S3 bucket to be used later in a reporting process.\nThe EC2 instances load their application from S3 on startup.\nThe application release logs show that there have been many updates to the application in the last month.\nThe data security and monitoring team wants to check the application logs to ensure that the logs do not contain any sensitive information.\nHow can you complete this process effectively without interrupting the Auto Scaling or the application release cycle?",
  "answer" : "Correct Answer: C because Cloudwatch logs provide information on the application behavior.\nA Lambda function can process CloudWatch Logs.\nOption A is INCORRECT because the CloudTrail only monitors the API calls made to AWS resources and does not record the application logs.\nOption B is INCORRECT because suspending the ASG terminate Instances is not effective and may affect the application release cycle during high utilization period.\nAdministrators should be able to get the logs without even log in to EC2 Instances.\nOption D is INCORRECT because this method is not straight-forward or cost-effective.\nOption A: Enable CloudTrail and redirect all the system logs to an S3 bucket. Download the log files from the S3 bucket and check for sensitive information.\nCloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It records all the API calls made within your account and delivers log files to an S3 bucket that you specify. In this scenario, enabling CloudTrail and directing all the system logs to an S3 bucket would ensure that all the API calls made to the S3 buckets are recorded. This would include the logs generated by the financial application. Once the logs are recorded, they can be downloaded and checked for sensitive information. This solution does not interrupt the Auto Scaling or the application release cycle.\nOption B: Suspend the Auto Scaling termination process, and then log into the machines that started recently and check the logs.\nThis option requires suspending the Auto Scaling termination process, which means that new EC2 instances will not be launched if the number of instances falls below the desired capacity. This could affect the availability of the application if the suspended process is not resumed promptly. Additionally, logging into the machines that started recently would not be effective in this scenario since the EC2 instances are launched and terminated by the Auto Scaling group.\nOption C: Install the CloudWatch Logs Agent and publish the logs to CloudWatch. Trigger a Lambda function to process the logs.\nCloudWatch is a monitoring service for AWS resources and applications. It can collect and track metrics, collect and monitor log files, and set alarms. In this option, installing the CloudWatch Logs Agent on the EC2 instances and publishing the logs to CloudWatch would allow the logs to be monitored in real-time. A Lambda function can be triggered to process the logs and check for sensitive information. This option does not interrupt the Auto Scaling or the application release cycle.\nOption D: Take daily snapshots of the EC2 instance volumes, and then mount the recent snapshots to another instance and check the logs.\nThis option requires taking daily snapshots of the EC2 instance volumes, which could result in a large number of snapshots being created, leading to increased storage costs. Additionally, mounting the recent snapshots to another instance would require stopping the original instance, which would result in downtime for the application. This option is not practical in this scenario since it requires stopping the EC2 instances, which would interrupt the Auto Scaling and the application release cycle.\nBased on the given scenario, option A or option C would be the best solutions since they do not interrupt the Auto Scaling or the application release cycle. Option A involves enabling CloudTrail and redirecting all system logs to an S3 bucket, which can be downloaded and checked for sensitive information. Option C involves installing the CloudWatch Logs Agent and publishing the logs to CloudWatch, which can be monitored in real-time, and a Lambda function can be triggered to process the logs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable CloudTrail and redirect all the system logs to an S3 bucket. Download the log files from the S3 bucket and check for sensitive information.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Suspend the Auto Scaling termination process, and then log into the machines that started recently and check the logs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Install the CloudWatch Logs Agent and publish the logs to CloudWatch. Trigger a Lambda function to process the logs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Take daily snapshots of the EC2 instance volumes, and then mount the recent snapshots to another instance and check the logs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 173,
  "query" : "Your company runs a video analysis software that is available in both Free and Paid plans.\nTo support the Free and Paid versions effectively, your company uses a spot pricing framework that does the bidding as per the subscription plan to ensure the availability of the spot instances and maintain the cost.\nThe instances are running behind a load balancer created for the analysis job with the auto-scaling group attached to that.\nPaid subscription jobs are set to run into multiple availability zones to maintain high availability.\nThe application using the spot engine adds the desired instances based on active jobs and terminates instances when the processing is completed.\nBy looking at the CloudWatch logs, you noticed that sometimes the Auto Scaling launches new instances before terminating the old ones.\nWhat could the cause and the corresponding resolution for this issue?",
  "answer" : "Correct Answer: B.\nOption A is INCORRECT because the desired capacity is meant to maintain a minimum number of instances in an Auto Scaling group.\nOption B is CORRECT because suspending the AZRebalance will disable the instance balancing activity if the availability zones have a different number of instances.\nOption C is INCORRECT for the current situation.\nIf the Auto Scaling runs into a single availability zone, the AZRebalance event will not trigger.\nBut at the same time, it will not be fault-tolerant against availability zone failures.\nOption D is INCORRECT because the min/max count does not affect the availability zone rebalance process.\nThe issue observed in the CloudWatch logs where new instances are launched before terminating the old ones could be caused by various reasons, such as scaling policies, instance termination policies, or instance warm-up time. To resolve the issue, the cause needs to be identified and addressed accordingly.\nOne potential cause of this issue could be the scaling policies that are defined for the Auto Scaling group. Scaling policies determine when new instances are launched or old ones are terminated based on the metrics and thresholds specified. It is possible that the scaling policies are not configured optimally and are triggering the launch of new instances before terminating the old ones. To address this, the desired capacity of the Auto Scaling group can be updated based on the number of active jobs. By doing so, the Auto Scaling group will launch only the required number of instances to handle the workload and terminate the instances once the processing is completed.\nAnother possible cause of the issue could be related to the instance termination policies. Instance termination policies define the order in which instances are terminated when scaling down. It is possible that the termination policies are not configured optimally and are terminating instances randomly. This could result in the launch of new instances before terminating the old ones. To resolve this issue, the instance termination policies can be adjusted to prioritize the termination of instances that are not currently processing any jobs.\nIn addition to the above, it is also possible that the instance warm-up time is causing the issue. Instance warm-up time is the time required for an instance to become fully operational after launch. During this time, the instance may not be able to handle the full workload, and launching new instances before terminating the old ones may be necessary to maintain high availability. To address this, the instance warm-up time can be reduced by pre-warming the instances before launching them into the Auto Scaling group.\nAnother potential cause of the issue could be related to the AZRebalance process of Auto Scaling. AZRebalance is a feature of Auto Scaling that ensures that the number of instances in each availability zone is balanced. If the number of instances in different availability zones is not matching after terminating instances, Auto Scaling may launch new instances to balance the availability zones. To resolve this issue, the AZRebalance process of Auto Scaling can be suspended to avoid the rebalancing.\nFinally, running all the instances into a single availability zone could also be a potential solution to avoid launching new instances before terminating the old ones. However, this could compromise high availability, as a single availability zone is more prone to failures than multiple availability zones. Therefore, this solution should be adopted only if the workload is not critical and high availability is not a major concern.\nIn summary, the cause of the issue needs to be identified first, and then an appropriate solution can be adopted. Adjusting the scaling policies, instance termination policies, instance warm-up time, or AZRebalance process of Auto Scaling can help resolve the issue. Running all the instances into a single availability zone could also be a potential solution, but it should be adopted only if high availability is not a major concern.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Update the desired capacity of the Auto Scaling Group based on the number of active jobs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Auto Scaling triggers the AZRebalance event if the number of instances in availability zones are not matching after terminating instances. Suspend the AZRebalance process of Auto Scaling to avoid the rebalancing.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Run all the instances into a single availability zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Adjust the min and max capacity of the Auto Scaling Group after the jobs are completed.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 174,
  "query" : "You have an existing application that runs on your premise and currently uses a non-relational database.\nYour team has decided to move the application to the cloud environment and the database to DynamoDB to use some of its features like scaling and data streaming.\nAs per the management outline, for post-migration, all the communication between the application and the DynamoDB must be secure and scalable as the load will increase in the near future.\nEC2 instances should use their private IP addresses to access DynamoDB with no exposure to the public internet.\nWhat combinations can be used to design the migration and the post-migration activities, including the secure connection to DynamoDB? Select three options.",
  "answer" : "E.\nCorrect Answer: A, D, and E.\nOption A is CORRECT because the question is asking for migration and migrating the application to AWS.\nSo, EC2 would be the best fit here.\nOption B is INCORRECT because VPC gateway endpoints should be used.\nOption C is INCORRECT because VPC endpoints are only accessible from EC2 instances inside a VPC.\nIn case you are running it from your local premise, you will have to run it via a proxy that redirects to VPC based resource and then towards the endpoint.\nOption D is CORRECT because an Auto Scaling group can dynamically scale the EC2 instances to maintain steady performance.\nIn response to actual traffic patterns, Amazon DynamoDB auto scaling can dynamically adjust provisioned throughput capacity on your behalf.\nOption E is CORRECT because VPC Endpoints provide a secure link to access the AWS resources from a VPC.\nPlease check the reference in https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html.\nSure, I'd be happy to provide a detailed explanation!\nThe scenario presented in this question involves migrating an existing application that uses a non-relational database to the cloud environment and specifically to DynamoDB. The goal is to take advantage of DynamoDB's scaling and data streaming capabilities. Additionally, the management outline specifies that all communication between the application and DynamoDB must be secure and scalable.\nTo achieve this, we need to consider a few different aspects of the migration and post-migration activities. These might include things like network connectivity, security, scalability, and more.\nLet's go through each answer option in turn to see how they fit into the overall picture.\nOption A: Migrate the on-premises application to AWS EC2\nMigrating the on-premises application to AWS EC2 could be a valid option, depending on the specific requirements of the application. However, it's worth noting that this option alone doesn't address the requirement for secure and scalable communication with DynamoDB. It's possible that some additional steps would be required to achieve this.\nOption B: Use the HTTP endpoint of DynamoDB to make sure all communication is secure\nThis option is not correct. While it is true that DynamoDB provides an HTTPS endpoint for secure communication, this alone does not guarantee that all communication is secure. There are other factors to consider, such as authentication and authorization, that are necessary to ensure the security of the overall system.\nOption C: Connect your on-premises network to AWS using VPN to access DynamoDB via VPC endpoints\nThis option is a strong possibility. By using a VPN connection to connect the on-premises network to the VPC hosting DynamoDB, it's possible to ensure that all communication is secure and private. This can also be done using VPC peering if the on-premises network is on another VPC. Additionally, by using VPC endpoints to access DynamoDB, it's possible to ensure that the communication remains within the AWS network, which can help improve performance and reduce latency.\nOption D: Host the application in an Auto Scaling group and enable Amazon DynamoDB auto-scaling\nThis option is also a strong possibility. By hosting the application in an Auto Scaling group and enabling DynamoDB auto-scaling, it's possible to ensure that the system is scalable and can handle increasing load over time. Additionally, by using DynamoDB auto-scaling, it's possible to ensure that the capacity of the database scales up or down automatically based on the actual usage of the system, which can help reduce costs.\nOption E: Use the VPC gateway endpoint to connect with your DynamoDB\nThis option is also correct. By using the VPC gateway endpoint, it's possible to access DynamoDB securely without exposing it to the public internet. This can help improve security and ensure that all communication remains within the AWS network. It's worth noting that this option may not be as performant as using VPC endpoints, as traffic may need to traverse the internet to reach the gateway endpoint.\nIn summary, the three correct answer options for designing the migration and post-migration activities, including the secure connection to DynamoDB, are:\nConnect your on-premises network to AWS using VPN to access DynamoDB via VPC endpoints.\nHost the application in an Auto Scaling group and enable Amazon DynamoDB auto-scaling.\nUse the VPC gateway endpoint to connect with your DynamoDB.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Migrate the on-premises application to the AWS EC2.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the HTTP endpoint of the DynamoDB to make sure all the communication is secure.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Connect your on-premises network to AWS using the VPN to access the DynamoDB via the VPC endpoints.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Host the application in an Auto Scaling group and enable Amazon DynamoDB auto-scaling.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the VPC gateway endpoint to connect with your DynamoD.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 175,
  "query" : "To follow the new security compliances, your company has hired an external auditor to assess the security perimeter around your SaaS platform.\nThe application, S3 and DynamoDB are running in multiple regions, and the application uses load balancers within each region for high availability.\nThe instances load sensitive configurations from an S3 bucket at the start, and the DynamoDB is used as a primary database.\nThe auditor has advised tightening the security groups and NACLs based on the application requirement and use the private network instead of using the public endpoints to access the AWS services.\nYour team decided to use the VPC Endpoints as it uses the AWS internal network for all the communication.\nAfter a detailed examination, they realize that the data used for the applications also need to be replicated in different regions.\nPlease select three valid options, including the modification for endpoints and objects replications across regions.",
  "answer" : "E.\nCorrect Answers: A, B, E.\nInterface endpoints.\nAn interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet.\nIt serves as an entry point for traffic destined to a supported AWS service or a VPC endpoint service.\nInterface endpoints are powered by AWS PrivateLink.\nGateway Load Balancer endpoints.\nA Gateway Load Balancer endpoint is an elastic network interface with a private IP address from the IP address range of your subnet.\nGateway Load Balancer endpoints are powered by AWS PrivateLink.\nThis type of endpoint serves as an entry point to intercept traffic and route it to a service that you've configured using Gateway Load Balancers, for example, for security inspection.\nGateway endpoints.\nA gateway endpoint is for the following supported AWS services:\nAmazon S3\nDynamoDB.\nYou specify a gateway endpoint as a route table target for traffic that is destined for the supported AWS services.\nOption A is CORRECT as the DynamoDB Global Tables will create a Multi-Region, Multi-Master database that can be accessed internally from each region.\nOption B is CORRECT as creating the VPC Endpoints for services like S3 and DynamoDB will allow the application to use them via the AWS network.\nOption C is INCORRECT because NAT gateways are used to communicate with the Internet via a private subnet.\nThis is to secure the private resources like Database and Application servers which do not require and ideally should not have public connectivity.\nOption D is INCORRECT because DynamoDB uses the Gateway Endpoint instead of the Interface Endpoint.\nOption E is CORRECT because VPC Endpoints are regional points and can not be accessed outside of those regions.\nTo use the endpoints, we need to copy the S3 objects to all the regions from which they will be used.\nS3 Cross-Region Replication can do this effectively without any manual support.\nNOTE:Please read the question as follows.\nTo follow the new security compliance's your company has hired an external auditor to assess the security perimeter around your SaaS platform.\nThe application, S3, and DynamoDB are running in multiple regions, and the application uses load balancers within each region for high availability.\nThe instances load sensitive configurations from an S3 bucket at the start, and the DynamoDB is used as a primary database.\nThe auditor has advised furthering\n1\ntighten the security groups and NACLs based on the application requirement and\n2\nuse the private network instead of using the public endpoints to access the AWS services.\nYour team decided to use the VPC Endpoints as it uses the AWS internal network for all the communication.\nAfter a detailed examination, they realize that the data used for the applications also need to be replicated in different regions.\nPlease select three valid options, including the modification for endpoints and objects replications across regions.\nReference:\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\nSure, I'd be happy to explain each option in detail.\nA. Configure the DynamoDB Global Tables to replicate the data into multi-regions.\nDynamoDB Global Tables allow you to replicate your table data automatically across multiple AWS regions, providing a fully managed, multi-master, multi-region database capability. By configuring DynamoDB Global Tables, you can ensure that your data is available in multiple regions, improving availability and fault tolerance.\nB. Create VPC Endpoints for S3 and DynamoDB.\nVPC endpoints allow you to connect privately to AWS services such as S3 and DynamoDB without going over the internet. By creating VPC endpoints for S3 and DynamoDB, you can ensure that your data does not leave your VPC and can only be accessed by resources within your VPC. This improves security and reduces data transfer costs.\nC. Use the NAT Gateway for all the egress communication to these AWS services.\nA NAT gateway is a managed service that allows resources in a private subnet to access the internet while blocking incoming traffic from the internet. By using a NAT gateway for egress communication to S3 and DynamoDB, you can ensure that traffic from your private subnets to these services is secure and goes through a single point of exit, reducing complexity.\nD. Set up VPC gateway endpoint for S3 and interface endpoint for DynamoDB to communicate with these services over the private AWS network.\nA VPC gateway endpoint allows you to connect to S3 from within your VPC over the AWS network, while an interface endpoint allows you to connect to DynamoDB from within your VPC over the AWS network. By setting up these endpoints, you can ensure that your data remains within the AWS network and does not go over the internet. This improves security and reduces data transfer costs.\nE. Use the S3 Cross-Region Replication to save the configurations in the multiple regions.\nS3 Cross-Region Replication allows you to automatically replicate objects from one S3 bucket to another S3 bucket in a different region. By using S3 Cross-Region Replication, you can ensure that your data is available in multiple regions, improving availability and fault tolerance.\nIn summary, options A, B, and D are all valid options to improve security and availability while ensuring that data is available in multiple regions. Option C is not necessary if VPC endpoints are used, and Option E only applies to replicating data in S3, not DynamoDB.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure the DynamoDB Global Tables to replicate the data into multi-regions.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create VPC Endpoints for S3 and DynamoD.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the NAT Gateway for all the egress communication to these AWS services.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up VPC gateway endpoint for S3 and interface endpoint for DynamoDB to communicate with these services over the private AWS network.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the S3 Cross-Region Replication to save the configurations in the multiple regions.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 176,
  "query" : "Your organization is planning to shift one of the high-performance data analytics applications running on Linux servers purchased from the 3rd party vendor to the AWS.\nCurrently, the application works in an on-premises load balancer, and all the data is stored in a very large shared file system for low-latency and high throughput purposes.\nThe management wants minimal disruption to existing service and also wants to do stepwise migration for easy rollback.\nPlease select 3 valid options from below.",
  "answer" : "E.\nCorrect Answer: C, D, E.\nOptions C, D, and E are correct because network extension via VPN or Direct Connect will allow the on-premises instances to use the AWS resources like EFS.\nEFS is elastic file storage that can be mounted on EC2 and other instances.\nIt is inherently durable and scalable.\nEFS stores the data by default at multiple availability zones.\nWith Route 53 Weighted policy, the requests can be distributed to on-premise and AWS resources easily in a controlled manner.\nOption A is INCORRECT because S3 will work as shared, durable storage.\nBut it may not be a suitable choice for low-latency, high throughput load processing.\nAs the application cannot be easily modified, presenting the S3 as a local file system will be another task and has to be done via File Storage Gateway.\nOption B is INCORRECT because the purpose is to use a shared file system solution (EFS)\nRAID1 for EBS is not necessary as the application requires data from EFS rather than the local storage.\nThe organization wants to migrate a high-performance data analytics application from an on-premises environment to AWS while minimizing disruption and ensuring easy rollback. Here are the explanations for the three valid options:\nOption A: Save all the data on S3 and use it as shared storage. Use an application load balancer with EC2 instances to share the processing load.\nThis option involves storing all the data in Amazon S3, which can be accessed by multiple EC2 instances that are load balanced using an application load balancer. This allows for easy scaling of compute resources as the processing load increases. Additionally, using S3 for storage provides durability and availability guarantees, and can reduce the need for expensive, high-performance storage options.\nOption B: Create a RAID 1 storage using EBS and run the application on EC2 with application-level load balancers to share the processing load.\nThis option involves creating a RAID 1 storage using Elastic Block Store (EBS) volumes, which provide high-performance, low-latency block-level storage for EC2 instances. The EC2 instances can be load balanced using application-level load balancers, which can distribute the processing load across multiple instances. This option provides high-performance storage and scalable compute resources while maintaining compatibility with the current application architecture.\nOption C: Use the VPN or Direct Connect to create a link between your company premise and AWS regional data center.\nThis option involves using a VPN or Direct Connect to create a secure, high-speed connection between the organization's on-premises environment and the AWS regional data center. This allows for easy migration of data and applications between the two environments, while maintaining network and data security. This option can also provide low-latency access to data stored in AWS, which is important for high-performance data analytics applications.\nOption D and E are not valid options for this scenario:\nOption D: Create an EFS with provisioned throughput and share the storage between your on-premise instances and EC2 instances.\nThis option involves using Amazon Elastic File System (EFS) to create a shared file system that can be accessed by both on-premises instances and EC2 instances. While this option provides shared storage, it may not provide the necessary performance for high-performance data analytics applications, which typically require low-latency access to data.\nOption E: Setup a Route 53 record to distribute the load between on-premises and AWS load balancer with the weighted routing policy.\nThis option involves using Route 53 to distribute traffic between on-premises and AWS load balancers using the weighted routing policy. While this option can provide load balancing between on-premises and AWS environments, it may not provide the necessary performance for high-performance data analytics applications, which typically require low-latency access to data. Additionally, this option may introduce additional complexity to the architecture, which can increase the risk of disruption during migration.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Save all the data on S3 and use it as shared storage. Use an application load balancer with EC2 instances to share the processing load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a RAID 1 storage using EBS and run the application on EC2 with application-level load balancers to share the processing load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the VPN or Direct Connect to create a link between your company premise and AWS regional data center.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an EFS with provisioned throughput and share the storage between your on-premise instances and EC2 instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Setup a Route 53 record to distribute the load between on-premises and AWS load balancer with the weighted routing policy.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 177,
  "query" : "The new mobile application that you are building will accept images and videos from the mobile app and will store, enhance, watermark, and deliver them to other users.\nAs per the initial research, it seems that EFS will be a suitable option as the application will be running behind a load balancer.\nThe file system will be shared among the instances so that the data can be delivered fast.\nYour company has decided to use the CloudFront before the load balancer to geocache the contents and serve it faster.\nWhich of the following statements of EFS are correct when working with the application? (Select TWO.)",
  "answer" : "E.\nCorrect Answer: C, E.\nOption A is INCORRECT because Amazon EFS data is distributed across multiple Availability Zones (AZs), providing a high durability and availability level.\nOption B is INCORRECT because EFS supports two throughput modes to choose from for your file system, Bursting Throughput, and Provisioned Throughput.\nWith Bursting Throughput mode, throughput on Amazon EFS scales as your file system grows.\nFile-based workloads are typically spiky, driving high levels of throughput for short periods of time and low levels of throughput the rest of the time.\nTo accommodate this, EFS is designed to burst to high throughput levels for periods of time.\nOption C is CORRECT because EFS serves as a local file system and is not directly accessible.\nAll the read/write operations have to be done via the EC2 with which it is attached.\nOption D is INCORRECT because EFS provides both in-transit and at-rest encryption options using the AWS KMS service.\nOption E is CORRECT because EFS uses a credit system to determine when file systems can burst.\nEach file system earns credits over time at a baseline rate determined by the size of the file system and uses credits whenever it reads or writes data.\nSo if you have proportionally high read/write compared to overall data, you may face the burst capacity issues and opt to the Throughput Provisioned Mode.\nSpecifying Throughput with Provisioned Mode.\nAWS EFS supports Provisioned Throughput mode, which allows applications configured to provision the throughput irrespective of the data stored inside the file system.\nThis means applications with higher throughput requirements can use this mode to achieve higher performance.\nKeep in mind that Provisioned Throughput is billed separately from the data storage depending on the capacity.\nThe correct statements regarding EFS when working with the application are: B. The throughput of EFS increases with storage capacity, so the download is always faster with more data. E. EFS performance is dependent on storage size. Under heavy load such as when the throughput limit exceeds 1024 MiB/s, EFS may start to throttle unexpectedly.\nExplanation: Amazon Elastic File System (EFS) is a scalable, fully managed, cloud-based NFS file system for Linux-based workloads. It can be used to store and share files across multiple instances in a scalable and highly available way. When working with the mobile application, EFS can be used as a shared file system to store and deliver images and videos to users.\nOption A is incorrect. EFS is highly available and automatically replicates data across multiple Availability Zones (AZs) within a region to provide durability and availability. However, it is recommended to periodically backup the data to ensure no data is lost.\nOption B is correct. The throughput of EFS is directly proportional to the amount of data stored in it. This means that the more data stored in EFS, the higher the throughput. This is because EFS automatically scales the throughput based on the size of the file system.\nOption C is incorrect. EFS is mounted in the local file system of EC2 instances, but the file system is accessible through NFS, which means that the application can read and write files to the file system just like a local file system.\nOption D is incorrect. EFS provides encryption in transit using Transport Layer Security (TLS) and at rest using encryption keys managed by AWS Key Management Service (KMS). This means that data stored in EFS can be encrypted both in transit and at rest.\nOption E is correct. EFS performance is dependent on the amount of data stored in it. Under heavy load, such as when the throughput limit exceeds 1024 MiB/s, EFS may start to throttle unexpectedly. This is because EFS provides throughput on a per-byte basis, and if a file system exceeds its allowed throughput, EFS may throttle the throughput to ensure that other file systems in the same Availability Zone are not affected.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "EFS is not highly-available and will require periodic backup to ensure no data is at a loss.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The throughput of EFS increases with storage capacity, so the download is always faster with more data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The EFS is mounted in the local file system of EC2 instances and your application needs to handle all the upload/download processing.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Encryption in transit and at the storage level is not available. So in the future, if your application needs encryption, EFS will not be the right choice.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "EFS performance is dependent on storage size. Under heavy load such as when the throughput limit exceeds 1024 MiB/s, EFS may start to throttle unexpectedly.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 178,
  "query" : "Your company runs a popular map service as a SaaS platform.\nYour dynamic multi-page application users are spread across the world, but not all of them heavily use the system.\nSo the load is high in some of the regions but not all.\nThe application uses the NoSQL database and runs it on a cluster of EC2 machines and using the custom tools to replicate the data across different regions.\nThe current database size is around 10PB.\nAs the popularity of the application grows, the database is also growing rapidly.\nNow the application is serving millions of requests from your SaaS platform.\nThe management has decided to develop a plan to re-design the architecture dynamically, both from the application availability and infrastructure cost perspective.\nPlease suggest the necessary changes.\nSelect 3 Options.",
  "answer" : "E.\nCorrect Answer: A, C, D.\nOption A is CORRECT because the AWS Route53 Latency-based routing will redirect the lowest latency region's request to help serve the application faster.\nOption B is INCORRECT because the AWS S3 can only be used for Static Website Hosting or Single Page Applications.\nHowever, in the current case, the application is a SaaS platform and the question mentions that the multi-page application is dynamic.\nCloudFront is suitable to serve requests from different regions.\nBut as S3 is incorrect, this option is wrong.\nOption C is CORRECT because the DynamoDB global tables provide replication capabilities.\nWith global tables, you can have a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications.\nOption D is CORRECT because, in the current case, the size of the database is into petabytes.\nWe can use ElasticCache to provide secure and fast performance.\nOption E is INCORRECT because AWS RDS is a relational database service, and the application in the context is using the NoSQL database.\nIt would require a significant engineering effort to re-design the application to fit the relational database and scale for such a huge amount of data.\nTo re-design the architecture dynamically from both application availability and infrastructure cost perspective, the following three options can be selected:\nOption A: Route53 with Latency-Based Routing Policy\nBy using Route53 with the Latency-Based Routing Policy, the application can be deployed in the regions where the heavy load is being generated. This policy will redirect the users to the lowest latency region from their location, ensuring that they have the best possible experience. This option helps to improve application availability by providing users with low latency and reducing the load on other regions.\nOption C: Use DynamoDB Global Tables\nDynamoDB Global Tables can be used to replicate the data into multiple regions. This option provides high availability and low latency by replicating data across different regions. The application can read and write data to any of the replicas, and DynamoDB will automatically synchronize the data between the replicas, ensuring that the data is always consistent.\nOption E: Use RDS with Read Replicas\nUsing RDS with Read Replicas into multiple regions can help improve availability and reduce the load on the primary database. The read replicas can be used to serve traffic, and the primary database can be used to handle writes. This option provides lower latency for read operations and helps improve application availability.\nOptions B and D are not suitable for the given scenario. Migrating the application to S3 and using CloudFront edge locations will not work for a dynamic multi-page application that uses a NoSQL database. And deploying ElastiCache to cache requests to the DynamoDB table may not be useful in regions with low traffic.\nTherefore, options A, C, and E are the most suitable options for re-designing the architecture dynamically from both application availability and infrastructure cost perspective.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Route53 with Latency-based routing policy to redirect to the lowest latency region and deploy the application into regions from where the heavy load is generating.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Migrate the application on S3 and use CloudFront edge locations to serve the requests.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use DynamoDB global tables to replicate the data into multiple regions.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Deploy the ElastiCache to cache the requests to the DynamoDB table.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use RDS with the Read Replicas into multiple regions, application servers will use the read replicas to serve the traffic.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 179,
  "query" : "You have developed a web application to collect monthly expense reports.\nAs the nature of the application and looking at the usage statistics, it is mostly used around the last week of the month and the first week of the month.\nTo increase the application performance, you added a caching layer in front of the application servers.\nSo the reports are cached and served immediately.\nYou started off with Elasticache Redis with a \"cache.t2.small\" node type.\nThe application has been running fine.\nWhen looking at the performance activity into the CloudWatch, hardly 50% of the requests are served by the cache, and the cache cannot cope with additional content requirements.\nYou want to improve the application with minimal changes and resources.\nPlease select a valid option.",
  "answer" : "Correct Answer: A.\nOption A is CORRECT because we can modify the cache node type from \"cache.t2.small\" to \"cache.t2.medium\" in the console.\nWe must increase the size of the Redis instance for the server to serve more requests from the cache.\nOption B is INCORRECT because creating a new Elasticache instance with a \"cache.t2.micro\" node type is not needed here.\nOption C is INCORRECT because migrating to Beanstalk will simply not save the cost.\nAlso, Beanstalk has an RDS layer but no caching layer.\nOption D is INCORRECT because S3 and CloudFront will incur additional costs for such a minimal use case.\nThere are two things here.\nElastic Cache Instance -&gt; this supports the underlying Elasticache engine.\nThis is similar to our ec2 instance like \"t2.micro\" , \"t2.mall\" etc.\nThis instance type can be modified either through the AWS console or through the CLI.\nRedis -&gt; This is the engine that runs the elasticache cluster.\nThis engine can only be \"upgraded\" and cannot be \"downgraded\".\nPlease refer to page 99 of the below link-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/redis-ug.pdf\nBased on the usage statistics of the web application, it is observed that the majority of the requests are made during the last week and first week of the month. To optimize the performance, a caching layer was added in front of the application servers. However, the current Elasticache Redis instance with \"cache.t2.small\" node type is not able to serve all the requests and there is a need to improve the performance of the application.\nOption A: Modify the ElastiCache instance from t2 small to t2 medium, as t2 medium is more suitable for the given requirement. This option suggests upgrading the current Elasticache Redis instance from \"cache.t2.small\" to \"cache.t2.medium\". This may improve the performance of the application as t2.medium has higher specifications than t2.small. However, it may not be a cost-effective solution as it involves an increase in instance size, which can be expensive, and may not be required to handle the spike in traffic during the peak usage period.\nOption B: Create a new ElastiCache instance with t2 micro and terminate the t2 small instance. This option suggests creating a new Elasticache Redis instance with \"cache.t2.micro\" node type and terminating the existing \"cache.t2.small\" instance. While t2.micro instances are smaller than t2.small instances, this option may work because the majority of the requests for the application are concentrated in a short period of time. Additionally, t2.micro instances are less expensive than t2.small instances, making it a cost-effective solution.\nOption C: Migrate the application to Elastic Beanstalk to use auto-scaling and set the desired and min capacity to 1, use the RDS and Cache layer of Beanstalk to save the cost. This option suggests migrating the web application to Elastic Beanstalk, which will allow the use of auto-scaling with a desired and minimum capacity of 1. Additionally, the RDS and cache layers of Beanstalk can be utilized to optimize performance and save costs. However, this option involves a significant amount of effort to migrate the application and may not be the most efficient solution for the given requirement.\nOption D: Run the web application from S3 and serve with CloudFront. This option suggests running the web application from Amazon S3 and serving it with Amazon CloudFront. While Amazon S3 can be used to host static websites, it may not be a suitable solution for dynamic web applications. Additionally, CloudFront is a content delivery network that can improve the performance of web applications by caching content at the edge, but it may not be a complete solution to optimize performance for the given requirement.\nBased on the given options, the best solution for optimizing the application's performance with minimal changes and resources would be to choose option B. Creating a new Elasticache Redis instance with t2.micro node type and terminating the t2.small instance will provide a cost-effective solution to handle the spike in traffic during the peak usage period of the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Modify the ElastiCache instance from t2 small to t2 medium, as t2 medium is more suitable for the given requirement.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new ElastiCache instance with t2 micro, and terminate the t2 small instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Migrate the application to Elastic Beanstalk to use auto-scaling and set the desired and min capacity to 1, use the RDS and Cache layer of Beanstalk to save the cost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Run the web application from S3 and serve with CloudFront.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 180,
  "query" : "You are an IT administrator, and you are responsible for managing several on-premises databases in VMware vSphere environments.\nThe R&D team has just created several RDS instances on VMware to utilize the latest AWS RDS on VMware features.\nThen, those new databases can be managed by using the RDS console, API, and CLI.\nWhich activities does the Amazon RDS on VMware manage on your behalf? (Select FOUR)",
  "answer" : "E.\nCorrect Answer - A,B,C ,D.\nAmazon Relational Database Service (RDS) on VMware lets you deploy managed databases in on-premises VMware environments using the Amazon RDS technology.\nFor what Amazon RDS on VMware manages on your behalf, refer to https://aws.amazon.com/rds/vmware/faqs/ and https://docs.aws.amazon.com/AmazonRDS/latest/RDSonVMwareUserGuide/rds-feature-support.html.\nOption A is CORRECT: Because RDS on VMware takes care of the patching for databases.\nOption B is CORRECT: Because RDS on VMWare has instance health monitoring and failover capabilities.\nOption C is CORRECT: Because after being configured, RDS on VMware takes care of the backup and retention just like what it does in AWS RDS.\nOption D is CORRECT: For a similar reason as Option.\nC.Option E is incorrect: Because RDS on VMware communicates with AWS RDS using a dedicated VPN channel.\nThere is no public IP allocated by AWS VPC.\nAmazon RDS on VMware is a service that enables you to run RDS instances on-premises in your VMware vSphere environment. It provides a simple and cost-effective way to manage databases in a hybrid environment that spans both on-premises and cloud environments. When you use RDS on VMware, certain tasks are managed on your behalf, allowing you to focus on your applications and data.\nThe activities that Amazon RDS on VMware manages on your behalf are as follows:\nA. The patching of the RDS on-premises operating systems and database engines: Amazon RDS on VMware manages the patching of the underlying operating systems and database engines of your RDS instances. This ensures that your databases are always up-to-date with the latest security patches and software updates.\nB. Instance health monitoring and failover capabilities of the on-premises instances: Amazon RDS on VMware monitors the health of your RDS instances and provides automatic failover capabilities in case of a failure. This ensures that your applications remain available and responsive to your users.\nC. Automated backups based on retention policies of databases in RDS VMware: Amazon RDS on VMware provides automated backups of your databases based on retention policies that you define. You can specify how long you want to retain backups and how many backups you want to keep. This ensures that your data is always safe and available for recovery.\nD. Point-in-time restore from on-premises instances and cloud backups when needed: Amazon RDS on VMware allows you to restore your databases to a specific point in time. You can restore your databases from on-premises instances or from cloud backups, depending on your needs. This ensures that you can recover your data quickly in case of a disaster or data loss.\nE. IP management such as a dedicated public IP has been allocated by AWS VP: Amazon RDS on VMware manages the IP addresses of your RDS instances, including allocating a dedicated public IP address for your instance. This ensures that your applications can communicate with your databases over the internet.\nIn summary, Amazon RDS on VMware provides several benefits to IT administrators, such as automatic patching, instance health monitoring, automated backups, point-in-time restore, and IP management. These activities are managed on your behalf, allowing you to focus on your applications and data.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The patching of the RDS on-premises operating systems and database engines.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Instance health monitoring and failover capabilities of the on-premises instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Automated backups based on retention policies of databases in RDS VMware.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Point-in-time restore from on-premises instances and cloud backups when needed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "IP management such as a dedicated public IP has been allocated by AWS VP.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 181,
  "query" : "You are an AWS solutions architect and are in charge of maintaining an RDS on VMware database that is deployed on-premises.\nYou have created a read replica in the ap-south-1 region to share some read traffic.\nThe system has run smoothly for a while.\nThen the company decides to migrate all the products to AWS, including the on-premises RDS instance.\nOther than that, the instance needs to have another replica in another region ap-southeast-1\nThe migration process should be straightforward.\nWhat actions should you take to fulfill this requirement?",
  "answer" : "Correct Answer - D.\nAmazon RDS on VMware database instances can be easily migrated to Amazon RDS database instances in AWS with no impact on uptime, giving you the ability to rapidly deploy databases in all AWS regions without interrupting your customer experience.\nThe process is as below.\nOption A is incorrect: Because the Data Migration Service is not needed.\nYou need to promote the read-replica to be the new RDS instance.\nOption B is incorrect: Same reason as.\nOption A.\nAlso, “migrating the instance” is incorrect.\nOption C is incorrect: Because the read replica in ap-southeast-1 is still syncing with the original on-premise RDS instance.\nA new read replica should be created from the instance in ap-south-1.\nOption D is CORRECT: Because the database can be easily migrated by promoting the read replica in ap-south-1\nPlease note that during migration, the performance may be impacted.\nIn the question, it asks for a straightforward method so this option is appropriate.\nThe correct answer is A. Use Data Migration Service to migrate the on-premises database to an RDS instance in AWS. Create a read replica in the ap-southeast-1 region afterward.\nExplanation:\nWhen migrating an on-premises database to AWS, AWS provides a service called AWS Database Migration Service (DMS) to simplify the process of migrating databases to the cloud. DMS helps you migrate databases to AWS easily and securely.\nTo fulfill the requirement, you should follow these steps:\nStep 1: Use Data Migration Service (DMS) to migrate the on-premises database to an RDS instance in AWS.\nYou should use DMS to migrate the on-premises database to an RDS instance in AWS. DMS can migrate data from on-premises databases to AWS with minimal downtime. DMS can migrate data to and from most widely used commercial and open-source databases.\nTo do this, you need to follow these steps:\nCreate a new RDS instance in the AWS region of your choice (e.g., ap-southeast-1).\nCreate a new migration task in DMS.\nConfigure the source and target endpoints in DMS.\nConfigure the migration task to migrate the database schema and data from the source to the target database.\nStart the migration task and monitor its progress.\nStep 2: Create a read replica in the ap-southeast-1 region afterward.\nOnce the on-premises database has been migrated to an RDS instance in AWS, you should create a read replica in the ap-southeast-1 region to share read traffic.\nTo do this, you need to follow these steps:\nNavigate to the RDS console.\nSelect the RDS instance that you want to create a read replica for (in this case, the RDS instance that you just created after migrating the on-premises database).\nClick the \"Actions\" button and select \"Create Read Replica.\"\nSpecify the region that you want to create the read replica in (ap-southeast-1).\nConfigure the read replica as desired (e.g., choose the appropriate instance class, configure the backup retention period, etc.).\nStart the creation of the read replica and monitor its progress.\nOption B is incorrect because clicking \"migrating the instance\" in the RDS console is not a valid option. Option C is incorrect because promoting a read replica to be the new RDS instance will not migrate the original on-premises database to AWS. Option D is incorrect because you should create the read replica in ap-southeast-1 after migrating the on-premises database to an RDS instance in AWS, not before.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Data Migration Service to migrate the on-premises database to an RDS instance in AWS. Create a read replica in the ap-southeast-1 region afterward.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In RDS console, click “migrating the instance” to create a new RDS instance. Then create a new read replica in the ap-southeast-1 region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create another read replica in the ap-southeast-1 region to share the read traffic for the RDS instance on VMware. Promote the RDS read replica in ap-south-1 to be the new RDS instance so that the original on-premise database is migrated in AWS with a replica in ap-southeast-1.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Promote the RDS read replica in ap-south-1 to be the new RDS instance. Create another read replica in ap-southeast-1 for this new instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 182,
  "query" : "You are a development lead, and your team has maintained an emailing service for the company's major applications.\nThe emailing service is deployed on-premises with several RDS on VMware databases to store users' metadata.\nThere is no plan to migrate the on-premises RDS databases to AWS RDS.\nHowever, you need an appropriate approach to backup the databases to AWS so that the database can be quickly restored.\nWhich steps should you take to fulfill this requirement? (Select THREE)",
  "answer" : "E.\nCorrect Answer - A, D, E.\nWhen using Amazon RDS on VMware, it also makes it very easy to take advantage of the AWS scale for disaster recovery, cloud backups, and read replica scaling.\nThe same RDS console, APIs, or CLI can be used to manage RDS databases running on VMware or RDS databases running on AWS.\nRefer to https://aws.amazon.com/rds/vmware/ on how to do the hybrid cloud backups and scaling.\nOption A is CORRECT: Because the Amazon RDS on VMware backup/restore system enables point-in-time restore for all supported engines and allows you to specify an automated backup retention period of up to 35 days.\nOption B is incorrect: Because for Amazon RDS on VMware, you can only create a read replica in the same region as the source DB instance.\nDetails please check https://aws.amazon.com/rds/vmware/faqs/.\nOption C is incorrect: Because AWS Glacier is unsuitable for a quick restore.\nOption D is CORRECT: Because the read replica can be created in the same region.\nRDS on VMware is available in AWS US East (Northern Virginia) Region.\nReferences can be found in https://aws.amazon.com/rds/vmware/faqs/\nPlease note that a Read Replica can be promoted if the database needs to be restored in AWS RDS.\nOption E is CORRECT: Because this method can create a manual snapshot in AWS, if needed.\nTo fulfill the requirement of backing up the on-premises RDS databases to AWS, there are a few steps that should be taken, as follows:\n1.\nSpecify an automated backup every day to store the snapshot to the S3 bucket so that the backup has high availability and durability. This will ensure that the backups are taken regularly and stored in a highly available and durable location such as S3, which provides a cost-effective and secure way to store data.\n2.\nConfigure a read-replica in a different region from the one that the RDS on VMware instance connects to. This will provide an additional layer of redundancy and protection against disaster scenarios, such as if the primary database becomes unavailable due to a regional outage. By having a read replica in a different region, data can still be accessed and the application can continue to function without significant disruption.\n3.\nCreate a automated backup schedule in RDS and save the daily snapshots to AWS Glacier for long-term backup. This will ensure that long-term backups are taken and stored in a cost-effective and secure manner using AWS Glacier, which is designed for archiving data. It is important to note that Glacier is designed for long-term storage and may have slower retrieval times than other storage options.\nIt is important to also note that creating a read-replica in the same region for disaster recovery as long as the region supports RDS (Option D) can also be a valid approach to ensure that the data is still available if the primary database becomes unavailable. However, this may not provide the same level of protection as having a read-replica in a different region.\nAdditionally, creating a manual backup of the on-premise database if needed (Option E) is always a good practice to ensure that a backup can be taken at any point in time if required. However, relying solely on manual backups may not be sufficient for ensuring regular and consistent backups.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Specify an automated backup every day to store the snapshot to the S3 bucket so that the backup has high availability and durability.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure a read-replica in a different region from the one that the RDS on VMware instance connects to.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an automated backup schedule in RDS and save the daily snapshots to AWS Glacier for long-term backup.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a read-replica in the same region for disaster recovery as long as the region supports RDS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a manual backup of the on-premise database if needed.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 183,
  "query" : "Which of the following should be used when migrating virtual machines hosted in an on-premises location to an AWS VPC?",
  "answer" : "Answers: C and D.\nOption A is incorrect because having a NAT instance is not going to help in this scenario.\nNAT instance is used so that the instances in the private subnet can communicate with the internet.\nOption B is incorrect, as a static IP address will not help in \"migrating a legacy application\" that is located on an on-premise location.\nOption C is CORRECT because AWS SMS is a dedicated and agentless service that simplifies and expedites the migration to the cloud.\nOption D is CORRECT because the VM Import facility provided by AWS can be used to import virtual machine images.\nhttps://docs.aws.amazon.com/vm-import/latest/userguide/what-is-vmimport.html.\nWhen migrating virtual machines hosted in an on-premises location to an AWS VPC, the recommended method is to use AWS Server Migration Service (SMS).\nAWS Server Migration Service is a fully managed service that allows you to migrate on-premises servers, virtual machines, or cloud instances to AWS. It simplifies the migration process by automating many of the manual steps involved in migrating workloads to the cloud, including replication of on-premises servers and their data to AWS, and the creation of Amazon Machine Images (AMIs) in preparation for launch in the cloud.\nUsing a NAT instance to route traffic from the instance in the VPC (Option A) or using a static IP address on the VPC instance (Option B) are not directly related to the migration process and do not provide any benefit in terms of migrating virtual machines.\nVM Import (Option D) is a service that allows you to import virtual machine images from your existing virtualization environment to Amazon EC2 instances. It can be used to migrate virtual machines to AWS, but it requires a manual process of creating an image of the virtual machine and uploading it to AWS.\nTherefore, the best option for migrating virtual machines hosted in an on-premises location to an AWS VPC is to use AWS Server Migration Service (SMS).",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a NAT instance to route traffic from the instance in the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a Static IP address on the VPC instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Server Migration Service (SMS).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the VM Import provided by AWS.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 184,
  "query" : "Your team is building up a smart home iOS APP.\nThe end users have used your company's camera-equipped home devices such as baby monitors, webcams, and home surveillance systems.\nThen the videos are uploaded to AWS.\nAfterward, through the mobile APP, users can play the on-demand or live videos using the format of HTTP Live Streaming (HLS)\nWhich combinations of steps should you use to accomplish this task? (Select TWO)",
  "answer" : "Correct Answers - B, D.\nAWS provides a live streaming solution that combines AWS Elemental MediaLive and AWS Elemental MediaPackage with Amazon CloudFront to build a highly resilient and scalable architecture that delivers your live content worldwide.\nThe diagram below presents the live streaming video architecture you can automatically deploy using the solution's implementation guide and accompanying AWS CloudFormation template.\nOption A is INCORRECT because Kinesis Data Firehose is not used for live streaming; instead, it is used for streaming data delivery.\nOption B is CORRECT because AWS provides a live streaming solution that combines AWS Elemental MediaLive and AWS Elemental MediaPackage with Amazon CloudFront to build a highly resilient and scalable architecture that delivers your live content worldwide.\nAWS Elemental MediaStore is a video origination and storage service that offers the high performance and immediate consistency required for live and on-demand media.\nYou can use AWS Elemental MediaStore to store assets that MediaLive retrieves and uses when transcoding, and as a destination for output from MediaLive.\nOption C is incorrect because transforming the stream data to HLS compatible data using Kinesis Data Analytics or customer code in EC2/Lambda is not needed or irrelevant here.\nOption D is CORRECT because the GetHLSStreamingSessionURL API is called to retrieve the HLS streaming session URL.\nWhen you have the HLS streaming session URL, provide it to the video player, which will be able to play the video.\nAWS Docs for reference:\nhttps://aws.amazon.com/solutions/live-streaming-on-aws/ https://docs.aws.amazon.com/medialive/latest/ug/medialive-ug.pdf#what-is\nTo accomplish the task of playing on-demand or live videos using HTTP Live Streaming (HLS) through the mobile APP, you should use the following two combinations of steps:\nB. Use AWS Elemental MediaLive and AWS Elemental MediaPackage with Amazon CloudFront.\nAWS Elemental MediaLive is a video processing service that enables you to create high-quality live video streams for delivery to broadcast televisions and internet-connected devices, such as smartphones, tablets, and connected TVs. You can use MediaLive to encode live video streams into HLS format, which is compatible with most iOS devices. AWS Elemental MediaPackage is a video origination and just-in-time packaging service that prepares video content for delivery over the internet. MediaPackage supports the creation of HLS video streams with features such as encryption, ad insertion, and time-shifted TV. Amazon CloudFront is a content delivery network (CDN) service that caches and delivers the HLS video streams to users around the world, providing low latency and high transfer speeds. By using AWS Elemental MediaLive and AWS Elemental MediaPackage with Amazon CloudFront, you can create, package, and deliver HLS video streams to iOS devices for on-demand or live streaming.\nC. Transform the stream data to HLS compatible data by using Kinesis Data Analytics or customer code in EC2/Lambda. Then in the mobile application, use HLS protocol to display the video stream by using the converted HLS streaming data.\nIn this approach, you need to transform the video stream data from the users' home devices into HLS compatible data. You can do this by using Kinesis Data Analytics, which is a fully managed service that enables you to analyze streaming data using SQL or Java. Alternatively, you can use customer code in EC2/Lambda to transform the stream data. After the stream data is transformed into HLS compatible data, you can use the HLS protocol to display the video stream in the mobile application. The mobile application can access the converted HLS streaming data and use it to play on-demand or live videos using HLS format.\nIn conclusion, to accomplish the task of playing on-demand or live videos using HTTP Live Streaming (HLS) through the mobile APP, you can use the combination of steps B and C. By using AWS Elemental MediaLive and AWS Elemental MediaPackage with Amazon CloudFront, you can create, package, and deliver HLS video streams to iOS devices. Alternatively, you can transform the stream data into HLS compatible data using Kinesis Data Analytics or customer code in EC2/Lambda, and then use HLS protocol to display the video stream in the mobile application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a Kinesis Data Firehose to ingest, durably store, and encrypt the live videos from the users’ home devices.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Elemental MediaLive and AWS Elemental MediaPackage with Amazon CloudFront.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Transform the stream data to HLS compatible data by using Kinesis Data Analytics or customer code in EC2/Lambda. Then in the mobile application, use HLS protocol to display the video stream by using the converted HLS streaming data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the mobile application, use HLS to display the video stream by using the HLS streaming session URL.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 185,
  "query" : "An IoT company has a new product which is a camera device.\nThe device has installed several sensors and can record video as required.\nThe device has AWS Kinesis Video Streams SDK in the software and can transmit recorded video in real-time to AWS Kinesis.\nThen the end-users can use a desktop or web client to view, download, or share the video stream.\nThe client app should be simple and use a third-party player such as Google Shaka Player to display the video stream from Kinesis.\nHow should the client app be designed?",
  "answer" : "Correct Answer - B.\nThe most straightforward way to view or live playback the video in Kinesis Video Streams is by using HLS.\nOption A is incorrect: Because although GetMedia API may work, it is not as simple as HLS.\nYou may have to create a player that uses GetMedia and build it yourself.\nHowever, in this case, a third-party player is needed.\nReference is in https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/how-hls.html#how-hls-ex1-session.\nOption B is CORRECT: Because GetHLSStreamingSessionURL API is required for third-party players to play the HLS streams.\nOption C is incorrect: Because HTTP Live Streaming (HLS) should be used to playback the Kinesis Video Streams.\nOption D is incorrect: Same reason as Option.\nC.\nThe correct answer to this question is B. The client can use HLS for live playback. Use GetHLSStreamingSessionURL API to retrieve the HLS streaming session URL, then provide the URL to the video player.\nThe given scenario involves an IoT device that records video and sends it to AWS Kinesis Video Streams in real-time. The end-users can use a simple desktop or web client to view, download or share the video stream. The client app should use a third-party player such as Google Shaka Player to display the video stream from Kinesis.\nHTTP Live Streaming (HLS) is a protocol that allows streaming of live or pre-recorded multimedia content over HTTP. It is widely used for video streaming and is supported by many platforms, including web browsers and mobile devices. The client app can use HLS for live playback of the video stream from Kinesis.\nThe GetHLSStreamingSessionURL API is used to retrieve the HLS streaming session URL for a Kinesis video stream. This API returns a signed URL that is valid for a limited time period. The client app can use this URL to access the HLS stream from Kinesis.\nAdobe HTTP Dynamic Streaming (HDS) and Microsoft Smooth Streaming (MSS) are also protocols for streaming video content over HTTP. However, these protocols are not supported by all platforms and may require additional software or plugins to be installed on the client device. Therefore, using HLS is a better option for maximum compatibility and ease of use.\nIn summary, the client app should be designed to use HLS for live playback and should use the GetHLSStreamingSessionURL API to retrieve the HLS streaming session URL from Kinesis Video Streams. The URL should then be provided to the third-party video player for display.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The client can use HTTP Live Streaming (HLS) for live playback. Use GetMedia API to process and play Kinesis video streams.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The client can use HLS for live playback. Use GetHLSStreamingSessionURL API to retrieve the HLS streaming session URL, then provide the URL to the video player.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The client can use Adobe HTTP Dynamic Streaming (HDS) for live playback. Use GetHDSStreamingSessionURL API to retrieve the HDS streaming session URL, then provide the URL to the video player.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The client can use Microsoft Smooth Streaming (MSS) for live playback. Use GetMSSStreaming API to retrieve the MSS streaming to the video player.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 186,
  "query" : "Your company is a media company.\nYou need a fully managed AWS service that can stream live video from smartphones to AWS and help you build applications for real-time video processing.\nWhich of the following services is the most appropriate?",
  "answer" : "Correct Answer - D.\nOptions A, B and C are incorrect because all these services are not dedicated AWS services for streaming live videos to AWS.\nOption D is CORRECT because Amazon Kinesis Video Streams can capture massive amounts of live video from millions of sources, including smartphones to AWS.\nIt is the most appropriate service to achieve the requirement of the question.\nFor the introduction of Kinesis Video Streams, please check https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/what-is-kinesis-video.html.\nThe most appropriate AWS service for the given scenario would be D. Amazon Kinesis Video Streams.\nAmazon Kinesis Video Streams is a fully managed AWS service that makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), and other processing. It offers SDKs for mobile platforms, including iOS and Android, to enable live video streaming from smartphones to AWS. Kinesis Video Streams can also be used to build real-time video applications for a variety of use cases, including media and entertainment, security and surveillance, and industrial automation.\nOption A, Amazon Kinesis Data Streams, is a managed service that can be used for real-time streaming of large data sets. However, it is not designed specifically for video streaming and does not offer the same level of functionality as Kinesis Video Streams.\nOption B, Amazon Kinesis Delivery Streams, is a managed service that can be used to reliably load streaming data into AWS data stores, such as Amazon S3, Amazon Redshift, and Amazon Elasticsearch. However, it does not provide real-time video processing capabilities.\nOption C, Amazon Interactive Video Service, is a managed service that can be used to create interactive video experiences with low latency and high reliability. However, it is not designed for real-time video streaming and does not offer the same level of functionality as Kinesis Video Streams.\nTherefore, the most appropriate service for the given scenario is Amazon Kinesis Video Streams, as it offers a comprehensive solution for real-time video streaming from smartphones to AWS and enables the development of real-time video processing applications.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Amazon Kinesis Data Streams",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Kinesis Delivery Streams",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Interactive Video Service",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Kinesis Video Streams.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 187,
  "query" : "When a user creates an encrypted EBS volume and attaches it to a supported instance type, which of the following data types are encrypted?",
  "answer" : "Answer - A, C, and D.\nAmazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure.\nWhen you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:\n(i) Data at rest inside the volume.\n(ii) All data moving between the volume and the instance.\n(iii) All snapshots created from the volume.\n(iv) All volumes created from those snapshots.\nBased on this, options A, C, and D are all CORRECT.\nOption B is incorrect since the data that is copied to S3 is not encrypted.\nFor more information on this, please visit the link below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nWhen a user creates an encrypted EBS volume and attaches it to a supported instance type, the data at rest inside the volume is encrypted. This means that any data written to the volume is encrypted before it is written, and any data read from the volume is decrypted after it is read. The encryption is done using the AWS-managed keys, which can be managed by the user.\nOption A is correct. Data at rest inside the volume is encrypted when the user creates an encrypted EBS volume and attaches it to a supported instance type.\nOption B is incorrect. All data copied from the EBS volume to S3 is not encrypted by default, even if the EBS volume is encrypted. The user needs to take additional steps to encrypt the data when copying it to S3.\nOption C is incorrect. All data moving between the volume and the instance is not encrypted by default, even if the EBS volume is encrypted. The user needs to take additional steps to encrypt the data moving between the volume and the instance.\nOption D is incorrect. All snapshots created from the volume are encrypted if the volume is encrypted. This means that any snapshot created from an encrypted EBS volume will also be encrypted.\nIn summary, when a user creates an encrypted EBS volume and attaches it to a supported instance type, the data at rest inside the volume is encrypted, and any snapshot created from the volume will also be encrypted.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Data at rest inside the volume",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "All data copied from the EBS volume to S3",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All data moving between the volume and the instance",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "All snapshots created from the volume.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 188,
  "query" : "You are hired as an AWS solutions architect in a startup company.\nYou notice some issues with the backup strategy of EC2 instances, and there is no snapshot lifecycle management.\nUsers just create snapshots manually without a routine policy to control.\nYou want to suggest using a proper EBS Snapshot Lifecycle policy.\nHow would you persuade your team lead to approve this suggestion?(Select TWO)",
  "answer" : "Correct Answer - A, B.\nEBS Snapshot Lifecycle policy, as a backup strategy, can bring lots of benefits for EC2 users.\nAbout the details, please refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html.\nOption A is CORRECT: Because EC2 EBS volumes can have a routine backup which helps on the quality audit.\nOption B is CORRECT: Because this is the major benefit of the life cycle policy, preserving important data and EBS volumes can be easily restored via the snapshots.\nOption C is incorrect: The snapshot lifecycle policy can reduce storage costs by deleting outdated backups.\nHowever, the snapshots themselves still have costs.\nOption D is incorrect: Because you can create up to 100 lifecycle policies per Region.\nAs an AWS solutions architect, you have noticed issues with the backup strategy of EC2 instances and the lack of snapshot lifecycle management. You want to persuade your team lead to approve your suggestion of implementing a proper EBS Snapshot Lifecycle policy. To do so, you can use the following two arguments:\n1. A snapshot lifecycle policy helps to retain backups as required by auditors or internal compliance.\nOne of the main reasons to implement an EBS Snapshot Lifecycle policy is to ensure compliance with regulations and internal policies. Many organizations have to comply with specific regulatory requirements, such as GDPR, HIPAA, or PCI DSS, which mandate a minimum retention period for backups. Failing to meet these requirements can lead to hefty fines and damage to the company's reputation. By implementing a snapshot lifecycle policy, you can ensure that backups are retained for the required period and deleted when they are no longer needed, thus avoiding compliance issues.\n1. An EBS Snapshot Lifecycle helps to protect valuable data by enforcing a regular backup schedule.\nAnother crucial reason to implement a snapshot lifecycle policy is to ensure that backups are taken regularly and automatically. Without a proper backup schedule, data loss can occur due to hardware failures, human errors, or malicious attacks. By setting up a snapshot lifecycle policy, you can ensure that backups are taken automatically according to a predefined schedule, thus minimizing the risk of data loss. Moreover, you can configure the policy to keep a certain number of snapshots, so you can roll back to a specific point in time if needed.\nIt is also worth noting that implementing an EBS Snapshot Lifecycle policy can help reduce storage costs. While creating snapshots manually can be convenient, it can lead to the accumulation of unnecessary snapshots, which can increase storage costs. On the other hand, a snapshot lifecycle policy can help you manage the number of snapshots and delete them when they are no longer needed, thus reducing storage costs. However, this argument is not as strong as the two mentioned above.\nLastly, it is important to clarify that you can create multiple lifecycle policies to manage different volumes or instances, but there is a limit to the number of policies you can create.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "A snapshot lifecycle policy helps to retain backups as required by auditors or internal compliance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "An EBS Snapshot Lifecycle helps to protect valuable data by enforcing a regular backup schedule.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A proper snapshot lifecycle policy is able to reduce storage costs as the snapshots taken by the schedule policy are free.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can create an unlimited number of lifecycle policies.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 189,
  "query" : "A communication company has deployed several EC2 instances in region ap-southeast-1 which are used to monitor user activities.\nThe AWS administrator has configured an EBS lifecycle policy to create a snapshot every day for each EBS volume to preserve data.\nThe retention is configured as 5, which means the oldest snapshot will be deleted after 5 days.\nThe administrator plans to copy some snapshots manually to another region ap-southeast-2 as these snapshots contain some important data.\nCan these snapshots be retained?",
  "answer" : "Correct Answer - C.\nCopying a snapshot to a new Region is commonly used for geographic expansion, migration, disaster recovery, etc.\nEBS snapshots' lifecycle policies contain some rules.\nOne of the rules is that when you copy a policy's snapshot, the new copy is not influenced by the retention schedule.\nOption A is incorrect: Because the new snapshots will be kept.\nOption B is incorrect: Because no matter the new snapshots are in the same region or not, they can be retained.\nOption C is CORRECT: Because the new snapshots are not affected by the original policy.\nReference is in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html.\nOption D is incorrect: Because there is no delete protection option for snapshots.\nThe retention policy of the EBS lifecycle policy is set to create a snapshot every day for each EBS volume, and retain them for five days. This means that after five days, the oldest snapshot will be deleted automatically by AWS.\nNow, the administrator wants to copy some snapshots manually to another region ap-southeast-2 as they contain important data. The question is whether these manually copied snapshots can be retained or not.\nOption A is incorrect because the new snapshots that are manually copied to ap-southeast-2 are not affected by the retention policy of the EBS lifecycle policy in ap-southeast-1. The retention policy only applies to snapshots created by the EBS lifecycle policy, not to manually created snapshots.\nOption B is incorrect because the retention policy of the original snapshots is not carried over to the copied snapshots. The copied snapshots are treated as new snapshots in ap-southeast-2 and are not affected by the retention policy of ap-southeast-1. Therefore, the copied snapshots can be retained.\nOption C is correct. The retention schedule of the EBS lifecycle policy is not carried over to the copied snapshots, so they can be retained for as long as desired.\nOption D is incorrect because the retention policy of the EBS lifecycle policy only applies to snapshots in ap-southeast-1, and not to snapshots that are manually copied to ap-southeast-2.\nIn summary, option C is correct. The new snapshots that are manually copied to ap-southeast-2 can be retained as the retention schedule of the EBS lifecycle policy is not carried over to the copied snapshots.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "These new snapshots may be deleted after the retention period, as they are still affected by the retention policy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "These new snapshots can be kept only when they are copied to another region. Otherwise, they may be deleted by the retention policy. In this case, the snapshots can be kept.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "These new snapshots can be kept as the retention schedule is not carried over to the copy.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The new snapshots in region ap-southeast-2 will be deleted after 5 days unless the delete protection option is enabled.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 190,
  "query" : "An IT company has a big data analytics application that is deployed in EC2 in multiple availability zones.\nThese EC2 instances simultaneously access a shared Amazon EFS file system using a traditional file permissions model.\nA recent internal security audit has found a potential security risk, as the EFS file system is not encrypted for either at rest or in transit.\nWhat actions could be taken to address the potential security threat posed by the non-encryption of the EFS volume?",
  "answer" : "Correct Answer - A.\nBoth encryption of data in transit and at rest are supported for EFS.\nDue to this, Amazon EFS now offers a comprehensive encryption solution.\nBlog https://aws.amazon.com/blogs/aws/new-encryption-of-data-in-transit-for-amazon-efs/ has an introduction to this.\nOption A is CORRECT: For the encryption at rest, it can be enabled as an option when the EFS file system is created.\nFor the encryption in transit, it can be enabled when the EFS file system is mounted:\nsudo mount -t efs-o tls fs-12345678:/ /mnt/efs.\nReference is in https://docs.aws.amazon.com/efs/latest/ug/encryption.html.\nOption B is incorrect: Because the encryption of data in transit is enabled when the EFS file system is mounted.\nOption C is incorrect: Because the encryption of data at rest is enabled when the EFS file system is created.\nOption D is incorrect: Same reason as Option B &amp;\nC.\nSure, I'd be happy to explain this question in detail!\nFirst, let's briefly discuss what Amazon EFS is and how it works. Amazon Elastic File System (EFS) is a scalable, fully managed, cloud-based NFS file system that provides shared access to files across multiple EC2 instances. EFS allows for easy access to shared file storage from multiple EC2 instances, which is especially useful for big data analytics applications that require a shared storage system.\nNow, let's look at the question. The IT company in question has deployed a big data analytics application in EC2 across multiple availability zones, with all instances accessing a shared Amazon EFS file system using traditional file permissions. The internal security audit has found a potential security risk because the EFS file system is not encrypted for either at rest or in transit.\nTo address this potential security risk, the question asks what actions could be taken. Let's look at each answer option in turn:\nA. The encryption of data at rest has to be enabled when the Amazon EFS file system is created. The encryption of data in transit can be enabled when the file system is mounted in the EC2 instance.\nThis answer option is incorrect. While it is true that data at rest can be encrypted when an EFS file system is created, data in transit cannot be enabled when the file system is mounted. Encryption of data in transit requires the use of a secure network protocol, such as SSL/TLS or IPsec, which must be configured separately from the file system.\nB. The encryption of data at rest and in transit can be enabled when the Amazon EFS file system is created.\nThis answer option is correct. When creating an EFS file system, you can enable encryption of data at rest and in transit. Encryption of data at rest is done using AWS Key Management Service (KMS) and encryption of data in transit is done using Secure Sockets Layer (SSL) or Transport Layer Security (TLS).\nC. The encryption of data at rest and in transit can only be enabled when the Amazon EFS file system is mounted in the EC2 instance.\nThis answer option is incorrect. Encryption of data at rest and in transit can be enabled when the EFS file system is created, not just when it is mounted in the EC2 instance.\nD. The encryption of data at rest can be enabled when the Amazon EFS file system is mounted in the EC2 instance. The encryption of data in transit is enabled when the EFS file system is created using the AWS console or CLI.\nThis answer option is incorrect. Encryption of data at rest and in transit can both be enabled when the EFS file system is created, not just when it is mounted in the EC2 instance.\nSo, the correct answer is B. The encryption of data at rest and in transit can be enabled when the Amazon EFS file system is created. By enabling encryption of data at rest and in transit, the IT company can address the potential security risk posed by the non-encryption of the EFS volume, ensuring the confidentiality and integrity of the data stored in the file system.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The encryption of data at rest has to be enabled when the Amazon EFS file system is created. The encryption of data in transit can be enabled when the file system is mounted in the EC2 instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The encryption of data at rest and in transit can be enabled when the Amazon EFS file system is created.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The encryption of data at rest and in transit can only be enabled when the Amazon EFS file system is mounted in the EC2 instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The encryption of data at rest can be enabled when the Amazon EFS file system is mounted in the EC2 instance. The encryption of data in transit is enabled when the EFS file system is created using the AWS console or CLI.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 191,
  "query" : "An International company has deployed a multi-tier web application with DynamoDB in a single region.\nFor regulatory reasons, they need disaster recovery capabilities in a separate region with a Recovery Time Objective of 5 hours and a Recovery Point Objective 24 hours.\nThey should synchronize their data regularly and be able to provision the web application rapidly using CloudFormation.\nThe objective is to minimize changes to the existing web application and replicate data for the DynamoDB table efficiently between two regions.",
  "answer" : "Answer - C.\nOption A is INCORRECT because you need to use AWS Data Pipeline to export data from DynamoDB table to a file in an Amazon S3 bucket and then import data from Amazon S3 into another DynamoDB table in the second region.\nHowever, DynamoDB global tables can automatically synchronize data between two regions.\nOption B is INCORRECT because you have to build your own logic to replicate data.\nThis is not the most efficient way.\nOption C is CORRECT because DynamoDB global tables provide a fully managed solution to synchronize data for DynamoDB.\nUsers do not need to build their own replication solution.\nThis is the most efficient method.\nOption D is INCORRECT because this method does not explain how to replicate data for the DynamoDB table.\nPlease check the below link to know more about DynamoDB Global Tables.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\nThe scenario presented in the question requires a disaster recovery solution with a Recovery Time Objective (RTO) of 5 hours and a Recovery Point Objective (RPO) of 24 hours for a multi-tier web application with DynamoDB in a single region. The company needs to synchronize data regularly and provision the web application quickly using CloudFormation. The solution should minimize changes to the existing web application and replicate data efficiently between the two regions.\nOption A: Use AWS Data Pipeline to schedule a DynamoDB Cross-Region copy once a day and create a “Last updated” attribute in your DynamoDB table representing the timestamp of the last update and use it as a filter.\nThis solution suggests using AWS Data Pipeline to schedule a copy of the DynamoDB table to a different region once a day. It also recommends creating a \"Last updated\" attribute in the table to keep track of the last update timestamp and use it as a filter. However, this solution doesn't meet the RTO and RPO requirements specified in the scenario since the data would be copied only once a day, and the RTO is set to 5 hours.\nOption B: Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN operation and push it to DynamoDB in the second region.\nThis solution suggests using Elastic MapReduce (EMR) to write a custom script that retrieves data from DynamoDB using a SCAN operation in the current region and pushes it to DynamoDB in the second region. This solution does not meet the RTO requirement since the data synchronization is not real-time, and it only meets the RPO requirement of 24 hours. Additionally, using the SCAN operation can be inefficient and expensive, especially for large datasets.\nOption C: Configure DynamoDB global tables for deploying the multi-active database in two AWS Regions.\nThis solution suggests using DynamoDB Global Tables, which enables automatic, real-time, and bi-directional replication of data across multiple regions. This option meets both the RTO and RPO requirements and can provision the web application quickly using CloudFormation. Additionally, Global Tables minimize changes to the existing web application since they don't require any code changes.\nOption D: Send each item into an SQS queue in the second region; use an auto-scaling group behind the SQS queue to replay the write in the second region.\nThis solution suggests using Amazon Simple Queue Service (SQS) to send each item to a queue in the second region, and then using an auto-scaling group to replay the writes in the second region. While this option meets the RTO requirement since the data synchronization is real-time, it does not meet the RPO requirement since it may take longer than 24 hours to replay all the writes. Additionally, this solution may require significant modifications to the existing web application to implement the queuing mechanism.\nIn conclusion, Option C (Configure DynamoDB global tables for deploying the multi-active database in two AWS Regions) is the best solution since it meets both the RTO and RPO requirements and can provision the web application quickly using CloudFormation. Global Tables also minimize changes to the existing web application since they don't require any code changes.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use AWS Data Pipeline to schedule a DynamoDB Cross-Region copy once a day and create a “Last updated” attribute in your DynamoDB table representing the timestamp of the last update and use it as a filter.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN operation and push it to DynamoDB in the second region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure DynamoDB global tables for deploying the multi-active database in two AWS Regions.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Send each item into an SQS queue in the second region; use an auto-scaling group behind the SQS queue to replay the write in the second region.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 192,
  "query" : "For application development and testing purposes, your team has created several EFS volumes.\nAs the AWS operation engineer, you have been tasked with mounting some EFS file systems on EC2 Linux instances with encryption enabled in-transit.\nYou have already installed the EFS mount helper on the instances.\nTo use the mount helper properly when mounting the EFS volumes, which actions should you perform? (Select THREE)",
  "answer" : "E.\nCorrect Answer - A, C, E.\nEncryption of data in transit is enabled by connecting to Amazon EFS using TLS.\nAWS recommends using the mount helper because it's the simplest option.\nThe helper is in the amazon-efs-utils package which is an open-source collection of Amazon EFS tools.\nThis link https://docs.aws.amazon.com/efs/latest/ug/using-amazon-efs-utils.html#efs-mount-helper is the user guide for the helper.\nOption A is CORRECT: Because the command is “sudo mount -t efs fs-92758f7b -o tls /mnt/efs”\nThe “fs-92758f7b” is the EFS file system ID.Option B is incorrect: Because EFS encryption does not need the rule from the security group.\nFor more on how to use mounting with the EFS mount helper, refer to https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html#mounting-fs-mount-helper.\nOption C is CORRECT: Because mount targets are needed for the VPC's availability zones when the EFS file system is created.\nOption D is incorrect: Similar to Option B, there is no specific network ACL rule needed for EFS encryption in transit.\nOption E is CORRECT: Because the “-o tls” option is needed to enable the encryption in transit.\nSure, I'd be happy to help!\nTo use the mount helper properly when mounting the EFS volumes on EC2 Linux instances with encryption enabled in-transit, there are several actions that you should perform. These actions are as follows:\n1.\nGet the EFS file system's ID from the console or programmatically through the Amazon EFS API: Before you can mount an EFS file system on an EC2 instance, you need to know its file system ID. You can find this ID in the AWS Management Console or programmatically using the Amazon EFS API.\n2.\nCreate mount targets for your EC2 instances: To mount an EFS file system on an EC2 instance, you must first create a mount target in the same VPC and subnet as the instance. A mount target is an Amazon EFS resource that represents a file system's network endpoint in a specific subnet.\n3.\nMake sure that the security group of EC2 instances has opened the port 443 for SSL traffic: To enable encryption in transit between EC2 instances and the EFS file system, you must make sure that the security group attached to your EC2 instances has opened port 443 for SSL traffic. By default, Amazon EFS uses the Transport Layer Security (TLS) protocol to encrypt data in transit between EC2 instances and the EFS file system.\n4.\nIn the subnets of your EC2 Instances, create a network ACL rule to allow HTTPS traffic so that encryption in transit between EC2 and EFS file system is allowed: In addition to configuring the security group attached to your EC2 instances, you also need to configure the network ACLs in the subnets where your EC2 instances reside. Specifically, you must create a rule to allow HTTPS traffic on port 443 to enable encryption in transit between EC2 instances and the EFS file system.\n5.\nWhen the mount helper utility is used, add the encryption option: \"-o tls\": Finally, when you use the mount helper utility to mount an EFS file system on an EC2 instance, you should add the encryption option \"-o tls\" to enable encryption in transit between the EC2 instance and the EFS file system.\nOverall, by performing these actions, you can mount EFS volumes on EC2 Linux instances with encryption enabled in-transit using the mount helper utility.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Get the EFS file system`s ID from the console or programmatically through the Amazon EFS API.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Make sure that the security group of EC2 instances has opened the port 443 for SSL traffic.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create mount targets for your EC2 instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In the subnets of your EC2 Instances, create a network ACL rule to allow HTTPS traffic so that encryption in transit between EC2 and EFS file system is allowed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "When the mount helper utility is used, add the encryption option: “-o tls”.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 193,
  "query" : "You are an AWS consultant in an IT company.\nYour development manager just assigned you a task to evaluate if the EBS volume types of the EC2 instances were properly configured in all regions.\nThe major concern that you have found is that almost all EBS volumes are using the Provisioned IOPS SSD (io1) volume type which costs the company a lot.\nYou plan to change the volume type from io1 to other types.\nHowever, for which scenarios should you still use the EBS volume type of io1?",
  "answer" : "Correct Answer:B.\nRefer to https://aws.amazon.com/ebs/features/ for the use cases of different EBS volume types:\nVolume Type.\nEBS Provisioned IOPS SSD (io1)\nEBS General Purpose SSD (gp2)*\nThroughput Optimized HDD (st1)\nCold HDD (sc1)\nShort Description.\nHighest performance SSD volume designed for latency-sensitive transactional workloads.\nGeneral Purpose SSD volume that balances price performance for a wide variety of transactional workloads.\nLow-cost HDD volume designed for frequently accessed, throughput intensive workloads.\nLowest cost HDD volume designed for less frequently accessed workloads.\nUse Cases.\nI/O-intensive NoSQL and relational databases.\nBoot volumes, low-latency interactive apps, dev &amp; test.\nBig data, data warehouses, log processing.\nColder data requiring fewer scans per day.\nOption A is incorrect: Because according to the above introductions, this should be a gp2 for a boot volume.\nOption B is CORRECT: Because this is ideal for io1 as it needs the highest performance and lowest latency.\nOption C is incorrect: Because this should be an st1, it could lower the cost and meet the performance need.\nOption D is incorrect: Because sc1 is suitable for this as it is not frequently used.\nMoreover, because it contains a large amount of data, using io1 is not cost-efficient.\nSure, I'd be happy to provide a detailed explanation!\nFirstly, let's start with understanding what the EBS volume type of io1 is and what makes it different from other EBS volume types.\nProvisioned IOPS SSD (io1) is a type of Amazon Elastic Block Store (EBS) volume that is designed for applications that require high-performance storage with low-latency and consistent I/O performance. It offers the highest performance, lowest latency, and most consistent I/O performance among all EBS volume types.\nNow, let's analyze each of the given scenarios to determine whether the io1 volume type is still appropriate or if another volume type would be more suitable.\nA. A boot volume of a test server that is frequently used by the Quality Assurance team.\nThe boot volume of a test server is typically not I/O-intensive, and the workload is generally read-heavy with infrequent writes. For such workloads, the General Purpose SSD (gp2) volume type is usually sufficient, as it provides a good balance of price and performance. Therefore, io1 volume type is not needed in this scenario.\nB. A Cassandra database that needs extremely low latency and high performance when being processed.\nCassandra is a distributed NoSQL database that requires high-performance storage to ensure low latency and high throughput. The io1 volume type is a suitable option for such workloads, as it can provide the required IOPS and low latency. However, other options such as the Amazon Elastic File System (EFS) or Amazon S3 can also be considered for specific use cases.\nC. A data warehouse server that contains a huge amount of customer data. The data needs to be accessed and analyzed by a monitor process frequently.\nData warehouses require high throughput and large capacity to store and process data. The throughput is generally measured in MB/s rather than IOPS, so the Throughput Optimized HDD (st1) or Cold HDD (sc1) volume types are typically more cost-effective options. The io1 volume type is not recommended for such workloads since it's mainly designed for IOPS-intensive workloads.\nD. Some large and legacy cold data that is stored to trace customers' activities in the past. The database requires fewer scans per day.\nFor such workloads, where the data is infrequently accessed, and the performance requirements are low, the Cold HDD (sc1) volume type is the most cost-effective option. The io1 volume type is not recommended for this scenario, as it's overkill for such a workload.\nIn conclusion, the io1 volume type is suitable for workloads that require high-performance storage with low-latency and consistent I/O performance, such as databases and other I/O-intensive applications. However, for workloads with lower I/O requirements or higher throughput demands, other volume types such as gp2, st1, or sc1 can be more cost-effective.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "A boot volume of a test server that is frequently used by the Quality Assurance team.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A Cassandra database that needs extremely low latency and high performance when being processed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A data warehouse server that contains a huge amount of customer data. The data needs to be accessed and analyzed by a monitor process frequently.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Some large and legacy cold data that is stored to trace customers’ activities in the past. The database requires fewer scans per day.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 194,
  "query" : "An AWS Solution architect who uses EBS General Purpose SSD (gp2) volume type for his EBS volumes now wants to modify some of these volumes.\nWhat options would you suggest? (Select TWO)",
  "answer" : "Correct AnswerA, D.\nThe EBS volume types can be modified in flight without the volume being detached or the instance being restarted.\nHowever, some limitations need to be noticed.\nThe details are in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/modify-volume-requirements.html.\nOption A is CORRECT: Because the root volume can be changed to io1 such as:\nOption B is incorrect: Because a gp2 volume that is attached to an instance as a root volume cannot be modified to an st1 or sc1 volume.\nCheck https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/modify-volume-requirements.html.\nOption C is incorrect: Because there is no gp4 type of volume available.\nOption D is CORRECT: Because the volume size 1TB is suitable for Throughput Optimized HDD (st1):\nSure, I'd be happy to explain the options available for modifying EBS volumes from the General Purpose SSD (gp2) volume type.\nOption A: A 50GB gp2 root volume can be modified to an EBS Provisioned IOPS SSD (io1) after stopping the instance. Explanation: This option is correct because when you stop an instance, you can modify the EBS root volume type. In this case, the current gp2 volume can be modified to an EBS Provisioned IOPS SSD (io1) volume type, which provides higher performance and consistent IOPS for latency-sensitive workloads.\nOption B: A gp2 volume attached to an instance as a root volume can be modified to a Throughput Optimized HDD (st1) volume. Explanation: This option is incorrect because you cannot modify the volume type of a root volume. Root volumes are created from Amazon Machine Images (AMIs) and cannot be modified after the instance is launched.\nOption C: A 1GB gp2 volume attached to an instance as a non-root volume can be modified to a gp3 volume. Explanation: This option is partially correct because you can modify a gp2 volume to a gp3 volume, which provides better performance at a lower cost. However, the minimum size for gp3 volumes is 1 GiB, not 1 GB. GiB stands for gibibyte, which is a binary multiple of a byte and is equal to 2^30 bytes. On the other hand, GB stands for gigabyte, which is a decimal multiple of a byte and is equal to 10^9 bytes. Therefore, the correct size for a gp3 volume would be 1 GiB, not 1 GB.\nOption D: A 1TB gp2 volume attached to an instance as a non-root volume can be modified to a Throughput Optimized HDD (st1) volume without stopping the instance or detaching the volume. Explanation: This option is incorrect because you cannot modify the volume type of an EBS volume without creating a new volume and copying the data over. Therefore, to modify a gp2 volume to a Throughput Optimized HDD (st1) volume, you would need to create a new st1 volume, copy the data from the gp2 volume to the st1 volume, and then detach the gp2 volume and attach the st1 volume to the instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "A 50GB gp2 root volume can be modified to an EBS Provisioned IOPS SSD (io1) after stopping the instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A gp2 volume attached to an instance as a root volume can be modified to a Throughput Optimized HDD (st1) volume.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A 1GB gp2 volume attached to an instance as a non-root volume can be modified to a gp4 volume.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A 1TB gp2 volume attached to an instance as a non-root volume can be modified to a Throughput Optimized HDD (st1) volume without stopping the instance or detaching the volume.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 195,
  "query" : "Your team is working on a plant recognition application.\nAfter users upload photos of plants, the application can provide their names and properties.",
  "answer" : "Correct Answer A, B.\nAbout how to back up the instance store volume, refer to the document in.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/back-up-instance-store-ebs/.\nYou can back up data stored on an instance store volume in one of the two ways mentioned in options A &amp;\nB.Option A is CORRECT: Because it is useful to back up the database filestore to S3.\nOption B is CORRECT: This is recommended as the above link.\nFor example, tools such as sync for Linux or robocopy for Windows can be used to backup data.\nOption C is incorrect: There is no way to create an AMI from instance store volume using the console.\nTo do so, you must use AMI Tools from the command line.\nOption D is incorrect: Snapshots cannot create, for instance, store volumes.\nOnly EBS volumes can.\nAmong the given answers, option B seems to be the most appropriate for backing up the MySQL database for the plant recognition application.\nOption A, backing up the database file store to S3, may not be sufficient for a MySQL database, as a MySQL database consists of multiple files and directories, and requires a consistent backup to maintain data integrity.\nOption C, creating an image of the EC2 instance, is not an ideal solution for backing up a MySQL database. This is because creating an image of the EC2 instance captures the state of the instance at a specific point in time, including the operating system and application software. However, it does not guarantee the consistency of the database at the time of the image creation.\nOption D, creating snapshots of the instance store volume, is a possible solution for backing up the MySQL database, but it may not be ideal in terms of performance and reliability. Instance store volumes are ephemeral and are not designed for long-term data storage. Furthermore, creating snapshots can affect the performance of the instance, as it requires copying the data from the instance store to Amazon S3.\nOption B, creating a new EBS volume and attaching it to EC2, and then exporting the MySQL database to the EBS volume using a migration tool, is a recommended solution for backing up the MySQL database. This method allows for consistent backups of the database, as well as the ability to restore the database to a different EC2 instance or region. EBS volumes are designed for durability and high availability, and can be easily backed up using EBS snapshots. Moreover, EBS volumes provide better performance compared to instance store volumes, as they are designed for frequent and random I/O operations.\nIn summary, option B, creating a new EBS volume and attaching it to EC2, and then exporting the MySQL database to the EBS volume using a migration tool, is the most appropriate solution for backing up the MySQL database for the plant recognition application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Back up the database filestore to S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new EBS volume and attach the volume to EC2. Export the MySQL database to the EBS volume using disk management or migration tool.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In the EC2 AWS console, select the instance -> Actions -> Image -> Create Image. Then the created AMI is able to backup the MySQL data in the instance store.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create snapshots for the instance store volume where the MySQL database resides. Copy the snapshots to other regions for further backup.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 196,
  "query" : "A video provider has a big proportion of EC2 instance store volumes as many of their workloads require very quick disk IO.\nCompared with EBS counterparts, instance store volumes are physically attached to the host and have better performance.\nHowever, they still need to work out a suitable method to create an AMI for the instance store volume.\nHow can you do it?",
  "answer" : "Correct Answer D.\nThe following diagram summarizes the process of creating an AMI from an instance store-backed instance.\nOption A is incorrect: Because an AMI can be created to store volume according to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-instance-store.html.\nOption B is incorrect: Because AWS EC2 CLI can only be used to generate AMI for EBS volumes.\nOption C is incorrect: Because the AMI creation process for instance store volumes is different from Amazon EBS-backed AMIs.\nAMI tools such as ec2-bundle-vol and ec2-upload-bundle are required.\nOption D is CORRECT: Because this has used the correct procedures described in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-instance-store-ami.html#amazon_linux_instructions.\nInstance store volumes are directly attached to the host and provide better performance compared to EBS volumes, which are network attached. Instance store volumes are ideal for workloads that require very fast disk IO. However, creating an Amazon Machine Image (AMI) for instance store volumes can be a bit tricky.\nAn AMI is a pre-configured virtual machine image that is used to create EC2 instances. It contains all the necessary information required to launch an instance such as operating system, applications, data, and storage configuration. To create an AMI for an instance store volume, the following options are available:\nA. Instance store volumes cannot have AMI images that are only available for EBS volumes. This statement is incorrect because instance store volumes can have AMI images.\nB. They can only create an AMI using AWS EC2 CLI command for instance store volumes. This statement is incorrect because there are other options available to create an AMI for an instance store volume.\nC. The AMI can be created by using AWS EC2 console, CLI, or SDK for instance store volumes. This statement is correct. AMIs can be created using the AWS Management Console, AWS CLI, or AWS SDK. To create an AMI using the console, navigate to the EC2 console, select the instance you want to create an AMI for, and choose the \"Create Image\" option. You can also create an AMI using the AWS CLI by running the \"aws ec2 create-image\" command. Finally, you can create an AMI using the AWS SDK by calling the \"CreateImage\" API.\nD. The AWS CLI tools can be used to create a bundle for the volume and upload it to S3. Then register the AMI to the file in the S3 bucket. This statement is also correct. To create an AMI using this method, you need to first create a bundle of the instance store volume. The bundle contains the files required to boot the instance store volume. You can use the AWS CLI command \"ec2-bundle-instance\" to create the bundle. Once the bundle is created, you need to upload it to an S3 bucket using the \"ec2-upload-bundle\" command. Finally, you can register the AMI to the uploaded bundle using the \"ec2-register\" command.\nIn summary, options C and D are correct ways to create an AMI for an instance store volume. Option A is incorrect, and option B is not the only way to create an AMI for an instance store volume.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Instance store volume cannot have AMI images that are only available for EBS volumes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "They can only create an AMI using AWS EC2 CLI command for instance store volumes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The AMI can be created by using AWS EC2 console, CLI or SDK for instance store volumes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The AWS CLI tools can be used to create a bundle for the volume and upload it to S3. Then register the AMI to the file in the S3 bucket.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 197,
  "query" : "A media company is working on migrating its various on-premises products and services to the AWS platform.\nIn one web service, MongoDB was used to store user subscription information.\nThe AWS cloud engineer has migrated this NoSQL database to DynamoDB in AWS.",
  "answer" : "Correct AnswerB, D.\nDynamoDB can return various exceptions with different error messages and codes.\nThe reference is in https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html.\nOption A is incorrect: Because the AWS SDKs for DynamoDB automatically retry requests that receive this exception.\nThe request to DynamoDB is eventually successful unless the retry queue is too large to finish.\nOption B is CORRECT: Because the CloudWatch metrics can tell the difference between provisioned capacity and consumed capacity.\nIt helps understand the proper value to the provisioned capacity.\nOption C is incorrect: Because there are no ProvisionedThroughputExceededException metrics for the DynamoDB table.\nOption D is CORRECT: Because the straightforward way is to increase the provisioned capacity and monitor if it can help resolve the problem.\nSure, I'd be happy to provide a detailed explanation of the answer options for this question:\nA. In Java AWS SDK, implement a retry mechanism to retry the request when the exception ProvisionedThroughputExceededException happens.\nThis answer option suggests implementing a retry mechanism in the Java AWS SDK to handle the ProvisionedThroughputExceededException. This exception occurs when a request exceeds the provisioned throughput limit of a DynamoDB table. By implementing a retry mechanism in the Java AWS SDK, the application can automatically retry the request when this exception occurs. This approach can help to improve the resiliency of the application, but it may not be sufficient to solve the problem entirely. If the request is frequently hitting the provisioned throughput limit, the retry mechanism may be triggered frequently, leading to increased latency and reduced performance. Therefore, this option is only a partial solution to the problem.\nB. Open the Amazon CloudWatch console and view performance metrics for provisioned throughput vs. consumed throughput.\nThis answer option suggests using the Amazon CloudWatch console to monitor the provisioned throughput vs. consumed throughput metrics for the DynamoDB table. This approach can help to identify whether the table is hitting its provisioned throughput limits frequently, and whether additional capacity is needed. If the provisioned throughput limit is consistently being hit, it may be necessary to increase the provisioned capacity of the table to avoid performance issues. By monitoring these metrics, the AWS cloud engineer can identify the need for additional capacity before it becomes a problem.\nC. In the CloudWatch console and view the ProvisionedThroughputExceededException metrics to understand when it happens.\nThis answer option suggests using the CloudWatch console to monitor the ProvisionedThroughputExceededException metrics. This approach can help to identify when the exception occurs and whether it is happening frequently. By monitoring this metric, the AWS cloud engineer can identify the need to increase the provisioned capacity of the DynamoDB table. However, this option only provides visibility into when the exception occurs, and it does not provide a solution to the underlying problem.\nD. Increase the provisioned read, write capacity, and monitor the log system.\nThis answer option suggests increasing the provisioned read and write capacity of the DynamoDB table and monitoring the log system. This approach can help to prevent the ProvisionedThroughputExceededException from occurring by ensuring that the table has sufficient capacity to handle the workload. By monitoring the log system, the AWS cloud engineer can identify whether the capacity increase has been successful in preventing the exception from occurring. This approach is a comprehensive solution to the underlying problem and can ensure that the application performs well in production.\nOverall, option D is the best answer because it provides a comprehensive solution to the problem by increasing the provisioned capacity of the DynamoDB table and monitoring the log system. Option B is also a valid answer because it provides visibility into the provisioned throughput vs. consumed throughput metrics, which can help to identify the need for additional capacity. Option A is a partial solution to the problem, and option C only provides visibility into the exception, without providing a solution.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In Java AWS SDK, implement a retry mechanism to retry the request when the exception ProvisionedThroughputExceededException happens.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Open the Amazon CloudWatch console and view performance metrics for provisioned throughput vs. consumed throughput.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the CloudWatch console and view the ProvisionedThroughputExceededException metrics to understand when it happens.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Increase the provisioned read, write capacity, and monitor the log system.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 198,
  "query" : "You are hired as an AWS engineer.\nYour major responsibility is to migrate existing services to the AWS platform.\nThe company's on-premises main page is made of Node.js and PostgreSQL database.\nYou just migrated the frontend and backend to an EC2 instance.\nAlso, you created a PostgreSQL instance in RDS.",
  "answer" : "Correct Answer A, D.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.SQLServer.Connect\nOption A is CORRECT: Because if the used endpoint or port is incorrect, the error would be “Connection timed out”.\nOption B is incorrect: Because the AWS SDK version does not cause the issue to communicate with the RDS instance.\nOption C is incorrect: Because the low configured capacity does not lead to a “Connection timed out” issue.\nOption D is CORRECT: Because the most common problem when attempting to connect to a PostgreSQL DB instance is that the security group assigned to the DB instance has incorrect access rules.\nAs an AWS engineer responsible for migrating existing services to the AWS platform, you have already migrated the on-premises main page to an EC2 instance and created a PostgreSQL instance in RDS. However, after the migration, you are experiencing issues with connecting to the RDS instance. In such a scenario, there are several things you can check to diagnose the issue:\nA. Check that if the hostname is the DB instance endpoint and the port number is correct: The first thing to check is the endpoint and port number used to connect to the RDS instance. Ensure that the hostname matches the DB instance endpoint and that the port number is correct. By default, PostgreSQL uses port 5432, so ensure that you are connecting to the correct port.\nB. Check if the AWS SDK version is correct as there are issues for certain versions to communicate with RDS instances: The AWS SDK is a collection of tools and libraries that enable developers to interact with various AWS services. Ensure that the AWS SDK version being used is compatible with RDS. Some older versions of the AWS SDK may have issues communicating with RDS instances, so make sure you are using a compatible version.\nC. Check if the RDS instance has configured enough read and write capacity. If not, the “Connection timed out” issue may appear: RDS instances come with a default set of resources, including compute, storage, and network resources. Ensure that the RDS instance has enough read and write capacity to handle the traffic from the EC2 instance. If the RDS instance is running out of resources, you may experience \"Connection timed out\" issues.\nD. Check that the security group assigned to the DB instance has the necessary rules to allow access through the EC2 instance: Security groups are used to control inbound and outbound traffic to AWS resources. Ensure that the security group assigned to the RDS instance has the necessary rules to allow access through the EC2 instance. By default, RDS instances do not allow inbound traffic, so you must create a security group rule that allows inbound traffic from the EC2 instance.\nIn summary, when experiencing issues with connecting to an RDS instance after migrating services to the AWS platform, ensure that the hostname and port number are correct, the AWS SDK version is compatible, the RDS instance has enough read and write capacity, and the security group assigned to the RDS instance has the necessary rules to allow access through the EC2 instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Check that if the hostname is the DB instance endpoint and the port number is correct.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Check if the AWS SDK version is correct as there are issues for certain versions to communicate with RDS instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check if the RDS instance has configured enough read and write capacity. If not, the “Connection timed out” issue may appear.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check that the security group assigned to the DB instance has the necessary rules to allow access through the EC2 instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 199,
  "query" : "A company has several departments, and its AWS specialist created an AWS organization in its master account that is owned by the operation team.\nThen he invited other departments such as Development, QA, and HR to join the organization.\nAfter all the invitations were accepted, the payer account and linked accounts have been set up successfully.\nWhat are some of the features of consolidated billing? (Select TWO)",
  "answer" : "Correct Answer B, C.\nThe consolidated billing feature in AWS Organizations is used to consolidate billing and payment for multiple AWS accounts.\nIt means that in the organization, the master pays the charges of all the member accounts.\nOption A is incorrect: Because although the owners of the linked accounts aren't charged, they can still see their usage and charges by going to their AWS Bills pages.\nOption B is CORRECT: Because the usage is combined in AWS Organization so that the discount is possible.\nOption C is CORRECT: This is clearly stated in https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html.\nOption D is incorrect: Because member accounts that you invited to join your organization did not automatically get an administrator role created.\nAn OrganizationAccountAccessRole is needed at first in linked accounts.\nThe related document is in https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role.\nConsolidated billing is a feature of AWS Organizations that allows the payer account to consolidate and pay for all the AWS charges accrued across all linked accounts. It is a convenient way for organizations to manage their AWS costs and usage across multiple accounts.\nThe features of consolidated billing include:\nB. The usage across all accounts can share the volume pricing discounts and Reserved Instance discounts. When an organization sets up consolidated billing, all the linked accounts become part of a single payment entity. This means that the organization can take advantage of volume pricing discounts and Reserved Instance discounts based on the combined usage of all the accounts. This can result in cost savings for the organization.\nA. It becomes more secure as only the payer account can see the usage and charges across all the accounts. Owners of the linked accounts cannot see their usage and charges. Consolidated billing improves security by centralizing access to billing information. With consolidated billing, only the payer account has access to the billing information across all the linked accounts. The owners of the linked accounts can only access their own account information, and they cannot see the usage and charges of other accounts. This ensures that sensitive financial information is only accessible to authorized personnel.\nC. The consolidated billing feature does not bring additional costs. Consolidated billing is a free feature of AWS Organizations. There are no additional costs associated with setting up or using consolidated billing.\nD. AWS Organizations automatically creates a root user and an IAM role for all linked accounts so that the master account can access and administer the member accounts. This statement is incorrect. AWS Organizations does not automatically create a root user or an IAM role for linked accounts. Instead, the master account can use AWS Organizations to create and manage IAM roles and policies for the linked accounts. This allows the master account to control the level of access that linked accounts have to AWS resources and services.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It becomes more secure as only the payer account can see the usage and charges across all the accounts. Owners of the linked accounts cannot see their usage and charges.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The usage across all accounts can share the volume pricing discounts and Reserved Instance discounts.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The consolidated billing feature does not bring additional costs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Organizations automatically creates a root user and an IAM role for all linked accounts so that the master account can access and administer the member accounts.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 200,
  "query" : "Two departments A and B have been added into a consolidated billing organization.\nDepartment A has 5 reserved RDS instances with DB Engine as MySQL.\nDuring one particular hour, department A used three DB Instances, and department B used two RDS instances for a total of 5 DB Instances on the consolidated bill.\nHow should the RDS instances in department B be configured so that all five instances are charged as Reserved DB Instances?",
  "answer" : "E.\nCorrect Answer E.\nIn order to receive the cost-benefit from Reserved DB Instances, all the attributes of DB Instances (DB Engine, DB Instance class, Deployment type, and License Model) in another account should match the attributes of the Reserved DB Instances.\nOption A &amp; D are all required, and do find the reference in the following link.\nOption E is CORRECT: Because all of the other options are needed.\nThe reference is in https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html.\nWhen multiple AWS accounts are consolidated under a single payer account, it is referred to as consolidated billing. In this scenario, it is important to ensure that all the resources are utilized efficiently to minimize costs. Reserved Instances (RIs) are a way to save costs in AWS by committing to use a certain amount of resources for a specified period.\nIn this scenario, department A has 5 reserved RDS instances with DB Engine as MySQL. During one hour, department A used three instances, and department B used two RDS instances for a total of 5 DB Instances on the consolidated bill. To ensure that all five instances are charged as Reserved DB Instances, the following configurations should be in place:\nA. Department B should launch DB instances in the same region as Reserved Instances in department A. The first configuration required is that department B should launch DB instances in the same region as the reserved instances in department A. The pricing of RIs is region-specific, and a reserved instance purchased in one region cannot be used in another region. Therefore, to ensure that department B instances are charged as reserved instances, they need to be launched in the same region as department A's reserved instances.\nB. The DB engine in Department B should be MySQL. The second configuration required is that the DB engine used by department B should be MySQL. Department A's reserved instances have a DB engine of MySQL, and AWS charges based on the DB engine used. If department B uses a different DB engine, then it will not be charged as a reserved instance.\nC. The DB Instance Class should be the same in both departments such as m1.large. The third configuration required is that the DB instance class should be the same in both departments. AWS charges based on the instance type, and if the instance types differ between the two departments, then department B's instances will not be charged as reserved instances. Therefore, to ensure that department B instances are charged as reserved instances, they need to use the same instance type as department A's reserved instances.\nD. The deployment type such as Multi-AZ should be the same in both department A and department B. The fourth configuration required is that the deployment type should be the same in both department A and department B. AWS charges based on the deployment type, and if the deployment type differs between the two departments, then department B's instances will not be charged as reserved instances. Therefore, to ensure that department B instances are charged as reserved instances, they need to have the same deployment type as department A's reserved instances.\nE. All the above are needed. Therefore, all the above configurations are required to ensure that department B's instances are charged as reserved instances, and there are no additional charges incurred.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Department B should launch DB instances in the same region as Reserved Instance in department",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The DB engine in Department B should be MySQL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The DB Instance Class should be the same in both departments such as m1.large.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The deployment type such as Multi-AZ should be the same in both department A and department",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All the above are needed.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 201,
  "query" : "Your company's on-premises content management system has the following architecture:",
  "answer" : "Answer - A.\nOption A is CORRECT because (i) it deploys the Oracle database on EC2 instance by restoring the backups from S3 which is quick, and (ii) it generates the EBS volume of static content from Storage Gateway.\nDue to these points, option A meets the best RTO compared to all the remaining options.\nOption B is incorrect because restoring the backups from the Amazon Glacier will be slow and will not meet the RTO.\nOption C is incorrect because there is no need to attach the Storage Gateway as an iSCSI volume.\nYou can just easily and quickly create an EBS volume from the Storage Gateway.\nThen you can generate snapshots from the EBS volumes for better recovery time.\nOption D is incorrect as restoring the content from Virtual Tape Library will not fit into the RTO.\nThe question presents a scenario where the company's on-premises content management system needs to be migrated to AWS. The architecture of the current system is not specified, but it is implied that there is an Oracle database, a JBoss application server, and static content that needs to be transferred to AWS.\nOption A suggests deploying the Oracle database and the JBoss app server on EC2. RMAN backups of the Oracle database are stored in Amazon S3, which can be used to restore the database. An EBS volume of static content can be generated from the Storage Gateway, which can then be attached to the JBoss EC2 server. This option appears to be a feasible solution as it allows the existing components to be deployed on EC2 while using AWS services for backup and storage.\nOption B suggests deploying the Oracle database on RDS and the JBoss app server on EC2. RMAN backups of the Oracle database are stored in Amazon Glacier. An EBS volume of static content can be generated from the Storage Gateway, which can then be attached to the JBoss EC2 server. This option appears to be a feasible solution as well, as it leverages RDS for the database and EC2 for the JBoss app server.\nOption C suggests deploying the Oracle database and the JBoss app server on EC2. RMAN backups of the Oracle database are stored in Amazon S3, which can be used to restore the database. Static content is restored by attaching an AWS Storage Gateway running on Amazon EC2 as an iSCSI volume to the JBoss EC2 server. This option also appears to be a feasible solution as it uses EC2 instances for both the database and application server, while using the Storage Gateway for static content.\nOption D suggests deploying the Oracle database and the JBoss app server on EC2. RMAN backups of the Oracle database are stored in Amazon S3. Static content is restored from an AWS Storage Gateway-VTL running on Amazon EC2. This option seems less feasible, as it relies on a Virtual Tape Library (VTL) solution for restoring static content, which may not be as efficient as other options.\nIn summary, options A, B, and C all seem to be feasible solutions for migrating the content management system to AWS. Option D appears to be less efficient and may not be the best choice. Ultimately, the choice between these options may depend on specific requirements and constraints of the content management system.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy the Oracle database and the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon S3. Generate anEBS volume of static content from the Storage Gateway and attach it to the JBoss EC2 server.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Deploy the Oracle database on RDS. Deploy the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon Glacier. Generate an EBS volume of static content from the Storage Gateway and attach it to the JBoss EC2 server.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy the Oracle database and the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon S3. Restore the static content by attaching an AWS Storage Gateway running on Amazon EC2 as an iSCSI volume to the JBoss EC2 server.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy the Oracle database and the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon S3. Restore the static content from an AWS Storage Gateway-VTL running on Amazon EC2",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 202,
  "query" : "An ERP application is deployed in multiple Availability Zones in a single region.\nThe application uses a MySQL database deployed in EC2\nIn the event of failure, the RTO must be less than 3 hours, and the RPO is 15 minutes.\nThe customer realizes that data corruption occurred roughly 10 Mins ago.\nWhich DR strategy can be used to achieve this RTO and RPO in the event of this kind of failure?",
  "answer" : "Answer - C.\nOption A is incorrect because restoring the backups from Amazon Glacier would be slow and will definitely not meet the RTO and RPO.\nOption B is incorrect because you cannot go back to the point in time recovery with the synchronous replication.\nYou will always have the latest data.\nOption C is CORRECT because it takes hourly backups to Amazon S3, restoring the backups quickly.\nSince the transaction logs are stored in S3 every 5 minutes, it will help restore the application to a state within the RPO of 15 minutes.\nOption D is incorrect because instance store volume is ephemeral.\ni.e.\nthe data can get lost when the instance is terminated.\nNote:\nAlthough Glacier supports expedited retrieval (On-Demand and Provisioned), it is an expensive option and is recommended only for the occasional urgent request for a small number of archives.\nHaving said this (and even if we go with the Glacier as a solution), the option also mentions taking database snapshots every 15 minutes.\nNow, if you keep taking backups every 15 mins, the database users will face a lot of outages during the backup (due to I/O suspension especially in non-AZ deployment)\nAlso, within 15 minutes, the backup process may not even finish!\nAs an architect, you need to use the database change (transaction) logs along with the backups to restore your database to a point in time.\nSince option (c) stores the transaction details up to the last 5 minutes, you can easily restore your database and meet the RPO of 15 minutes.\nHence, C is the best choice.\nTo achieve a recovery time objective (RTO) of less than 3 hours and a recovery point objective (RPO) of 15 minutes in the event of a failure, the following DR strategy can be used:\nB. Use synchronous database master-standby replication between two Availability Zones.\nExplanation: Synchronous database replication is a DR strategy that involves replicating changes from a primary database to a secondary database in real-time, ensuring that the secondary database is always in sync with the primary database. In the event of a failure, the secondary database can be promoted to become the primary database, with minimal data loss and downtime.\nUsing synchronous replication between two Availability Zones in a single region will ensure that the secondary database is always in sync with the primary database. In the event of a failure, the secondary database can be promoted to become the primary database with minimal data loss and downtime.\nOption A, taking 15-minute database backups stored in Amazon Glacier, with transaction logs stored in Amazon S3 every 5 minutes, is not suitable for achieving the RPO of 15 minutes, as the data loss can be up to 15 minutes.\nOption C, taking hourly database backups to Amazon S3, with transaction logs stored in S3 every 5 minutes, is not suitable for achieving the RPO of 15 minutes, as the data loss can be up to 1 hour.\nOption D, taking hourly database backups to an Amazon EC2 instance store volume, with transaction logs stored in Amazon S3 every 5 minutes, is not suitable for achieving the RPO of 15 minutes, as the data loss can be up to 1 hour.\nIn conclusion, using synchronous database master-standby replication between two Availability Zones in a single region is the best DR strategy to achieve the RTO of less than 3 hours and the RPO of 15 minutes in the event of a failure.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Take 15-minute DB backups stored in Amazon Glacier, with transaction logs stored in Amazon S3 every 5 minutes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use synchronous database master-standby replication between two Availability Zones.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Take hourly DB backups to Amazon S3, with transaction logs stored in S3 every 5 minutes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Take hourly DB backups to an Amazon EC2 instance store volume, with transaction logs stored in Amazon S3 every 5 minutes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 203,
  "query" : "The Marketing Director in your company asked you to create an app that lets users post sightings of good deeds known as random acts of kindness in 80-character summaries.\nYou decided to write the application in JavaScript to run on the broadest range of phones, browsers, and tablets.\nYour application should provide access to Amazon DynamoDB to store the good deed summaries.\nInitial testing of a prototype shows that there aren't large spikes in usage.\nWhich option provides the most cost-effective and scalable architecture for this application?",
  "answer" : "Answer - B.\nThis scenario asks to design a cost-effective and scalable solution where a multi-platform application needs to communicate with DynamoDB.\nFor such scenarios, federated access to the application is the most likely solution.\nOption A is incorrect because the Token Vending Machine (STS Service) is implemented on a single EC2 instance which is a single point of failure.\nThis is not a scalable solution either, as the instance can become the performance bottleneck.\nOption B is CORRECT because, (i) it authenticates the application via federated identity provider such as Amazon, Google, Facebook, etc, (ii) it sets up the proper permission for DynamoDB access, and (iii) S3 website which supports Javascript - is a highly scalable and cost-effective solution.\nThe application is authenticated through the \"assumeRolewithWebIdenity\" API via the federated identity provider.\nOption C is incorrect because deploying EC2 instances in the Auto Scaled environment is not as cost-effective solution as the S3 website, even though it is scalable.\nOption D is incorrect because it suggests running the website on an Auto Scaling group of EC2 instances with an ELB in the front end.\nThis option is not the most cost-effective solution provided.\nHence this is invalid.\nFor this application, we need to provide access to Amazon DynamoDB to store the good deed summaries. The application is written in JavaScript, and it needs to run on a broad range of phones, browsers, and tablets. We also know that initial testing of a prototype shows that there aren't large spikes in usage. Based on these requirements, the most cost-effective and scalable architecture for this application is option B:\nOption B: Register the application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow S3 gets and DynamoDB puts. You serve your application out of an S3 bucket enabled as a website. Your client updates DynamoDB.\nExplanation: In option B, the JavaScript application is registered with a Web Identity Provider like Amazon, Google, or Facebook. This registration enables the application to authenticate users through these providers. After registration, we create an IAM role for the provider and set up permissions for that role to allow S3 gets and DynamoDB puts.\nNext, we serve the application out of an S3 bucket enabled as a website. This option provides a simple, cost-effective, and scalable way to serve static websites. Since there aren't any large spikes in usage, we don't need to worry about scaling the server infrastructure. We can simply use S3 to host the application and serve it to the client's browser.\nFinally, the client updates DynamoDB to store the good deed summaries. We can use the AWS SDK for JavaScript in the client-side code to interact with DynamoDB. The SDK provides a set of APIs for accessing and manipulating data in DynamoDB.\nIn summary, option B provides a cost-effective and scalable architecture that meets the requirements of the application. It uses S3 to serve the application and DynamoDB to store the good deed summaries. The Web Identity Provider authentication enables users to authenticate through popular providers like Amazon, Google, or Facebook, which can help increase adoption of the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Provide the JavaScript client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance to provide signed credentials mapped to an Amazon Identity and Access Management (IAM) user allowing DynamoDB puts and S3 gets. You serve your mobile application out of an S3 bucket enabled as a website. Your client updates DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Register the application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow S3 gets and DynamoDB puts. You serve your application out of an S3 bucket enabled as a website. Your client updates DynamoD.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Provide the JavaScript client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials mapped to an IAM user allowing DynamoDB puts. You serve your mobile application out of Apache EC2 instances that are load-balanced and autoscaled. Your EC2 instances are configured with an IAM role that allows DynamoDB puts. Your server updates DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Register the JavaScript application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow DynamoDB puts. You serve your mobile application out of Apache EC2 instances that are load-balanced and autoscaled. Your EC2 instances are configured with an IAM role that allows DynamoDB puts. Your server updates DynamoD.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 204,
  "query" : "You are building a website that will retrieve and display highly sensitive information to users.\nThe amount of traffic the site will receive is known and not expected to fluctuate.\nThe site will leverage SSL to protect the communication between the clients and the web servers.\nDue to the nature of the site, you are very concerned about the security of your SSL private key.\nYou want to ensure that the key cannot be accidentally or intentionally moved outside your environment.\nAdditionally, while the data the site will display is stored on an encrypted EBS volume, you are also concerned that the web servers' logs might contain sensitive information.\nTherefore, the logs must persist so that employees of your company can only decrypt them.\nWhich of these architectures meets all of the requirements?",
  "answer" : "Answer - C.\nOptions A and D are both incorrect because the logs containing sensitive information are written to ephemeral volume.\nSo there are chances that the data can get lost upon termination of the EC2 instance.\nOption B is incorrect because it does not use a secure way of managing the SSL private key for SSL transactions.\nOption C is CORRECT because it uses CloudHSM for performing the SSL transaction without requiring any additional way of storing or managing the SSL private key.\nThis is the most secure way of ensuring that the key will not be moved outside of the AWS environment.\nAlso, it uses the highly available and durable S3 service for storing the logs.\nMore information on AWS CloudHSM:\nThe AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) appliances within the AWS cloud.\nWith CloudHSM, you control the encryption keys and cryptographic operations performed by the HSM.\nFor more information on AWS CloudHSM, please refer to the link.\nhttps://aws.amazon.com/cloudhsm/\nThe best architecture for this scenario is D. Use Elastic Load Balancing to distribute traffic to a set of web servers. Configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.\nExplanation:\nOption A is not the best choice as it is not secure enough for highly sensitive information. Uploading the private key to the load balancer increases the risk of accidental or intentional exposure, as it can be accessed by multiple individuals. Additionally, storing logs on an ephemeral volume may lead to data loss in case of a hardware failure.\nOption B is not the best choice as it does not provide the best security measures for highly sensitive information. While using a private Amazon S3 bucket and server-side encryption for storing logs is a good option, retrieving the private key from an S3 bucket during boot time is not a secure method. An attacker could potentially access the key during the boot process, leading to a security breach.\nOption C is a better option than A and B, but it is not the best. Using an AWS CloudHSM to perform SSL transactions is a good choice, as the private key never leaves the HSM. However, CloudHSM is expensive and requires additional setup, which can increase complexity. Moreover, writing logs to a private Amazon S3 bucket using Amazon S3 server-side encryption is a good choice, but it still does not ensure that employees can only decrypt the logs.\nOption D is the best option for this scenario. Using Elastic Load Balancing to distribute traffic to a set of web servers with TCP load balancing is a secure option, as it enables secure transmission of SSL traffic. Additionally, using AWS CloudHSM to perform SSL transactions ensures that the private key is never exposed outside the CloudHSM. Writing logs to an ephemeral volume that has been encrypted using a randomly generated AES key ensures that the logs cannot be accessed by unauthorized users, and the use of ephemeral volumes ensures that logs are not stored on permanent storage, which can increase the risk of data breaches.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Elastic Load Balancing to distribute traffic to a set of web servers. To protect the SSL private key, upload the key to the load balancer, and configure the load balancer to offload the SSL traffic. Write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Elastic Load Balancing to distribute traffic to a set of web servers. Use TCP load balancing on the load balancer and configure your web servers to retrieve the private key from a private Amazon S3 bucket on boot. Write your web server logs to a private Amazon S3 bucket using Amazon S3 server-side encryption.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Elastic Load Balancing to distribute traffic to a set of web servers, configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to a private Amazon S3 bucket using Amazon S3 server-side encryption.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Elastic Load Balancing to distribute traffic to a set of web servers. Configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 205,
  "query" : "You are working for a large company.\nYou have set up the AWS consolidated billing with a Management account and several member accounts.\nHowever, the management account's cost allocation report does not use the AWS generated cost allocation tags to organize the resource costs.",
  "answer" : "Answer - D.\nAWS provides two types of cost allocation tags: AWS-generated tags and user-defined tags.\nAWS defines, creates, and applies the AWS-generated tags for you, and users define, create, and apply user-defined tags.\nTo use the AWS-generated tags, a management account owner must activate them in the Billing and Cost Management console.\nWhen a management account owner activates the tag, the tag is also activated for all member accounts.\nOption A is incorrect: Because AWS-generated tags should be activated.\nOption B is incorrect: Because AWS-generated tags can only be activated in the management account.\nOption C is incorrect: Same reason as Option.\nB.\nAlso, it is not user-defined tags.\nOption D is CORRECT: Because the tag can be activated in “Billing -&gt; Cost Management.\nReferences:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/aws-tags.html\nAWS consolidated billing is a feature that allows companies to consolidate billing and payment for multiple AWS accounts under a single paying account, called the management account. This enables companies to get a better view of their AWS usage and cost across all accounts, as well as simplify their billing process.\nIn this scenario, the problem is that the Management account's cost allocation report does not use the AWS generated cost allocation tags to organize the resource costs. This means that the cost allocation report is not providing accurate information about the cost of resources used in each account, which can make it difficult to track and optimize costs.\nTo resolve this issue, there are several options:\nOption A: Use the Management account to log in to the AWS console and activate the user-defined tags in the Billing and Cost Management console. This option suggests activating user-defined tags in the Management account. User-defined tags are tags that are defined by the user and applied to AWS resources. However, this option does not address the issue of AWS generated cost allocation tags not being used.\nOption B: For both, the Management account and member accounts, use AWS CLI to activate AWS generated tags for Billing and Cost Management. This option suggests using the AWS Command Line Interface (CLI) to activate AWS generated cost allocation tags for both the Management account and member accounts. AWS generated tags are automatically applied by AWS to resources, and are used to identify the cost of resources used by each account. This is a more appropriate solution to the problem.\nOption C: Log in to the AWS console of both Management account and member accounts, activate the user-defined tags in Billing -> Cost Explorer -> Cost Allocation Tags. This option is similar to Option A, but suggests activating user-defined tags in the Cost Explorer section of the AWS console. However, this option does not address the issue of AWS generated cost allocation tags not being used.\nOption D: Log in to the AWS console using the Management account and activate the AWS-generated tags in the Billing and Cost Management console. This option suggests activating AWS generated cost allocation tags in the Billing and Cost Management console of the Management account. This is a more appropriate solution to the problem, as it addresses the issue of AWS generated cost allocation tags not being used.\nIn summary, Option B and Option D are the most appropriate solutions to the problem described in the scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the Management account to log in to the AWS console and activate the user-defined tags in the Billing and Cost Management console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For both, the Management account and member accounts, use AWS CLI to activate AWS generated tags for Billing and Cost Management.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Log in to the AWS console of both Management account and member accounts, activate the user-defined tags in Billing -> Cost Explorer -> Cost Allocation Tags.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Log in to the AWS console using the Management account and activate the AWS-generated tags in the Billing and Cost Management console.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 206,
  "query" : "You are designing network connectivity for your thick client application.\nThe application is designed for business travellers who must be able to connect to it from their hotel rooms, cafes, public Wi-Fi hotspots, and elsewhere on the Internet.\nBut you do not want to publish the application on the Internet.",
  "answer" : "E.\nAnswer - D.\nOption A is incorrect because AWS Direct Connect is not a cost-effective solution compared to using a VPN solution.\nOption B is incorrect because it does not mention how the application would be accessible only to business travellers and not to the public.\nOption C is incorrect because IPsec requires third-party client software, so it is more expensive to set up and maintain.\nAnd it would be better to move the instances to a private subnet.\nOption D is CORRECT because configuring the SSL VPN solution is cost-effective and allows access only to business travelers.\nSince the application servers are in a private subnet, the application is not accessible via the internet.\nFor more details, please refer to section \"AWS Client VPN Categories\" in the below link-\nhttps://docs.aws.amazon.com/vpn/latest/clientvpn-user/client-vpn-user-what-is.html\nThe scenario requires designing network connectivity for a thick client application that is accessed by business travellers from different locations on the Internet. However, the application should not be published on the public Internet. Let's analyze each of the given options to determine the best solution.\nOption A: Implement AWS Direct Connect, and create a private interface to your VPC.\nAWS Direct Connect is a service that provides dedicated network connectivity between on-premises infrastructure and AWS. It can be used to establish a private, dedicated network connection between a VPC and an on-premises data center or other remote location. However, this option may not be the best fit for this scenario, as it does not provide access to the application for public users.\nOption B: Create a public subnet and place your application servers in it.\nThis option suggests placing the application servers in a public subnet, which is accessible from the public Internet. However, this contradicts the requirement of not publishing the application on the Internet.\nOption C: Implement Elastic Load Balancing with an SSL listener that terminates the back-end connection to the application.\nElastic Load Balancing (ELB) is a managed load balancing service provided by AWS that distributes incoming traffic across multiple targets, such as EC2 instances. It provides the option to configure SSL/TLS encryption for traffic between the client and the load balancer. This option could work well in the scenario, as it allows terminating SSL connections at the load balancer and forwarding traffic to the application servers over a private network connection within the VPC.\nOption D: Configure an IPsec VPN connection, and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it.\nThis option suggests creating an IPsec VPN connection to the VPC and placing the application servers in a public subnet. Users would need to have the VPN client software installed on their computers to access the application. While this option would provide secure access to the application for users, it does not align with the requirement of not publishing the application on the Internet.\nOption E: Configure an SSL VPN solution in a public subnet of your VPC, then install and configure SSL VPN client software on all user computers. Create a private subnet in your VPC and place your application servers in it.\nThis option suggests configuring an SSL VPN solution in a public subnet of the VPC and placing the application servers in a private subnet. Users would need to have SSL VPN client software installed on their computers to access the application. This option aligns with the requirement of not publishing the application on the public Internet and provides secure access to the application for users.\nBased on the analysis of the given options, option E is the most suitable solution for this scenario. It provides secure access to the application for users while keeping the application private from the public Internet.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Implement AWS Direct Connect, and create a private interface to your VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a public subnet and place your application servers in it.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Implement Elastic Load Balancing with an SSL listener that terminates the back-end connection to the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an IPsec VPN connection, and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure an SSL VPN solution in a public subnet of your VPC, then install and configure SSL VPN client software on all user computers. Create a private subnet in your VPC and place your application servers in it.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 207,
  "query" : "There are two departments in a company.\nBoth departments own several EC2 instances.",
  "answer" : "Correct Answer - A.\nMultiple policies can be created to take snapshots for an EBS volume, as long as each policy targets a unique tag on the volume.\nIn this case, the EBS volumes owned by two departments should have two tags: tag A is the target for policy A to create a snapshot every 12 hours for Department A, and tag B is the target for policy B to create a snapshot every 24 hours for Department.\nB.\nAmazon DLM creates snapshots according to the schedules for both policies.\nDetails refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html.\nOption A is CORRECT because when an EBS volume has two tags, multiple policies can run simultaneously.\nOption B is incorrect because there is no schedule conflict for this scenario.\nOption C is incorrect because 12 hours schedule does not take priority over 24 hours, and both schedules can run in parallel.\nOption D is incorrect because the EBS volumes owned by two departments can add another tag and be included in the policy for Department.\nB.\nThe correct answer is A.\nHere is an explanation of why:\nThe requirement is to take snapshots of EBS volumes owned by two departments on a schedule that is every 12 hours and every 24 hours. The best approach to achieving this requirement is by using AWS EBS Lifecycle policies with tags.\nEach department should add tags to their EBS volumes. The tags will be used to set up the lifecycle policies for the EBS volumes. For the volumes owned by both departments, the lifecycle policy should be set up to take snapshots every 12 hours and every 24 hours.\nOption B is incorrect because it does not address the requirement of taking snapshots for EBS volumes owned by two departments.\nOption C is incorrect because it suggests that snapshots should be taken every 12 hours only. This does not meet the requirement of taking snapshots every 12 hours and every 24 hours.\nOption D is incorrect because it only addresses the EBS volumes owned by Department B outright and does not address the EBS volumes owned by both departments.\nTo summarize, the best approach is to use AWS EBS Lifecycle policies with tags. Each department adds tags to their EBS volumes, and the lifecycle policies are set up based on the tags. For the EBS volumes owned by two departments, the lifecycle policy should be set up to take snapshots every 12 hours and every 24 hours.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Each Department adds tags on its EBS volume(s). Set up lifecycle policies based on the tags. For EBS volumes owned by two departments, snapshots will be taken every 12 hours and every 24 hours.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Each Department adds tags on its EBS volume(s). Set up lifecycle policies based on the tags. For EBS volumes owned by two departments, snapshots will not be taken as there is a schedule conflict between the two policies. Other EBS volumes are not affected.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Each Department adds tags on its EBS volume(s). Set up a lifecycle policy based on the tag. For EBS volumes owned by two departments, snapshots will be taken every 12 hours since the 12-hour schedule takes priority.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a tag on the EBS volume(s) that Department B owns outright (not including the tag on any shared volumes). Set up a lifecycle policy based on the tag. For the EBS volumes owned by two departments, snapshots are taken every 12 hours due to the policy of Department",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 208,
  "query" : "As an AWS specialist, you are in charge of configuring consolidated billing in a multinational IT company.\nIn the linked accounts, users have set up AWS resources using a tag called Department, which is used to differentiate resources.\nThere are some other user-created tags such as Phase, CICD, Trial, etc.",
  "answer" : "Correct Answer - A.\nUser-Defined Cost Allocation Tags can be selected and activated in the Cost Allocation Tags.\nOption A is CORRECT because using this method only, the user-defined tag Department will appear in the cost allocation report.\nOption B is incorrect because it should be the Cost Allocation Tags rather than the Cost Explorer console.\nMoreover, by default, all user-defined tags are deactivated.\nOption C is incorrect because it should be the Cost Allocation Tags rather than the Cost Explorer console.\nOption D is incorrect because only a management account can activate or deactivate the user-defined tags.\nBesides, the tag does not appear on earlier reports before it is activated.\nConsolidated billing in AWS allows the management of multiple AWS accounts as a single entity, consolidating the payment of multiple accounts into a single payment. Cost allocation tags in AWS enable you to categorize your AWS resources and services' costs, so you can easily identify and track your expenses.\nIn this scenario, the users have set up AWS resources using a tag called Department, which differentiates resources based on the department they belong to. There are also other user-created tags such as Phase, CICD, Trial, etc.\nTo configure consolidated billing and cost allocation tags, there are a few steps to follow. Here are the explanations of each answer option:\nA. In the Billing and Management console of the management account, select Cost allocation tags and then select the Department tag in the User-Defined Cost Allocation Tags area and activate it. The tag starts appearing on the cost allocation report after it is applied but does not appear on earlier reports.\nThis answer is partially correct. In the management account, you should select the Cost allocation tags option, and then select the Department tag in the User-Defined Cost Allocation Tags area and activate it. By doing this, the Department tag is applied to all the linked accounts. However, the tag does not immediately appear on the cost allocation report, and it only applies to resources used after it is applied. The tag does not appear on earlier reports, which means that the cost allocation report only reflects costs incurred after the tag is applied.\nB. In the Cost Explorer console of the management account, deactivate all the other tags except the Department tag in the User-Defined Cost Allocation Tags area. By default, all user-defined tags are activated.\nThis answer is incorrect. The Cost Explorer console is a reporting tool used to analyze your costs and usage data in AWS. It does not have any impact on cost allocation tags. Therefore, deactivating other tags does not affect the Department tag's activation status.\nC. In the Cost Explorer console of the management account, select the Department tag in the User-Defined Cost Allocation Tags area and activate it. Make sure that other tags are inactive at the same time.\nThis answer is partially correct. In the Cost Explorer console, you can select the Department tag in the User-Defined Cost Allocation Tags area and activate it. However, it is not necessary to make sure other tags are inactive at the same time. This option does not activate the Department tag in the linked accounts.\nD. In the Billing and Management console of the management account and member accounts, select Cost allocation tags and then select the Department tag in the User-Defined Cost Allocation Tags area and activate it. The tag starts appearing on the cost allocation report after it is applied and also appears on earlier reports after 1 hour.\nThis answer is correct. In the Billing and Management console, you should select the Cost allocation tags option, and then select the Department tag in the User-Defined Cost Allocation Tags area and activate it in both the management account and linked member accounts. After activating the tag, it appears on the cost allocation report, and it also appears on earlier reports after an hour.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the Billing and Management console of the management account, select Cost allocation tags and then select the Department tag in the User-Defined Cost Allocation Tags area and activate it. The tag starts appearing on the cost allocation report after it is applied but does not appear on earlier reports.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In the Cost Explorer console of the management account, deactivate all the other tags except the Department tag in the User-Defined Cost Allocation Tags area. By default, all user-defined tags are activated.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the Cost Explorer console of the management account, select the Department tag in the User-Defined Cost Allocation Tags area and activate it. Make sure that other tags are inactive at the same time.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the Billing and Management console of the management account and member accounts, select Cost allocation tags and then select the Department tag in the User-Defined Cost Allocation Tags area and activate it. The tag starts appearing on the cost allocation report after it is applied and also appears on earlier reports after 1 hour.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 209,
  "query" : "An organization has created multiple components of a single application.\nCurrently, all the components are hosted on a single EC2 instance.\nDue to security reasons, the organization wants to implement 2 separate SSL certificates for the separate modules.",
  "answer" : "Answer - A.\nIt can be useful to assign multiple IP addresses to an instance in your VPC to do the following.\n(1) Host multiple websites on a single server by using multiple SSL certificates on a single server and associating each certificate with a specific IP address.\n(2) Operate network appliances, such as firewalls or load balancers, have multiple IP addresses for each network interface.\n(3) Redirect internal traffic to a standby instance if your instance fails by reassigning the secondary IP address to the standby instance.\nOption A is CORRECT because, as mentioned above, if you have multiple elastic network interfaces (ENIs) attached to the EC2 instance, each network IP can have a component running with a separate SSL certificate.\nOption B is incorrect because having separate rules in the security group as well as NACL does not mean that the instance supports multiple SSLs.\nOption C is incorrect because an EC2 instance cannot have multiple subnets.\nOption D is incorrect because the NAT address is not related to supporting multiple SSLs.\nFor more information on Multiple IP Addresses, please refer to the link below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html\nThe most appropriate solution in this scenario would be to create separate EC2 instances for each component and install the SSL certificate on each instance separately. However, out of the given options, the best solution would be to:\nA. Create an EC2 instance that has multiple network interfaces with multiple elastic IP addresses.\nExplanation: This solution involves creating an EC2 instance with multiple network interfaces, each with a separate elastic IP address. The application components can be hosted on separate network interfaces, and the SSL certificates can be installed on each interface separately. This approach provides better security by isolating the components and allows for better management of the certificates.\nB. Creating an EC2 instance that has both an ACL and the security group attached to it and have separate rules for each IP address is not the best solution because it involves managing multiple rules for each IP address, which can be complex and difficult to manage.\nC. Creating an EC2 instance that has multiple subnets attached to it and each will have a separate IP address is not the best solution because it involves managing multiple subnets, which can be complex and difficult to manage.\nD. Creating an EC2 instance with a NAT address is not the best solution because NAT instances are typically used for outbound traffic from a private subnet to the internet, and not for hosting application components.\nIn summary, the best solution to implement separate SSL certificates for separate components of a single application hosted on a single EC2 instance is to create an EC2 instance with multiple network interfaces and multiple elastic IP addresses.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an EC2 instance that has multiple network interfaces with multiple elastic IP addresses.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an EC2 instance that has both an ACL and the security group attached to it and have separate rules for each IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an EC2 instance that has multiple subnets attached to it and each will have a separate IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an EC2 instance with a NAT address.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 210,
  "query" : "Your company produces customer commissioned one-of-a-kind skiing helmets combining high fashion with custom technical enhancements.\nCustomers can show off their individuality on the ski slopes and have access to head-up-displays.\nGPS rear-view cams and any other technical innovation they wish to embed in the helmet.\nThe current manufacturing process is data-rich and complex, including assessments to ensure that the custom electronics and materials used to assemble the helmets are the highest standards.\nAssessments are a mixture of human and automated assessments.\nYou need to add a new set of assessments to model the electronics' failure modes using GPUs with CUDA across a cluster of servers with low latency networking.\nWhat architecture would allow you to automate the existing process using a hybrid approach and ensure that the architecture can support the evolution of processes over time?",
  "answer" : "Answer - B.\nThe main point to consider in this question is that the assessments include human interaction as well.\nIn most such cases, always look for AWS Step Functions in the options.\nOption A is incorrect because this will be useful during the batch jobs, which deal with the automated assessments.\nFor the human assessment, this will not be a useful option.\nOption B is CORRECT because (a)it enables assessment via human interaction, (b) uses Auto Scaled G2 instances that are efficient in automated assessments due to their GPU and low latency networking.\nPlease refer the below link for AWS Step Functions.\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\nOption C is incorrect because although SWF can be used for human tasks, C3 instances and SR-IOV will not provide the required GPU.\nOption D is incorrect because (a) this will be useful during the batch jobs, which deal with the automated assessments.\nThis will not be a useful option for the human assessment, and (b) C3 instances and SR-IOV will not provide the required GPU.\nTo automate the existing process of adding a new set of assessments for modeling electronics failure modes using GPUs with CUDA across a cluster of servers with low latency networking and ensure that the architecture can support the evolution of processes over time, we need an architecture that provides scalability, high availability, and fault tolerance.\nOption A: Use AWS Data Pipeline to manage the movement of data & metadata and assessments. Use an Auto Scaling group of G2 instances in a placement group.\nAWS Data Pipeline is a web service that allows you to move data between different AWS services, such as Amazon S3, Amazon RDS, and Amazon DynamoDB. It can also be used to automate the execution of data-driven workflows. In this option, we can use AWS Data Pipeline to manage the movement of data and metadata and assessments. However, G2 instances are designed for graphics-intensive workloads and not for GPU-based computing, which is required for this scenario. Also, G2 instances are based on an older architecture and have limited scalability, which makes this option less suitable for the task.\nOption B: Use AWS Step Functions to manage assessments, movement of data, and metadata. Use an Auto Scaling group of G2 instances in a placement group.\nAWS Step Functions is a serverless workflow service that allows you to coordinate the components of distributed applications and microservices. It provides visual workflows to build and run applications using AWS services like AWS Lambda, Amazon SNS, and Amazon SQS. In this option, we can use AWS Step Functions to manage assessments, movement of data, and metadata. However, G2 instances are not suitable for this scenario, as explained above.\nOption C: Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and metadata. Use an Auto Scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization).\nAmazon SWF is a workflow service for building scalable, distributed applications. It can be used to coordinate the execution of tasks across multiple machines and services, allowing you to build complex, multi-step workflows. In this option, we can use Amazon SWF to manage assessments, movement of data, and metadata. C3 instances with SR-IOV are optimized for compute-intensive workloads and provide high network performance. They can also be used for GPU-based computing, which makes this option suitable for the task.\nOption D: Use AWS Data Pipeline to manage the movement of data & metadata and assessments. Use an Auto Scaling group of C3 with SR-IOV (Single Root I/O virtualization).\nThis option is similar to Option C, but it uses AWS Data Pipeline instead of Amazon SWF to manage the movement of data and metadata and assessments. AWS Data Pipeline can be used to automate the execution of data-driven workflows, making it a suitable alternative to Amazon SWF. However, Amazon SWF is more flexible and provides greater control over the workflow execution.\nIn summary, Option C or Option D would be the most suitable options for the scenario described. Both options use a combination of Amazon SWF or AWS Data Pipeline and an Auto Scaling group of C3 instances with SR-IOV to provide scalability, high availability, and fault tolerance. The choice between the two options will depend on the specific requirements of the workflow and the preferences of the solution architect.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use AWS Data Pipeline to manage the movement of data & meta-data and assessments. Use an Auto Scaling group of G2 instances in a placement group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Step Functions to manage assessments, movement of data, & meta-data. Use an Auto Scaling group of G2 instances in a placement group.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Amazon Simple Workflow (SWF) to manage assessments movement of data & meta-data. Use an Auto Scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS data Pipeline to manage the movement of data & meta-data and assessments. Use an Auto Scaling group of C3 with SR-IOV (Single Root I/O virtualization).",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 211,
  "query" : "A web company is looking to implement an external payment service into their highly available application deployed in a VPC.\nTheir application EC2 instances are behind a public-facing ELB with NAT instances and Public IP s in place.\nAuto Scaling is used to add additional instances as traffic increases under normal load.\nThe application runs 2 instances in the Auto Scaling group, but it can scale 3x in size at the peak.\nThe application instances need to communicate with the payment service over the Internet, which requires whitelisting all public IP addresses to communicate with it.\nA maximum of 4 whitelisting IP addresses are allowed at a time and can be added through an API.\nHow should they architect their solution?",
  "answer" : "Answer - A.\nOption A is CORRECT because (a) the requests originated from the instances in the subnet would be routed through the NAT, so they would have the NAT's IP address (which is whitelisted), and (b) two NAT instances would provide high availability.\nOption B is incorrect because (a) Internet Gateway (IGW) can only route the traffic.\nIt cannot whitelist any particular IP and payment requests, and (b) EC2 instances with public IP addresses in a public subnet are routed through the gateway.\nStill, they will keep their own IP address.\nSo they will not get whitelisted.\nOption C is incorrect because the outbound traffic cannot be routed through an ELB.Option D is incorrect because the ASG will have 6 servers during the peak load, and the payment service only allows 4 to be whitelisted.\nSo, it will exceed the allowed 4 IP addresses.\nThe correct answer is option D: Automatically assign public IP addresses to the application instances in the Auto Scaling group and run a script on boot that adds each instance's public IP address to the payment validation whitelist API.\nHere's the explanation:\nThe web company has a highly available application deployed in a VPC, with EC2 instances behind a public-facing ELB with NAT instances and Public IP addresses in place. Auto Scaling is used to add additional instances as traffic increases under normal load.\nThe application instances need to communicate with an external payment service over the Internet, and this service requires whitelisting all public IP addresses to communicate with it. However, a maximum of 4 whitelisting IP addresses are allowed at a time and can be added through an API.\nTo solve this problem, we need to find a solution that allows the application instances to communicate with the payment service while staying within the limitations of the whitelisting IP addresses.\nOption A suggests routing payment requests through two NAT instances set up for high availability and whitelisting the Elastic IP addresses attached to the NAT instances. However, this solution does not work because the payment service only allows four whitelisting IP addresses, and the number of NAT instances does not matter.\nOption B suggests whitelisting the VPC Internet Gateway Public IP and routing payment requests through the Internet Gateway. However, this solution is not practical because the VPC Internet Gateway Public IP changes every time the Internet Gateway is deleted and recreated.\nOption C suggests whitelisting the ELB IP addresses and routing payment requests from the application servers through the ELB. However, this solution does not work because the payment service requires whitelisting of public IP addresses, and the ELB IP addresses are not public.\nOption D is the correct answer. This option suggests automatically assigning public IP addresses to the application instances in the Auto Scaling group and running a script on boot that adds each instance's public IP address to the payment validation whitelist API. With this solution, we can stay within the limitations of the whitelisting IP addresses while allowing the application instances to communicate with the payment service. The script that adds the public IP address to the payment validation whitelist API must be triggered on every instance launch to ensure that the new instances are added to the whitelist.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Route payment requests through two NAT instances setup for High Availability and whitelist the Elastic IP addresses attached to the NAT instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Whitelist the VPC Internet Gateway Public IP and route payment requests through the Internet Gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Whitelist the ELB IP addresses and route payment requests from the Application servers through the EL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Automatically assign public IP addresses to the application instances in the Auto Scaling group and run a script on boot that adds each instance`s public IP address to the payment validation whitelist API.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 212,
  "query" : "You are an AWS Cloud Architect in a big company.\nYour company is under a planning phase for a fresh project.\nThe project needs to be developed and deployed completely in AWS and various deployment services are now considered.\nYour team members are debating on which service should be used between OpsWorks and CloudFormation.\nFor the below options, which ones should you consider helping to choose the most appropriate service? Select 3.",
  "answer" : "E.\nCorrect Answer - A, C, D.\nThe question asks for the items that help with choosing the service of OpsWorks or CloudFormation.\nOne major feature of OpsWorks is that it has used Chef.\nFor OpsWorks, it is very common that a custom recipe is needed.\nThat might be a simple task if the team has a Chef expert.\nBut if there is not, there is a pretty steep learning curve.\nNo matter in which way, the project's schedule should be always considered.\nOption A is CORRECT because the experience of Chef and Recipes is a key factor to choose OpsWorks or not.\nOption B is incorrect because the OpsWorks ( except Opsworks-chef automate ) and CloudFormation templates and stacks themselves do not bring cost.\nYou only need to pay for the resources that are set up in stacks.\nOption C is CORRECT because the project's schedule is also a key factor to consider.\nOption D is CORRECT because CloudFormation is better at a lower level scripting if the team prefers to have a deeper infrastructure control with code.\nOption E is incorrect: OpsWorks supports spot EC2 instances too.\nRefer tohttps://aws.amazon.com/blogs/devops/registering-spot-instances-with-aws-opsworks-stacks/.\nAs an AWS Cloud Architect, you are responsible for choosing the appropriate service for a fresh project in AWS. Your team members are debating between OpsWorks and CloudFormation. Here are the three factors that you should consider to help choose the most appropriate service:\n1.\nThe experience of Chef and Recipe: OpsWorks is a service that offers Chef automation to manage infrastructure. It can automate tasks such as software configurations and deployments. However, if the team lacks knowledge of Chef, OpsWorks may not be the best choice as the learning curve can be steep. In this case, CloudFormation may be a better choice as it does not require knowledge of Chef.\n2.\nThe budget of the whole project: OpsWorks (except Opsworks-chef automate) and CloudFormation templates and stacks are charged differently. If the project has a tight budget, you may want to choose the service that fits better with your budget. For example, if OpsWorks is more cost-effective, it may be a better choice than CloudFormation.\n3.\nThe schedule of the project: If the timeline is not under pressure, the team can still choose to learn new skills such as Chef for OpsWorks or JSON scripting for CloudFormation. However, if the timeline is tight, the team may not have enough time to learn new skills. In this case, the team may need to choose the service that requires less time to deploy.\nAdditional factors to consider:\n1.\nDeeper control of infrastructure setup: If the team prefers to have deeper control over the infrastructure setup, CloudFormation may be a better choice. CloudFormation allows users to define the entire infrastructure in a template, including networking, EC2 instances, and other resources. This gives users more control over the infrastructure setup. In contrast, OpsWorks can take care of some basic configurations automatically, but it may not offer the same level of control as CloudFormation.\n2.\nSpot EC2 instances: If the project requires the use of spot EC2 instances, CloudFormation should be used as OpsWorks does not support spot instances.\nIn conclusion, choosing the appropriate service between OpsWorks and CloudFormation depends on various factors such as the team's experience with Chef, the budget of the project, the schedule of the project, the level of control over infrastructure setup, and the use of spot EC2 instances. By considering these factors, you can choose the service that best fits the project's requirements.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The experiences of Chef and Recipe. If the team lacks Chef knowledge, OpsWorks may not be considered as the learning curve would be steep.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The budget of the whole project. Except for the resources such as EC2, the OpsWorks ( except Opsworks-chef automate ) and CloudFormation templates and stacks are charged differently.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The schedule of the project. If the timeline is not under pressure, the team can still choose to learn new skills such as Chef for OpsWorks or JSON scripting for CloudFormation.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Understand if the team prefers to have deeper control of infrastructure setup. If yes, CloudFormation may be a better choice. Otherwise, OpsWorks can take care of some basic configurations automatically.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "If spot EC2 instances are required, only CloudFormation should be used as OpsWorks does not support spot instances.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 213,
  "query" : "Your company currently has a 2-tier web application running in an on-premises data center.\nYou have experienced several infrastructure failures in the past two months resulting in significant financial losses.\nYour CIO strongly agrees to move the application to AWS.\nWhile working on achieving buy-in from the other company executives, he asks you to develop a disaster recovery plan to deploy the application, post its AMI creation, and help to improve Business continuity in the short term.\nHe specifies a target Recovery Time Objective (RTO) of 4 hours and a Recovery Point Objective (RPO) of 1 hour or less.\nHe also asks you to implement the solution within 2 weeks.\nYour database is 200GB in size, and you have a 20Mbps Internet connection.\nWhich of the following solutions is the most suitable?",
  "answer" : "Answer - A.\nOption A is CORRECT because (a) with AMIs, the newly created EC2 instances will be ready with the pre-installed application.\nThus, reducing the RTO, (b) with CloudFormation, the entire stack can be automatically provisioned, and (c) since there are no additional services used, the cost will stay low.\nOption B is incorrect because although this could work, (a) deploying EC2 instances for this scenario will be expensive, and (b) in case of disaster, the recovery will potentially be slower since the new EC2 need to be manually updated with the application software and patches, especially since it does not use the AMIs.\nOption C is incorrect because it has a low-performance issue.\n(a) Backing up a local DB of 200GB on a 20Mbps connection every hour will be very slow, and (b) even with the incremental backup, recovering from the incremental backup take time and might not satisfy the given RTO.\nOption D is incorrect because (a) the EC2 instance is a single point of failure, which needs to be made highly available via Auto Scaling, and (b) it can only handle the average load of the application.\nSo, in case of peak load, it may fail, and (c) AWS Direct Connection will be an expensive solution compared to the setup of option A.\nThe most suitable solution for this scenario is A. Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template that includes your AMI and the required Auto Scaling group, and ELB resources to support deploying the application across Multiple Availability Zones. Asynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection.\nHere is a detailed explanation of each option:\nOption A:\nCreate an EBS backed private AMI which includes a fresh install of your application. This AMI will serve as a template for the application deployment.\nDevelop a CloudFormation template that includes the AMI and the required Auto Scaling group, and ELB resources to support deploying the application across multiple Availability Zones. This template will automate the deployment of the application infrastructure.\nAsynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection. This will ensure that the AWS database is up-to-date with the on-premises database.\nThis solution meets the target RTO and RPO objectives by providing a fully automated and scalable infrastructure that can be deployed within 4 hours. The asynchronous replication of the database ensures that the RPO is met, and the use of multiple Availability Zones and Auto Scaling ensures that the application can withstand infrastructure failures.\nOption B:\nDeploy your application on EC2 instances within an Auto Scaling Group across multiple Availability Zones.\nAsynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection.\nThis solution does not provide a fully automated infrastructure, and the deployment process may take longer than 4 hours. Additionally, there is no mention of using a load balancer, which means that the application may not be highly available in the event of an infrastructure failure.\nOption C:\nCreate an EBS backed private AMI which includes a fresh install of your application.\nSetup a script in your data center to back up the local database every 1 hour and encrypt and copy the resulting file to an S3 bucket using multi-part upload.\nThis solution does not provide a highly available infrastructure, and the RTO objective may not be met since the deployment process is not automated. Additionally, the RPO objective may not be met since the database backup is only done every hour.\nOption D:\nInstall your application on a compute-optimized EC2 instance capable of supporting the application's average load.\nSynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure Direct Connect connection.\nThis solution does not provide a highly available infrastructure, and the RTO objective may not be met since the deployment process is not automated. Additionally, the synchronous replication of the database may introduce latency and affect the application performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template that includes your AMI and the required Auto Scaling group, and ELB resources to support deploying the application across Multiple Availability Zones. Asynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Deploy your application on EC2 instances within an Auto Scaling Group across multiple Availability Zones. Asynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an EBS backed private AMI which includes a fresh install of your application. Setup a script in your data center to back up the local database every 1 hour and encrypt and copy the resulting file to an S3 bucket using multi-part upload.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Install your application on a compute-optimized EC2 instance capable of supporting the application’s average load. Synchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure Direct Connect connection.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 214,
  "query" : "An organization is planning to set up a management network on the AWS VPC.\nThe organization is trying to secure the web server on a single EC2 instance such that it allows the internet traffic and back-end management traffic.\nThe organization wants to ensure that the back-end management network interface can only receive SSH traffic from a selected IP range.\nAt the same time, the Internet-facing web server will have an IP address that can receive traffic from all the internet IPs.",
  "answer" : "Answer - B.\nAn Elastic Network Interface (ENI) is a virtual network interface that you can attach to an instance in a VPC.\nNetwork interfaces are available only for instances running in a VPC.A network interface can include the following attributes.\nA primary private IPv4 address.\nOne or more secondary private IPv4 addresses.\nOne Elastic IP address (IPv4) per private IPv4 address.\nOne public IPv4 address.\nOne or more IPv6 addresses.\nOne or more security groups.\nA MAC address.\nA source/destination check flag.\nA description.\nSee an example below how the route table can be configured to allow the IP based access via multiple ENIs.\nFor more information on ENI, please refer to the below link.\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\nThe organization wants to set up a management network on AWS VPC, and secure a web server on a single EC2 instance to allow internet traffic and back-end management traffic. The organization also wants to ensure that the back-end management network interface can only receive SSH traffic from a selected IP range while the Internet-facing web server can receive traffic from all internet IPs.\nOption A: \"It is not possible to have 2 IP addresses for a single instance.\" This statement is incorrect. It is possible to have multiple IP addresses for a single instance, using multiple network interfaces. Each network interface can have its own private IP address and public IP address.\nOption B: \"The organization should create 2 network interfaces, one for the internet traffic and the other for the backend traffic.\" This is the correct answer. To achieve the desired outcome, the organization should create two network interfaces for the EC2 instance, one for internet traffic and one for backend management traffic. The internet-facing network interface should have a public IP address and be associated with a security group that allows inbound traffic from all internet IPs. The backend management network interface should have a private IP address and be associated with a security group that allows inbound SSH traffic only from the selected IP range for back-end management.\nOption C: \"The organization should create 2 EC2 instances as this is not possible with one EC2 instance.\" This statement is incorrect. It is possible to create multiple network interfaces on a single EC2 instance, as mentioned in Option B. There is no need to create two separate EC2 instances.\nOption D: \"This is not possible.\" This statement is also incorrect. The desired outcome can be achieved by creating multiple network interfaces on a single EC2 instance, as mentioned in Option B.\nIn conclusion, the correct answer is Option B: The organization should create 2 network interfaces, one for the internet traffic and the other for the backend traffic.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It is not possible to have 2 IP addresses for a single instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The organization should create 2 network interfaces, one for the internet traffic and the other for the backend traffic.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The organization should create 2 EC2 instances as this is not possible with one EC2 instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "This is not possible.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 215,
  "query" : "A web design company currently runs several FTP servers for their 20 customers to upload and download large graphic files.\nThey want to move this system to AWS.\nThey also want to maintain customer privacy and keep the costs incurred on AWS side to a minimum.",
  "answer" : "Answer - A.\nThe main considerations in this scenario are: (1) the architecture should be scalable, (2) customer privacy should be maintained, and (3) the solution should be cost-effective.\nOption A is correct because (a) it creates permissions via IAM policy where each user will have access to only those objects ( files ) named with their username, and (b) S3 is a cost-effective and highly scalable solution.\nNote: Even though creating one IAM User per user/customer is not the best way forwards, but given the other choices, this is the best option.\nOption B is incorrect because creating one bucket per user is not a scalable architecture.\nOne S3 bucket is enough for this scenario.\nOption C is incorrect because creating an Auto Scaling group of FTP servers is a costly solution compared to creating buckets on S3 and appropriate IAM policies.\nOption D is incorrect because (a) creating one bucket per user is not a scalable architecture.\nCurrently, the number of customers is 20\nBut in the future, the number can grow.\nIf it does, it will put limits on the number of buckets, and (b) you configure buckets to be Requester Pays when you want to share the data but not incur charges associated with others accessing the data.\nThis will keep the cost down for the company but will increase the cost for the customer who will access the buckets.\nThe best option among the given choices is A.\nOption A involves using Amazon S3 (Simple Storage Service) as the storage solution for the web design company's FTP server. S3 provides highly scalable, reliable, and cost-effective storage with built-in security and access controls.\nTo implement this option, the company can create a single S3 bucket to store all the customer files. Then, they can create an IAM user for each customer and put them in an IAM group with an IAM policy that permits access to objects (files) within the bucket using the \"username\" policy variable. This approach ensures that each customer can only access their own files and not those of other customers.\nAdditionally, the company can ask their customers to use an S3 client instead of an FTP client to upload and download files. This approach reduces the cost of maintaining FTP servers and ensures secure and reliable file transfers using S3's built-in encryption and data integrity features.\nOption B involves creating a single S3 bucket with Reduced Redundancy Storage (RRS) turned on, which provides lower durability but lower storage costs compared to standard S3 storage. The company would create a separate bucket for each customer with a Bucket Policy that permits access only to that one customer. However, this approach is less flexible and scalable than Option A and requires more management overhead, making it less ideal for the web design company's needs.\nOption C involves creating an Auto Scaling Group of FTP servers that automatically scale-in when network traffic on the group falls below a given threshold. The company would load a central list of FTP users from S3 as part of the user data startup script on each instance. While this approach ensures high availability and scalability, it also requires more infrastructure management and maintenance than Option A and may incur higher costs.\nOption D involves creating a single S3 bucket with Requester Pays turned on, which requires the customer to pay for the data transfer costs incurred when accessing the bucket. The company would create a separate bucket for each customer with a Bucket Policy that permits access only to that one customer. However, this approach may not be feasible if the company wants to cover the data transfer costs themselves or if customers are unwilling to pay for the costs of accessing their files.\nIn summary, Option A is the best choice for the web design company as it provides a cost-effective, scalable, and secure solution for file storage and transfer while maintaining customer privacy.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Ask their customers to use an S3 client instead of an FTP client. Create a single S3 bucket. Create an IAM user for each customer. Put the IAM Users in a Group with an IAM policy that permits access to objects ( files ) within the bucket via the use of the ‘username’ policy variable.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a single S3 bucket with Reduced Redundancy Storage turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket for each customer with a Bucket Policy that permits access only to that one customer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an Auto Scaling Group of FTP servers with a scaling policy to automatically scale-in when minimum network traffic on the auto-scaling group is below a given threshold. Load a central list of FTP users from S3 as part of the user data startup script on each Instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a single S3 bucket with Requester Pays turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket for each customer with a bucket policy that permits access only to that one customer.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 216,
  "query" : "Your company has a logging microservice used to generate logs when users have entered certain commands in another application.\nThis logging service is implemented via an SQS standard queue that an EC2 instance is listening to.\nHowever, you have found that on some occasions, the order of the logs is not maintained.\nAs a result, it becomes harder to use this service to trace users' activities.\nHow should you simply fix this issue?",
  "answer" : "Correct Answer - B.\nThe FIFO queue improves upon and complements the standard queue.\nThe most important features of this queue type are FIFO (First-In-First-Out) delivery and exactly-once processing.\nThe FIFO queue is mainly used to process the messages in the queue that needs to be guaranteed without any items being out of order or duplicated.\nOption A is incorrect: Because you can't convert an existing standard queue into a FIFO queue.\nThis is clarified in.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html.\nOption B is CORRECT: Because the FIFO queue can guarantee the sequence for users' operations so that the issue of the logging system is fixed.\nOption C is incorrect: Because this is not a straightforward method by changing the whole microservice to SWF.\nOption B is much simpler than this option.\nOption D is incorrect: Refer to the explanations in Option.\nB.\nThe correct answer is A. Convert the existing standard queue into a FIFO queue. Add a deduplication ID for the messages that are sent to the queue.\nExplanation: Amazon Simple Queue Service (SQS) is a managed message queue service that enables decoupling between services. There are two types of SQS queues: standard and FIFO (First-In-First-Out). Standard queues provide a highly scalable and flexible queuing service, while FIFO queues are designed to ensure that messages are processed in the exact order in which they are sent.\nSince the logging microservice is using a standard SQS queue, it does not guarantee the order of the logs. To fix this issue, we can convert the standard queue into a FIFO queue. FIFO queues ensure that messages are processed in the exact order in which they are sent, and they provide the additional benefits of deduplication and exactly-once processing.\nIn addition, adding a deduplication ID to the messages that are sent to the queue will help to prevent duplicate messages from being processed. This ensures that logs are not repeated and prevents them from being out of order.\nOption B is incorrect because deleting and recreating the queue as a FIFO queue is not necessary. It can result in loss of data, and it is not the recommended solution.\nOption C is incorrect because migrating the microservice to SWF (Amazon Simple Workflow Service) is not necessary. SWF is a fully-managed workflow service that makes it easy to build applications that coordinate work across distributed components. However, it is not the solution for ensuring the order of messages in an SQS queue.\nOption D is incorrect because it is not accurate. While SQS does not guarantee the order of messages in a standard queue, it is possible to ensure the order of messages in a FIFO queue.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Convert the existing standard queue into a FIFO queue. Add a deduplication ID for the messages that are sent to the queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Delete the existing standard queue and recreate it as a FIFO queue. As a result, the order for the messages to be received is ensured.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Migrate the whole microservice application to SWF so that the operation sequence is guaranteed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The wrong order of timestamps is a limitation of SQS, which does not have a fix.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 217,
  "query" : "In Amazon Cognito, your mobile app authenticates with the Identity Provider (IdP) using the provider's SDK.\nOnce the end-user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito.\nIn addition to the access token, which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?",
  "answer" : "Answer - C.\nIf you're allowing unauthenticated users, you can retrieve a unique Amazon Cognito identifier (identity ID) for your end-user immediately.\nIf you're authenticating users, you can retrieve the identity ID after setting the login tokens in the credentials provider.\nFor more information on Cognito ID, please refer to the below link.\nhttp://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html\nWhen using Amazon Cognito as an identity provider for mobile apps, once the end-user is authenticated with the Identity Provider (IdP) using the provider's SDK, the OAuth or OpenID Connect token returned from the IdP is passed by the mobile app to Amazon Cognito.\nIn addition to the access token, Amazon Cognito also returns a set of temporary, limited-privilege AWS credentials for the user. These credentials allow the user to access AWS services, such as Amazon S3, that require AWS Identity and Access Management (IAM) credentials.\nThe temporary credentials are provided in the form of an AWS Security Token Service (STS) token. The STS token includes an access key, a secret access key, and a session token. These temporary credentials are valid for a limited time, typically one hour, and can be used to make API calls to AWS services.\nTo obtain the temporary credentials, the mobile app needs to call the AWS Security Token Service (STS) AssumeRoleWithWebIdentity API. This API call requires the following information:\nThe Amazon Resource Name (ARN) of the IAM role that the user is assuming\nThe web identity token obtained from the identity provider\nA unique identifier for the user, such as the Amazon Cognito Identity ID\nAn optional set of policy documents that define the permissions granted to the user\nTherefore, the correct answer to the question is: C. Cognito Identity ID. This is the unique identifier for the user that is required to obtain the temporary AWS credentials from the Security Token Service (STS) when using Amazon Cognito as an identity provider for mobile apps.\nThe other answer options, A. Cognito SDK, B. Cognito Key Pair, and D. Cognito API, are not directly related to obtaining temporary AWS credentials when using Amazon Cognito as an identity provider for mobile apps. However, the Cognito SDK can be used to interact with Amazon Cognito, the Cognito Key Pair can be used for signing and encrypting data, and the Cognito API can be used for various operations related to Amazon Cognito, such as creating user pools, configuring authentication providers, and managing user accounts.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Cognito SDK",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Cognito Key Pair",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Cognito Identity ID",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Cognito API.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 218,
  "query" : "You have been asked to design network connectivity between your existing data centers and AWS.\nYour application's EC2 instances must be able to connect to existing backend resources located in your data center.\nNetwork traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months.\nThe success of your application is dependent upon getting to market quickly.\nWhich of the following design options will allow you to meet your objectives?",
  "answer" : "Answer - C.\nThe most important considerations in this scenario are: (1) the network traffic would be initially small, and will increase in the future, and (2) the application should be up quickly.\nSo time is crucial.\nOne thing should be noted that it takes time initially to set up the AWS Direct Connect (See the link below for the latest information).\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html\nOption A is incorrect because setting up Direct Connect will take time.\nSo, the backend servers will not be connected quickly.\nOption B is incorrect because provisioning VPN is not a long-term solution since the traffic would increase to over 10Gbps.\nOption C is CORRECT because (a) it provides a quick connection between the on-premise data center and AWS via VPN, and (b) it also initiates the provision of a Direct Connect solution to tackle the requirement of higher bandwidth (for 10Gbps network) for later.\nOption D is incorrect because setting up Direct Connect will take time, and the application will not be up within time as it is time-critical.\nIt needs to establish a VPN connection firstly.\nFor more information on VPN and Direct Connect, please visit the link below.\nhttps://datapath.io/resources/blog/aws-direct-connect-vs-vpn-vs-direct-connect-gateway/\nOption A: Quickly create an internal ELB for your backend applications, submit a Direct Connect request to provision a 1 Gbps cross-connect between your data center and VPC, then increase the number or size of your Direct Connect connections as needed.\nThis option suggests setting up an internal Elastic Load Balancer (ELB) for the backend applications, which will help distribute traffic across the backend servers. This is a good approach for scaling the backend resources as traffic increases. Additionally, the option suggests using Direct Connect to connect the data centers and VPC, which provides a dedicated, private connection with high bandwidth and low latency. This option allows for increasing the number or size of Direct Connect connections as needed to accommodate increasing traffic.\nOption B: Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on-premises equipment.\nThis option suggests using Elastic IPs (EIPs) and an Internet Gateway to provide temporary access to backend resources from VPC instances. While this approach may work in the short term, it may not be sufficient for scaling to 10s of GB per second over several months. Additionally, it suggests provisioning a VPN connection between the VPC and the data center, which is less performant than Direct Connect and may not be sufficient for high bandwidth traffic.\nOption C: Provision a VPN connection between a VPC and existing on-premises equipment, submit a Direct Connect partner request to provision cross-connects between your data center and the Direct Connect location, then cut over from the VPN connection one or more Direct Connect connections as needed.\nThis option suggests starting with a VPN connection between the VPC and data center and later transitioning to Direct Connect as traffic increases. While this approach may be feasible, it requires additional steps to transition from VPN to Direct Connect, which may cause downtime or disruptions. Additionally, this option suggests using a Direct Connect partner, which adds another layer of complexity to the network setup.\nOption D: Quickly submit a Direct Connect request to provision a 1 Gbps cross-connect between your data center and VPC, then increase the number or size of your Direct Connect connections as needed.\nThis option is similar to Option A but does not include setting up an internal ELB for the backend applications. This option still allows for Direct Connect to provide a dedicated, private connection with high bandwidth and low latency, and allows for increasing the number or size of Direct Connect connections as needed to accommodate increasing traffic.\nOverall, the best option for meeting the objectives of quickly connecting the existing data centers to AWS and scaling to 10s of GB per second over several months is Option A, which suggests setting up an internal ELB for the backend applications and using Direct Connect to connect the data centers and VPC, allowing for easy scalability.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Quickly create an internal ELB for your backend applications, submit a Direct Connect request to provision a 1 Gbps cross-connect between your data center and VPC, then increase the number or size of your Direct Connect connections as needed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on-premises equipment.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Provision a VPN connection between a VPC and existing on-premises equipment, submit a Direct Connect partner request to provision cross-connects between your data center and the Direct Connect location, then cut over from the VPN connection one or more Direct Connect connections as needed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Quickly submit a Direct Connect request to provision a 1 Gbps cross-connect between your data center and VPC, then increase the number or size of your Direct Connect connections as needed.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 219,
  "query" : "Your company has developed an operations system for an e-commerce organization having multiple offices across the country.\nThe operations team pushes the new changes to the production frequently.\nThe product uses AWS RDS Multi-AZ MySQL with read-replicas.\nThe web application is running on EC2 with ELB in front.\nThe last feature update created an issue with the database, and part of the system is down due to that.\nThe development team has rolled out a couple of more patches to fix the issue.\nBut some users are still facing the outages.\nThe management has finally decided to roll back the update to the last known stable version until all the issues are resolved in the QA environment.\nWhich of the following options is the most suitable to perform the rollback cost-efficiently?",
  "answer" : "Correct Answer: A.\nOption A is CORRECT because point-in-time recovery helps to recover the database to a certain point in time.\nThe EC2 servers can be recovered with the previous AMI.\nOption B is INCORRECT because this method is not cost-efficient as a new set of resources need to be configured.\nOption C is INCORRECT because it does not mention how to roll back the application.\nOption D is INCORRECT because the database problem is still there, and the traffic still goes to the existing problematic database.\nThe best option to perform a rollback cost-efficiently in this scenario is A. Use the DB snapshot with Point-in-time recovery to rollback the database and rollback the EC2 servers with a previously working AMI.\nExplanation:\nOption A suggests using the DB snapshot with Point-in-time recovery to rollback the database and rollback the EC2 servers with a previously working AMI. This approach is cost-efficient because it only involves rolling back the components that have issues and not creating a completely new environment.\nTo implement this approach, the following steps can be taken:\n1.\nRollback the database: AWS RDS Multi-AZ MySQL with read-replicas allows for easy rollbacks using DB snapshots with Point-in-time recovery. This means a snapshot of the database can be taken before the last feature update was applied, and the database can be rolled back to that point in time. This will ensure that the database is in a stable state and working correctly.\n2.\nRollback the EC2 servers: Once the database is rolled back, the next step is to roll back the EC2 servers to the last known stable version. This can be done by using a previously working AMI. This will ensure that the web application is running on a stable version and any issues caused by the last feature update are resolved.\nOnce the rollback of both the database and EC2 servers is complete, the system should be in a stable state, and users should no longer face outages.\nOption B suggests creating a totally new environment with the last known stable build and using the Route53 Weighted policy to redirect traffic to the new environment. While this approach can also work, it is not as cost-efficient as option A because it involves creating a completely new environment. Additionally, this approach can result in downtime during the transition period as traffic is redirected to the new environment.\nOption C suggests using the DB snapshot with Point-in-time recovery and restoring it on top of the original database. This approach is not recommended because restoring the snapshot on top of the original database can cause data loss and other issues. It is better to create a new instance from the snapshot and then use it to replace the original instance.\nOption D suggests creating a CloudFormation template with RollingUpdate policy to create a new frontend and change the Route53 record to redirect traffic to the new frontend with the existing database. This approach is not recommended because it involves creating a new frontend, which is not necessary since the issue is with the database. Additionally, using CloudFormation can be complex and time-consuming, making it not suitable for a quick rollback.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the DB snapshot with Point-in-time recovery to rollback the database, and rollback the EC2 servers with a previously working AMI.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a totally new environment with the last known stable build. Use the Route53 Weighted policy to redirect the traffic to the new environment. Delete the old environment.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the DB snapshot with Point-in-time recovery and restore on top of the original database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CloudFormation template with RollingUpdate policy to create a new frontend and change the Route53 record to redirect traffic to the new frontend with the existing database.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 220,
  "query" : "Which of the following items are required to allow an application deployed on an EC2 instance to write data to a DynamoDB table? Assume that no security keys are allowed to be stored on the EC2 instance.",
  "answer" : "E.\nAnswer - A and.\nE.To enable an AWS service to access another one, the most important requirement is to create an appropriate IAM Role and attach that role to the service that needs access.\nOption A is CORRECT because it creates the appropriate IAM Role for accessing the DynamoDB table.\nOption B is INCORRECT because this is not a best practice, and we need to use IAM Role.\nOptions C and D are incorrect because IAM Role is preferred and more secure way than IAM User.\nOption E is CORRECT because it launches the EC2 instance after attaching the required role.\nSee the steps below.\n1\nCreate the IAM Role with appropriate permissions.\n2\nLaunch an EC2 instance with this role.\n3\nAttach the role to a running EC2\nReference Link:\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html https://aws.amazon.com/about-aws/whats-new/2017/02/new-attach-an-iam-role-to-your-existing-amazon-ec2-instance/\nIn order to allow an application deployed on an EC2 instance to write data to a DynamoDB table, we need to follow the AWS security best practices and avoid storing security keys on the EC2 instance. With this constraint in mind, the following options are possible:\nA. Create an IAM Role that allows write access to the DynamoDB table. This is the recommended option as it follows the AWS security best practices. We can create an IAM Role that has permissions to write data to the DynamoDB table and assign this role to the EC2 instance. The EC2 instance will then assume the role and have the necessary permissions to write data to the table. This approach does not require storing any security keys on the EC2 instance.\nB. Encode the IAM User credentials into the application. This option is not recommended as it involves storing security keys on the EC2 instance. This approach violates the AWS security best practices and can expose the security keys to unauthorized access.\nC. Create an IAM User that allows write access to the DynamoDB table. This option is not recommended as it involves storing security keys on the EC2 instance. This approach violates the AWS security best practices and can expose the security keys to unauthorized access.\nD. Add an IAM User to a running EC2 instance. This option is not recommended as it involves storing security keys on the EC2 instance. This approach violates the AWS security best practices and can expose the security keys to unauthorized access.\nE. Launch an EC2 Instance with the IAM Role included in the launch configuration. This option is similar to option A and is recommended as it follows the AWS security best practices. We can launch an EC2 instance with an IAM Role that has permissions to write data to the DynamoDB table. The EC2 instance will then assume the role and have the necessary permissions to write data to the table. This approach does not require storing any security keys on the EC2 instance.\nIn summary, the recommended options are A and E, as they follow the AWS security best practices and do not involve storing security keys on the EC2 instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an IAM Role that allows write access to the DynamoDB table.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Encode the IAM User credentials into the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM User that allows write access to the DynamoDB table.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add an IAM User to a running EC2 instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Launch an EC2 Instance with the IAM Role included in the launch configuration.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 221,
  "query" : "Your customer wishes to deploy an enterprise application on AWS, consisting of several web servers, several application servers, and a small (50GB) Oracle database.\nThe information is stored both in the database and the file systems of the various servers.\nThe backup system must support database recovery, whole server, and whole disk restores, and individual file restores with a recovery time of no more than two hours.\nThey have chosen to use RDS Oracle as the database.",
  "answer" : "Answer - A.\nOption A is CORRECT because (a) it uses automated daily backups from which the recovery can be made quickly, (b) the file-level backup to S3 will ensure that the recovery can be made at the individual file level - which satisfies the requirements.\nOption B is incorrect because Multi-AZ deployment is for Disaster Recovery, not for data backup.\nOption C is incorrect because Glacier is an archival solution and most certainly will not meet the criteria of RTO of 2 hours.\nOption D is INCORRECT because just backing up the EC2 instances alone would not suffice.\nHere the RDS database would also need to be backed up to S3.\nFor more information on this topic, please visit the links below.\nhttps://aws.amazon.com/rds/details/backup/ https://d0.awsstatic.com/whitepapers/Backup_and_Recovery_Approaches_Using_AWS.pdf https://blogs.oracle.com/pshuff/amazon-rds\nThe correct answer is B: Backup RDS using a Multi-AZ Deployment. Backup the EC2 instances using AMIs and supplement by copying file system data to S3 to provide file-level restore.\nExplanation: The customer's requirement is to have a backup system that supports database recovery, whole server and whole disk restores, and individual file restores with a recovery time of no more than two hours.\nRDS Oracle is chosen as the database, and a Multi-AZ deployment should be used for high availability and automatic failover. This provides the necessary redundancy and resilience for the database. Additionally, automated daily backups should be enabled for the RDS instance, which will allow for fast and easy database recovery if needed.\nFor the web servers, application servers, and file systems, EC2 instances should be used, and they should be backed up using Amazon Machine Images (AMIs). AMIs are the best option for backing up entire instances since they can be easily restored and launched in case of a failure. This provides the necessary backups for whole server and whole disk restores.\nFor individual file restores, the file system data should be copied to Amazon S3, which can be achieved using the AWS DataSync service. This provides the necessary file-level restore capabilities, and with the use of S3 lifecycle policies, the data can be stored cost-effectively in the long term.\nOption A is incorrect because using traditional enterprise backup software to provide file-level restore is not the most cost-effective solution, and EBS snapshots are not suitable for backing up file-level data.\nOption C is also incorrect because using Amazon Glacier for file-level backup is not suitable for fast restores since it is a cold storage service designed for long-term archiving.\nOption D is incorrect because EBS snapshots are not suitable for backing up file-level data, and they do not provide the necessary backups for whole server restores.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Backup RDS using automated daily DB backups. Backup the EC2 instances using AMIs and supplement with file-level backup to S3 using traditional enterprise backup software to provide file-level restore.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Backup RDS using a Multi-AZ Deployment. Backup the EC2 instances using AMIs and supplement by copying file system data to S3 to provide file-level restore.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Backup RDS using automated daily DB backups. Backup the EC2 instances using EBS snapshots and supplement with file-level backups to Amazon Glacier using traditional enterprise backup software to provide file-level restore.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Backup the EC2 instances using AMIs and supplement with EBS snapshots for individual volume restore.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 222,
  "query" : "API gateway and Lambda proxy integrations have been chosen to implement an application by a software engineer.\nThe application is a data analysis tool that returns some statistical results when the HTTP endpoint is called.\nThe Lambda needs to communicate with some back-end data services such as Keen.io.\nHowever, there are chances that error happens, such as wrong data requested, bad communications, etc.\nThe Lambda is written using Java.\nTwo exceptions may be returned which are BadRequestException and InternalErrorException.\nWhat should the software engineer do to map these two exceptions in the API gateway with proper HTTP return codes? For example, BadRequestException and InternalErrorException are mapped to HTTP return codes 400 and 500 respectively.\nSelect 2.",
  "answer" : "E.\nCorrect Answer - B, D.\nWhen an API gateway is established, there are four parts.\nMethod Request/Method Response mainly deals with API gateways.\nThey are the API's interface with the API's frontend (a client), whereas Integration Request and Integration Response are the API's interface with the backend.\nIn this case, the backend is a Lambda.\nFor the mapping of exceptions that come from Lambda, Integration Response is the correct place to configure.\nHowever, the corresponding error code (400) on the method response should be created first.\nOtherwise, API Gateway throws an invalid configuration error response at runtime.\nBelow is an example of mapping BadRequestException to HTTP return code 400:\nOption A is incorrect: Because HTTP error codes are defined as firstly in Method Response instead of Integration Response.\nOption B is CORRECT: Because HTTP error codes are defined as firstly in Method Response instead of Integration Response.\n(Same reason as A).\nOption C is incorrect: Because Integration Response in API gateway should be used.\nRefer to https://docs.aws.amazon.com/apigateway/latest/developerguide/handle-errors-in-lambda-integration.html on “how to Handle Lambda Errors in API Gateway”.\nOption D is CORRECT: Because BadRequest or InternalError should be mapped to 400 and 500 in Integration Response settings.\nOption E is incorrect: Because Method Response is the interface with the frontend.\nIt does not deal with how to map the response from Lambda/backend.\nTo map exceptions in the API gateway with proper HTTP return codes, the software engineer should perform the following steps:\n1.\nAdd Integration Responses: In the API gateway, the software engineer should add Integration Responses, which define how to handle responses from the backend service. These responses can be associated with regular expression patterns, such as BadRequest or InternalError, and with corresponding HTTP status codes.\n2.\nAdd Method Responses: In the API gateway, the software engineer should add Method Responses, which define how to handle responses from the API endpoint. These responses can also be associated with regular expression patterns, such as BadRequest or InternalError, and with corresponding HTTP status codes, such as 400 and 500.\n3.\nAdd error codes on Integration Response: The software engineer should add the corresponding error codes (400 and 500) on the Integration Response in the API gateway. This ensures that when the backend service returns an error, the API gateway will map it to the appropriate HTTP status code.\n4.\nAdd error codes on Method Response: The software engineer should add the corresponding error codes (400 and 500) on the Method Response in the API gateway. This ensures that when the API endpoint returns an error, the API gateway will map it to the appropriate HTTP status code.\n5.\nPut the mapping logic into Lambda: The software engineer can put the mapping logic into the Lambda itself so that when an exception happens, error codes are returned at the same time in a JSON body. This approach provides more flexibility and control over the mapping logic.\nOverall, options A and B are correct answers. The software engineer should add Integration Responses and Method Responses and associate them with regular expression patterns and corresponding HTTP status codes. Options C, D, and E are incorrect as they either suggest putting the mapping logic into the Lambda or adding Integration Responses or Method Responses with regular expression patterns but not associating them with corresponding HTTP status codes.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Add the corresponding error codes (400 and 500) on the Integration Response in the API gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add the corresponding error codes (400 and 500) on the Method Response in the API gateway.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Put the mapping logic into Lambda itself so that when an exception happens, error codes are returned at the same time in a JSON body.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add Integration Responses where regular expression patterns are set, such as BadRequest or InternalError. Associate them with HTTP status codes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Add Method Responses where regular expression patterns are set, such as BadRequest or InternalError. Associate them with HTTP status codes 400 and 500.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 223,
  "query" : "A company needs to monitor the read and write IOPs metrics for their AWS MySQL RDS instance and send real-time alerts to their operations team.\nWhich AWS services can accomplish this?",
  "answer" : "E.\nAnswer - B and.\nE.Option A is incorrect as SNS would be a better choice for sending real-time notifications compared to SES.\nOption B is CORRECT because CloudWatch is used for monitoring the metrics pertaining to the AWS resources.\nOption C is incorrect because SQS can neither monitor any metrics nor send out any real-time notifications.\nOption D is incorrect because Route 53 cannot monitor any metrics.\nOption E is CORRECT because SNS is used for sending the real-time notifications based on the thresholds set in CloudWatch.\nFor more information on CloudWatch metrics, please refer to the link-\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CW_Support_For_AWS.html\nThe correct answer for this question is option B: Amazon CloudWatch, and option E: Amazon Simple Notification Service.\nAmazon CloudWatch is a monitoring service provided by AWS that allows users to monitor resources and applications on AWS cloud. CloudWatch can monitor metrics such as CPU utilization, network traffic, disk I/O, and more for various AWS services, including RDS. In this case, the company can use CloudWatch to monitor the read and write IOPs metrics for their AWS MySQL RDS instance.\nOnce the metrics are monitored, the company needs to send real-time alerts to their operations team. This can be done using Amazon Simple Notification Service (SNS). SNS is a messaging service provided by AWS that allows users to send messages or notifications to multiple recipients or subscribers. In this case, the company can configure CloudWatch alarms to send notifications to SNS topics whenever the read and write IOPs metrics exceed a certain threshold. The operations team can then subscribe to these SNS topics to receive real-time alerts about the issues.\nTherefore, the correct options to accomplish this task are B and E - Amazon CloudWatch and Amazon Simple Notification Service. Options A, C, and D are not relevant to this task.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Amazon Simple Email Service",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon CloudWatch",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon Simple Queue Service",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Route 53",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Simple Notification Service.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 224,
  "query" : "John is a software contractor and is working on a web application.\nSince the budget is limited and the schedule is tight, he decides to implement it using API gateway and Lambda so that he does not need to consider the server management, scalability, etc.\nThe customer has raised concerns that the APIs should be kept secure and there should be mechanisms to control the access to API endpoints.\nWhich below method can be used to help secure the API?",
  "answer" : "E.\nCorrect Answer - E.\nMultiple mechanisms can be used to control access to the API in the API gateway.\nAnd several methods can be used together to implement a very granular and secure application.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\nThe below mechanisms can be chosen.\nResource policies let you create resource-based policies to allow or deny access to your APIs and methods from the specified source IP addresses or VPC endpoints.\nIt can be configured in the API Gateway console:\nStandard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods.\nThe below is an IAM policy example of calling the Lambda function:\nLambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication and information described by headers, paths, query strings, stage variables, or context variables request parameters.\nIn the API Gateway console, lambda authorizers can be created in the below place.\nAmazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs.\nAs a result, option E is correct.\nSure, I'd be happy to explain the answer to this question in detail!\nTo recap, John is developing a web application using API Gateway and Lambda, and the customer is concerned about security and controlling access to API endpoints. The question asks which method can be used to help secure the API, and provides five possible answers. Let's go through each option and explain what it entails.\nA. Attach a resource policy to the API Gateway API, which controls access to the API Gateway resources. Access can be controlled by IAM condition elements, including conditions on AWS account, source VPC, etc.\nThis answer suggests using a resource policy to control access to the API Gateway API. A resource policy is a JSON document that specifies who has access to the API Gateway resources, and what actions they can perform. It can be used to control access based on a variety of conditions, including the AWS account making the request, the IP address of the client, the HTTP method being used, and more. Resource policies can be attached to API Gateway APIs, stages, and methods, and can be used to grant or deny access.\nIn this case, the answer suggests using IAM condition elements to control access. IAM condition elements allow you to specify additional conditions that must be met before a request is allowed. For example, you could require that the request come from a specific IP address range, or that the request includes a certain header or query string parameter. By using IAM condition elements in conjunction with a resource policy, you can create very granular access control rules.\nB. Use IAM permissions to control access to the API Gateway component. For example, in order to call a deployed API, the API caller must be granted permission to perform required IAM actions supported by the API execution component of API Gateway.\nThis answer suggests using IAM permissions to control access to the API Gateway component. IAM permissions are used to control access to AWS resources, including API Gateway. By default, IAM users do not have access to API Gateway, so you will need to grant them permissions in order to use it.\nTo control access to API Gateway using IAM permissions, you can create an IAM policy that grants or denies access to specific API Gateway resources. For example, you could create a policy that allows a user to call a specific API, but not to create or delete APIs. IAM policies can be very granular, allowing you to specify exactly what actions a user is allowed to perform on a given resource.\nC. Use a Lambda function as the authorizer. When a client calls the API, the API Gateway either supplies the authorization token that is extracted from a specified request header for the token-based authorizer or it passes in the incoming request parameters as the input to the request parameters-based authorizer Lambda function.\nThis answer suggests using a Lambda function as the authorizer for the API. An authorizer is a Lambda function that authenticates and authorizes incoming API requests. When a client makes a request to the API, the authorizer function is invoked to determine whether the client is allowed to access the requested resource.\nThere are two types of authorizers in API Gateway: token-based and request parameter-based. Token-based authorizers extract a token from a specified header in the request and pass it to the authorizer function, while request parameter-based authorizers pass the entire request to the function. The authorizer function then returns an IAM policy document that specifies whether the client is authorized to access the requested resource.\nUsing a Lambda function as an authorizer allows you to implement custom authentication and authorization logic. For example, you could use a Lambda function to authenticate users against a database, or to check whether they have the necessary permissions to access a resource.\nD. Use an Amazon Cognito user pool to control who can access the API in Amazon API Gateway. You need to use the Amazon",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Attach a resource policy to the API Gateway API, which controls access to the API Gateway resources. Access can be controlled by IAM condition elements, including conditions on AWS account, source VPC, etc.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use IAM permissions to control access to the API Gateway component. For example, in order to call a deployed API, the API caller must be granted permission to perform required IAM actions supported by the API execution component of API Gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a Lambda function as the authorizer. When a client calls the API, the API Gateway either supplies the authorization token that is extracted from a specified request header for the token-based authorizer or it passes in the incoming request parameters as the input to the request parameters-based authorizer Lambda function.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use an Amazon Cognito user pool to control who can access the API in Amazon API Gateway. You need to use the Amazon Cognito console, CLI/SDK, or API to create a user pool. Then, in the API Gateway, create an API Gateway authorizer with the chosen user pool.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All the above options are correct.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 225,
  "query" : "You have multiple Amazon EC2 instances running in various clusters within the same region.\nHow will you ensure that the EC2 instances will communicate between the clusters without any bandwidth restrictions and also perform with the highest network performance, low latency, and low jitter?",
  "answer" : "E.\nF.\nAnswer - A, B, and D.\nOption A is CORRECT because Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both.\nThe majority of the network traffic is between the instances in the group.\nOption B is CORRECT because Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types.\nOption C is incorrect because it is recommended to use HVM AMIs for better performance than PV AMIs.\nD.\nOption D is CORRECT because HVM AMIs are presented with a fully virtualized set of hardware and boot by executing the master boot record of the root block device of your image.\nThis virtualization type provides the ability to run an operating system directly on top of a virtual machine without any modification as if it were run on the bare-metal hardware.\nThe Amazon EC2 host system emulates some or all of the underlying hardware presented to the guest.\nHVM guests can take advantage of hardware extensions that provide fast access to the underlying hardware on the host system.\nOption E is incorrect because using Amazon Linux does not necessarily improve any performance.\nOption F is incorrect because if we enable a VPC Endpoint connection from a VPC EC2 instance to other AWS services, then the users experience slowness.\nFor more information on Enhanced Networking, please visit the URL\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\nLinux Amazon Machine Images use one of two types of virtualization: paravirtual (PV) or hardware virtual machine (HVM)\nThe main difference between PV and HVM AMIs is the way in which they boot and whether they can take advantage of special hardware extensions (CPU, network, and storage) for better performance.\nWe recommend that you use current generation instance types and HVM AMIs when you launch your instances for the best performance.\nFor more information on Enhanced Networking, please visit the URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html\nTo ensure that the EC2 instances will communicate between the clusters with high network performance, low latency, and low jitter, and without any bandwidth restrictions, the recommended solution is to use a Cluster placement group.\nA Cluster placement group is a logical grouping of instances within a single Availability Zone (AZ) that allows for low-latency, high-bandwidth connections between instances. The instances in a placement group are placed on hardware that is physically close together, which minimizes network latency and jitter, and maximizes network throughput.\nBy using a Cluster placement group, you can ensure that your EC2 instances are co-located and can communicate with each other over a high-speed, low-latency network. This is particularly important for applications that require high network performance and low latency, such as distributed databases, high-performance computing, and real-time analytics.\nEnhanced networking is another option to improve network performance, which utilizes SR-IOV (Single Root I/O Virtualization) to provide higher bandwidth, lower latency, and lower CPU utilization for network-intensive workloads. However, this option doesn't provide any guarantees on placement of instances within a region, as a cluster placement group does.\nAmazon PV AMI and Amazon HVM AMI are different virtualization types available for EC2 instances, and they don't provide any direct impact on networking performance between clusters.\nAmazon Linux is a popular operating system that can be used on EC2 instances, but it doesn't provide any specific optimization for inter-cluster communication.\nFinally, Amazon VPC Endpoints is a service that enables private connectivity between VPCs and other AWS services without using public IP addresses. While this service can improve network security and reduce data transfer costs, it doesn't provide any guarantees on network performance or placement of instances within a region.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Cluster placement group",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Enhanced networking",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon PV AMI",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon HVM AMI",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon Linux",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon VPC Endpoints.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 226,
  "query" : "You have a video transcoding application running on Amazon EC2\nEach instance polls a queue to find out which video should be transcoded and then runs a transcoding process.\nIf this process is interrupted, the videos will be transcoded by another instance based on the queuing system.\nYou have a large backlog of videos that need to be transcoded and reduce this backlog by adding more instances.\nYou will need these instances only until the backlog is reduced.\nWhich type of Amazon EC2 instances should you use to reduce the backlog in the most cost-efficient way?",
  "answer" : "Answer - B.\nSince this is like a batch processing job, the best type of instance to use is a Spot instance.\nSince these jobs don't last for the entire duration of the year, they can bid upon and be allocated and deallocated as requested.\nOptions A and C are incorrect because the application needs the instances only until the backlog is reduced.\nWith reserved/dedicated instances, there is a possibility that the instances might get idle after the backlog reduction.\nSo, this is a costly solution.\nOption B is CORRECT because (i) they are less expensive than reserved instances, (ii) interruption in the transcoding process is affordable since another instance will transcode the videos based on the queuing system.\nOption D is incorrect because (i) on-demand instances are most expensive, (ii) you can afford an interruption in the transcoding process, and (iii) on-demand instances would have been suited if there was no alternate way of transcoding the videos and interruption was not affordable.\nFor more information on Spot Instances, please visit the URL -\nhttps://aws.amazon.com/ec2/spot/\nThe most cost-efficient way to reduce the backlog of video transcoding using Amazon EC2 instances is by using spot instances.\nSpot instances are unused EC2 instances that are available for less than the On-Demand price. AWS allows users to bid for these instances at a price they are willing to pay. The spot price varies based on the supply and demand of available instances in a given Availability Zone. When the spot price goes above the user's bid, the instance is terminated.\nUsing spot instances for this workload is advantageous because the workload is interruptible, and there is no loss of data if the instance is terminated. Since the video transcoding process can be picked up by another instance, using spot instances allows for cost savings by bidding on the spot instances at a lower price than On-Demand instances.\nReserved instances, on the other hand, are best suited for long-term workloads where there is a predictable and consistent usage pattern. Since the workload in this case is expected to be short-term and unpredictable, using Reserved instances may not be cost-efficient.\nDedicated instances, which provide dedicated hardware for an instance, are best suited for compliance or regulatory requirements. Since the workload in this case is not a compliance or regulatory requirement, using Dedicated instances may not be necessary.\nFinally, On-Demand instances are ideal for workloads that require short-term, unpredictable usage patterns, but they are more expensive than Spot instances. Therefore, using On-Demand instances may not be cost-efficient in this scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Reserved instances",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Spot instances",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Dedicated instances",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "On-demand instances.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 227,
  "query" : "A company has run a major auction platform where people buy and sell a wide range of products.\nThe platform requires that transactions from buyers and sellers get processed in exactly the order received.\nAt the moment, the platform is implemented using RabbitMQ which is a light-weighted queue system.\nThe company consulted you to migrate the on-premise platform to AWS.\nHow should you design the migration plan? (Select TWO)",
  "answer" : "Correct Answer - A, C.\nSQS has 2 types - standard queue and FIFO queue.\nIn this case, the FIFO queue should be chosen as the order of message processing is critical to the application.\nFIFO queue has the below key features.\nOption A is CORRECT: Because the SQS FIFO queue can help with the message processing in the right order.\nOption B is incorrect: Because the SQS standard queue may have an issue that some messages are handled in the wrong sequence.\nOption C is CORRECT: Because the message group ID is a feature to help with the FIFO delivery.\nMessages that belong to the same message group are always processed one by one, in a strict order relative to the message group.\nOption D is incorrect: Because deduplication ID is a method to help on preventing messages to be processed duplicately, which is not used to guarantee the message order.\nThe key requirement for this migration is to ensure that transactions are processed in the order received. Since the current system is using RabbitMQ, a lightweight queue system, one way to maintain the same order is to migrate to an AWS queue service that supports ordering of messages.\nOption A and B both suggest using Amazon Simple Queue Service (SQS) as the queue system for the migrated platform. However, they differ in the type of SQS queue to be used.\nOption A suggests using an SQS FIFO (First-In-First-Out) queue, which is designed to maintain strict message order. This is an appropriate solution for the use case, as it ensures that the transactions are processed in the exact order received. Additionally, SQS FIFO queues guarantee that messages are delivered exactly once, which is important in the context of processing transactions.\nOption B suggests using an SQS standard queue, which is a highly scalable and reliable queue service. However, unlike SQS FIFO queues, the standard queue does not guarantee message ordering. While it's possible to ensure some degree of ordering by using message group IDs, it is not guaranteed to maintain the exact order of transactions received.\nOption C suggests using message group IDs to ensure message processing in strict order, but this is only relevant when using an SQS standard queue, as FIFO queues maintain strict order by design.\nOption D suggests using an EC2 or Lambda function to add a deduplication ID to messages before they are sent to the SQS queue. This ensures that only unique messages are processed, avoiding duplication. However, it does not address the requirement for strict message order.\nIn summary, options A and C are the correct choices for designing the migration plan. Specifically, the platform should use an SQS FIFO queue to maintain strict message order, and message group IDs can be added to ensure that messages within a group are processed in the correct order.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "When the bids are received, send the bids to an SQS FIFO queue before they are processed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "When the users have submitted the bids from the frontend, the backend service delivers the messages to an SQS standard queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a message group ID to the messages before they are sent to the SQS queue so that the message processing is in a strict order.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use an EC2 or Lambda to add a deduplication ID to the messages before the messages are sent to the SQS queue to ensure that bids are processed in the right order.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 228,
  "query" : "You deployed your company website using Elastic Beanstalk, and you enabled log file rotation to S3\nAn Elastic MapReduce job is periodically analyzing the logs on S3 to build a usage dashboard that you share with your CIO.\nYou recently improved the website's overall performance using CloudFront for dynamic content delivery and your website as the origin.\nAfter this architectural change, the usage dashboard shows that the traffic on your website dropped by order of magnitude.\nHow do you fix your usage dashboard?",
  "answer" : "E.\nAnswer - A.\nOption A is CORRECT because the website is now only accessible via CloudFront.\nSo, for the dashboard to have the up-to-date information via EMR, the logs from the CloudFront must be stored on S3 (to be analyzed by the EMR)\nOnce these logs are delivered to S3, the dashboard should show the correct traffic information.\nOption B is incorrect because the CloudTrail log will not show the required information.\nIt will only show the insights of the AWS services and APIs accessed by the application.\nOption C is incorrect because the dashboard must be showing the information about the traffic pertaining to the website.\nCloudWatch will show the information based on the metrics related to AWS resources (not the website).\nOption D is incorrect because the configuration of the Elastic Beanstalk environment is independent of the CloudFront setting.\nIn order to have the information related to the dynamic content, the logs created by CloudFront must be delivered to S3\n“Rebuild Environment” of Elastic Beanstalk will not be of any use.\nOption E is incorrect because “Restart App Server(s)” causes the environment to restart the application container server running on each Amazon EC2 instance.\nIt is totally unrelated to the information that is shown by the dashboard.\nThe correct answer is A. Allow CloudFront logs to be delivered to S3 and use them as input of the Elastic MapReduce job.\nExplanation: The reason the usage dashboard shows a drop in traffic is that CloudFront serves cached content directly to end-users instead of forwarding requests to the origin server. This behavior reduces the amount of traffic the Elastic Beanstalk environment receives, resulting in a decrease in log volume. To fix the usage dashboard, we need to use CloudFront access logs as input for the Elastic MapReduce job, as these logs contain detailed information about user requests.\nOption B, Turn on CloudTrail and use trail log tiles on S3 as input of the Elastic MapReduce job, is incorrect because CloudTrail logs are not suitable for analyzing website usage. CloudTrail logs capture AWS API calls made by users, applications, and AWS services, but they do not contain information about website requests.\nOption C, Change your log collection process to use CloudWatch ELB metrics as input of the Elastic MapReduce job, is incorrect because CloudWatch ELB metrics provide aggregated statistics about network traffic, HTTP errors, and response time, but they do not contain detailed information about user requests.\nOption D, Use Elastic Beanstalk \"Rebuild Environment\" option to update log delivery to the Elastic MapReduce job, is incorrect because rebuilding the Elastic Beanstalk environment does not change the log delivery process. The log rotation settings are configured at the Elastic Beanstalk environment level and apply to all instances in the environment.\nOption E, Use Elastic Beanstalk 'Restart App Server(s)\" option to update log delivery to the Elastic MapReduce job, is incorrect because restarting the app server(s) does not change the log delivery process. The log rotation settings are configured at the Elastic Beanstalk environment level and apply to all instances in the environment.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Allow CloudFront logs to be delivered to S3 and use them as input of the Elastic MapReduce job.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Turn on CloudTrail and use trail log tiles on S3 as input of the Elastic MapReduce job.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Change your log collection process to use CloudWatch ELB metrics as input of the Elastic MapReduce job.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Elastic Beanstalk \"Rebuild Environment\" option to update log delivery to the Elastic MapReduce job.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Elastic Beanstalk `Restart App Server(s)\" option to update log delivery to the Elastic MapReduce job.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 229,
  "query" : "You decide to configure a bucket for static website hosting.\nAs per the AWS documentation, you create a bucket named 'mybucket.com'\nYou also enable website hosting with an index document of 'index.html' and leave the error document as blank.\nYou then upload a file named 'index.html' to the bucket.\nAfter clicking on the endpoint of mybucket.com.s3-website-us-east-1.amazonaws.com you receive 403 Forbidden error.\nYou then change the CORS configuration on the bucket so that everyone has access.\nHowever, you still receive the 403 Forbidden error.\nWhat additional step do you need to do so that the endpoint is accessible to everyone?",
  "answer" : "Answer - D.\nYou are receiving the 403 Forbidden Error because you do not have the permissions to view the index.html file.\nOption A is incorrect because this is an S3 hosted website.\nRoute 53 does not come into the picture.\nOption B is incorrect because it is a static website hosted on S3\nThis issue is not related to DNS resolution.\nOption C is incorrect.\nEven if you add the error document, you will get the error because you need to set the proper permissions.\nOption D is CORRECT because it sets the appropriate permissions so that the user has access to the index.html.\nFor more information on web site hosting in S3, please visit the below link:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\nNote:\nThe question is referring to Configuring a bucket for Website Hosting.\nIn this scenario, Route53 is not required.\nHowever, extra configuration in S3 is needed other than making it public.\nFor more information, please refer to:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/IndexDocumentSupport.html\nThe 403 Forbidden error indicates that the user making the request does not have the necessary permissions to access the requested resource. In this case, you have enabled static website hosting on an S3 bucket named mybucket.com, configured the index document as index.html, and left the error document as blank. You have also uploaded an index.html file to the bucket.\nWhen you click on the endpoint of mybucket.com.s3-website-us-east-1.amazonaws.com, you receive a 403 Forbidden error, even though you have changed the CORS configuration on the bucket to allow everyone access. This suggests that the issue is not related to the CORS configuration, but rather some other permission-related issue.\nTo resolve this issue, you need to ensure that the objects in your bucket are publicly accessible. One way to achieve this is to modify the bucket policy to allow public access to the objects in the bucket.\nTo modify the bucket policy:\n1. Open the S3 console and navigate to the bucket mybucket.com.\n2. Click on the \"Permissions\" tab.\n3. Click on the \"Bucket Policy\" button.\n4. Paste the following policy into the policy editor, replacing mybucket.com with the name of your bucket:\njsonCopy code{   \"Version\":\"2012-10-17\",   \"Statement\":[     {       \"Sid\":\"PublicReadGetObject\",       \"Effect\":\"Allow\",       \"Principal\": \"*\",       \"Action\":[\"s3:GetObject\"],       \"Resource\":[\"arn:aws:s3:::mybucket.com/*\"]     }   ] } \n1. Click on \"Save changes\".\nThis policy grants public read access to all objects in the mybucket.com bucket. After saving the policy, you should be able to access the website at mybucket.com.s3-website-us-east-1.amazonaws.com without receiving a 403 Forbidden error.\nOption A (Register mybucket.com on Route53) and option B (Wait for the DNS change to propagate) are not relevant to this issue, as the issue is related to permissions rather than DNS. Option C (Add a name for the error document) is also not relevant, as leaving the error document field blank is a valid configuration for a static website hosted on S3. Option D (Change the permissions on the index.html file) may help in some cases, but in this case, it is not sufficient to resolve the issue.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Register mybucket.com on Route53",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Wait for the DNS change to propagate",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You need to add a name for the error document, because it is a required field",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Change the permissions on the index.html file also, so that everyone has access.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 230,
  "query" : "Server-side encryption is about data encryption at rest.\nThat is, Amazon S3 encrypts your data at the object level as it writes it to disk in its data centers and decrypts it for you when you go to access it.\nA few different options are depending on how you choose to manage the encryption keys.\nOne of the options is called 'Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)'\nWhich of the following best describes how this encryption method works?",
  "answer" : "Answer - B.\nServer-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption.\nAmazon S3 encrypts each object with a unique key.\nAs an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.\nAmazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\nOption A is incorrect because there are no separate permissions to the key that protects the data key.\nOption B is CORRECT because as mentioned above, each object is encrypted with a strong unique key and that key itself is encrypted by a master key.\nOption C is incorrect because the keys are managed by the AWS.\nOption D is incorrect because there is no randomly generated key and the client does not do the encryption.\nFor more information on S3 encryption, please visit the links-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\nThe correct answer is C: \"You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disk, and decryption when you access your objects.\"\nServer-Side Encryption with Amazon S3-Managed Keys (SSE-S3) is a server-side encryption method in which Amazon S3 manages the encryption process for objects stored in S3, but the encryption keys are managed by the user. This method uses Advanced Encryption Standard (AES) 256-bit encryption to encrypt data at rest.\nWhen you upload an object to Amazon S3 with SSE-S3 enabled, Amazon S3 encrypts the object using a unique encryption key, which is also referred to as a data encryption key (DEK). The DEK is generated using the AES-256 encryption algorithm, and it is used to encrypt the object data. The DEK is then encrypted with a master key, which is managed by Amazon S3, and stored alongside the encrypted object. This ensures that the DEK is not accessible by unauthorized parties.\nWhen you want to access an object that has been encrypted with SSE-S3, Amazon S3 retrieves the encrypted object and the encrypted DEK. It then decrypts the DEK using the master key and uses the DEK to decrypt the object data. The decrypted object is then made available to the user.\nIt's worth noting that SSE-S3 does not provide any additional protection against unauthorized access to your objects beyond what is already provided by AWS Identity and Access Management (IAM) permissions. SSE-S3 is primarily intended to protect your data at rest, in case your storage media is lost or stolen.\nIn summary, SSE-S3 provides a simple way to enable server-side encryption for objects stored in Amazon S3. With SSE-S3, you manage the encryption keys while Amazon S3 manages the encryption and decryption of your objects. This ensures that your data is encrypted and protected at rest.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "There are separate permissions for the use of an envelope key (a key that protects your data`s encryption key) that provides added protection against unauthorized access of your objects in S3 and also provides you with an audit trail of when your key was used and by whom.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Each object is encrypted with a unique key employing strong encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disk, and decryption when you access your objects.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A randomly generated data encryption key is returned from Amazon S3, which is used by the client to encrypt the object data.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 231,
  "query" : "Your company is running a high-volume online e-commerce site.\nThe main database is deployed in RDS MySQL configured with Multi-AZ and few read replicas.\nThe main website runs on EC2 instances behind a load balancer, and the setup also includes a cluster of Redis to offload repeated queries from the main database.\nThe backup window is set in the early morning when the traffic is usually very low.\nDue to the holiday season, the site is receiving a huge amount of traffic during the daytime.\nThe development team has recently finished a critical feature and delivered it to the QA team for the final testing.\nThe management has decided to perform the full-scale DR exercise around midnight before rolling out new features.\nThe resources in one Availability Zone will be taken down during the test.\nSelect the option which would restore the RDS database to the latest point.",
  "answer" : "Correct Answer: C.\nOption A is INCORRECT because restoring from the last automated snapshot will not be the latest data.\nAs per the timeline mentioned, the backup window is around early morning, and the exercise time is around midnight.\nSo the automated snapshot will be nearly a day old.\nOption B is INCORRECT because it does not restore the database to the latest point.\nOption C is CORRECT.\nBecause as Multi-AZ is enabled, when the primary DB instance has an issue, it will automatically switch to the standby replica in another AZ.\nOption D is INCORRECT because there is no Copy Database option available in the RDS.\nIn this scenario, the company is running a high-volume online e-commerce site with a main database deployed in RDS MySQL configured with Multi-AZ and a few read replicas. The main website runs on EC2 instances behind a load balancer, and there is also a cluster of Redis to offload repeated queries from the main database. The backup window is set in the early morning when the traffic is usually very low. Due to the holiday season, the site is receiving a huge amount of traffic during the daytime. The development team has recently finished a critical feature and delivered it to the QA team for the final testing. The management has decided to perform the full-scale DR exercise around midnight before rolling out new features. The resources in one Availability Zone will be taken down during the test.\nIn the event of a disaster, it is important to have a backup and recovery plan in place to restore the RDS database to the latest point. There are several options available to restore the RDS database to the latest point, including:\nA. Restore the database from the last automated snapshot onto the new database. This option involves restoring the database from the last automated snapshot onto a new database. The snapshot is taken automatically by RDS during the backup window. However, this option will result in some data loss as the restored database will only contain data up until the time of the last snapshot.\nB. Restore the database by promoting the read-replica and modify the application to use the new DB instance as the database master. This option involves promoting one of the read replicas to become the new master database instance. The application is then modified to use the new DB instance as the database master. This option has minimal data loss and minimal downtime, as the read replica is already up-to-date with the master database instance.\nC. Amazon RDS automatically switches to the standby replica in another Availability Zone. This option involves relying on Amazon RDS to automatically switch to the standby replica in another Availability Zone if the primary database instance becomes unavailable. This option has minimal data loss and minimal downtime, but it is not suitable for a planned DR exercise.\nD. Use the RDS Copy Database option to create a new database from an existing database. This option involves using the RDS Copy Database option to create a new database from an existing database. This option has minimal data loss and minimal downtime, as the new database will be a copy of the existing database at a specific point in time.\nGiven the scenario described, the best option would be B. Restore the database by promoting the read-replica and modify the application to use the new DB instance as the database master. This option has minimal data loss and minimal downtime, as the read replica is already up-to-date with the master database instance. Option A would result in data loss, Option C is not suitable for a planned DR exercise, and Option D would take longer and result in more downtime.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Restore the database from the last automated snapshot onto the new database.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Restore the database by promoting the read-replica and modify the application to use the new DB instance as the database master.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon RDS automatically switches to the standby replica in another Availability Zone.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the RDS Copy Database option to create a new database from an existing database.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 232,
  "query" : "A customer implemented AWS Storage Gateway with a gateway-cached volume at their main office.\nYou need to restore the Storage Gateway data in AWS.\nHow would you implement this?",
  "answer" : "Answers - D.\nOption A is incorrect because all gateway-cached volume data and snapshot data are stored in Amazon S3 encrypted at rest using server-side encryption (SSE) and cannot be visible or accessed with S3 API or any other tools.\n(Ref: https://forums.aws.amazon.com/thread.jspa?threadID=109748)\nOption B is incorrect because the lifecycle policy does not help to restore the data.\nOption C is incorrect because the cached volumes are never stored in Glacier.\nOption D is CORRECT because you can take point-in-time snapshots of gateway volumes made available in the form of Amazon EBS snapshots.\nA new EBS volume can be created from the snapshot which can be mounted to an existing EC2 instance.\nFor more information on this topic, please refer to the AWS FAQs.\nhttps://aws.amazon.com/storagegateway/faqs/\nTo restore the data in AWS, the best approach will depend on the configuration and use case of the AWS Storage Gateway deployment.\nIn the given scenario, the customer implemented a gateway-cached volume at their main office. This means that frequently accessed data is stored in the local cache at the office, and less frequently accessed data is stored in Amazon S3. So, if data needs to be restored, it may be located in the Amazon S3 bucket.\nOption A suggests using an HTTPS GET to the Amazon S3 bucket where the files are located. This could work if the data is readily accessible in the S3 bucket and can be downloaded directly. However, it may not be the most efficient option for large amounts of data or if the data is stored in an encrypted format.\nOption B suggests restoring by implementing a lifecycle policy on the Amazon S3 bucket. A lifecycle policy can automatically transition data to Amazon S3 Glacier for archival or deletion based on rules that the user sets. While this can be a useful feature for managing data lifecycle, it may not be the best approach for restoring specific data quickly.\nOption C suggests making an Amazon Glacier Restore API call to load the files into another Amazon S3 bucket within four to six hours. This option can be useful if the data is stored in Amazon Glacier and needs to be restored to S3 before it can be accessed. However, it may not be the fastest option for restoring data.\nOption D suggests creating an Amazon EBS volume from a gateway snapshot and mounting it to an Amazon EC2 instance. This can be a good option if the data is stored in the gateway snapshot and needs to be accessed quickly. This option can provide faster access to data than downloading from S3 or restoring from Glacier, but it may not be the most cost-effective option for large amounts of data.\nOverall, the best option will depend on the specific situation and needs of the customer. If the data is stored in the S3 bucket, option A may be the most straightforward approach. If the data is stored in a gateway snapshot, option D may provide the fastest access to the data. If the data is stored in Glacier, option C may be the most appropriate option.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use an HTTPS GET to the Amazon S3 bucket where the files are located.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Restore by implementing a lifecycle policy on the Amazon S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Make an Amazon Glacier Restore API call to load the files into another Amazon S3 bucket within four to six hours.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an Amazon EBS volume from a gateway snapshot and mount it to an Amazon EC2 instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 233,
  "query" : "You created three S3 buckets - “mydomain.com” ( root domain ), “downloads.mydomain.com” ( subdomain ), and “www.mydomain.com” ( subdomain )\nYou uploaded your files ( including the \"index.html\" and \"error.html\" files ), enabled static website hosting, specified both of the default documents under the “enable static website hosting” header, and set the “Make Public” permission for the objects in each of the three buckets.\nAfter creating Route 53 Aliases for the three buckets, you will have your end-users test your websites by browsing to http://mydomain.com/error.html, http://downloads.mydomain.com/index.html, and http://www.mydomain.com.\nWhat problems will your testers encounter?",
  "answer" : "Answer - C.\nPreviously we are only allowed domain prefix, when creating AWS Route53 aliases for AWS S3 static websites was the “www”.\nHowever, this is no longer the case.\nYou can now use other sub-domains.\nFor more information on S3 web site hosting, please visit the below link-\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\nBased on the information provided, you created three S3 buckets: \"mydomain.com\" for the root domain, \"downloads.mydomain.com\" for the subdomain, and \"www.mydomain.com\" for another subdomain. You uploaded files, including the \"index.html\" and \"error.html\" files, enabled static website hosting, specified both of the default documents under the \"enable static website hosting\" header, and set the \"Make Public\" permission for the objects in each of the three buckets.\nAfter creating Route 53 Aliases for the three buckets, your end-users will test your websites by browsing to the following URLs:\nhttp://mydomain.com/error.html\nhttp://downloads.mydomain.com/index.html\nhttp://www.mydomain.com\nLet's examine each URL and see if there are any problems:\n1.\nhttp://mydomain.com/error.html This URL should work if you have set the \"index.html\" and \"error.html\" files as the default documents under the \"enable static website hosting\" header. Therefore, Option A is incorrect.\n2.\nhttp://downloads.mydomain.com/index.html This URL should work if you have created the \"downloads.mydomain.com\" subdomain and created an S3 bucket with that name. Additionally, you must have set up Route 53 Aliases correctly for the subdomain. Therefore, Option D is incorrect.\n3.\nhttp://www.mydomain.com This URL may not work because it does not include a file name at the end of it. By default, S3 will look for \"index.html\" as the default document if no file is specified in the URL. If you have set \"index.html\" as the default document for the \"www.mydomain.com\" bucket, this URL should work. However, if you have not set the default document, this URL may not work. Therefore, Option B is correct.\n4.\nAll three sites should work. Option C is incorrect because as explained above, there is a possibility that the URL \"http://www.mydomain.com\" may not work if the default document is not set up correctly.\nIn conclusion, the correct answer is Option B: http://www.mydomain.com will not work because the URL does not include a file name at the end of it.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "http://mydomain.com/error.html will not work because you did not set a value for the error.html file.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "http://www.mydomain.com will not work because the URL does not include a file name at the end of it.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "There will be no problems, all three sites should work.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "http://downloads.mydomain.com/index.html will not work because the “downloads” prefix is not a supported prefix for S3 websites using Route 53 aliases.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 234,
  "query" : "Your supervisor is upset about the fact that SNS topics that he subscribed are now cluttering up his email inbox.\nHow can he stop receiving the email from SNS without disrupting other users' ability to receive the email from SNS?",
  "answer" : "Answer - A and D.\nEvery request has a unsubscribe URL that can be used.\nAlso, from the AWS console, one can just delete the subscription.\nOption A is CORRECT because deleting the user's subscription from the SNS topic will ensure that he will not receive any notifications (basically just unsubscribing him).\nOption B is incorrect because you cannot delete the endpoint from the SNS subscription.\nOption C is incorrect because if you delete the topic, none of the subscribers will get any notifications.\nOption D is CORRECT because the notifications have an option to unsubscribe which the user can avail to stop receiving the notifications.\nFor more information on SNS subscription, please visit the below link.\nhttp://docs.aws.amazon.com/sns/latest/api/API_Subscribe.html\nThe solution to this problem is to remove the subscription from the SNS topic for your supervisor without affecting other users who still need to receive email notifications from the SNS topic. Therefore, options A, B, and C are not viable since they would remove the subscription or the topic, thereby disrupting other users' ability to receive email notifications.\nThe correct solution is option D. The unsubscribe information provided in the emails allows your supervisor to stop receiving email notifications without affecting other users' ability to receive the emails. By clicking the unsubscribe link in the email, your supervisor can opt-out of receiving future email notifications from the SNS topic while leaving the topic and other subscriptions intact for other users.\nIt is important to note that removing a subscription or deleting a topic should be done with caution as it can have unintended consequences for other users who rely on those notifications. In addition, it is recommended to review the SNS topic's subscription policies to ensure that the appropriate users are subscribed to the topic and receiving notifications relevant to their roles.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "He can delete the subscription from the SNS topic responsible for the emails.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "He can delete the endpoint from the SNS subscription responsible for the emails.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "He can delete the SNS topic responsible for the emails.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "He can use the unsubscribe information provided in the emails.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 235,
  "query" : "You have created an Elastic Load Balancer with Duration-Based sticky sessions enabled in front of your six EC2 web application instances in US-West-2\nFor High Availability, there are three web application instances in Availability Zone 1 and three web application instances in Availability Zone 2\nTo load test, you set up a software-based load tester in Availability Zone 2 to send traffic to the Elastic Load Balancer, as well as letting several hundred users browse to the ELB's hostname.",
  "answer" : "Answer - B and C.\nWhen you create an elastic load balancer, a default level of capacity is allocated and configured.\nAs Elastic Load Balancing sees changes in the traffic profile, it will scale up or down.\nThe time required for Elastic Load Balancing to scale can range from 1 to 7 minutes, depending on the changes in the traffic profile.\nWhen Elastic Load Balancing scales, it updates the DNS record with the new list of IP addresses.\nTo ensure that clients are taking advantage of the increased capacity, Elastic Load Balancing uses a TTL setting on the DNS record of 60 seconds.\nIt is critical that you factor this changing DNS record into your tests.\nIf you do not ensure that DNS is re-resolved or use multiple test clients to simulate increased load, the test may continue to hit a single IP address when Elastic Load Balancing has actually allocated many more IP addresses.\nBecause your end-users will not all be resolving to that single IP address, your test will not be a realistic sampling of real-world behavior.\nOption A is incorrect because creating a load tester in US-East-1 is inconsequential since it is in a different region.\nOption B is CORRECT because if you do not ensure that DNS is re-resolved, the test may continue to hit the single IP address.\nOption C is CORRECT because if the requests come from globally distributed users, the DNS will not be resolved to a single IP address.\nThe traffic would be distributed evenly across multiple instances.\nOption D is incorrect because the traffic will be routed to the same back-end instances as the users continue to access your application.\nThe load will not be evenly distributed across the AZs.\nPlease refer to the below article for more information.\nhttp://aws.amazon.com/articles/1636185810492479\nThe scenario described above involves an Elastic Load Balancer with Duration-Based sticky sessions enabled in front of six EC2 web application instances spread across two Availability Zones. A software-based load tester is used to simulate traffic to the Elastic Load Balancer and several hundred users browse to the ELB's hostname.\nThe objective of load testing is to identify potential bottlenecks in the application or infrastructure under test, determine the maximum capacity that can be handled by the system, and measure the response time and performance metrics under different traffic scenarios.\nOption A suggests creating a software-based load tester in US-East-1 and testing from there. However, this option does not address the high availability aspect of the architecture and may not accurately reflect the real-world scenario where users may be accessing the application from different locations.\nOption B suggests forcing the software-based load tester to re-resolve DNS before every request. This option can help distribute the load across multiple EC2 instances behind the load balancer, but it does not address the need for global testing and may not accurately reflect the performance in real-world scenarios.\nOption C suggests using a third-party load-testing service to send requests from globally distributed clients. This option can help simulate real-world traffic patterns, identify regional performance issues, and provide detailed performance metrics. However, this option may incur additional costs and may require additional setup and configuration.\nOption D suggests switching to application-controlled sticky sessions. This option can help distribute the load across multiple EC2 instances based on the application's logic, but it may not be suitable for all applications and may require additional development effort.\nIn summary, Option C is the most suitable answer as it can provide a more realistic load testing scenario, identify potential performance issues, and provide detailed performance metrics from globally distributed clients. However, depending on the specific requirements and constraints of the project, other options may also be considered.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a software-based load tester in US-East-1 and test from there.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Force the software-based load tester to re-resolve DNS before every request.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use a third party load-testing service to send requests from globally distributed clients.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Switch to application-controlled sticky sessions.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 236,
  "query" : "A Fintech company has its major applications deployed in AWS US East (N.\nVirginia) region.\nSince the applications need extremely fast processing, stable and high speed of network broadband is required.\nA Direct Connect has been set up in the region which connects its on-premises network with the Virtual Private Gateway in AWS VPC.\nDirect Connect has already run for two months smoothly.\nNow, as the business grows, new applications have been deployed in AWS US West region VPC recently.\nThey also need to communicate with the on-premises datacenter which is located in Virginia using a high-speed network.\nCan the existing Direct Connect be reused for the VPC in the AWS US West region?",
  "answer" : "C.\nF.\nCorrect Answer - C.\nDirect Connect Gateway is a service that helps on connecting AWS Direct Connect connection over a private virtual interface to one or more VPCs in the same account that are located in the same or different Regions.\nThe reinvent session https://www.youtube.com/watch?v=Pj11NFXDbLY has very good introductions of this feature.\nAs below picture, the Direct Connect gateway enables you to use the AWS Direct Connect connection in the US East (N.\nVirginia) Region to access VPCs in your account in both the US East (N.\nVirginia) and US West (N.\nCalifornia) Regions.\nFor the documentation of Direct Connect Gateway, check the below link in https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html.\nOption A is incorrect because by using Direct Connect Gateway, the existing Direct Connect can be reused.\nIt does not need to create another new one which is time-consuming.\nOption B is incorrect because, via Direct Connect Gateway, VPCs in different regions can use the same AWS Direct Connect connection.\nOption C is CORRECT because to use Direct Connect Gateway, a private virtual interface is needed for the AWS Direct Connect connection to communicate with the Direct Connect gateway.\nIn the meantime, attach a private virtual interface in VPC to the Direct Connect gateway.\nOption D is incorrect because a private virtual interface instead of a private VPN IPsec tunnel is needed between the AWS Direct Connect connection and the Direct Connect gateway.\nThe answer is C. Yes. A Direct Connect Gateway is required to associate with the virtual private gateway for the VPC in the AWS US West region.\nDirect Connect is a network service that allows you to establish a dedicated network connection from your premises to AWS. This service provides a more reliable and consistent network experience compared to internet-based connections. Direct Connect can be used to access AWS services in any region, but a separate connection is required for each region.\nIn this scenario, the company already has a Direct Connect set up in the AWS US East region to connect its on-premises datacenter to its VPC. Now, the company has deployed new applications in the AWS US West region VPC, and they need to communicate with the on-premises datacenter using a high-speed network.\nTo reuse the existing Direct Connect connection, the company needs to set up a Direct Connect Gateway, which is a new type of virtual interface that allows traffic to flow between VPCs in different regions. The Direct Connect Gateway needs to be associated with the virtual private gateway for the VPC in the AWS US West region.\nOnce the Direct Connect Gateway is set up and associated with the virtual private gateway, the company can create a private virtual interface for the AWS Direct Connect connection to the Direct Connect Gateway. This will enable the new VPC in the AWS US West region to communicate with the on-premises datacenter over the same dedicated network connection that is already in place.\nTherefore, the correct answer is C. Yes. A Direct Connect Gateway is required to associate with the virtual private gateway for the VPC in the AWS US West region.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "No. Direct Connect cannot be reused. For this scenario, another new Direct Connect is required. Order a Direct Connect as soon as possible in the new region to save time.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No. Direct Connect can only be reused if both AWS VPCs are located in the same region. Since the second AWS VPC is in another region, it is impossible to reuse the same Direct Connect.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Yes. A Direct Connect Gateway is required to associate with the virtual private gateway for the VP.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Then create a private virtual interface for the AWS Direct Connect connection to the Direct Connect gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Yes. A Direct Connect Gateway is required to associate with the virtual private gateway for the VP.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Then create a private VPN IPsec tunnel between the AWS Direct Connect connection and the Direct Connect gateway.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 237,
  "query" : "You are using DynamoDB to store data in your application.\nYou create a table named \"Users\" with \"UserID\" as the Primary Key.\nHowever, you envision that, in some cases, you might need to query the table by \"UserName\" which cannot be set as the primary key.\nWhat changes would you make to this table to be able to query using UserName?",
  "answer" : "Answer - C.\nSome applications might need to perform many kinds of queries, using a variety of different attributes as query criteria.\nTo support these requirements, you can create one or more global secondary indexes and issue\nQuery.\nrequests against these indexes in Amazon DynamoDB.To speed up queries on non-key attributes, you can create a global secondary index.\nA global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table.\nThe index key does not need to have any of the key attributes from the table.\nIt doesn't even need to have the same key schema as a table.\nOption A is incorrect because creating another table is costly and unnecessary.\nOption B is incorrect because UserName cannot be the primary key.\nOption C is CORRECT because, as mentioned above, creating a global secondary index on UserName would allow the user to efficiently access the table via querying on this attribute rather than UserID which is the primary key.\nOption D is incorrect because DynamoDB tables are partitioned based on the primary key, and you cannot make UserName as the primary key.\nAWS Reference Docs:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\nTo enable querying by a non-primary key attribute in DynamoDB, you can create a Global Secondary Index (GSI). A GSI is a separate index with its own partition key and sort key that allows you to query the table using different attributes than the primary key.\nIn this scenario, creating a GSI with \"UserName\" as the partition key would allow you to query the \"Users\" table by this attribute. To create a GSI, you can use the AWS Management Console, AWS CLI, or an SDK.\nThe other options provided in the answer choices are not ideal solutions for this scenario:\nA. Creating a separate table would lead to data duplication and management overhead. B. Changing the primary key would require recreating the entire table and migrating the data. D. Partitioning the table using \"UserName\" as the partition key would not allow for efficient querying and would result in hot partitions.\nTherefore, the correct answer is C. Create a Global Secondary Index.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a second table that contains all the information, but make UserName the primary key.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a hash and change the primary key.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Global secondary index.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Partition the table using UserName rather than UserI.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 238,
  "query" : "A large trading company is using an on-premise system to analyze the trade data.\nAfter the trading day closes, the data, including the day's transaction costs, execution reporting, and market performance, is sent to a Redhat server that runs big data analytics tools for next-day trading predictions.\nA bash script is used to configure resources and schedule when to run the data analytics workloads.\nHow should the on-premise system be migrated to AWS with appropriate tools? (Select THREE)",
  "answer" : "E.\nCorrect Answer - A, C and E.\nThere are several parts of the on-premise system.\nThe first is the place to store the data from several sources.\nThe second is the bash script that is used to schedule the data to analyze the task.\nAnd the third part is the big data analysis.\nAll of these three parts need to be considered when being migrated.\nRefer to the below chart as a reference.\nOption A is CORRECT: Because S3 is an ideal place to store trade data as it is highly available, durable, and cost-efficient.\nOption B is incorrect: Because the SQS queue is inappropriate to store source data.\nThe trade data is very large which needs a durable store such as S3.\nOption C is CORRECT: Because AWS Batch is suitable to run a bash script using a job.\nThe AWS Batch scheduler evaluates when, where, and how to run jobs.\nOption D is incorrect: Because you should set up compute resources through AWS Batch instead of ECS.\nECS is not supported by AWS Batch.\nOption E is CORRECT: Because CloudWatch Events can be used to schedule and trigger AWS Batch jobs to perform data analytics.\nThe on-premise system that analyzes trade data can be migrated to AWS using several appropriate tools. Here are the explanations for the three selected answers:\nA. Create an S3 bucket to store the trade data that is used for post-processing. Storing trade data in an S3 bucket is a good option as it provides a scalable, durable, and highly available object storage solution. This will enable the big data analytics tools to access the data and perform post-processing. The data can also be easily secured by using access control policies and versioning options.\nC. Use AWS Batch to execute the bash script using a proper job definition. AWS Batch is a fully-managed service that allows you to run batch computing workloads at any scale. With AWS Batch, you can define and manage batch computing workloads using job definitions, and you can easily execute those jobs across a pool of EC2 instances. This option can be used to execute the bash script for configuring resources and scheduling when to run the data analytics workloads.\nE. Use CloudWatch Events to schedule data analytics jobs. CloudWatch Events is a service that allows you to schedule events that trigger actions. You can use CloudWatch Events to schedule data analytics jobs at specific times or intervals. This will enable the big data analytics tools to perform next-day trading predictions based on the trade data that is collected during the previous day.\nD. Use AWS ECS to handle the big data analytics workloads is incorrect as it is a container orchestration service that is used to deploy, manage, and scale containerized applications. It is not suitable for big data analytics workloads that require processing large amounts of data.\nB. Send the trade data from various sources to a dedicated SQS queue is incorrect as Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It is not suitable for trade data analysis use case as it doesn't perform any analysis on the data.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an S3 bucket to store the trade data that is used for post-processing.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Send the trade data from various sources to a dedicated SQS queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Batch to execute the bash script using a proper job definition.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS ECS to handle the big data analytics workloads.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudWatch Events to schedule data analytics jobs.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 239,
  "query" : "In your CloudFormation template, you have an \"EnvironmentType\" input parameter (Dev/Staging/Production)\nAWS CloudFormation creates an Amazon EC2 instance for a production environment and attaches a volume to the instance.\nFor other environments, AWS CloudFormation creates only the Amazon EC2 instance.\nWhich section in your CloudFormation template would you add to implement this logic?",
  "answer" : "Answer - D.\nThe optional Conditions section includes statements that define when a resource is created or when a property is defined.\nFor example, you can compare whether a value is equal to another value.\nBased on the result of that condition, you can conditionally create resources.\nIf you have multiple conditions, separate them with commas.\nFor more information on Cloudformation conditions, please visit the below link-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\nNote:\nAs per AWS documentation,\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment.\nIn your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs.\nFor the production environment, you might include Amazon EC2 instances with certain capabilities.\nHowever, for the test environment, you want to use reduced capabilities to save money.\nWith conditions, you can define which resources are created and how they're configured for each environment type.\nRefer to page 276 on the link: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-ug.pdf.\nConditions are evaluated based on input parameter values that you specify when you create or update a stack.\nWithin each condition, you can reference another condition, a parameter value, or a mapping.\nAfter you define all your conditions, you can associate them with resources and resource properties in the Resources and outputs sections of a template.\nOptions A, B, C are incorrect because they cannot define the circumstances under which entities are created or configured or not.\nFor more details, please check the below AWS Docs.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\nTo implement this logic in the CloudFormation template, you would add the logic to the \"Conditions\" section.\nThe \"Conditions\" section of a CloudFormation template allows you to specify conditions that determine whether certain resources or properties are created or updated during stack creation or update.\nHere's an example of how you could use the \"Conditions\" section to create an EC2 instance and attach a volume only for the production environment:\njsonCopy code\"Conditions\": {   \"IsProduction\": {\"Fn::Equals\": [{\"Ref\": \"EnvironmentType\"}, \"Production\"]} },  \"Resources\": {   \"EC2Instance\": {     \"Type\": \"AWS::EC2::Instance\",     \"Properties\": {       \"ImageId\": \"ami-0abc123\",       \"InstanceType\": \"t2.micro\",       \"KeyName\": \"myKeyPair\",       \"Tags\": [         {\"Key\": \"Name\", \"Value\": \"My EC2 Instance\"}       ],       \"BlockDeviceMappings\": [         {           \"DeviceName\": \"/dev/sda1\",           \"Ebs\": {             \"VolumeSize\": 50,             \"VolumeType\": \"gp2\"           }         }       ],       \"Condition\": \"IsProduction\"     }   } } \nIn this example, the \"IsProduction\" condition is defined using the \"Fn::Equals\" intrinsic function, which compares the value of the \"EnvironmentType\" parameter to the string \"Production\". If the condition is true, then the EC2 instance and volume resources are created. If the condition is false, then only the EC2 instance resource is created.\nNote that the \"Condition\" property is set to \"IsProduction\" for the EC2 instance resource. This tells CloudFormation to only create the EC2 instance when the \"IsProduction\" condition is true.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Outputs",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Resources",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Mappings",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Conditions.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 240,
  "query" : "There are currently multiple applications hosted in a VPC.\nDuring monitoring, it has been noticed that multiple port scans are coming in from a specific IP Address block.\nThe internal security team has requested that all offending IP Addresses be denied for the next 24 hours.\nWhich of the following is the best method to quickly and temporarily deny access from the specified IP Addresses?",
  "answer" : "Answer - B.\nA network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\nOptions A and D are incorrect because (a) it will only work for Windows-based instances, and (b) a better approach is to block the traffic at the subnet layer via NACL rather than instance layer (windows firewall).\nOption B is CORRECT because the best way to allow or deny IP address-based access to the resources in the VPC is to configure rules in the Network access control list (NACL), which are applied at the subnet level.\nOption C is incorrect because (a) you cannot explicitly deny access to particular IP addresses via security group, and (b) a better approach is to block the traffic at the subnet layer via NACL rather than instance layer (security group).\nFor more information on network ACL's, please refer to the below link-\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\nThe best method to quickly and temporarily deny access from the specified IP Addresses in a VPC is to modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block. This is because Network ACLs operate at the subnet level and provide stateless filtering of traffic, which makes them a suitable choice for blocking incoming traffic from specific IP addresses.\nOption A, creating an AD policy to modify the Windows Firewall settings on all hosts in the VPC to deny access from the IP Address block, is not a suitable option because it is specific to Windows hosts and would not block traffic to non-Windows systems in the VPC.\nOption C, adding a rule to all of the VPC Security Groups to deny access from the IP Address block, is not the best option because security groups operate at the instance level, and adding a rule to all of them could be time-consuming and error-prone.\nOption D, modifying the Windows Firewall settings on all AMIs that your organization uses in that VPC to deny access from the IP address block, is not a good option because it only applies to instances that are launched from those specific AMIs and would not block traffic to instances launched from other AMIs or non-Windows instances.\nTherefore, modifying the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block is the best option to quickly and temporarily deny access from the specified IP Addresses.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an AD policy to modify the Windows Firewall settings on all hosts in the VPC to deny access from the IP Address block.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Add a rule to all of the VPC Security Groups to deny access from the IP Address block.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the Windows Firewall settings on all AMI`s that your organization uses in that VPC to deny access from the IP address block.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 241,
  "query" : "You have an Auto Scaling group associated with an Elastic Load Balancer (ELB)\nYou have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not terminated.\nWhat do you need to do to ensure that the instances marked unhealthy by the ELB will be terminated and replaced?",
  "answer" : "Answer - B.\nTo discover the availability of your EC2 instances, an ELB periodically sends pings, attempts connections, or sends requests to test the EC2 instances.\nThese tests are called health checks.\nThe status of the instances that are healthy at the time of the health check is InService.\nThe status of any instances that are unhealthy at the time of the health check is OutOfService.\nWhen you allow the Auto Scaling group (ASG) to receive the traffic from the ELB, it gets notified when the instance becomes unhealthy, and then it terminates it.\nSee the images in the \"More information...\" section for more details.\nOption A is incorrect because changing the threshold will not enable ASG to know about the unhealthy instances.\nOption B is CORRECT because when you associate the ELB with ASG, you allow the ASG to receive the traffic from that ELB.\nWhen the health check type is ELB, the ASG will get aware of the unhealthy instances and terminate them.\nOption C is incorrect because increasing the interval will still not communicate the information about the unhealthy instances to the ASG.Option D is incorrect because this setting will not communicate the information about the unhealthy instances to the ASG either.\nMore information on ELB with Auto Scaling Group:\nFor more information on ELB, please visit the below URL-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\nWhen an Auto Scaling group is associated with an Elastic Load Balancer (ELB), the ELB regularly performs health checks on the instances in the Auto Scaling group to ensure that they are healthy and able to handle incoming traffic. If an instance is found to be unhealthy, the ELB marks it as such, and may optionally remove it from the pool of instances that it sends traffic to.\nIn this scenario, the issue is that instances marked as unhealthy by the ELB are not being terminated and replaced. This could be due to a number of factors, including the thresholds set on the Auto Scaling group health check, the type of health check being used, or the interval at which the ELB performs health checks.\nTo ensure that unhealthy instances are terminated and replaced as soon as possible, we need to identify and address the root cause of the issue. Here are the possible options:\nA. Change the thresholds set on the Auto Scaling group health check. By default, an Auto Scaling group uses the EC2 instance status checks to determine the health of instances. These checks monitor the underlying health of the instance, such as whether it has network connectivity, and can detect issues such as a crashed operating system. You can also configure the Auto Scaling group to use a custom health check that you define. By changing the thresholds set on the health check, you can adjust the criteria that are used to determine whether an instance is healthy or not. For example, you could increase the timeout for the health check, or adjust the number of consecutive successful checks required before an instance is marked as healthy. However, this option alone may not solve the problem of unhealthy instances not being terminated.\nB. Change the type of the health check to \"Elastic Load Balancing\" in your Auto Scaling group. By default, an Auto Scaling group uses the EC2 instance status checks to determine the health of instances, but you can also configure it to use a health check based on the Elastic Load Balancer. When you select this option, the Auto Scaling group relies on the health status reported by the ELB, rather than performing its own health checks. This means that instances will be terminated and replaced as soon as the ELB marks them as unhealthy. This is likely the best option to solve the problem of unhealthy instances not being terminated.\nC. Increase the value for the Health check interval set on the Elastic Load Balancer. The health check interval determines how frequently the ELB checks the health of instances. By increasing the value of the health check interval, you can reduce the load on the ELB and potentially reduce the number of false positives (i.e., instances that are marked as unhealthy but are actually healthy). However, this option may not address the root cause of the issue, which is why unhealthy instances are not being terminated.\nD. Change the health check on the Elastic Load Balancer to use TCP rather than HTTP checks. By default, the ELB uses HTTP checks to determine the health of instances. However, if instances are behind a firewall or have a non-standard HTTP configuration, they may fail the health check even if they are otherwise healthy. By changing the health check to use TCP checks, the ELB simply checks whether it can establish a connection to the instance's IP address and port, without checking the contents of the response. This can reduce the likelihood of false positives, but may not solve the problem of unhealthy instances not being terminated.\nIn summary, option B (Change the type of the health check to \"Elastic Load Balancing\" in your Auto Scaling group) is likely the best option to ensure that unhealthy instances are terminated and replaced as soon as possible. However, you may want to consider other options as well, depending on the specific circumstances of your deployment.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Change the thresholds set on the Auto Scaling group health check.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Change the type of the health check to \"Elastic Load Balancing\" in your Auto Scaling group.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Increase the value for the Health check interval set on the Elastic Load Balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Change the health check on the Elastic Load Balancer to use TCP rather than HTTP checks.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 242,
  "query" : "You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets.\nOne instance is running a database, and the other instance an application that will interface with the database.",
  "answer" : "Answer - A and D.\nIn order to have the instances communicate with each other, you need to properly configure both Security Group and Network access control lists (NACLs)\nFor the exam, remember that the Security Group operates at the instance level, whereas the NACL operates at the subnet level.\nOption A is CORRECT because the security groups must be defined to allow the webserver to communicate with the database server.\nAn example image from the AWS documentation is given below:\nOption B is incorrect because it is not necessary to have the two instances of the same type or the same key-pair.\nOption C is incorrect because configuring NAT instance or NAT gateway will not enable the two servers to communicate with each other.\nNAT instance/NAT gateway is used to enable the communication between instances in the private subnets and the Internet.\nOption D is CORRECT because the two servers are in two separate subnets.\nIn order for them to communicate with each other, you need to configure the NACLas shown below:\nFor more information on VPC and Subnets, please visit the below URL-\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\nIn this scenario, you have two EC2 instances in the same AZ but in different subnets. One instance is running a database, and the other instance an application that will interface with the database. To enable communication between these two instances, you need to configure the network settings properly.\nA. Security groups are set to allow the application host to talk to the database on the right port/protocol. Security groups act as virtual firewalls for your instances. You can configure security groups to allow or deny inbound and outbound traffic based on protocols, ports, and IP addresses. In this scenario, you need to ensure that the security group associated with the database instance allows inbound traffic from the security group associated with the application instance on the port and protocol used by the database.\nB. Both instances are the same instance class and using the same key-pair. The instance class determines the computing resources allocated to an instance, such as CPU, memory, and network performance. In this scenario, the instance class may not matter as long as the instances meet the minimum requirements for running the database and application. Using the same key-pair may not be necessary unless you need to establish a secure SSH connection between the instances.\nC. The default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate. The default route determines where the network traffic should be directed if there is no explicit route defined for a particular destination. In this scenario, you need to ensure that the default route in the route table associated with both subnets points to a NAT instance or IGW, depending on whether the instances need to communicate with the internet or not.\nD. A network ACL that allows communication between the two subnets. Network ACLs act as a firewall for your VPC and can be used to control inbound and outbound traffic at the subnet level. In this scenario, you need to ensure that the network ACL associated with the subnets allows inbound traffic from the other subnet on the port and protocol used by the database.\nIn conclusion, A and C are the most relevant answers to enable communication between the two instances, but B and D may also be necessary depending on your specific requirements.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Security groups are set to allow the application host to talk to the database on the right port/protocol.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Both instances are the same instance class and using the same key-pair.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A network ACL that allows communication between the two subnets.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 243,
  "query" : "You are managing a legacy application inside VPC with hard-coded IP addresses in its configuration.\nWhich mechanisms will allow the application to failover to new instances without much reconfiguration?",
  "answer" : "Answer - B and D.\nOption A is incorrect because rerouting to a failover instance cannot be done through a Traffic Manager.\nOption B is CORRECT because the attributes of a network interface follow it as it's attached or detached from an instance and reattached to another instance.\nWhen you move a network interface from one instance to another, network traffic is redirected to the new instance.\nOption C is incorrect because Route 53 cannot reroute the traffic to the failover instance with the same IP address.\nOption D is CORRECT because you can have a secondary IP address that can be configured on the primary ENI of the failover instance.\nBest Practices for Configuring Network Interfaces.\nYou can attach a network interface to an instance when it's running (hot attach), when it's stopped (warm attach), or when the instance is being launched (cold attach).\nYou can detach secondary (ethN) network interfaces when the instance is running or stopped.\nHowever, you can't detach the primary (eth0) interface.\nYou can attach a network interface in one subnet to an instance in another subnet in the same VPC; however, both the network interface and the instance must reside in the same Availability Zone.\nWhen launching an instance from the CLI or API, you can specify the network interfaces to attach to the instance for both the primary (eth0) and additional network interfaces.\nLaunching an Amazon Linux or Windows Server instance with multiple network interfaces automatically configures interfaces, private IPv4 addresses, and route tables on the operating system of the instance.\nA warm or hot attach of an additional network interface may require you to manually bring up the second interface, configure the private IPv4 address, and modify the route table accordingly.\nInstances running Amazon Linux or Windows Server automatically recognize the warm or hot attach and configure themselves.\nAttaching another network interface to an instance (for example, a NIC teaming configuration) cannot be used as a method to increase or double the network bandwidth to or from the dual-homed instance.\nIf you attach two or more network interfaces from the same subnet to an instance, you may encounter networking issues such as asymmetric routing.\nIf possible, use a secondary private IPv4 address on the primary network interface instead.\nFor more information, see Assigning a Secondary Private IPv4 Address.\nFor more information on Network Interfaces, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\nThe best answer for this scenario would be option B, which suggests creating a secondary Elastic Network Interface (ENI) that can be moved to the failover instance.\nExplanation: When managing a legacy application inside a Virtual Private Cloud (VPC) with hard-coded IP addresses in its configuration, the goal is to provide high availability without changing the existing configuration. To achieve this, the failover mechanism should involve minimal reconfiguration.\nOption A, which suggests using a traffic manager, may not be suitable in this case as it would require changing the configuration to include the traffic manager's endpoint. Additionally, if the application has hard-coded IP addresses, it may not be possible to change the configuration easily.\nOption C, which suggests using Route53 health checks, can help reroute the traffic to the failover instance in case of a failure. However, it requires changing the DNS configuration, which may not be possible in a legacy application with hard-coded IP addresses.\nOption D, which suggests assigning a secondary private IP address to the primary ENI of the failover instance, may not be the best solution as it still requires changing the configuration to include the secondary IP address.\nTherefore, option B is the best choice as it involves creating a secondary ENI that can be moved to the failover instance. This allows the failover instance to have the same IP address as the primary instance, which means that the application can continue to operate without any reconfiguration. In case of a failure, the secondary ENI can be moved to the failover instance, and the traffic will automatically start flowing to the failover instance, ensuring high availability without any significant changes to the existing configuration.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the traffic manager to route the traffic to the failover instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a secondary ENI that can be moved to the failover instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Route53 health checks to reroute the traffic to the failover instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Assign a secondary private IP address to the primary ENI of the failover instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 244,
  "query" : "A media company produces new video files on-premises every day with a total size of around 100GB after compression.\nAll files have a size of 1-2 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3 AM and 5 AM.\nThe current upload takes almost 3 hours, although less than half of the available bandwidth is used.\nWhat step(s) would ensure that the file uploads can complete in the allotted time window?",
  "answer" : "Answer - B.\nWhen uploading large videos, it's always better to use AWS multipart file upload, especially when the bandwidth is not fully utilized.\nOption A is incorrect because the existing bandwidth itself is not fully utilized.\nIncreasing the bandwidth is not going to help; in fact, it will add to the cost.\nOption B is CORRECT because Multipart Upload will fully utilize the available bandwidth and increase the throughput.\nIt also has additional benefits, as mentioned below in the \"More Information\" section.\nOption C is incorrect because there is a restriction on the size of upload in a single PUT operation.\nYou cannot upload a file of size more than 5GB in a single upload.\nSo this option is not going to help at all.\nYou need to use multipart upload.\nOption D is incorrect because this option requires you to put all the files daily on a storage drive and send it to AWS.\nSince the data has to be uploaded in a certain time frame and there is sufficient bandwidth already available, multipart upload is the best option compared to AWS Import/Export.\nAdvantages of Multipart Upload.\nImproved throughput-you can upload parts in parallel to improve throughput.\nQuick recovery from any network issues-smaller part size minimizes the impact of restarting a failed upload due to a network error.\nPause and resume object uploads-you can upload object parts over time.\nOnce you initiate a multipart upload, there is no expiry; you must explicitly complete or abort the multipart upload.\nBegin an upload before you know the final object size-you can upload an object as you are creating it.\nFor more information on Multi-part file upload for S3, please visit the URL -\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html\nThe correct answer for this question is B. Use Multipart upload to upload the data to s3.\nExplanation:\nOption A - Increasing network bandwidth can certainly help in increasing the throughput to Amazon S3. However, it may not guarantee that the file uploads can complete in the allotted time window. Besides, the question states that less than half of the available bandwidth is being used currently, which implies that there is no network bottleneck currently.\nOption B - Multipart upload is designed to upload large objects to Amazon S3 in parts, which makes it more efficient and faster than uploading the object in a single operation. Multipart upload is recommended for objects greater than 100 MB in size and can be used to upload objects up to 5 TB in size. Since the video files are around 1-2 GB in size, using Multipart upload can speed up the upload process significantly and ensure that the file uploads can complete within the allotted time window.\nOption C - Packing all files into a single archive and uploading it to Amazon S3 and then extracting the files in AWS may not help in reducing the upload time since the compressed file size will still be around 100GB.\nOption D - AWS Import/Export is a service designed to transfer large amounts of data into and out of Amazon S3 using portable storage devices. While AWS Import/Export can be used to transfer large amounts of data, it may not be the best solution in this case since the video files are being produced on-premises every day.\nTherefore, option B is the correct answer as using Multipart upload to upload the data to S3 can ensure that the file uploads can complete in the allotted time window.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Increase your network bandwidth to provide faster throughput to S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Multipart upload to upload the data to s3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Pack all files into a single archive, upload it to S3, and then extract the files in AWS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Import/Export to transfer the video files.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 245,
  "query" : "Your team is excited about using AWS because now they have access to \"programmable Infrastructure”\nYou have been asked to manage your AWS infrastructure in a manner similar to the way you might manage application code.\nYou want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert to previous versions, and identify what versions are running at any particular time (development, test, QA, and production)\nWhich approach addresses this requirement?",
  "answer" : "Answer - D.\nYou can use AWS Cloud Formation's sample templates or create your own templates to describe the AWS resources, and any associated dependencies or runtime parameters required to run your application.\nYou don't need to figure out the order for provisioning AWS services or the subtleties of making those dependencies work.\nCloudFormation takes care of this for you.\nAfter the AWS resources are deployed, you can modify and update them in a controlled and predictable way, in effect applying version control to your AWS infrastructure the same way you do with your software.\nYou can also visualize your templates as diagrams and edit them using a drag-and-drop interface with the AWS CloudFormation Designer.\nOption A is incorrect because Cost Allocation Reports are not helpful for the purpose of the question.\nOption B is incorrect because CloudWatch is used for monitoring the metrics about different AWS resources.\nOption C is incorrect because it does not have the concept of programmable Infrastructure.\nOption D is CORRECT because AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\nFor more information on CloudFormation, please visit the link-\nhttps://aws.amazon.com/cloudformation/\nThe approach that addresses the given requirements is to use AWS CloudFormation and a version control system like GIT to deploy and manage the infrastructure.\nAWS CloudFormation is a service that allows users to define and provision AWS infrastructure as code (IaC) using templates, enabling them to automate the process of creating and managing resources. CloudFormation templates are written in JSON or YAML and can be version-controlled using tools like GIT.\nUsing CloudFormation templates in conjunction with version control systems allows for the management of infrastructure to be treated in a similar manner to application code. By defining infrastructure as code, infrastructure changes can be tested and validated before they are deployed to a production environment, just as application code changes are validated. This approach enables the deployment of exact copies of different versions of infrastructure and the ability to revert to previous versions.\nMoreover, CloudFormation supports stack creation rollback and update rollback to simplify the process of undoing changes. By staging infrastructure changes in different environments, such as development, test, QA, and production, teams can ensure that infrastructure changes are thoroughly tested and validated before they are promoted to a production environment.\nIn addition, using version control systems enables teams to track changes to infrastructure code and identify what versions are running at any particular time. This allows for easier management of multiple environments, such as testing and production, as well as the ability to audit and track changes made to the infrastructure over time.\nTherefore, option D (Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure) is the correct approach to address the given requirements.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 246,
  "query" : "What would happen to an RDS (Relational Database Service) Multi-Availability Zone deployment if the primary DB instance fails?",
  "answer" : "Answer - D.\nOption A is incorrect because the IP address of the primary and standby instances remain the same and are not changed.\nOption B is incorrect because the CNAME record of the primary DB instance changes to the standby instance.\nOption C is incorrect because there is no new instance created in the standby AZ.\nOption D is CORRECT because the CNAME of the primary DB instance changes to the standby instance so that there is no impact on the application setting or any reference to the primary instance.\nMore information on Amazon RDS Multi-AZ deployment:\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads.\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ)\nEach AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.\nIn case of an infrastructure failure (for example, instance hardware failure, storage failure, or network disruption), Amazon RDS performs an automatic failover to the standby so that you can resume database operations as soon as the failover is complete.\nAnd as per the AWS documentation, the CNAME is changed to the standby DB when the primary one fails.\nhttps://aws.amazon.com/rds/faqs/\nFor more information on Multi-AZ RDS, please visit the link-\nhttps://aws.amazon.com/rds/details/multi-az/\nIn an RDS Multi-Availability Zone (Multi-AZ) deployment, a standby replica is automatically created in a different Availability Zone (AZ) from the primary DB instance. This replica is kept in sync with the primary through synchronous replication of the database writes.\nIf the primary DB instance fails, AWS automatically promotes the standby replica to the primary instance, and reconfigures the DNS name of the DB instance endpoint to point to the new primary. This process is automated and occurs without any manual intervention.\nTherefore, the correct answer is:\nA. The IP address of the primary DB instance is switched to the standby DB instance.\nOption B is incorrect because the primary instance cannot remain as primary if it fails. Option C is also incorrect because a new DB instance is not created, but rather the standby instance is promoted to be the new primary. Option D is incorrect because the canonical name record (CNAME) is not re-pointed from the primary to the secondary instance, but rather the endpoint is reconfigured to point to the new primary instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The IP address of the primary DB instance is switched to the standby DB instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The primary RDS (Relational Database Service) DB instance reboots and remains as primary.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A new DB instance is created in the standby availability zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The canonical name record (CNAME) is re-pointed from the primary to the secondary instance (standby).",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 247,
  "query" : "A user is trying to save some cost on the AWS services.\nWhich of the below mentioned options will not help him to save cost?",
  "answer" : "Answer - B.\nOption A is incorrect because EBS volumes have a costing aspect, hence deleting the unutilized volumes will save some cost.\nOption B is CORRECT because an unused Auto Scaling launch configuration will not cost anything.\nOption C is incorrect because non-associated Elastic IP will cost you if not released.\nOption D is incorrect because an ELB without any instances behind it incurs costs.\nFor more information on AWS Pricing, please visit the link-\nhttps://aws.amazon.com/pricing/services/\nAll the options listed above are relevant for cost savings, except for one. Let's examine each option in more detail:\nA. Delete the unutilized EBS volumes once the instance is terminated: When an EC2 instance is terminated, its associated EBS (Elastic Block Store) volumes continue to exist and will continue to incur charges until they are explicitly deleted. Therefore, deleting unutilized EBS volumes once the instance is terminated is an essential step in managing costs.\nB. Delete the Auto Scaling launch configuration after the instances are terminated: Auto Scaling launch configuration provides the necessary information to launch new EC2 instances automatically. Once the instances are terminated, it is necessary to delete the launch configuration as it will continue to incur charges as long as it exists. Therefore, deleting the Auto Scaling launch configuration is an essential step in managing costs.\nC. Release the elastic IP if not required once the instance is terminated: An Elastic IP (EIP) is a static IP address associated with your AWS account. When an EC2 instance is launched, it can be associated with an EIP. Once the instance is terminated, the EIP remains associated with the account, and you continue to incur charges. Therefore, releasing the elastic IP if not required once the instance is terminated is an essential step in managing costs.\nD. Delete the AWS ELB after all the instances behind it are terminated: An Elastic Load Balancer (ELB) distributes incoming traffic across multiple EC2 instances. When all the instances behind the ELB are terminated, the ELB still exists and will continue to incur charges. Therefore, deleting the ELB after all the instances behind it are terminated is an essential step in managing costs.\nOption D is incorrect as it suggests deleting the AWS ELB after all the instances behind it are terminated, which is the right thing to do in order to reduce cost.\nIn conclusion, all the options except for option D will help the user to save costs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Delete the unutilized EBS volumes once the instance is terminated.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Delete the Auto Scaling launch configuration after the instances are terminated.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Release the elastic IP if not required once the instance is terminated.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Delete the AWS ELB after all the instances behind it are terminated.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 248,
  "query" : "An organization is planning to use AWS for its production rollout.\nThe organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3, and set up the ELB.\nWhich of the below mentioned AWS services meets the requirement for making an easy and orderly deployment of the software?",
  "answer" : "Answer - A.\nThe Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services.\nWe can simply upload the code.\nElastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, Auto Scaling to application health monitoring.\nMeanwhile, we can retain full control over the AWS resources used in the application and can access the underlying resources at any time.\nHence, A is the CORRECT answer.\nFor more information on launching a LAMP stack with Elastic Beanstalk ( click on \"Getting started with the Implementation Guide\" at the bottom of the page ):\nhttps://aws.amazon.com/getting-started/projects/launch-lamp-web-app/faq/\nWe can do it on AWS CloudFormation as well in a harder way, and it will be less Native.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/deploying.applications.html\nThe AWS service that can meet the organization's requirement for an easy and orderly deployment of a LAMP stack, downloading the latest PHP installable from S3, and setting up the ELB is AWS CloudFormation.\nAWS CloudFormation is a service that provides a way to create and manage AWS resources using templates. A CloudFormation template is a JSON or YAML formatted text file that describes the AWS resources that you want to create and configure. In this case, the organization can create a CloudFormation template that includes instructions to create an EC2 instance, install Apache, MySQL, PHP, and other required packages to set up the LAMP stack, download the latest PHP installable from S3, and set up the ELB.\nAWS Elastic Beanstalk is a platform-as-a-service (PaaS) offering from AWS that helps developers deploy and scale web applications and services. Elastic Beanstalk provides preconfigured platforms for popular programming languages and frameworks such as PHP, Java, Ruby, and Node.js. However, Elastic Beanstalk is not well-suited for deploying custom software stacks, as it requires using the preconfigured platforms.\nAWS CloudFront is a content delivery network (CDN) that delivers content, including websites, videos, and APIs, to end-users with low latency and high transfer speeds. However, CloudFront is not designed for deploying software stacks or configuring servers.\nAWS DevOps is not a specific AWS service but rather a collection of tools and practices for automating software development and deployment processes. DevOps tools such as AWS CodePipeline and AWS CodeDeploy can be used to automate the deployment of software stacks on AWS. However, DevOps is not a standalone service and requires the use of multiple AWS services to build a complete deployment pipeline.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Elastic Beanstalk",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Cloudfront",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Cloudformation",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS DevOps.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 249,
  "query" : "A media and entertainment company utilizes several scripts to schedule daily tasks that drive lots of EC2 instances to enable accelerated and automated processing of data.\nThis mainly aims for the compilation and processing of files, graphics, and visual effects.\nHowever, the operation manager is unsatisfied with the way that the tasks are scheduled and assigned you a task to choose AWS Batch instead.\nWhich components of AWS Batch do you need to create? (Select TWO)",
  "answer" : "Correct Answer - B, C.\nAs a fully managed service, AWS Batch enables you to run batch computing workloads of any scale.\nThe key components of AWS Batch are as below.\n1, Jobs: A unit of work (such as a shell script, a Linux executable, or a Docker container image) that you submit to AWS Batch.\n2, Job Definitions: A job definition specifies how jobs are to be run; you can think of it as a blueprint for the resources in your job.\n3, Job Queues: When you submit an AWS Batch job, you submit it to a particular job queue, where it resides until it is scheduled onto a compute environment.\n4, Compute Environment: A compute environment is a set of managed or unmanaged compute resources used to run jobs.\nOption A is incorrect: Because in AWS Batch, you do not need to configure the Auto Scaling settings that AWS Batch takes care of.\nOption B is CORRECT: Because a job is required for AWS Batch to understand the task.\nOption C is CORRECT: Because the compute environment is where AWS Batch executes the specific tasks.\nOption D is incorrect: Because AWS Batch uses a job queue rather than an SQS queue where you submit the AWS Batch job.\nSure, I can help you with that!\nAWS Batch is a service that enables you to run batch computing workloads on the AWS Cloud. It allows you to define and execute batch jobs using Docker containers. The service handles the details of scheduling and launching instances, managing the environment, and monitoring and logging the progress of your batch jobs.\nIn the scenario you provided, the media and entertainment company is currently using EC2 instances to run daily batch processing tasks. The operation manager wants to switch to AWS Batch to improve the scheduling and execution of these tasks. To do so, you will need to create the following components:\n1.\nA compute environment: A compute environment is a set of resources that AWS Batch uses to run your batch jobs. These resources can be EC2 instances, EC2 Spot instances, or Fargate resources. You can specify the minimum and maximum number of instances in the compute environment, as well as the type of instance or resource to use. You can also configure the environment to use Auto Scaling, which automatically adds or removes instances based on the workload.\n2.\nA job definition: A job definition describes the parameters and resources required for a batch job to run. It includes the Docker image to use, the command to run inside the container, the memory and CPU requirements, and any environment variables or data volumes needed. You can also specify any dependencies or constraints for the job.\nWith these two components in place, you can submit batch jobs to AWS Batch, which will schedule and run them on the compute environment you created. When you submit a job, you specify the job definition to use, as well as any additional parameters or inputs required. AWS Batch will handle the scheduling and execution of the job, scaling the compute environment as needed to meet the demand.\nNow, let's take a look at the answer options:\nA. An Auto Scaling configuration for the AWS Batch computer resources to use.\nThis is partially correct. You do need to configure Auto Scaling for the compute environment, but it is not the only component you need to create. Without a compute environment or job definition, Auto Scaling alone cannot enable the execution of batch jobs.\nB. A job that runs as a containerized application on an Amazon EC2 instance, using parameters that you specify in a job definition.\nThis is incorrect. The question specifically asks about AWS Batch, which runs batch jobs using Docker containers, not directly on EC2 instances.\nC. A compute environment that is a set of compute resources for running jobs.\nThis is correct. As explained earlier, a compute environment is a required component of AWS Batch that provides the resources for running batch jobs.\nD. An SQS queue that AWS Batch uses to execute tasks. When there is a message in the queue, it is scheduled onto an AWS Batch compute environment.\nThis is incorrect. AWS Batch does not use an SQS queue to execute tasks. Instead, it uses the job queue feature to manage and prioritize the submission of batch jobs.\nTherefore, the correct answers to the question are A and C, since you need to create a compute environment and configure Auto Scaling for it. However, it's important to note that a job definition is also a necessary component of AWS Batch, which was not mentioned in the answer options.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "An Auto Scaling configuration for the AWS Batch computer resources to use.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A job that runs as a containerized application on an Amazon EC2 instance, using parameters that you specify in a job definition.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A compute environment that is a set of compute resources for running jobs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "An SQS queue that AWS Batch uses to execute tasks. When there is a message in the queue, it is scheduled onto an AWS Batch compute environment.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 250,
  "query" : "An IT company owns a web product in AWS that provides discount restaurant information to customers.\nIt has used one S3 Bucket (my-bucket) to store restaurant data such as pictures, menus, etc.\nThe product is deployed in VPC subnets.\nThe company's Cloud Architect decides to configure a VPC endpoint for this S3 bucket to enhance the performance.\nTo be compliant with security rules, it is required that the new VPC endpoint is only used to communicate with this specific S3 Bucket.\nOn the other hand, the S3 bucket allows the read/write operations to come from this VPC endpoint.\nWhich two options should the Cloud Architect choose to meet the security needs?",
  "answer" : "E.\nCorrect Answer - A, D.\nIn this case, two restrictions are required.\n1, For the VPC endpoint, restricting access to the specific S3 Bucket “my-bucket”\nA VPC Endpoint policy is needed.\n{\n\"Statement\": [\n{\n\"Sid\": \"Access-to-my-bucket-only\",\n\"Principal\": \"*\",\n\"Action\": [\n\"s3:GetObject\",\n\"s3:PutObject\"\n],\n\"Effect\": \"Allow\",\n\"Resource\": [\"arn:aws:s3:::my-bucket\",\n\"arn:aws:s3:::my-bucket/*\"]\n}\n]\n}\n2, For the S3 bucket “my-bucket”, restricting access to the new VPC Endpoint.\nS3 Bucket policy is required.\n{\n\"Version\": \"2012-10-17\",\n\"Id\": \"Policy1415115909152\",\n\"Statement\": [\n{\n\"Sid\": \"Access-to-specific-VPCE-only\",\n\"Principal\": \"*\",\n\"Action\": \"s3:*\",\n\"Effect\": \"Deny\",\n\"Resource\": [\"arn:aws:s3:::my-bucket\",\n\"arn:aws:s3:::my-bucket/*\"],\n\"Condition\": {\n\"StringNotEquals\": {\n\"aws:sourceVpce\": \"vpce-1a2b3c4d\"\n}\n}\n}\n]\n}\nIn terms of the S3 bucket policy for VPC Endpoint, the aws:SourceIp condition can not be used as for either NotIpAddress or IpAddress, the condition fails to match any specified IP address or IP address range.\nInstead, the specific endpoint ID should be used for the S3 bucket policy.\nOption A is CORRECT because the VPC Endpoint policy helps to restrict which entity can use the VPC Endpoint.\nIt is an IAM resource policy that you attach to an endpoint when you create or modify the endpoint.\nOption B is incorrect because users cannot configure a \"deny\" for the outbound traffic in a security group.\nOption C is incorrect because, for the S3 bucket policy, the NotIpAddress condition is always met for the VPC endpoint so that it cannot help restrict the traffic from the VPC endpoint.\nOption D is CORRECT because, in the S3 bucket policy, a rule can be set up to deny all actions if the incoming traffic is not from the VPC Endpoint ID.Option E is incorrect: Same reason as option.\nC.\nOption A is the correct answer.\nExplanation: To enhance performance and improve security, the company's Cloud Architect wants to configure a VPC endpoint for the S3 bucket that is only used to communicate with the specific S3 bucket. This will provide a more secure and efficient way to access the S3 bucket from the VPC subnets.\nOption A suggests using a VPC endpoint policy for Amazon S3 to restrict access to the S3 bucket \"my-bucket\" so that the VPC endpoint is only allowed to perform S3 actions on \"my-bucket\". This option is the most appropriate because it restricts the VPC endpoint access to the specific S3 bucket while allowing read/write operations to come from the VPC endpoint.\nOption B suggests modifying the security group of the EC2 instance to limit the outbound actions to the VPC endpoint by denying the outgoing traffic to the destination S3 bucket \"my-bucket\". This option is not correct because it does not address the requirement of allowing read/write operations to come from the VPC endpoint.\nOption C suggests adding an S3 bucket policy to deny all actions if the source IP address is not equal to the EC2 public IP. This option is not correct because it does not address the requirement of allowing the VPC endpoint to perform S3 actions on the S3 bucket.\nOption D suggests using an S3 bucket policy that denies all actions if the source VPC endpoint is not equal to the endpoint ID that is created. This option is not correct because it does not allow the VPC endpoint to perform S3 actions on the S3 bucket.\nOption E suggests creating an S3 bucket policy that denies all actions unless the source IP address is equal to the EC2 public IP. This option is not correct because it does not allow the VPC endpoint to perform S3 actions on the S3 bucket.\nIn summary, option A is the correct answer because it meets the security needs of the company by restricting access to the S3 bucket only to the VPC endpoint while allowing read/write operations to come from the VPC endpoint.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use a VPC Endpoint policy for Amazon S3 to restrict access to the S3 Bucket “my-bucket” so that the VPC Endpoint is only allowed to perform S3 actions on “my-bucket”.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Modify the security group of the EC2 instance to limit the outbound actions to the VPC Endpoint by denying the outgoing traffic to the destination S3 bucket “my-bucket”.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the S3 bucket “my-bucket”, add an S3 bucket policy in which all actions are denied if the source IP address is not equal to the EC2 public IP (use “NotIpAddress” condition).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For the S3 bucket “my-bucket”, use an S3 bucket policy that denies all actions if the source VPC Endpoint is not equal to the endpoint ID that is created.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an S3 bucket policy in the S3 bucket “my-bucket” which denies all actions unless the source IP address is equal to the EC2 public IP (use “IpAddress” condition).",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 251,
  "query" : "You have large EC2 instances in your AWS infrastructure which you have recently set up.\nThese instances carry out the task of creating JPEG files and store them on an S3 bucket and occasionally need to perform high computational tasks.\nAfter close monitoring, you see that the CPUs of these instances remain idle most of the time.",
  "answer" : "Answer - C.\nIn this scenario, the problem is that the large EC2 instances are mostly remaining unused.\nHence, the solution should be to use instances that can cost less but still be able to carry out occasional high computational tasks.\nT2 instances are Burstable Performance Instances that provide a baseline level of CPU performance with the ability to burst above the baseline.\nThe baseline performance and ability to burst are governed by CPU Credits.\nT2 instances accumulate CPU Credits when they are idle and consume CPU Credits when they are active.\nT2 instances are the lowest-cost Amazon EC2 instance option designed to dramatically reduce costs for applications that benefit from the ability to burst to full core performance whenever required.\nOption A is incorrect because there is no issue with the current use of S3.\nOption B is incorrect because adding another large instance is, on the contrary, an expensive solution and would add to the existing cost.\nOption C is CORRECT because T2 instances are cost-effective and also provide a baseline level of CPU performance with the ability to burst above the baseline whenever required.\nOption D is incorrect because this option is not going to make efficient use of the current instances.\nIt will not lower the cost of the architecture.\nFor more information on Instance types, please visit the below URL-\nhttps://aws.amazon.com/ec2/instance-types/t2/\nOption A is not a viable solution as Amazon Glacier is an archival storage service and not suitable for storing frequently accessed files like JPEG files. Moreover, retrieving files from Amazon Glacier can take several hours, which is not acceptable in this scenario.\nOption B is also not a suitable solution as adding additional instances to handle a low load is not cost-effective and can increase operational complexity.\nOption C can be a good solution as T2 instances provide burstable CPU performance, which means that they can handle occasional high computational tasks without incurring high costs. However, it is important to note that T2 instances have a CPU credit system, which means that they can only sustain high CPU usage for a limited time. If the high computational tasks are long-running, it may be more appropriate to use a different instance type.\nOption D is not a viable solution as it suggests using larger files to handle more load, which is not a scalable or efficient solution. The size of the files should be based on their intended use, and increasing their size may not necessarily improve performance.\nBased on the scenario described, a more suitable solution would be to use Auto Scaling groups with EC2 instances optimized for high-performance computing, such as C5, M5, or R5 instances. This would allow for automatic scaling based on demand, ensuring that there are enough resources to handle high computational tasks while minimizing idle time. Additionally, using Amazon S3's Transfer Acceleration feature can improve file transfer speeds, reducing the time it takes to store the JPEG files in S3. Finally, implementing a queuing system like Amazon SQS can help manage and distribute computational tasks, ensuring that instances are utilized efficiently.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Amazon glacier instead of S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add additional large instances by introducing a task group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use T2 instances if possible.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Ensure the application hosted on the EC2 instances uses larger files on S3 to handle more load.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 252,
  "query" : "A large IT company has an on-premises website that provides real-estate information such as rent, house prices, and the latest news to users.\nThe website has a Java backend and a NoSQL MongoDB database used to store subscribers' data.\nYou are a cloud analyst and need to migrate the whole application to the AWS platform.\nYour manager requires that a similar structure should be deployed in AWS for high availability.",
  "answer" : "E.\nCorrect Answer - A, C, D.\nAs this case needs to migrate the on-premises system to AWS using a similar structure, DocumentDB is more appropriate than RDS Aurora.\nDocumentDB is also a NoSQL database compatible with MongoDB.\nBesides, AWS X-Ray is a service that collects data about requests that your application serves and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.\nReference for AWS X-Ray is at https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html.\nOption A is CORRECT because autoscaling would provide high availability.\nOption Bis incorrect because RDS Aurora is a SQL database that is inappropriate in this scenario and brings extra unnecessary efforts.\nOption C is CORRECT because Amazon DocumentDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability and compatible with MongoDB.Option D is CORRECT because AWS X-Ray is suitable to work as a tracing framework.\nIt can monitor the requests from the frontend and requests to the DocumentDB database.\nBelow is a graph that AWS X-Ray can provide.\nNote: Please read \"DynamoDB\" as \"DocumentDB\" instead in the diagram given below.\nPlease refer to the below link for \"DocumentDB\"\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/developerguide.pdf\nOption E is incorrect: Theoretically, CloudWatch can be used to trace the incoming and outgoing requests, although it may bring extra efforts.\nHowever, the service should be CloudWatch Logs rather than CloudWatch Events.\nAmazon CloudWatch Events describe changes in Amazon Web Services (AWS) resources.\nTo migrate the on-premises real estate website with a Java backend and NoSQL MongoDB database to AWS platform, we need to ensure high availability. Here are the possible solutions:\nA. Deploy an Auto Scaling group of Java backend servers to provide high availability.\nAuto Scaling group is a group of instances that automatically scale based on the traffic load. By deploying an Auto Scaling group of Java backend servers, we can ensure high availability by adding more instances when the traffic increases and removing instances when the traffic decreases. This approach also provides load balancing by distributing traffic evenly among all the instances in the group.\nHowever, deploying only an Auto Scaling group does not solve the issue of database high availability.\nB. Use RDS Aurora as the database for the subscriber data because it is highly available and can scale up to 15 Read Replicas.\nRDS Aurora is a high-performance, scalable, and highly available relational database service provided by AWS. Aurora provides automatic replication, automatic failover, and automatic backups to ensure high availability. Aurora also provides up to 15 read replicas that can be used to offload read traffic from the primary database instance. Using Aurora as the database for subscriber data ensures high availability and scalability, and it integrates well with other AWS services.\nC. Create a DocumentDB database to hold subscriber data. Set up an Auto Scaling policy for the read/write throughput.\nDocumentDB is a fully managed, highly available, and scalable NoSQL document database service provided by AWS. DocumentDB is compatible with MongoDB, which makes it easy to migrate MongoDB workloads to DocumentDB. DocumentDB provides automatic backups, point-in-time recovery, and automatic failover to ensure high availability. By setting up an Auto Scaling policy for the read/write throughput, we can ensure that the database can handle increased traffic load.\nD. Use AWS X-Ray SDK to record data about incoming and outgoing requests. View the statistics graph in the X-Ray console.\nAWS X-Ray is a service that provides end-to-end visibility of applications running on AWS. By using the X-Ray SDK, we can record data about incoming and outgoing requests, such as the time taken by each service and any errors that occur. This information can be viewed in the X-Ray console, which provides a statistics graph of the application's performance. However, using X-Ray alone does not provide high availability for the application.\nE. Trace the requests using AWS JAVA SDK and send logs to AWS CloudWatch Events. Create a CloudWatch dashboard to view the statistics.\nAWS CloudWatch is a service that provides monitoring and logging capabilities for AWS resources and applications. By using the AWS Java SDK, we can trace requests and send logs to CloudWatch Events. We can create a CloudWatch dashboard to view statistics such as request latency, errors, and traffic load. However, using CloudWatch alone does not provide high availability for the application.\nConclusion:\nThe best solution for ensuring high availability for the real estate website with a Java backend and NoSQL MongoDB database would be to use RDS Aurora as the database for the subscriber data because it provides automatic replication, automatic failover, and automatic backups to ensure high availability. Additionally, deploying an Auto Scaling group of Java backend servers can help provide load balancing and ensure high availability for the application layer.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy an Auto Scaling group of Java backend servers to provide high availability.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use RDS Aurora as the database for the subscriber data because it is highly available and can scale up to 15 Read Replicas.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a DocumentDB database to hold subscriber data. Set up an Auto Scaling policy for the read/write throughput.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS X-Ray SDK to record data about incoming and outgoing requests. View the statistics graph in the X-Ray console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Trace the requests using AWS JAVA SDK and send logs to AWS CloudWatch Events. Create a CloudWatch dashboard to view the statistics.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 253,
  "query" : "You've been brought in as a solutions architect to assist an enterprise customer with their migration of an e-commerce platform to Amazon Virtual Private Cloud (VPC)\nThe previous architect has already deployed a 3- tier VPC.\nThe configuration is as follows.",
  "answer" : "Answer - D.\nOption A is incorrect because the route should be pointing to NAT.\nOption B is incorrect because adding IGW to route rtb-238bc44b would expose the application and database server to the internet.\nBastion and NAT should be in the public subnet.\nOption C is incorrect because the route should point to NAT and not Internet Gateway else it would be internet accessible.\nOption D is CORRECT because Bastion and NAT should be in the public subnet.\nAs Web Server has direct access to the Internet, the subnet subnet-258bc44d should be public and Route rtb-218bc449 pointing to IGW.\nRoute rtb-238bc44b for private subnets should point to NAT for outgoing internet access.\nSure, I'd be happy to explain the options available for configuring a 3-tier VPC to enable the migration of an e-commerce platform to Amazon Virtual Private Cloud (VPC).\nFirst, let's review the components of a typical 3-tier VPC architecture:\nPublic subnet: contains resources that are exposed to the internet, such as web servers.\nPrivate subnet: contains resources that should not be exposed to the internet, such as application servers and databases.\nData subnet: contains resources that store data, such as databases.\nOption A proposes to create a bastion and NAT instance in subnet-258bc44d and add a route to rtb-238bc44b pointing to subnet-258bc44d.\nThis option involves creating a bastion host in the public subnet (subnet-258bc44d) to allow secure access to instances in the private subnet. A NAT instance is also created in the public subnet to allow private instances to access the internet. The route table (rtb-238bc44b) is updated to direct traffic to the NAT instance in the public subnet.\nOption B proposes to add a route to rtb-238bc44b pointing to igw-2d8bc445 and add a bastion and NAT instance within Subnet-248bc44c.\nThis option involves creating a NAT gateway in the public subnet (subnet-248bc44c) to allow private instances to access the internet. A bastion host is also created in the public subnet for secure access to instances in the private subnet. The route table (rtb-238bc44b) is updated to direct traffic to the NAT gateway in the public subnet.\nOption C proposes to create a Bastion and NAT Instance in subnet-258bc44d. Add a route to rtb-238bc44b pointing to igw-2d8bc445, and a new NACL that allows access between subnet-258bc44d and subnet-248bc44c.\nThis option is similar to Option A, but instead of adding a route to the NAT instance, a new network access control list (NACL) is created to allow traffic between the public and private subnets.\nOption D proposes to create a Bastion and NAT instance in subnet-258bc44d and add a route to rtb-238bc44b pointing to the NAT instance.\nThis option is similar to Option A, but instead of creating a bastion host, only a NAT instance is created in the public subnet (subnet-258bc44d) to allow private instances to access the internet. The route table (rtb-238bc44b) is updated to direct traffic to the NAT instance in the public subnet.\nIn general, all of these options involve creating a bastion host and NAT instance in the public subnet to allow secure access and internet connectivity for instances in the private subnet. The choice between these options will depend on the specific requirements of the enterprise customer and the existing VPC configuration.\nI hope this explanation helps!",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a bastion and NAT Instance in subnet-258bc44d and add a route to rtb-238bc44b pointing to subnet-258bc44d.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a route to rtb-238bc44b pointing to igw-2d8bc445 and add a bastion and NAT instance within Subnet-248bc44c.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Bastion and NAT Instance in subnet-258bc44d. Add a route to rtb-238bc44b pointing to igw-2d8bc445, and a new NACL that allows access between subnet-258bc44d and subnet-248bc44c.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a Bastion and NAT instance in subnet-258bc44d and add a route to rtb-238bc44b pointing to the NAT instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 254,
  "query" : "You work in a video game company, and your team is working on a feature that tells how many times certain web pages have been viewed or clicked.\nYou also created an AWS Lambda function to show some key statistics of the data.\nYou tested the Lambda function, and it worked perfectly.",
  "answer" : "Correct Answer- A.\nPotentially, more than one option may work.\nHowever, this question asks the most cost-efficient and straightforward method that needs to be considered.\nOption A is CORRECT because the AWS CloudWatch Events rule is free and quite easy to begin with.\nTo schedule a daily event at 8:00 AM GMT, you just need to set up a cron rule, as given in the below screenshot.\nOption B is incorrect: Because launching a new EC2 instance for this task is not cost-efficient.\nOption C is incorrect: Because this is not something AWS Batch works.\nFor AWS Batch, it runs as a containerized application on an Amazon EC2 instance in your computing environment.\nOption D is incorrect: Because firstly, it should be “Create rule” rather than “Create Event”\nSecondly, the Cron expression of “ * ? * * 08 00” is incorrect.\nFor More information, Please check below AWS Docs:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\nThe best option for this scenario would be option D: In AWS CloudWatch Events console, click “Create Event” using the cron expression “ * ? * * 08 00”. Configure the target as the Lambda function.\nExplanation: AWS Lambda is a serverless compute service that allows developers to run code without the need to provision or manage servers. AWS Lambda integrates with other AWS services, such as Amazon S3, Amazon DynamoDB, and Amazon Kinesis, to trigger code execution based on events from these services. AWS CloudWatch Events is a service that can be used to schedule and trigger events based on time, resource state changes, or custom events.\nOption A: Create an AWS CloudWatch Events rule that is scheduled using a cron expression. Configure the target as the Lambda function. This option is almost correct, as it uses AWS CloudWatch Events to trigger the Lambda function based on a scheduled cron expression. However, it doesn't provide a specific cron expression to run the Lambda function at 8:00 AM, which is required in the scenario. It is possible to create a cron expression for 8:00 AM using this option, but it requires more effort.\nOption B: Create an Amazon Linux EC2 T2 instance and set up a Cron job using Crontab. Use AWS CLI to call your AWS Lambda every 8:00 AM. This option is not ideal for this scenario because it requires setting up and managing an EC2 instance, which increases the operational cost and complexity. Additionally, it doesn't use AWS CloudWatch Events, which is the recommended way to schedule Lambda functions.\nOption C: Use Amazon Batch to set up a job with a job definition that runs every 8:00 AM for the Lambda function. This option is also not ideal for this scenario because Amazon Batch is mainly used for running batch computing workloads at scale, and it requires more setup and management compared to AWS CloudWatch Events. It is also not the recommended way to schedule Lambda functions.\nOption D: In AWS CloudWatch Events console, click “Create Event” using the cron expression “ * ? * * 08 00”. Configure the target as the Lambda function. This option is the best one for this scenario because it uses AWS CloudWatch Events to schedule the Lambda function execution based on a cron expression that runs at 8:00 AM. It also requires minimal setup and management, making it a cost-effective and efficient solution. The AWS CloudWatch Events console makes it easy to create and manage rules, and it integrates seamlessly with Lambda functions, making it the recommended solution for scheduling Lambda functions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an AWS CloudWatch Events rule that is scheduled using a cron expression. Configure the target as the Lambda function.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an Amazon Linux EC2 T2 instance and set up a Cron job using Crontab. Use AWS CLI to call your AWS Lambda every 8:00 AM.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon Batch to set up a job with a job definition that runs every 8:00 AM for the Lambda function.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In AWS CloudWatch Events console, click “Create Event” using the cron expression “ * ? * * 08 00”. Configure the target as the Lambda function.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 255,
  "query" : "A company has its major business on selling second-hand products.\nIts online trading system is deployed on AWS EC2 instances.\nIn Route53, a domain name has been configured to route the traffic to a Classic Load Balancer.\nAs Classic Load Balancer is quite old and in AWS, there are new types of load balancers that Classic Load Balancer can easily migrate to.\nThe operation team decides to migrate the Load Balancer.\nThey want all the connections between clients and EC2 instances to be kept secure using certificates that they created and want a secure data encryption in transit in adherence to TLS protocol between the clients and EC2 instances.\nWhich choices should be used together to meet the needs?",
  "answer" : "E.\nCorrect Answer - B, E.\nIn order to make the full path secure, there are two parts to be considered if Network Load Balancer is used.\n1, TLS termination on Load Balancer.\nA listener with TLS and port 443 is required.\n2, Configure targets using protocol TLS and port 443.\nThe details can be found in https://network.exampleloadbalancer.com/nlbtls_demo.html.\nOption A is incorrect because the ALB terminates SSL traffic, and the requirement is to get secure traffic up to the EC2 instances.\nOption B is CORRECT because this is the correct way to implement the TLS termination on the load balancer.\nOption C is incorrect because the ALB terminates SSL traffic, and the requirement is to get secure traffic up to the EC2 instances.\nOption D is incorrect because although this option is partially correct, as the listener is based on TLS as Option B, the target protocol cannot be HTTPS.\nOption E is CORRECT because the TLS connection is set up between load balancer and targets.\nTogether with Option B, the connections between clients and EC2 instances are kept secure.\nThe best option to meet the requirements of secure connections and adherence to TLS protocol is to use option A: In the “Create Load Balancer” console, create an Application Load Balancer, add a listener with protocol TLS and port 443 so that the TLS connections terminate at the Load Balancer.\nExplanation:\nClassic Load Balancer is an older type of load balancer in AWS and there are newer types of load balancers that provide better features and performance. The operation team wants to migrate the Classic Load Balancer to a new one. There are two types of load balancers available in AWS, Application Load Balancer (ALB) and Network Load Balancer (NLB).\nALB is a Layer 7 (application layer) load balancer, which means it can route traffic based on application-level information such as HTTP headers and cookies. ALB also provides SSL/TLS termination, which allows clients to terminate their SSL/TLS sessions at the load balancer, and then communicate with the backend instances over HTTP or HTTPS. This way, the load balancer can offload SSL/TLS encryption and decryption from the backend instances, which can help improve performance.\nNLB is a Layer 4 (transport layer) load balancer, which means it can route traffic based on IP protocol data such as TCP and UDP. NLB also provides SSL/TLS termination, which allows clients to terminate their SSL/TLS sessions at the load balancer, and then communicate with the backend instances over TCP or TLS. NLB supports TLS offloading, which can help improve performance.\nTo meet the requirement of secure connections and adherence to TLS protocol, the best option is to use ALB with a listener that terminates TLS connections at the load balancer. This can be achieved by creating an ALB with a listener that listens to the traffic with protocol TLS and port 443. This way, all the connections between clients and ALB are secured with TLS protocol.\nOption B is not the best choice because NLB is a Layer 4 load balancer, which means it does not support SSL/TLS termination at the load balancer. Therefore, it cannot meet the requirement of terminating TLS connections at the load balancer.\nOption C is not the best choice because it uses HTTPS instead of TLS. While HTTPS is a secure protocol that uses SSL/TLS encryption, it is not the same as using TLS directly. Also, it is recommended to use port 443 for HTTPS, not port 80.\nOption D is not the best choice because it only sets up the connections between the load balancer and the backend instances securely. It does not address the requirement of terminating TLS connections at the load balancer.\nOption E is not the best choice because it uses TLS as the target protocol, which is not supported by ALB. Also, it does not address the requirement of terminating TLS connections at the load balancer.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the “Create Load Balancer” console, create an Application Load Balancer, add a listener with protocol TLS and port 443 so that the TLS connections terminate at the Load Balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Go to the Load Balancer console, create a Network Load Balancer with a listener that listens to the traffic with protocol as TLS and port as 443.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an Application Load Balancer with a listener that listens to the traffic with protocol as HTTPS and port 80.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For the new Load Balancer, select the target protocol as HTTPS and port as 443, setting up the connections with targets securely.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For the new Load Balancer, select the target protocol as TLS and port as 443. As a result, the connections between Load Balancer and targets are secure.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 256,
  "query" : "Another team just transferred your team a task to maintain an AWS CodeBuild project that aims to create a docker image for the QA lab environment.\nPreviously, this CodeBuild project was triggered manually at about 20:00 every working day (Mon to Fri) by a QA engineer, which is not quite appropriate.\nYou are considering setting up an automatic trigger for the project.\nAt the same time, when the job runs, using an SNS topic to notify the team by email.\nWhich procedures should you use?",
  "answer" : "Correct Answer- B.\nOption A is incorrect because the CodeBuild project needs to be scheduled every working day (Mon to Fri)\nThe cron expression of “0 20 ? * * *” is incorrect.\nOption B is CORRECT because, in order to schedule the CodeBuild project, the ARN is needed instead of the project name.\nOption C is incorrect because a rule can have 5 targets synchronously.\nOne rule is sufficient.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/cloudwatch_limits_cwe.html.\nOption D is incorrect because CodeBuild is a supported target for AWS CloudWatch Events.\nPlease refer to the below reference on how to schedule automated builds using CodeBuild.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-codebuild.html\nSure, I'd be happy to explain each answer choice and help you understand the best approach for setting up an automatic trigger for the CodeBuild project and configuring SNS notifications.\nAnswer A suggests configuring a new CloudWatch Events rule with a cron expression of \"0 20 ? * * *\", and including the CodeBuild project name and SNS topic as targets. This option is not ideal because it will trigger the CodeBuild project every day at 8:00 PM UTC, regardless of whether it's a weekday or not. This may not be desirable if the project is only meant to run on weekdays.\nAnswer B suggests creating a new CloudWatch Events rule with a cron expression of \"0 20 ? * MON-FRI *\", targeting the CodeBuild project ARN and the SNS topic name. This option is a better approach because it only triggers the CodeBuild project on weekdays at 8:00 PM UTC, and it also sends a notification to the specified SNS topic. This ensures that the team is notified of the build status and can take appropriate action if necessary.\nAnswer C suggests configuring two new CloudWatch Events rules with a cron expression of \"0 20 ? * MON-FRI *\". The first rule targets the CodeBuild project ARN, and the second rule triggers the SNS topic. This approach is not necessary because a single CloudWatch Events rule with both targets can accomplish the same result.\nAnswer D is incorrect because CloudWatch Events does support triggering CodeBuild projects, so an EC2 or on-premise server is not required. Additionally, scheduling the SNS notification separately from the CodeBuild project trigger is not necessary because a single CloudWatch Events rule can accomplish both tasks.\nIn summary, the best approach is to create a new CloudWatch Events rule with a cron expression of \"0 20 ? * MON-FRI *\", targeting the CodeBuild project ARN and the SNS topic name. This will trigger the CodeBuild project at 8:00 PM UTC on weekdays and notify the team of the build status via email.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure a newly created CloudWatch Events rule that has a cron expression of “0 20 ? * * *”. The targets for the rule include the CodeBuild project name and the SNS topic.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new CloudWatch Events rule that has a cron expression of “0 20 ? * MON-FRI *”. The targets for this rule are the CodeBuild project ARN and the SNS topic name.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure two new CloudWatch Events that have a cron expression of “0 20 ? * MON-FRI *”. The target for the first rule is the CodeBuild project ARN and the trigger for the second rule is the SNS topic.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "As AWS CloudWatch Events rule does not support CodeBuild service, an EC2 or an on-premise lightweight server is required to schedule the docker image build of the project. Use a new CloudWatch Events rule to schedule the SNS notification.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 257,
  "query" : "You are an AWS administrator.\nYour company has two key EC2 instances owned by AWS account A.\nThe users in AWS account B may start/stop these EC2 instances from time to time.\nThese users are under the same IAM user group called “Group_QA”\nYou already created a cross-account role “EC2Update” in account A.",
  "answer" : "Correct Answer- B and C.\nYou can grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own.\nThe user chooses the account name on the navigation bar and chooses Switch Role.\nThe user specifies the account ID (or alias) and role name.\nAlternatively, the user can click on a link sent in an email by the administrator.\nThe link takes the user to the Switch Role page with the details already filled in.\nOption A is incorrect because, for AWS API/AWS CLI, the user in the group “Group_QA” should call the AssumeRole function to obtain credentials for the “EC2Update” role.\nOption D is incorrect because it should be “Switch Role” rather than “Switch Accounts,” and no key credentials are needed for switching rules to another account.\nReferences:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html\nAs an AWS administrator, your company has two EC2 instances owned by AWS account A that need to be accessed by users in AWS account B who belong to the \"Group_QA\" IAM user group. You have already created a cross-account role \"EC2Update\" in account A to grant the required permissions to these users.\nTo allow users in account B to start/stop the EC2 instances in account A, they need to assume the \"EC2Update\" role. There are several ways to do this, and the correct answer depends on the specific requirements of your organization.\nLet's go through each answer option in detail:\nA. With AWS CLI, the user calls the AssumeRoleWithSAML function to obtain credentials for the “EC2Update” role.\nThis option requires users to have the AWS Command Line Interface (CLI) installed and configured on their local machine. Users must also be familiar with the CLI and the AssumeRoleWithSAML function, which is used to obtain temporary credentials to assume the \"EC2Update\" role.\nAssuming the \"EC2Update\" role with the CLI involves executing a command similar to the following:\nrubyCopy codeaws sts assume-role-with-saml --role-arn arn:aws:iam::ACCOUNT-A-ID:role/EC2Update --principal-arn arn:aws:iam::ACCOUNT-B-ID:saml-provider/EXAMPLE-IDP --saml-assertion base64-encoded-SAML-assertion \nThis command requires the user to provide the ARN of the \"EC2Update\" role, the ARN of the SAML provider in account B, and a base64-encoded SAML assertion obtained from the SAML provider. Once the command is executed, the CLI returns a set of temporary credentials that can be used to access the resources in account A.\nB. The user chooses the account name on the navigation bar and clicks “Switch Role”. The user specifies the account ID (or alias) and role name.\nThis option requires users to have access to the AWS Management Console and to be familiar with the Switch Role feature. To assume the \"EC2Update\" role, users must click on the account name on the navigation bar and choose \"Switch Role\". They must then specify the account ID (or alias) of account A, the name of the \"EC2Update\" role, and optionally a display name for the role. Once the user clicks \"Switch Role\", they are granted access to the resources in account A.\nC. The user can click on a link sent in an email by the administrator which takes the user to the Switch Role page with the details already filled in. The link can be found when the role “EC2Update” was created.\nThis option involves sending a link to the users in account B that takes them directly to the Switch Role page with the necessary details already filled in. This can be done by copying the link provided by AWS when the \"EC2Update\" role was created and sending it to the users via email or another communication channel. This option requires users to have access to the AWS Management Console and to be familiar with the Switch Role feature.\nD. In the AWS console, the user clicks its account name and chooses “Switch Accounts”. The user then specifies the account ID, key credentials, and the role name for account A.\nThis option requires users to have multiple AWS accounts and to switch between them using the Switch Accounts feature. To assume the \"EC2Update\" role in account A, users must click on their account name in the AWS Management Console and choose \"Switch Accounts\". They must then specify the account ID of account A, their access key ID and secret access key, and the name of the \"EC2Update\" role. Once the user clicks \"Sign In",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "With AWS CLI, the user calls the AssumeRoleWithSAML function to obtain credentials for the “EC2Update” role.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user chooses the account name on the navigation bar and clicks “Switch Role”. The user specifies the account ID (or alias) and role name.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The user can click on a link sent in an email by the administrator which takes the user to the Switch Role page with the details already filled in. The link can be found when the role “EC2Update” was created.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In the AWS console, the user clicks its account name and chooses “Switch Accounts”. The user then specifies the account ID, key credentials, and the role name for account",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 258,
  "query" : "You are an AWS solutions architect in a large IT company, and your company has owned several AWS accounts.\nBy using IAM roles, access to resources in other accounts is granted.\nFor example, users in the Test account may switch roles and operate on DynamoDB resources that belong to the Dev account.",
  "answer" : "Correct Answer - A, D.\nThe below diagram is a reference on how to switch roles between accounts.\nOption A is CORRECT because you cannot switch to a role when you are signed in as the AWS account root user.\nThis is indicated in.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html.\nOption B is incorrect: Because multi-factor authentication (MFA) is optional when a role is created.\nOption C is incorrect because the alias of the target account is optional.\nDuring role switch, the user specifies the account ID (or alias) and role name.\nOption D is CORRECT because it is required for the user to have permission to assume the role that belongs to other accounts.\nIn a multi-account environment in AWS, IAM roles can be used to provide access to resources in other accounts. This allows users to switch roles and operate on resources that belong to a different account without having to sign out and sign in again as a user from that account.\nTo answer the question, we can break down each answer choice:\nA. The user is signed in as the AWS account non-root user. This answer choice is not completely correct or relevant to the scenario given. It is good practice to avoid using the root account for regular tasks and to create non-root users with specific permissions, but it doesn't directly relate to IAM roles or accessing resources in other accounts.\nB. The assuming entity has used multi-factor authentication (MFA) protection. MFA is a security feature that requires users to provide additional authentication factors besides their username and password when signing in to AWS accounts or assuming IAM roles. While MFA can add an extra layer of security, it is not a requirement for using IAM roles to access resources in other accounts.\nC. The target account that the user plans to switch to must use an alias. This answer choice is incorrect. An account alias is a name for an AWS account that is easier to remember and use instead of an account ID number. However, it is not a requirement for using IAM roles to access resources in other accounts.\nD. The user must be explicitly granted permission to assume the role. This answer choice is correct. In AWS, to assume an IAM role, a user must have permission to do so. The user can be granted permission by either adding the user to a group that has permission to assume the role or by attaching an inline policy to the user that allows them to assume the role. It is important to note that the user must have the correct permissions in both the source and target accounts to successfully switch roles and access the resources.\nIn conclusion, the correct answer to the question is D. The user must be explicitly granted permission to assume the role.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The user is signed in as the AWS account non-root user.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The assuming entity has used multi-factor authentication (MFA) protection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The target account that the user plans to switch to must use an alias.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user must be explicitly granted permission to assume the role.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 259,
  "query" : "A supermarket chain had a big data analysis system deployed in AWS.\nThe system has the raw data such as clickstream or process logs in S3\nAn m3.large EC2 instance transformed the data into other formats and saved it to another S3 bucket.\nIt was then moved to Amazon Redshift.",
  "answer" : "Correct Answer - A, D.\nAWS Glue is a service to discover data, transform it, and make it available for search and querying.\nAWS Glue can make all your data in S3 immediately available for analytics without moving the data.\nOption A is CORRECT because Crawler is a key component in AWS Glue that can scan data in all kinds of repositories, classify it, extract schema information from it, and store the metadata automatically in the AWS Glue Data Catalog.\nOption B is incorrect because AWS Glue will generate ETL code in Scala or Python rather than Java.\nOption C is incorrect because AWS Glue does not generate triggers by default.\nMoreover, Cron expressions that lead to rates faster than 5 minutes are not supported.\nOption D is CORRECT because the Glue Data Catalog stores the metadata in the AWS Cloud which is readily available for analysis.\nThe given scenario describes a big data analysis system in AWS, where raw data is stored in S3, and an EC2 instance transforms it and saves it to another S3 bucket before moving it to Amazon Redshift. The question asks which of the following statements is true about AWS Glue, a fully managed extract, transform, and load (ETL) service that makes it easy to move data between data stores.\nA. AWS Glue contains a crawler that connects to the S3 bucket and scans the dataset. Then the service creates metadata tables in the data catalog.\nThis statement is correct. AWS Glue includes a crawler that can connect to various data sources, including Amazon S3, JDBC databases, and DynamoDB tables, and extract metadata such as table definitions, column types, and file formats. The crawler scans the S3 bucket containing the raw data, and then the service creates metadata tables in the Glue Data Catalog, a central metadata repository that stores information about the data assets in your account.\nB. AWS Glue automatically generates code in Java to extract data from the source and transform it to match the target schema.\nThis statement is partially true. AWS Glue provides an ETL workflow that allows you to create and run Python or Scala scripts that extract, transform, and load data. While AWS Glue doesn't automatically generate code in Java, it does provide a visual interface that allows you to create ETL jobs without writing any code.\nC. By default, AWS Glue creates a scheduler to trigger the activated tasks every minute.\nThis statement is false. AWS Glue provides a scheduler that you can use to trigger ETL jobs on a schedule, but it doesn't create a scheduler by default. You have to configure the scheduler to run your jobs according to your requirements.\nD. AWS Glue has a central metadata repository (Glue Data Catalog). The Glue Data Catalog is available for analysis immediately.\nThis statement is correct. AWS Glue provides a central metadata repository called the Glue Data Catalog, which stores metadata about data assets such as databases, tables, and partitions. The Glue Data Catalog is available for analysis immediately and can be queried using SQL or the Glue API. The metadata is stored in a highly available and durable manner, making it accessible to all AWS services and accounts within the same region.\nIn summary, option A and D are correct statements about AWS Glue, while options B and C are false or partially true. AWS Glue can be used to automate the ETL process for big data analysis systems, and it provides a centralized metadata repository that stores information about your data assets.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Glue contains a crawler that connects to the S3 bucket and scans the dataset. Then the service creates metadata tables in the data catalog.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Glue automatically generates code in Java to extract data from the source and transform it to match the target schema.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "By default, AWS Glue creates a scheduler to trigger the activated tasks every minute.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Glue has a central metadata repository (Glue Data Catalog). The Glue Data Catalog is available for analysis immediately.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 260,
  "query" : "Your company runs an online 3D Printing E-commerce site where users can either select or upload their designs and provide the necessary details.\nAfter the payment, the order goes to the production system.\nThe application uses the SQS FIFO queue to decouple the payment from the main printing job.\nOnce the payment information is successfully received, the system sends the job request to the SQS FIFO queue consumed by the spot EC2 instances configured with Auto Scaling.\nThe spot instances prepare the print job and submit it to the queue responsible for the printing.\nThe instances need to wait for the printing job to finish before finishing the message processing.\nDue to the possibility of complex designs, the rendering job may take a while to finish.",
  "answer" : "Correct Answers: B.\nOption A is incorrect because the spot instances are not the issue, as the question does not mention that the instances are terminated.\nOption B is CORRECT because this method can increase the timer for the instances to process the message so that the messages will not return to the queue again.\nOption C is incorrect because the spot instances do not cause the issue.\nChanging the instances to on-demand ones will not help.\nOption D is incorrect because it takes longer to process the messages.\nBut they are still processed successfully.\nA dead-letter queue does not help.\nThe 3D Printing E-commerce site uses the SQS FIFO queue to decouple the payment from the printing job. After the payment is received, the job request is sent to the SQS FIFO queue, which is consumed by the spot EC2 instances configured with Auto Scaling. The spot instances prepare the print job and submit it to the queue responsible for printing. However, the rendering job may take a while to finish, and the instances need to wait for the printing job to finish before finishing the message processing.\nOption A - Increase the price reserved by the spot instances and make sure that AWS does not terminate the spot instances. This option is not recommended since increasing the price reserved by the spot instances can result in higher costs, and it does not address the issue of waiting for the printing job to finish.\nOption B - Increase the visibility timeout of the SQS queue. This option is a possible solution. By increasing the visibility timeout, the spot instances have more time to process the messages before they become visible again in the queue. However, this may not solve the issue entirely if the rendering job takes a long time to finish.\nOption C - Modify the Auto Scaling group to use on-demand instances instead of spot instances. This option is not recommended since using on-demand instances can result in higher costs. Also, it does not address the issue of waiting for the printing job to finish.\nOption D - Configure a dead-letter queue for the messages that are processed unsuccessfully. This option is not related to the issue of waiting for the printing job to finish.\nIn conclusion, the best option in this scenario is to increase the visibility timeout of the SQS queue to allow the spot instances more time to process the messages before they become visible again. However, this solution may not solve the issue entirely if the rendering job takes a long time to finish.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Increase the price reserved by the spot instances and make sure that AWS does not terminate the spot instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Increase the visibility timeout of the SQS queue.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Modify the Auto Scaling group to use on-demand instances instead of spot instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a dead-letter queue for the messages that are processed unsuccessfully.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 261,
  "query" : "Your company manages a high-end auctioning site.\nThe members register their products with the company and manage the actions related to the products.\nThe application is built using NodeJS and MongoDB.\nIt also uses the Redis to cache the current auctioning data and pub/sub to distribute the auction-related events like bidding count and price fluctuations.",
  "answer" : "E.\nCorrect Answers: D and E.\nOption A is INCORRECT because in the Standard queue the order is not preserved.\nOption B is INCORRECT because SNS is mainly used to fan out the messages to multiple consumers.\nIt delivers the same message to multiple consumers and does not guarantee unique message delivery.\nOption C is INCORRECT because SNS does not have a FIFO queue.\nAfter all, it is a notification service (not a queueing service).\nOption D is CORRECT because AWS Lambda can be configured to process the messages from the SQS FIFO queue.\nLambda is serverless and cost-efficient.\nSee more info here: https://aws.amazon.com/blogs/compute/new-for-aws-lambda-sqs-fifo-as-an-event-source/\nOption E is CORRECT because, for an SQS FIFO queue, all the messages with the same Message Group Identifier will be delivered in order.\nMessages with different message group ID values might be sent and received out of order.\nThe scenario described involves an auctioning site built using NodeJS and MongoDB. Redis is being used for caching current auctioning data, while pub/sub is used to distribute auction-related events such as bidding count and price fluctuations. The question asks about the best option for managing message queues.\nA. Use the Simple Queue Service (SQS) standard queue. The standard queue of the Simple Queue Service (SQS) provides a reliable, highly scalable message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. However, it does not guarantee the order in which messages are received, which may not be appropriate for an auctioning site where the order of messages is critical.\nB. Use the Simple Notification Service (SNS), which is similar to the Redis pub/sub, but automatically supports unlimited messages and scales. The Simple Notification Service (SNS) is a fully managed pub/sub messaging service that enables the distribution of messages to multiple recipients, such as email, SMS, and HTTP endpoints. SNS is a suitable option for an auctioning site because it is similar to Redis pub/sub but scales automatically and supports an unlimited number of messages.\nC. Create and use the Simple Notification Service (SNS) FIFO queue. The Simple Notification Service (SNS) FIFO queue guarantees the order in which messages are sent and received. However, SNS FIFO queues do not support deduplication of messages, which can cause problems in some scenarios.\nD. Use the Simple Queue Service (SQS) FIFO queue and migrate the bid processing module to the Lambda. The Simple Queue Service (SQS) FIFO queue is similar to the standard queue, but it guarantees that messages are received in the order they were sent. This option is appropriate for an auctioning site because the order of messages is critical. The use of AWS Lambda for processing bids can help reduce the load on the application servers.\nE. Configure a message group identifier to represent a distinct ordered message group within the FIFO queue. The Simple Queue Service (SQS) FIFO queue supports the use of a message group identifier to represent a distinct ordered message group within the queue. This feature can be used to group related messages together, which can help ensure that messages are processed in the correct order.\nIn conclusion, the best option for managing message queues for an auctioning site built using NodeJS and MongoDB is option D: Use the Simple Queue Service (SQS) FIFO queue and migrate the bid processing module to the Lambda. This option guarantees the order of messages while reducing the load on application servers by processing bids using AWS Lambda.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the Simple Queue Service (SQS) standard queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the Simple Notification Service (SNS), which is similar to the Redis pub/sub, but automatically supports unlimited messages and scales.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create and use the Simple Notification Service (SNS) FIFO queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the Simple Queue Service (SQS) FIFO queue and migrate the bid processing module to the Lambda.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure a message group identifier to represent a distinct ordered message group within the FIFO queue.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 262,
  "query" : "A Node.js software team has developed a recipe application that end-users can search and share recipes.\nThe team deployed the app in AWS EC2 instances with a network load balancer.\nA recent security audit has discovered an issue that the load balancer listener is not using TLS.\nThat means the application is at a security risk and is prone to be threatened by man in the middle attack.\nWhich below choices are correct to help fix this issue? Select 2.",
  "answer" : "E.\nCorrect Answer - A, C.\nFor network load balancer, if TLS is used for the listener, a certificate must be identified for the listener to use as below.\nThe certificate can come from ACM or IAM.\nACM integrates with Elastic Load Balancing so that you can deploy the certificate on your load balancer.\nHowever, there are regions that ACM is not supported.\nA certificate in IAM would be another alternative.\nDetails can be found in https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html.\nOption A is CORRECT because configuring a certificate in ACM is a recommended method when TLS is chosen for the listener.\nOption B is incorrect because HTTPS is for an application load balancer rather than a network load balancer.\nOption C is CORRECT because a certificate in IAM can be used for TLS listeners.\nOption D is incorrect because KMS is not a valid service to provide a certificate for TLS listeners.\nAnd the protocol of SSL is inaccurate for network load balancer as well.\nOption E is incorrect because the Secrets Manager cannot provide a certificate for TLS listeners.\nThe correct options to fix the issue of a Node.js application deployed in AWS EC2 instances with a network load balancer, where the load balancer listener is not using TLS and is at security risk and prone to man-in-the-middle attacks, are A and B.\nOption A: Modify the listener protocol to TLS and choose an existing certificate in ACM (AWS Certificate Manager).\nThis option modifies the listener protocol to TLS, which is a more secure protocol than HTTP or SSL. TLS provides encryption and authentication mechanisms to ensure that data transmitted over the network is secure and cannot be intercepted by attackers. Choosing an existing certificate from ACM (AWS Certificate Manager) ensures that the certificate is valid and trusted by clients. This option is a good choice for applications that require high security.\nOption B: Change the listener protocol to HTTPS. Import a new certificate to ACM (AWS Certificate Manager) and use it in the load balancer listener.\nThis option changes the listener protocol to HTTPS, which is a combination of HTTP and TLS. HTTPS provides authentication, encryption, and integrity protection mechanisms, making it a more secure protocol than HTTP or SSL. Importing a new certificate to ACM ensures that the certificate is valid and trusted by clients. This option is a good choice for applications that require high security.\nOption C: Edit the listener by changing the protocol to TLS. Use an existing certificate from IAM.\nThis option modifies the listener protocol to TLS, which is a more secure protocol than HTTP or SSL. However, using a certificate from IAM is not recommended as IAM is not a certificate authority and does not provide certificate management services. This option is not a good choice for applications that require high security.\nOption D: Modify the listener protocol to SSL and choose an existing certificate in KMS (AWS Key Management Service).\nThis option modifies the listener protocol to SSL, which is an older protocol that has been deprecated in favor of TLS. SSL is less secure than TLS and is no longer recommended for use. Choosing an existing certificate from KMS is also not recommended as KMS is a key management service and does not provide certificate management services. This option is not a good choice for applications that require high security.\nOption E: Edit the listener by changing the protocol to TLS. Upload a new key to the Secrets Manager and link the key for the listener to use.\nThis option modifies the listener protocol to TLS, which is a more secure protocol than HTTP or SSL. However, uploading a new key to the Secrets Manager is not recommended as Secrets Manager is a secrets management service and does not provide certificate management services. This option is not a good choice for applications that require high security.\nIn summary, options A and B are the best choices to fix the issue of a Node.js application deployed in AWS EC2 instances with a network load balancer, where the load balancer listener is not using TLS and is at security risk and prone to man-in-the-middle attacks. These options provide a more secure protocol (TLS or HTTPS) and ensure that the certificate is valid and trusted by clients.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Modify the listener protocol to TLS and choose an existing certificate in ACM (AWS Certificate Manager).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Change the listener protocol to HTTPS. Import a new certificate to ACM (AWS Certificate Manager) and use it in the load balancer listener.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Edit the listener by changing the protocol to TLS. Use an existing certificate from IAM.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Modify the listener protocol to SSL and choose an existing certificate in KMS (AWS Key Management Service).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Edit the listener by changing the protocol to TLS. Upload a new key to the Secrets Manager and link the key for the listener to use.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 263,
  "query" : "A customer is deploying an SSL-enabled Web application on AWS that reads the certificate from ACM.\nThey would like to implement a separation of roles between the EC2 service administrators entitled to login to Instances and make API calls and the security officers who maintain and have exclusive access to the application's X.509 certificate contains the private key.\nWhich configuration option would satisfy the above requirements?",
  "answer" : "Answer - A.\nOption A is CORRECT because (a) only the security officers have access to the certificate store, and (b) the certificate is not stored on EC2 instances, hence avoiding giving access to it to the EC2 service administrators.\nOption B is incorrect because it will still involve storing the certificate on the EC2 instances and additional configuration overhead to access the security officers, which is unnecessary.\nOptions C and D are both incorrect because giving EC2 instances access to the certificate should be avoided.\nIt is better to let ELB manage the SSL certificate, instead of the EC2 web servers.\nFor more information, please refer to the links given below.\nhttp://docs.aws.amazon.com/IAM/latest/APIReference/API_UploadServerCertificate.html https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/ https://docs.aws.amazon.com/acm/latest/userguide/authen-overview.html\nThe customer wants to deploy an SSL-enabled web application on AWS, and they want to separate the roles between the EC2 service administrators and the security officers. The EC2 service administrators should only have access to login to instances and make API calls, whereas the security officers should maintain and have exclusive access to the X.509 certificate containing the private key.\nTo satisfy these requirements, the following configuration options could be considered:\nOption A: Configure IAM policies authorizing access to the ACM certificate store only to the authorized security officers. This option involves using AWS Identity and Access Management (IAM) policies to restrict access to the ACM certificate store. Only the authorized security officers will be able to access the certificate store. This option provides the desired separation of roles and limits access to the certificate to only the security officers. The web servers can retrieve the certificate from the ACM certificate store during boot time.\nOption B: Configure system permissions on the web servers to restrict access to the certificate only to the authorized security officers. This option involves configuring system permissions on the web servers to restrict access to the certificate. Only the authorized security officers will have access to the certificate. This option provides some level of separation of roles, but it is not as secure as Option A because system administrators with privileged access to the web servers could potentially bypass the restrictions and gain access to the certificate.\nOption C: Upload the certificate on an S3 bucket owned by the security officers and accessible only by the EC2 role of the web servers. This option involves uploading the certificate to an S3 bucket owned by the security officers and restricting access to the bucket to only the EC2 role of the web servers. This option provides some level of separation of roles, but it is not as secure as Option A because the certificate is stored in an S3 bucket, which is a shared resource, and there is a potential for unauthorized access.\nOption D: Configure the web servers to retrieve the certificate upon boot from a CloudHSM that is managed by the security officers. This option involves using AWS CloudHSM to store the certificate and configure the web servers to retrieve the certificate during boot time. Only the authorized security officers will have access to the CloudHSM, providing the desired separation of roles. However, this option is more complex to implement and requires additional resources and configuration.\nIn conclusion, Option A is the best option for satisfying the customer's requirements. It provides the desired separation of roles and limits access to the certificate to only the security officers.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure IAM policies authorizing access to the ACM certificate store only to the authorized security officers.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure system permissions on the web servers to restrict access to the certificate only to the authorized security officers.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Upload the certificate on an S3 bucket owned by the security officers and accessible only by the EC2 role of the web servers",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the web servers to retrieve the certificate upon boot from a CloudHSM that is managed by the security officers.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 264,
  "query" : "Your company runs a customer-facing event registration site built with a 3-tier architecture with web and application tier servers and a MySQL database.\nThe application requires 6 web tier servers and 6 application tier servers for normal operation but can run on a minimum of 65% server capacity and a single MySQL database.\nWhen deploying this application in a region with three availability zones (AZs) which architecture provides high availability?",
  "answer" : "Answer - D.\nIn this scenario, the application can run on a minimum of 65% of the overall server capacities.\nI.e.\nit can run on a minimum of 4 web and 4 application servers.\nSince there are 3 AZs, there are many ways the instances can be put across them.\nThe most important point to consider is that even if an entire AZ becomes unavailable, there should be a minimum of 4 servers running.\nSo, placing 3 servers in 2 AZs is not a good architecture.\nBased on this, options A and C are incorrect.\nThe best solution would be to have 2 servers in each AZ.\nSo, in case of an entire AZ being unavailable, the application still has 4 servers available.\nNow, regarding the RDS instance, the high availability is provided by the Multi-AZ deployment, not by read replicas (although they improve the performance in case of read-heavy workload)\nSo, option B is incorrect.\nHence, option D is CORRECT because (a) it places 2 EC2 instances in each of the 3 AZs, and (b) it uses the Multi-AZ deployment of RDS.\nThe correct answer is D: A web tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS deployment.\nExplanation:\nThe question requires us to design a high availability architecture for a customer-facing event registration site that uses a 3-tier architecture with web and application tier servers and a MySQL database. The application requires 6 web tier servers and 6 application tier servers for normal operation but can run on a minimum of 65% server capacity and a single MySQL database. The architecture needs to be deployed in a region with three availability zones (AZs).\nOption A proposes deploying the web tier across 2 AZs and the application tier across 2 AZs with a single RDS instance deployed in another AZ with read replicas. While this option provides some level of fault tolerance, it doesn't meet the requirements of the application, which requires 6 web tier servers and 6 application tier servers for normal operation. Additionally, a single RDS instance with read replicas does not provide the required level of availability as the primary RDS instance is a single point of failure.\nOption B proposes deploying the web tier and application tier across all 3 AZs with 2 instances in each AZ and a single RDS instance with read replicas deployed in two other AZs. While this option deploys the required number of instances, it still relies on a single RDS instance as the primary database, which is a single point of failure.\nOption C proposes deploying the web and application tiers across 2 AZs with 3 instances in each AZ and a Multi-AZ RDS deployment. While this option provides some level of fault tolerance, it doesn't take full advantage of the availability zones available in the region and may not provide enough capacity to handle the normal operation of the application.\nOption D proposes deploying the web and application tiers across all 3 AZs with 2 instances in each AZ and a Multi-AZ RDS deployment. This option meets the requirements of the application by deploying the required number of instances and takes full advantage of the availability zones available in the region. Additionally, the use of a Multi-AZ RDS deployment provides high availability for the database tier.\nIn summary, Option D is the best choice for a high availability architecture for the customer-facing event registration site.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the two other AZs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database services) deployment.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 265,
  "query" : "Your company has developed a suite of business analytics services as a SaaS application used by hundreds of customers worldwide.\nRecently there has been an acquisition of a product, and the management has decided to integrate the product with the main service.\nThe product also runs onto the AWS platform.\nThe initial phase required the product software to use some private resources of the main SaaS service.",
  "answer" : "E.\nCorrect Answers: A and D.\nOption A is CORRECT because the request is made from the product's AWS account and the resource was part of the main AWS account.\nThe user will have to check the log trail of both the accounts and match the user token being used.\nOption B is INCORRECT because the SaaS application's CloudTrail logs will not reveal the user identity.\nThe cross-account role issues a token, and all the further interaction is logged with that token.\nTo know which user the token belongs to, the auditor will have to look into the product account's log trail as well.\nOption C is INCORRECT because the DeleteBucket will not have the user identity information.\nThe log will have the user token information only, as the API was invoked with a cross-account role.\nOption D is CORRECT because, at the time of assuming the role into the main AWS account, the product team's AWS account must have created an entry with the sharedEventId and the userIdentity information.\nSharedEventId helps to identify the real user, and userIdentity provides the IAM ARN that performs the action.\nThese two can help to find who has executed the DeleteBucket API.\nPlease check the references in https://docs.aws.amazon.com/awscloudtrail/latest/userguide/shared-event-ID.html and https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html.\nOption E is INCORRECT because the userIdentity information will only be available inside the product team's AWS account in response to the AssumeRole operation.\nThe sharedEventId will be available in both the account's log trail though.\nBased on the information provided, it seems that there was an attempt to access private resources of the main SaaS service from the product software running on AWS. To investigate this issue, the auditing team can use AWS CloudTrail, which is a service that provides a record of actions taken by a user, role, or an AWS service in your AWS account.\nOption A suggests that the auditing team will need the CloudTrail logs detail of both the SaaS and the product AWS accounts as the call was made from the product application's AWS account. While this might provide some information about the call made, it does not specifically address the need to find the details about the private resources accessed.\nOption B suggests that the auditing team can find the details only from the SaaS application's AWS account as the bucket was part of that account. However, this option assumes that the private resources accessed were stored in a bucket, which may not necessarily be the case.\nOption C suggests that the auditing team should look for the DeleteBucket API record in the SaaS application's AWS account CloudTrail logs. While this option may provide some information about the bucket that was deleted, it does not address the fact that the private resources were accessed.\nOption D suggests that the auditing team should look for the sharedEventId and the userIdentity for the DeleteBucket API event in both AWS accounts. This option seems to be more relevant as it addresses the need to find the details about the private resources accessed. By looking at the sharedEventId and userIdentity, the auditing team can determine the user or role that was responsible for the action and identify the specific private resources accessed.\nOption E suggests that the auditing team should look for the sharedEventId and the userIdentity for the AssumeRole API event in both AWS accounts. This option may not be relevant as it assumes that the action was performed using an assumed role, which may not necessarily be the case.\nIn conclusion, Option D seems to be the most appropriate answer, as it addresses the need to find the details about the private resources accessed by looking for the sharedEventId and userIdentity for the DeleteBucket API event in both AWS accounts.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The auditing team will need the CloudTrail logs detail of both the SaaS and the product AWS accounts as the call was made from the product application’s AWS account.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The auditing team can find the detail only from the SaaS application’s AWS account, as the bucket was part of that account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Look for the DeleteBucket API record into the SaaS application’s AWS account CloudTrail logs. It should have a user Id and the bucket detail as part of the log detail.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Look for the sharedEventId and the userIdentity for the DeleteBucket API event in both AWS accounts.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Look for the sharedEventId and the userIdentity for the AssumeRole API event in both AWS accounts.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 266,
  "query" : "Your company runs a successful medical sampling application onto the AWS cloud and uses various AWS services like EC2, EBS, S3, DynamoDB, etc.\nDue to their business nature, they have an internal audit and compliance team that regularly audits the security posture and takes up various compliance-related activities on a strict basis.\nThe management has decided to go for an external tool to add to the internal auditing process.\nThe management has decided to use a 3rd-party tool that helps them quickly do the auditing and compliance scanning and generate reports.",
  "answer" : "E.\nAnswers: B and D.\nOption A is INCORRECT because it is advisable to use the ExternalId to secure your AssumeRole calls further.\nThis feature is only available via the CLI and API and not via the console.\nOption B is CORRECT because the auditing team is correct, and the AssumeRole can be further secure down with the use of ExternalId while giving access to the external tools.\nOption C is INCORRECT because it's just the ExternalId and not the OwnerId which you can pass along the AssumeRole API.\nOption D is CORRECT because you can use the ExternalId parameter while making the AssumeRole API call.\nOption E is INCORRECT because if the cross-account role is set with the ExternalId, the policy should be modified to add the ExternalId.\nThe correct answer to this question is D. Use the ExternalId with the AssumeRole API.\nExplanation:\nWhen using a 3rd-party tool for auditing and compliance scanning, it is important to ensure that the tool has the necessary permissions to access the AWS resources required for scanning. One way to provide this access is by using AWS IAM roles with AssumeRole API.\nAssumeRole API allows you to grant access to AWS resources to an external account, IAM user, or 3rd-party tool. It is important to ensure that this access is secure and limited to the intended recipient. This is where ExternalId comes in.\nExternalId is a unique identifier that can be passed along with AssumeRole API to provide an additional layer of security. When you use ExternalId, you ensure that only the intended recipient of the IAM role can assume it.\nIn this scenario, the management has decided to use a 3rd-party tool for auditing and compliance scanning. To ensure that this tool has the necessary permissions to access the AWS resources, you can use an IAM role with AssumeRole API and provide an ExternalId.\nBy using ExternalId, you ensure that only the intended 3rd-party tool can assume the IAM role and access the AWS resources. This adds an extra layer of security and helps protect against unauthorized access.\nOption A is incorrect because it assumes that no additional security is needed, which is not true. It is important to ensure that access to AWS resources is secure and limited to the intended recipient.\nOption B is incorrect because it assumes that the ExternalId should be used with the OwnerId, which is not necessary. ExternalId can be used on its own with the AssumeRole API.\nOption C is incorrect because it assumes that both OwnerId and ExternalId are required, which is not true. ExternalId can be used on its own with the AssumeRole API.\nOption E is incorrect because it assumes that no modification is needed to the IAM policy, which is not true. You need to add the ExternalId condition to the IAM policy to ensure secure access.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The security team is correct, and you do not need any extra security to verify the access.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The auditing team is correct, and you can use the External Id to secure the access further.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the OwnerId and ExternalId with the AssumeRole API.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the ExternalId with the AssumeRole API.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You do not need to modify the IAM policy to add the ExternalId condition.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 267,
  "query" : "Your organization has hundreds of developers using AWS accounts.\nBased on the organization policy, when a developer joins the company, a new AWS account is created for that user and added to the AWS Organisation for development and testing purposes.",
  "answer" : "E.\nCorrect Answers: A, B, C, and E.\nOption A is CORRECT because the SCP or Service Control Policies can be used to allow/deny different AWS services depending on the requirement.\nOption B is CORRECT because the CloudWatch Events can be tracked based on CloudTrail API calls.\nThis can be the starting point for collecting the information from the user accounts.\nFor example, some CloudWatch Events can be triggered for an EC2 instance has been launched.\nHowever, this alone will not work, and some form of analysis will require to run on these events.\nOption C is CORRECT because the CloudTrail logs can be aggregated to the central S3 bucket and analyzed there.\nOption D is INCORRECT because this will not be an effective and scalable solution.\nThere will be thousands of events, and it may not make any sense to process each of them for malicious activities.\nOption E is CORRECT because Users and roles must still be granted permissions with appropriate IAM permission policies.\nA user without any IAM permission policies has no access at all, even if the applicable SCPs allow all services and all actions.\nThe organization has a policy to create a new AWS account for each developer that joins the company, and these accounts are added to the AWS Organization for development and testing purposes. To ensure security and compliance, the following measures can be taken:\nA. Implement Service Control Policies (SCPs) to whitelist or blacklist different AWS services depending on the user role. SCPs are used to set boundaries for what actions users can take in an AWS account. By creating SCPs that restrict certain services and actions based on the user role, the organization can ensure that users are only allowed to use the services that are necessary for their job functions.\nB. Use CloudWatch Events to track the user activities. CloudWatch Events can be used to monitor user activities, including API calls, console sign-in events, and more. By setting up event rules, the organization can be notified when specific events occur, such as failed login attempts or changes to security group rules.\nC. Enable CloudTrail in the user accounts to track and log user activities, and redirect the logs to the organization-wide S3 bucket for processing. CloudTrail provides a record of API calls and other activities that occur within an AWS account. By enabling CloudTrail in the user accounts, the organization can track and log user activities and redirect the logs to a central S3 bucket for processing and analysis.\nD. Run AWS Lambda on individual user accounts to check for malicious activities. AWS Lambda can be used to run custom code in response to specific events, such as API calls or changes to AWS resources. By running Lambda functions in individual user accounts, the organization can detect and respond to malicious activities in real-time.\nE. Assign IAM policies to only allow certain activities. IAM policies can be used to grant permissions to specific AWS resources and actions. By assigning IAM policies that only allow users to perform necessary activities, the organization can restrict access to sensitive resources and prevent users from performing unauthorized actions.\nOverall, by implementing these measures, the organization can ensure that its AWS accounts are secure and compliant with industry standards and regulations.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Implement Service Control Policies to whitelist or blacklist different AWS services depending on the user role.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the CloudWatch Events to track the user activities.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Enable CloudTrail in the user accounts to track and log user activities, and redirect the logs to the organization-wise S3 bucket for processing.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Run AWS Lambda on individual user accounts to check for malicious activities.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Assign IAM policies to only allow certain activities.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 268,
  "query" : "Your company decided to improve on the current batch processing model and start using the AWS Batch instead of the currently running SQS and EC2 with Auto Scaling configurations.\nYour processing jobs are using Docker containers.\nYou also expect the jobs to grow in future.\nThese jobs pull messages from the AWS SQS queue and write the data into DynamoDB tables once the processing is completed.\nYou have created a small Proof-of-concept application to try out the AWS Batch before you actually migrate your job containers.",
  "answer" : "E.\nCorrect Answers: B, C, and D.\nOption Ais INCORRECT because the CloudWatch Log group does not impact the Batch job's running status.\nPlease refer to the following link:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/batch-job-stuck-runnable-status/\nOption B is CORRECT because no internet access for the compute resources may result in this issue.\nRefer to the link: https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html.\nOption C is CORRECT because if the job has asked for more resources than the attached compute environment has, the Batch will not schedule the job to execute, and the job will stay in the RUNNABLE state forever.\nOption D is CORRECT because the job may require starting new instances to run the additional containers.\nIf the AWS account has reached its limit, the service will not start new containers, and the job will remain in the RUNNABLE state.\nOption E is INCORRECT because the question has not mentioned connecting to the SQS privately from the VPC.\nVPC Endpoint is not required for AWS Batch to work.\nSure, I can provide a detailed explanation of the answer options for this scenario.\nOption A: Make sure a CloudWatch Log group is set up for the Batch job. Explanation: AWS Batch logs can be sent to Amazon CloudWatch Logs. It's important to set up a CloudWatch Log group for the Batch job, as it enables logging, monitoring, and debugging of the Batch job. You can also set up alarms and notifications based on the log data to help with troubleshooting.\nOption B: Make sure Internet Access is available to your container. Explanation: Docker containers running on AWS Batch require internet access to pull images from the container registry. If internet access is not available, the containers cannot be created. Hence, it's essential to ensure internet access is available for the Batch job to run.\nOption C: Make sure suitable CPU and RAM are allocated for the Batch job. Explanation: AWS Batch allows specifying CPU and memory resources for each job. Proper allocation of CPU and RAM is necessary to ensure that the job runs efficiently and within the specified time. Oversizing or undersizing can cause performance issues or excessive costs.\nOption D: Make sure your AWS account has not reached the EC2 limit. Explanation: AWS Batch uses EC2 instances to run jobs, and it's important to ensure that your AWS account has enough EC2 instances available to run your Batch jobs. You can check your EC2 instance limits using the AWS Service Quotas console.\nOption E: Make sure the VPC Endpoint is set to access the SQS queue from the containers inside the VPC. Explanation: If your Batch job is running within a VPC, it's essential to set up a VPC endpoint to access the SQS queue from within the VPC. This eliminates the need to route traffic outside the VPC, which improves security and network performance. Without this, the containers would not be able to access the SQS queue.\nIn summary, all the answer options are essential considerations when migrating batch processing from SQS and EC2 to AWS Batch with Docker containers. You should ensure that a CloudWatch Log group is set up, internet access is available to containers, suitable CPU and RAM are allocated, EC2 instance limits are not reached, and the VPC endpoint is set up to access the SQS queue from within the VPC.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Make sure a CloudWatch Log group is set up for the Batch job.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Make sure Internet Access is available to your container.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Make sure suitable CPU and RAM are allocated for the Batch job.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Make sure your AWS account has not reached to the EC2 limit.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Make sure the VPC Endpoint is set to access the SQS queue from the containers inside the VP.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 269,
  "query" : "Your company is running a premium photo-sharing application.\nUsers can upload their creative photos and license them to be used by others.\nThe application allows the users to watermark and do other signatures before the images can be visible to everyone in the public stream.\nThe watermarking and signature are based on the user-selected plan and allowed to do a certain number of images in a given timeframe.\nAlong with this, the company has collaborated with some users and groups.\nThey are allowed to process images faster compared to others whenever they submit their images.",
  "answer" : "Correct Answer: C.\nOption A is INCORRECT because while the SQS will definitely help decouple and streamline the message processing.\nIt will still not be processed any desired order.\nThe consumer application will receive the SQS queue messages and will have to use the additional logic to schedule and prioritize the processing.\nOption B is INCORRECT because just saving the processing information to the DynamoDB will not help in the scheduling or ordering the messages.\nThe customer application will still receive the message from the SQS in a different order and will have to run the appropriate logic to schedule the tasks.\nOption C is CORRECT because it always allows the consumer to check the premium queue first.\nIf the premium queue is empty, it will pull from the member queue, satisfying both the requirements to re-architect and processing some images before others.\nYou can also scale your EC2 instances, processing the images based on need as well.\nPlease refer to the following link-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\nOption D is INCORRECT because using AWS Batch to create jobs is not necessary here.\nThe best solution for this scenario would be option C - Use two SQS queues instead of Lambda Trigger. One with high priority messages and another for the low priority messages. Check for the messages into the high priority queue before processing any messages from the low priority queue.\nHere's why:\nThe application allows users to upload their photos and apply watermarks or signatures before they are publicly visible. These watermarks/signatures are based on user-selected plans that allow a certain number of images to be processed in a given time frame. Additionally, some users and groups are allowed to process images faster than others.\nSQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It provides a reliable, highly scalable, and secure way to transmit any volume of data between software components.\nIn this scenario, using SQS would allow the application to handle processing the images in a more efficient manner. Instead of using a Lambda trigger, two separate SQS queues could be created - one for high priority messages and another for low priority messages.\nWhen a user uploads a photo, the photo is added to the appropriate SQS queue based on their selected plan. Users with higher priority plans would have their photos added to the high priority queue, while users with lower priority plans would have their photos added to the low priority queue.\nA worker instance would then be responsible for pulling messages from the high priority queue first and processing them accordingly. Only after all high priority messages have been processed would the worker instance then start processing messages from the low priority queue. This ensures that users with higher priority plans get their photos processed more quickly.\nFurthermore, processing information could be saved to DynamoDB before saving the image to S3. When the AWS Lambda trigger runs, it can pull the information from DynamoDB and process accordingly.\nUsing two separate SQS queues and prioritizing messages based on user-selected plans would help to ensure that the application is able to handle processing the images in an efficient and fair manner. It would also help to reduce the processing time for users with higher priority plans, which would improve their overall experience with the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use SQS queues instead of Lambda Trigger. Use the priority order messages to process the images.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Save the processing information to DynamoDB before saving the image to S3. When the AWS Lambda trigger runs, pull the information from DynamoDB and process accordingly.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use two SQS queues instead of Lambda Trigger. One with high priority messages and another for the low priority messages. Check for the messages into the high priority queue before processing any messages from the low priority queue.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the AWS Batch to create jobs with priority job queues and use the combination of EC2 On-Demand and Spot instances to process the messages.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 270,
  "query" : "Your company runs a business management applications on the AWS cloud and has hundreds of customers.\nDuring the peak load, they receive hundreds of requests per second.\nThe overall application uses many AWS services like API Gateway, Lambda, DynamoDB, Kinesis, RDS, etc.\nThe product allows users to customize their environment based on their requirements.",
  "answer" : "E.\nCorrect Answers: C and E.\nOption A is INCORRECT because the CloudWatch Logs generated from these individual requests will, by default, not have any correlation identifier, and it will have to be done manually to tie up the original requests with all the logs.\nOption B is INCORRECT because while it may be possible to do, it will also require some usage of a correlation identifier to isolate interactions among different services.\nOption C is CORRECT because the X-Ray integrates well into the AWS services, and it can manage the request relation concerning other AWS services.\nAWS X-Ray provides wrappers for other AWS services as well as traces requests with more detailed information.\nOption D is INCORRECT because while this is doable, it may require an additional effort to log and generate custom metrics data.\nCloudWatch custom metrics are generally used where the custom and application-specific monitoring is required, like Disk Space Consumed or Payment Gateway Error, etc.\nOption E is CORRECT because, with the use of AWS X-Ray segments, detail tracing can be enabled.\nAnnotations can help pinpoint specific areas of the application in the tracing records to isolate the issues and impact area further.\nThis question pertains to optimizing the performance of a business management application on AWS cloud during peak load. The application uses various AWS services such as API Gateway, Lambda, DynamoDB, Kinesis, and RDS, among others. The application also allows customization of user environments.\nThe best answer to this question is C - Use X-Ray to monitor the performance of your application.\nX-Ray is a powerful AWS service that enables application tracing to help developers analyze and debug their applications' performance. X-Ray helps to identify the root cause of performance bottlenecks and errors in a distributed environment.\nBy using X-Ray, the performance of individual services can be monitored, and their response time can be tracked. X-Ray can also help identify the dependencies between services in the application architecture. This information can help optimize the application for peak performance and identify potential issues that may arise during peak load.\nOption A is incorrect because using CloudWatch Logs and Kinesis is not a direct solution to monitor application performance. It is primarily used for analyzing logs and identifying issues in real-time.\nOption B is incorrect because using a third-party tool adds additional complexity and cost to the solution. It is better to use the AWS-provided services for monitoring and analyzing the performance of the application.\nOption D is incorrect because CloudWatch custom metrics are not sufficient for monitoring the performance of a distributed application. Custom metrics only provide information about the specific metric being monitored and not about the application's overall performance.\nOption E is incorrect because although X-Ray segments and annotations are useful for group-level tracing, they do not provide a comprehensive view of the application's overall performance.\nIn conclusion, the best solution for monitoring the performance of a business management application on AWS during peak load is to use X-Ray to monitor and optimize the application for peak performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Publish logs using the CloudWatch Logs and redirect the logs to Kinesis to inspect the issues in real-time.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Due to the distributed nature of the application and multiple services used, use the 3rd party tool and integrate it with it.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use X-Ray to monitor the performance of your application.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use CloudWatch custom metrics to publish and generate events for the notifications.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use X-Ray segments and annotations for group-level tracing.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 271,
  "query" : "Your company hosts an on-premises legacy engineering application with 900GB of data shared via a central file server.\nThe engineering data consists of thousands of individual files ranging in size from megabytes to multiple gigabytes.\nEngineers typically modify 5-10 percent of the files a day.\nYour CTO would like to migrate this application to AWS, with minimal downtime during normal business hours ( days )\nYou calculate that it will take a minimum of 48 hours to transfer 900GB of data using your company's existing 45-Mbps Internet connection.",
  "answer" : "Answer - B.\nIn this scenario, the following important points need to be considered - (i) only the fraction of the data (5-10%) is modified every day, (ii) there are only 48 hrs for the migration, (iii) downtime should be minimized, and (iv) there should be no data loss.\nOption A is incorrect because even though it is theoretically possible to transfer 972GB of data in 48 hours with 45Mbps speed, this option will only work if you consistently utilize the bandwidth to the max.\nThis option will have less time in hand if there are any problems with the multipart upload.\nHence, not a practical solution.\nOption B is a proactive approach, which is CORRECT because the data changes are limited and can be propagated over the week.\nAlso, the bandwidth would be used efficiently, and you would have sufficient time and bandwidth in hand, should there be any unexpected issues while migrating.\nOption C is incorrect because physically shipping the disk to Amazon would involve many external factors such as shipping delays, loss of shipping, damage to the disk, and the time would not be sufficient to import the data in a day.\nThis is a very risky option and should not be exercised.\nOption D is incorrect because AWS Storage Gateway involves creating S3 snapshots and synchronizing.\nThis option is slow and may not meet the limitation of 48 hrs downtime.\nBesides, using EBS volume to store a large amount of data is not cost-efficient.\nPlease view the below video for best practices for cloud migration to AWS:\nhttps://www.youtube.com/watch?v=UpeV4OqB6Us&amp;list=PL_RVC-cMNyYTz8zlxq117O1bfji-knooI&amp;index=23\nSure, I'll be happy to explain the answer options for this question in detail:\nOption A: Copy the data to Amazon S3 using multiple threads and multi-part upload for large files over the weekend, and work in parallel with your developers to reconfigure the replicated application environment to leverage Amazon S3 to serve the engineering files.\nThis option suggests copying the on-premises engineering application data to Amazon S3 over the weekend using multiple threads and multi-part upload for large files. This approach can help speed up the transfer process by allowing the transfer of multiple files concurrently. Once the data is in S3, you can work with your developers to reconfigure the application environment to use Amazon S3 to serve the engineering files. This approach can minimize downtime as you can gradually switch over to the AWS-based solution.\nOption B: Sync the application data to Amazon S3 starting a week before the migration. On Friday night, perform the final sync, and copy the entire data set to your AWS file server after the sync completes.\nThis option suggests syncing the on-premises engineering application data to Amazon S3 starting a week before the migration. You can perform the final sync on Friday night and copy the entire data set to your AWS file server after the sync completes. This approach can help minimize downtime as you can copy the data to the AWS file server during off-business hours. However, it may not be an optimal solution as syncing the data can take longer than transferring it directly to AWS.\nOption C: Copy the application data to a 1-TB USB drive on Friday and immediately send overnight, with Saturday delivery, the USB drive to AWS Import/Export to be imported as an EBS volume, mount the resulting EBS volume to your AWS file server on Sunday.\nThis option suggests copying the on-premises engineering application data to a 1-TB USB drive on Friday and sending it overnight to AWS Import/Export to be imported as an EBS volume. You can then mount the resulting EBS volume to your AWS file server on Sunday. This approach can minimize downtime as you can copy the data during off-business hours. However, it may not be an optimal solution as it can be time-consuming to copy the data to a USB drive, and the shipping time can vary.\nOption D: Leverage the AWS Storage Gateway to create a Gateway-Stored volume. On Friday, copy the application data to the Storage Gateway volume. After the data has been copied, perform a snapshot of the volume, and restore the volume as an EBS volume to be attached to your AWS file server on Sunday.\nThis option suggests using the AWS Storage Gateway to create a Gateway-Stored volume. You can copy the on-premises engineering application data to the Storage Gateway volume on Friday. After the data has been copied, you can perform a snapshot of the volume and restore it as an EBS volume to be attached to your AWS file server on Sunday. This approach can minimize downtime as you can copy the data during off-business hours. However, it may not be an optimal solution as the transfer speed can depend on your network bandwidth and the volume of data being transferred.\nIn summary, option A seems to be the most optimal solution as it allows for parallel transfer of data to Amazon S3 and provides flexibility to reconfigure the application environment gradually. Option B can also work but may take longer as the data needs to be synced first. Option C and D can minimize downtime but may not be the most efficient solutions due to the time and effort required to copy the data to a USB drive or use the AWS Storage Gateway.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Copy the data to Amazon S3 using multiple threads and multi-part upload for large files over the weekend, and work in parallel with your developers to reconfigure the replicated application environment to leverage Amazon S3 to serve the engineering files.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Sync the application data to Amazon S3 starting a week before the migration. On Friday night, perform the final sync, and copy the entire data set to your AWS file server after the sync completes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Copy the application data to a 1-TB USB drive on Friday and immediately send overnight, with Saturday delivery, the USB drive to AWS Import/Export to be imported as an EBS volume, mount the resulting EBS volume to your AWS file server on Sunday.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Leverage the AWS Storage Gateway to create a Gateway-Stored volume. On Friday, copy the application data to the Storage Gateway volume. After the data has been copied, perform a snapshot of the volume, and restore the volume as an EBS volume to be attached to your AWS file server on Sunday.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 272,
  "query" : "You are a software engineer.\nYou are developing an online food order web application.\nThe Node.js backend needs to get the client's IP to understand users' locations.The application is deployed in AWS EC2 with a network load balancer to distribute traffic.\nFor the network load balancer, the target is specified using instance id.\nTLS is also terminated on the Network Load Balancer.\nYou are worried that the backend cannot get the client's IP due to the network load balancer.\nWhich below description is correct in this situation?",
  "answer" : "Correct Answer - C.\nNetwork Load Balancer supports TLS termination between the clients and the load balancer.\nYou can configure a target group so that you register targets by instance ID or IP address.\nIf you specify targets using an instance ID, the clients' source IP addresses are preserved and provided to your applications.\nIf you specify targets by IP address, the source IP addresses are the private IP addresses of the load balancer nodes.\nTherefore, in this case, the source IP is preserved since the targets are specified by instance ID.References are in https://aws.amazon.com/elasticloadbalancing/features/#compare and https://docs.amazonaws.cn/en_us/elasticloadbalancing/latest/network/elb-ng.pdf.\nOption A is incorrect because the proxy protocol is not required in this case since the source IP is preserved.\nOption B is incorrect because X-Forwarded-For is an HTTP header instead of a TCP header.\nAlso, it is not needed in this scenario.\nOption C is CORRECT because since the source IP is preserved, nothing else needs to be done.\nOption D is incorrect because changing the protocol to TCP will have a security issue.\nAgain it is not required.\nIn this scenario, the Node.js backend of an online food order web application needs to get the client's IP to understand user locations. The application is deployed in AWS EC2 with a network load balancer to distribute traffic. However, the software engineer is worried that the backend cannot get the client's IP due to the network load balancer.\nOption A suggests enabling the proxy protocol using AWS CLI for the network load balancer so that the client IP can be obtained in the backend service. The Proxy Protocol is a Layer 4 protocol that sends connection information between the client and the server as part of the network packet. By enabling the proxy protocol, the network load balancer can send the client's IP address to the backend instances using Proxy Protocol. This option is correct, and it is the recommended approach for preserving the client IP address in the backend service while using a network load balancer.\nOption B suggests getting the client IP from the TCP X-Forwarded-For header, which is used to identify the user's originating IP address connecting to the webserver. The X-Forwarded-For header is an HTTP header field that identifies the originating IP address of a client connecting to a web server through an HTTP proxy or load balancer. This option is incorrect because the Node.js backend is not a web server, and the X-Forwarded-For header is not supported by the network load balancer.\nOption C suggests that the source IP continues to be preserved to the backend applications when TLS is terminated on the Network Load Balancer. This option is incorrect because when TLS is terminated on the network load balancer, the client IP address is replaced with the IP address of the network load balancer.\nOption D suggests changing the listener protocol to TCP or changing the load balancer to an application or classic load balancer. This option is incorrect because changing the listener protocol to TCP or using an application or classic load balancer does not guarantee preserving the client IP address in the backend service.\nTherefore, the correct option is A, which suggests enabling the proxy protocol using AWS CLI for the network load balancer to preserve the client IP address in the backend service.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable proxy protocol using AWS CLI for the network load balancer so that you can get the client IP in the backend service.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You just need to get the client IP from the TCP X-Forwarded-For header, which is used to identify the user`s originating IP address connecting to the webserver.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Source IP continues to be preserved to your back-end applications when TLS is terminated on the Network Load Balancer in this case.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Change listener protocol to TCP or change the load balancer to the application or classic load balancer. Otherwise, the client IP cannot be preserved.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 273,
  "query" : "Which of the following are Lifecycle events available in OpsWorks?",
  "answer" : "Answer - A, C, and D.\nSetup occurs on a new instance after it successfully boots.\nConfigure occurs on all of the stack's instances when an instance enters or leaves the online state.\nDeploy occurs when you deploy an app.\nUndeploy occurs when you delete an app.\nShutdown occurs when you stop an instance.\nFor more information on Lifecycle events, please refer to the below URL-\nhttp://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\nOpsWorks is a configuration management service offered by AWS that allows you to automate the deployment and management of your applications. It provides a set of built-in Lifecycle events that can be used to control the various stages of an instance's lifecycle.\nThe four available lifecycle events in OpsWorks are:\nA. Setup: This event is triggered when an instance is launched and is used to configure the instance for use. You can define custom Chef recipes or use the built-in recipes to perform tasks such as installing packages, configuring settings, and creating users.\nB. Deploy: This event is triggered when a new version of an application is deployed to an instance. You can define custom Chef recipes or use the built-in recipes to update the application, restart services, and perform other tasks as required.\nC. Shutdown: This event is triggered when an instance is about to be terminated. You can define custom Chef recipes or use the built-in recipes to perform cleanup tasks such as removing log files, deleting temporary files, and stopping services.\nD. Decommission: This event is triggered when an instance is no longer needed and is being decommissioned. You can define custom Chef recipes or use the built-in recipes to perform tasks such as removing the instance from load balancers, deregistering it from DNS, and removing any associated resources.\nIn summary, the available lifecycle events in OpsWorks are Setup, Deploy, Shutdown, and Decommission. These events can be used to automate the configuration, deployment, cleanup, and decommissioning of your instances, helping to simplify your application management tasks.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Setup",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Decommision",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Deploy",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Shutdown.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 274,
  "query" : "You tried to integrate two systems including both the frontend and backend in a stack.\nThese systems don't store any state inside.\nAll of the state information is stored in a DynamoDB table.\nYou have launched each of the systems with separate AMIs.",
  "answer" : "E.\nAnswer - B and C.\nOption A is incorrect because Route53 cannot be used to deploy the fixes.\nOption B is CORRECT because Blue/Green deployment strategy can deploy new versions of AMIs without service disruption.\nOption C is CORRECT because you can modify the AMI used by the Auto Scaling group and adjust the desired number of the ASG to launch new instances with the new AMI.\nOption D is incorrect because deploying CloudFront is not needed in this situation.\nOption E is incorrect because SQS does not help to deploy the new fix.\nReference:\nhttps://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\nBased on the given scenario, where two systems including frontend and backend are integrated into a stack, and all state information is stored in a DynamoDB table, there are several options to ensure smooth integration.\nOption A - Use Route53:\nRoute53 is a DNS service that routes traffic between internet resources. Using Route53, you can create DNS records that direct traffic to specific servers or endpoints. While Route53 can be used to balance traffic between systems, it is not the best solution for this scenario, as it does not provide any specific features for integrating frontend and backend systems.\nOption B - Use Blue/Green deployment strategy by creating new Auto Scaling groups with the new AMIs:\nThe Blue/Green deployment strategy involves deploying a new version of an application, referred to as the \"Green\" version, alongside the existing, or \"Blue\" version. Once the Green version is fully tested and ready, traffic is shifted from the Blue version to the Green version. This can be done by creating new Auto Scaling groups with the new AMIs, testing them, and then redirecting traffic to them. This option is a good choice because it allows the testing of the new systems before redirecting traffic to them.\nOption C - Modify the AMIs used by the existing Auto Scaling groups and adjust the desired number of Auto Scaling group to deploy the new AMIs in the new instances:\nModifying the AMIs used by the existing Auto Scaling groups and adjusting the desired number of Auto Scaling groups to deploy the new AMIs in the new instances is a viable option. This approach requires careful consideration of the effects of making the changes and ensuring that the systems are fully tested before deploying them in production.\nOption D - Use Amazon CloudFront with access to the frontend server with origin fetch:\nAmazon CloudFront is a content delivery network that caches and distributes content from an origin server to multiple locations worldwide. In this scenario, CloudFront can be used to cache the frontend server and serve it to end-users. This option is not the best choice, as CloudFront does not provide any specific features for integrating frontend and backend systems.\nOption E - Use Amazon SQS between the frontend and backend subsystems:\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables decoupling and scaling microservices, distributed systems, and serverless applications. This option can be used to decouple the frontend and backend subsystems, making it easy to integrate them into the stack. However, this option requires additional work to ensure that the messages are processed correctly and that the systems are integrated seamlessly.\nOverall, Option B - Use Blue/Green deployment strategy by creating new Auto Scaling groups with the new AMIs is the best option as it provides the ability to test the new systems before redirecting traffic to them, ensuring smooth integration with the existing stack.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Route53.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Blue/Green deployment strategy by creating new Auto Scaling groups with the new AMIs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Modify the AMIs used by the existing Auto Scaling groups and adjust the desired number of Auto Scaling group to deploy the new AMIs in the new instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use Amazon CloudFront with access to the frontend server with origin fetch.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon SQS between the frontend and backend subsystems.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 275,
  "query" : "You are the new IT architect in a company that operates a mobile sleep tracking application.\nWhen activated at night, the mobile app sends collected data points of 100 kilobyte every 5 minutes to your backend.\nThe backend takes care of authenticating the user and writing the data points into an Amazon DynamoDB table.\nEvery morning, you scan the table to extract and aggregate last night's data on a per-user basis and store the results in Amazon S3\nUsers are notified via Amazon SNS mobile push notifications that new data is available, which is parsed and visualized by the mobile app.\nCurrently, you have around 100k users who are mostly based out of North America.\nYou have been tasked to optimize the architecture of the backend system to lower costs.",
  "answer" : "E.\nAnswers - C &amp; E.\nOption A is incorrect because accessing the DynamoDB table for read and write by 100k users will exhaust the read and write capacity, which will increase the cost drastically.\nOption B is incorrect because creating clusters of EC2 instances will be a very expensive solution in this scenario.\nOption C is CORRECT because (a) with SQS, the huge number of writes overnight will be buffered/queued which will avoid exhausting the write capacity (hence, cutting down on cost), and (b) SQS can handle a sudden high load, if any.\nOption D is incorrect because the data is not directly accessed from the DynamoDB table by the users.\nIt is accessed from S3\nSo, there is no need for caching.\nSince the results are stored in S3, introducing ElastiCache is unnecessary.\nOption E is CORRECT because once the aggregated data is stored on S3, there is no point in keeping the DynamoDB tables pertaining to the previous days.\nKeeping the tables for the latest data only will certainly cut the unnecessary costs, keeping the overall cost of the solution down.\nThe optimal solution for optimizing the architecture of the backend system to lower costs is to introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput.\nOption A, having the mobile app access Amazon DynamoDB directly instead of JSON files stored on Amazon S3, would not be cost-effective, as it would require a high provisioned throughput for writes and reads to DynamoDB, which could become expensive as the number of users grows.\nOption B, replacing both Amazon DynamoDB and Amazon S3 with an Amazon Redshift cluster, may not be the best solution because Redshift is more suitable for analytical workloads rather than transactional workloads.\nOption D, introducing Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput, could reduce costs, but it would only help with the read throughput, not the write throughput.\nOption E, creating a new Amazon DynamoDB table each day and dropping the one for the previous day after its data is on Amazon S3, may not be the best solution because it could lead to increased complexity in managing the data, as well as potential data loss if there are any issues in the process of dropping the table.\nTherefore, the best option is C, introducing an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput. This would allow for a lower provisioned throughput for writes to DynamoDB, as the queue can buffer writes during peak times and process them at a lower rate during off-peak times. Additionally, it can help with managing any spikes in traffic and reduce the chances of write capacity being exceeded. This option can also reduce costs as the provisioned throughput for writes to DynamoDB can be reduced, resulting in lower costs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Have the mobile app access Amazon DynamoDB directly instead of JSON files stored on Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new Amazon DynamoDB table each day and drop the one for the previous day after its data is on Amazon S3.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 276,
  "query" : "A user has created a VPC with CIDR 20.0.0.0/16\nThe user has created one subnet with CIDR 20.0.0.0/16 in this VPC.\nThe user is trying to create another subnet with the same VPC for CIDR 20.0.0.0/24\nWhat will happen in this scenario?",
  "answer" : "Answer - A.\nSince the CIDR of the new subnet overlaps with that of the first subnet, an overlap error will be displayed.\nSee the snapshot below.\nFor more information on VPC subnets, please refer to the below link.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\nThe correct answer is A. It will throw a CIDR overlap error.\nA Virtual Private Cloud (VPC) is a virtual network dedicated to the user's AWS account. When a user creates a VPC, they specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block. The CIDR block is a range of IP addresses in CIDR notation that is reserved for the VPC. For example, a CIDR block of 10.0.0.0/16 represents a range of IP addresses from 10.0.0.0 to 10.0.255.255.\nWhen a user creates a subnet in a VPC, they specify a range of IP addresses from the VPC CIDR block to allocate to the subnet. A subnet is a range of IP addresses in the VPC CIDR block that can be used to launch Amazon Elastic Compute Cloud (EC2) instances, Amazon Relational Database Service (RDS) instances, and other resources.\nIn this scenario, the user has created a VPC with CIDR 20.0.0.0/16 and one subnet with CIDR 20.0.0.0/16. The user is trying to create another subnet with CIDR 20.0.0.0/24 in the same VPC. However, this is not possible as the CIDR blocks of the subnets are overlapping.\nCIDR block 20.0.0.0/16 represents a range of IP addresses from 20.0.0.0 to 20.0.255.255. CIDR block 20.0.0.0/24 represents a range of IP addresses from 20.0.0.0 to 20.0.0.255. As we can see, the CIDR block 20.0.0.0/24 is a subset of CIDR block 20.0.0.0/16. This means that the second subnet's IP address range falls within the IP address range of the first subnet, causing an overlap.\nTherefore, when the user tries to create the second subnet with CIDR 20.0.0.0/24, it will throw a CIDR overlap error, and the user will not be able to create the subnet with the same IP address range as the existing subnet in the VPC.\nOption A is the correct answer as the CIDR overlap error will prevent the creation of a subnet with an overlapping IP address range. Option B is incorrect because it is possible to create a subnet with a different CIDR block within the VPC. Option C and D are incorrect because the second subnet cannot be created due to the CIDR overlap error.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It will throw a CIDR overlap error.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "It is not possible to create a subnet with the same CIDR as the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The second subnet will be created.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The VPC will modify the first subnet to allow this IP range.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 277,
  "query" : "Currently, a company uses Redshift to store its analyzed data.\nThey need to configure the Redshift cluster for a demo.\nWhat would be the minimum configuration that a user can choose for an SSD storage using Redshift in the console?",
  "answer" : "Answer - D.\nFor more information on Redshift, please refer to the below URL-\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-launch-sample-cluster.html\nRedshift is a data warehousing service provided by AWS that enables you to store and analyze large amounts of data in a scalable and cost-effective manner. When configuring a Redshift cluster, you can choose between two types of storage: SSD and HDD. SSD storage provides faster performance compared to HDD storage, but it is also more expensive.\nTo determine the minimum configuration for an SSD storage using Redshift, you need to consider the amount of data that needs to be stored and the performance requirements for the demo. The answer to this question depends on the size of the data and the query performance required.\nOption A, which suggests configuring three nodes with 320GB each, may not be sufficient for large data sets. Option B, which suggests configuring a single node with 320GB storage, may not be able to deliver the performance required for the demo.\nOption C, which suggests configuring two nodes with 128TB each, seems excessive for a demo, as this would be suitable for large-scale production workloads with petabytes of data.\nOption D, which suggests configuring a single node with 160GB of storage, is the most reasonable configuration for a demo, assuming that the data set is not very large and the query performance requirements are not very high.\nIn summary, the minimum configuration for SSD storage using Redshift for a demo would be one node with 160GB of storage, which is Option D.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Three nodes with 320GB each",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One node of 320GB",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Two nodes with 128 TB each",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One node of 160GB.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 278,
  "query" : "Your company has developed and managed a Cash Logistics application.\nThe application collects data from a variety of sources like Bank, Deposit Boxes, or ATMs.\nThen, it routes the physical cash to various points like Chain Stores, Banks, and ATM points to schedule the pickups and drop-offs.\nThe application receives requests in different formats from different sources like CSV, XML, JSON, or even encoded data.\nDue to the nature of the application, all the transfer requests come before 24 hours of the actual time of scheduling so that they can be processed and prepared for the scheduling.",
  "answer" : "E.\nCorrect Answer: A, B, and E.\nOption A is CORRECT because the AWS Glue is a fully serverless ETL framework that can process a large amount of data set effectively and can handle the transformation with little to no coding requirements.\nIt is possible to trigger the AWS Batch process once the AWS Glue import is completed.\nThis can invoke the route scheduling jobs based on the imported data.\nOption B is CORRECT because the AWS Batch can process a large number of jobs with varying compute requirements.\nThe jobs can be prioritized as well as scheduled to perform after one another.\nThe jobs run in the elastic containers and only consume the resources when there is anything to work on.\nThis can be integrated with AWS Glue for the post-processing once the data is available for execution.\nOption C is INCORRECT because running the data load and scheduling the jobs in the Lambda will not be possible as per the business requirements.\nAlso, in the question, it mentions that it may take few minutes to an hour to process the routing information.\nThe Lambda has a limited run time of 15 minutes per execution.\nSo some jobs may not fit into that.\nOption D is INCORRECT because different importers are required for the different file types, so there will be different EMR clusters that need to run and manage.\nEMR is technically the same as Glue, but it provides additional controls to manage the underlying Hadoop environment for the lower-level control.\nIt may not be a suitable fit in the current context.\nOption E is CORRECT because it can automatically sync the request files from the on-premise data center to the AWS environment.\nOnce the files are available in the S3, other processes can be triggered via the S3 Event Triggers.\nThe best solution for this scenario would be option C. Here's why:\nOption A, which suggests using AWS Glue to extract and import data to the source database for job scheduling, is not the best fit for the given scenario. AWS Glue is a fully managed ETL service that is used for data cataloging, cleaning, normalization, and preparation. It is not suitable for scheduling jobs.\nOption B, which suggests using AWS Batch to execute processing jobs for different importers, is also not the best fit. AWS Batch is used for running batch computing workloads on the AWS Cloud. It is not designed to handle file processing and job scheduling.\nOption D suggests using EFS to import data from on-premises data centers to AWS and running EMR jobs to import data into the database for job scheduling. This approach is expensive and time-consuming as it involves setting up an EFS file system and an EMR cluster. Moreover, it is not suitable for processing files of different formats and sources.\nOption E, which suggests importing data request files from on-premises data centers to S3 and storing them in one S3 bucket, is not optimal as it does not cater to the different file formats and sources of data. Furthermore, storing all data in a single bucket could lead to naming conflicts and make it difficult to manage and process data.\nOption C, which suggests creating different S3 buckets for each data source and using Lambda triggers to process the data, is the best fit for this scenario. Here's how it works:\nCreate different S3 buckets for each data source (e.g., Bank, Deposit Boxes, ATM points).\nAllocate a folder for each customer within the S3 bucket.\nWhen a new data request file arrives in a bucket, a Lambda function is triggered to process the file.\nThe Lambda function reads the file and extracts the data.\nThe extracted data is then imported into the target database for job scheduling.\nThis approach is cost-effective, scalable, and can handle different file formats and sources. Moreover, it allows for efficient management and processing of data.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the AWS Glue to perform the extract and import the data to the source database for job scheduling. Trigger the AWS Batch jobs once the data is imported to perform the route schedule.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the AWS Batch to execute the processing jobs for different importers. Create separate job definitions and compute environments to support different import files based on customers.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create different S3 buckets for a different source. Use the Lambda trigger to invoke after a new request has arrived. The Lambda will process the file and import the data into the target database for the job scheduling.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the EFS to import the data from on-premises data centers to the AWS. Run the EMR jobs which can pull the data from the EFS and import the data into the database for the job scheduling.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Import the files of data requests from the on-premises data center to S3. Store the files into one S3 bucket and allocate one folder for each customer.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 279,
  "query" : "A startup company has developed its on-premises SAAS product using standard Kubernetes.\nKubernetes has proved to be a success because of its feature of easily deploying, managing the worker nodes and managing containerized applications at scale.\nThe R&D team also gained adequate Kubernetes experiences, including related tools like kubectl.\nRecently, the CTO of the company has proposed to migrate the whole product to the AWS platform.\nThey need a managed service that runs Kubernetes on AWS without installing, operating, and maintaining their own Kubernetes control plane or nodes.\nHow should they easily design the migration and manage Kubernetes with the least code modification?",
  "answer" : "Correct Answer - B.\nOption A is incorrect because this option should be used when you want to manage Kubernetes by yourself fully.\nOption B is CORRECT because applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, and can be easily migrated, whether running in on-premises datacenters or public clouds.\nWith EKS, there is no need to refactor the code.\nAccording to https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html, Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes.\nOption C is incorrect because the appropriate service to manage and run Kubernetes should be EKS instead of ECS.\nOption D is incorrect because AWS ECR is a docker registry service.\nIt is not the service to manage the Kubernetes services.\nThe best option for the startup company to migrate their on-premises SAAS product to AWS with minimal code modifications is to use Amazon Elastic Kubernetes Service (EKS) - Option B.\nAmazon EKS is a managed Kubernetes service that makes it easy to run Kubernetes on AWS without needing to provision or manage master instances. With Amazon EKS, the startup can easily deploy, manage, and scale containerized applications using Kubernetes on AWS.\nHere are some benefits of using Amazon EKS:\n1.\nEasy to use: With Amazon EKS, the startup can easily deploy, manage, and scale containerized applications using Kubernetes on AWS. Amazon EKS automatically deploys the Kubernetes control plane across multiple availability zones to ensure high availability and scalability.\n2.\nScalability: Amazon EKS can easily scale up or down based on the workload requirements. This means that the startup can easily add or remove worker nodes as required to meet the demands of their application.\n3.\nSecurity: Amazon EKS is designed to be secure by default. It integrates with AWS Identity and Access Management (IAM) for authentication and authorization and encrypts all data at rest and in transit.\n4.\nHigh Availability: Amazon EKS runs the Kubernetes control plane across multiple availability zones to ensure high availability and resilience.\n5.\nCost-effective: With Amazon EKS, the startup only pays for the resources they use. They don't have to worry about the cost of maintaining and managing their own Kubernetes control plane or worker nodes.\nOption A (Provision and run Kubernetes on powerful EC2 instance types such as c5) requires the startup to manage the Kubernetes control plane and worker nodes themselves. This can be time-consuming and can take away resources from other important business operations.\nOption C (Store, encrypt, and manage container images for fast deployment in Amazon ECR and manage and run Kubernetes clusters in Amazon Elastic Container Service (ECS)) requires the startup to use Amazon ECS, which is not a true Kubernetes service. This could result in code modifications to their existing Kubernetes infrastructure and tools.\nOption D (Configure and run Kubernetes in AWS Elastic Container Registry without having to manage servers or clusters) is not a valid option as AWS Elastic Container Registry is a container registry and does not offer a managed Kubernetes service.\nIn conclusion, Amazon EKS is the best option for the startup company to migrate their on-premises SAAS product to AWS with minimal code modifications.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Provision and run Kubernetes on powerful EC2 instance types such as c5 to fully manage the Kubernetes deployment.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Run Kubernetes in Amazon Elastic Kubernetes Service (EKS) without needing to provision or manage master instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In Amazon ECR, store, encrypt, and manage container images for fast deployment. Manage and run Kubernetes clusters in Amazon Elastic Container Service (ECS).",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure and run Kubernetes in AWS Elastic Container Registry without having to manage servers or clusters.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 280,
  "query" : "You lead a team to use Kubernetes to develop some microservices in the local server and data center.\nTo align with the company's strategy to move to the AWS cloud, you need to consider the possibilities of migrating the projects that your team is working on.\nYou think that Amazon Elastic Container Service for Kubernetes (Amazon EKS) is a good candidate.\nIn order to start using EKS properly, which prerequisites must be met? (Select TWO)",
  "answer" : "Correct Answer - C, D.\nIn order to use the EKS service properly, some prerequisites must be met, which is mentioned in https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html.\nOption A is incorrect because EKS can pull Docker images from ECR or any other container registries, such as Docker Hub.\nOption B is incorrect because using Route53 alone will not help to access the cluster.\nIn addition to standard Amazon EKS permissions, your IAM user or role must have route53:AssociateVPCWithHostedZone permissions to enable the cluster's endpoint private access.\nRefer to page 41 on the below link under the 'Note' section.\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-ug.pdf\nOption C is correct because Amazon EKS needs the IAM role to create AWS resources such as a load balancer.\nOption D is correct because EKS has also provided a CloudFormation template to provide a suitable VPC for the EKS cluster.\nThe link https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html contains the instructions on how to create the cluster VPC for EKS.\nSure, I'd be happy to explain!\nTo start using Amazon Elastic Container Service for Kubernetes (Amazon EKS) properly, there are two prerequisites that must be met:\n1.\nAll related container images are registered in Amazon ECR since EKS can only pull Docker images from ECR. Amazon ECR is a fully-managed Docker container registry that makes it easy to store, manage, and deploy Docker container images. Amazon EKS can only pull Docker images from Amazon ECR, so all container images must be registered there before they can be used in an EKS cluster. This means that you need to have all your container images built and stored in Amazon ECR before you can start using EKS.\n2.\nAn IAM EKS service role should be created to allow Amazon EKS to manage clusters on your behalf. When you create an Amazon EKS cluster, you need to specify an IAM role that Amazon EKS can assume to create and manage resources on your behalf. This role is called the \"IAM EKS service role,\" and it allows Amazon EKS to create and manage resources such as Amazon EC2 instances, load balancers, and security groups. Without this role, Amazon EKS cannot create or manage any resources on your behalf.\nIn addition to the above prerequisites, there are other considerations to take into account when setting up an EKS cluster, such as:\nA VPC exists for the EKS cluster to use: An Amazon EKS cluster must be created within a Virtual Private Cloud (VPC). The VPC provides the network infrastructure for your cluster, and you need to have a VPC set up before you can create an EKS cluster.\nA Route53 should be in place for Amazon EKS to route internet traffic among various public subnets: Amazon EKS can be used to deploy Kubernetes applications across multiple Availability Zones, and you need to set up a Route53 DNS record to route traffic to your cluster's endpoints. This is only necessary if you plan to expose your Kubernetes services to the public internet.\nI hope this helps! Let me know if you have any further questions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "All related container images are registered in Amazon ECR since EKS can only pull docker images from ECR.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A Route53 should be in place for Amazon EKS to route internet traffic among various public subnets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An IAM EKS service role should be created to allow Amazon EKS to manage clusters on your behalf.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A VPC exists for the EKS cluster to use.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 281,
  "query" : "You've created a temporary application that accepts image uploads, stores them in S3, and records information about the image in RDS.\nAfter building this architecture and accepting images for the duration required, it's time to delete the CloudFormation template.\nHowever, your manager has informed you that the RDS data needs to be stored for archival reasons, and the S3 bucket with the images needs to remain.\nYour manager has also instructed you to ensure that the application can be restored by a CloudFormation template and run next year during the same period.",
  "answer" : "Answer - D.\nThe main points in this question are (i) the need for an ability by which the RDS data can be restored if needed and (ii) the S3 bucket with the images needs to retain.\nOption A is incorrect because it replicates the images into another bucket, but does not ensure that the bucket itself would retain.\nOption B is incorrect because RDS data does not need to be retained.\nYou just need the ability to restore the RDS data - for which you need to use a snapshot policy.\nOption C is incorrect because the S3 bucket itself needs to be retained.\nHence you need to use the retain policy for the S3 bucket.\nOption D is CORRECT because it uses a retain policy for the S3 bucket and a snapshot policy for RDS such that the data can be restored when needed.\nMore information on DeletionPolicy Options:\nDelete.\nAWS CloudFormation deletes the resource and all its content if applicable during stack deletion.\nRetain.\nAWS CloudFormation keeps the resource without deleting the resource or its contents when its stack is deleted.\nSnapshot.\nFor resources that support snapshots (AWS::EC2::Volume, AWS::ElastiCache::CacheCluster, AWS::ElastiCache::ReplicationGroup, AWS::RDS::DBInstance, AWS::RDS::DBCluster, and AWS::Redshift::Cluster), AWS CloudFormation creates a snapshot for the resource before deleting it.\nFor more information on CloudFormation deletion policy, please visit the below URL:\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\nSure, I can provide a detailed explanation of the options and help you understand which one is the correct answer.\nThe scenario here is that you have created a temporary application that accepts image uploads, stores them in S3, and records information about the image in RDS. You need to delete the CloudFormation template, but your manager has instructed you to ensure that the RDS data is stored for archival reasons and the S3 bucket with images remains. Additionally, the application needs to be restored by a CloudFormation template and run next year during the same period.\nOption A: Enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects, set the deletion policy for the RDS instance to snapshot.\nExplanation: S3 bucket replication can be used to replicate objects from one S3 bucket to another. While it can help maintain a copy of all S3 objects, it does not address the retention of the RDS data. Setting the deletion policy for the RDS instance to snapshot does ensure that a snapshot of the RDS instance is taken before deletion, but it does not retain the RDS data.\nOption B: For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to retain.\nExplanation: This option is a good solution. Setting the DeletionPolicy to retain ensures that the resources are not deleted when the CloudFormation template is deleted. The S3 bucket with the images and the RDS instance with the data will both be retained. This satisfies the requirement to store the RDS data for archival reasons and to maintain the S3 bucket with the images.\nOption C: Set the DeletionPolicy on the S3 resource to snapshot and the DeletionPolicy on the RDS resource to snapshot.\nExplanation: This option is not correct because the snapshot deletion policy does not address the requirement to retain the RDS data. While taking a snapshot is useful for backup purposes, it does not ensure that the data is retained.\nOption D: Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, set the RDS resource declaration DeletionPolicy to snapshot.\nExplanation: This option is not the best solution because it only retains the S3 bucket with the images and does not ensure the retention of the RDS data. While taking a snapshot of the RDS instance is useful for backup purposes, it does not address the requirement to retain the RDS data.\nTherefore, option B is the correct answer. It satisfies the requirement to store the RDS data for archival reasons and maintain the S3 bucket with the images, and it allows the application to be restored by a CloudFormation template and run next year during the same period.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects, set the deletion policy for the RDS instance to snapshot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to retain.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set the DeletionPolicy on the S3 resource to snapshot and the DeletionPolicy on the RDS resource to snapshot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, set the RDS resource declaration DeletionPolicy to snapshot.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 282,
  "query" : "You are an AWS administrator for a large organization, and you maintain several AWS accounts.\nYour manager recently asked you to generate the Cost & Usage Reports from the billing dashboard regularly so that he can have a review of the usage and cost status.\nThrough the AWS Billing & Cost Management console, you have configured the report successfully.\nHow should you present the reports to your manager?",
  "answer" : "Correct Answer - C.\nWhen the Billing Report is set up, an S3 bucket is essential to store the reports.\nThen AWS delivers the report files to that bucket and updates the report up to three times a day.\nOption A is incorrect because SNS cannot be used for the billing report notification.\nOption B is incorrect because you should give the least privilege to users.\nCreating an admin IAM user is improper.\nOption C is correct because appropriate permissions should be configured for the users who need access to the S3 bucket.\nOption D is incorrect because the bucket should NOT be publicly accessible.\nAnd the S3 bucket policy should only allow limited users to view the report.\nAs an AWS administrator for a large organization, you are responsible for generating Cost & Usage Reports from the billing dashboard regularly for your manager. You have successfully configured the reports using the AWS Billing & Cost Management console, and now you need to present the reports to your manager.\nThere are four options given in the question, and we need to choose the best one based on the requirements and security considerations. Let's analyze each option:\nOption A: Configure the billing report to use SNS to send the report to your manager with an email notification every day.\nThis option involves using Simple Notification Service (SNS) to send the Cost & Usage Reports to your manager's email every day. While this option is straightforward, it has some drawbacks. Firstly, it can be a security risk since the reports are being sent outside the AWS environment. Secondly, it can be difficult to manage and keep track of the emails sent, especially when the number of accounts or reports increases.\nOption B: Create an IAM admin user for your manager so that he can log in to the AWS billing console to view the Cost & Usage Reports.\nThis option involves creating an IAM admin user for your manager, which would allow them to log in to the AWS Billing & Cost Management console and view the reports. While this option is secure, it might not be the best solution since it gives the manager full access to the AWS console, which can be a security risk. Moreover, it can be time-consuming to manage the IAM user's permissions and roles.\nOption C: Configure an S3 bucket where AWS delivers the billing report files. Allocate a read access for your manager to this bucket.\nThis option involves configuring an S3 bucket where AWS delivers the Cost & Usage Reports, and then granting read access to your manager. This option is secure since the reports are stored within the AWS environment, and the manager has only read access to the reports. Additionally, it allows you to manage the reports easily since they are stored in a centralized location.\nOption D: Create a new S3 bucket for AWS to send the billing report files to. Make sure the bucket is publicly accessible by modifying the bucket policy so that your manager can see the report properly.\nThis option involves creating a new S3 bucket that is publicly accessible by modifying the bucket policy so that your manager can view the reports. This option is not secure since the reports are publicly accessible, and anyone with the URL can view them. It violates the AWS security best practices since it involves making resources publicly accessible.\nBased on the analysis, Option C is the best solution. It is secure, easy to manage, and provides centralized storage for the reports. Therefore, you should configure an S3 bucket where AWS delivers the Cost & Usage Reports, and then grant read access to your manager.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure the billing report to use SNS to send the report to your manager with an email notification every day.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM admin user for your manager so that he can log in to the AWS billing console to view the Cost & Usage Reports.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an S3 bucket where AWS delivers the billing report files. Allocate a read access for your manager to this bucket.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new S3 bucket for AWS to send the billing report files to. Make sure the bucket is publicly accessible by modifying the bucket policy so that your manager can see the report properly.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 283,
  "query" : "Consolidated Billing feature is used in a multinational company.\nA master AWS account is a setup to pay the charges of all the member accounts.\nYou are an AWS solutions architect.\nYou created the billing Cost and Usage Reports in the master AWS account.\nIn terms of this billing report configuration, which benefits can you get? (Select TWO)",
  "answer" : "Correct Answer - B, D.\nFor Cost and Usage Reports, if the consolidated billing feature in AWS Organizations is used, only the master account can create and view the reports.\nCheck the details in https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html.\nOption A is incorrect because member accounts cannot see their reports and the only master account can.\nBesides, the reports are updated up to three times a day instead of every hour.\nOption B is CORRECT because AWS has provided the integration with Redshift for the billing reports:\nFor more information:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage-upload.html\nOption C is incorrect because the report is a .csv file or a collection of .csv files instead of JSON format.\nOption D is CORRECT: This is correct according to.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html.\nConsolidated Billing is an AWS feature that allows a single AWS account, referred to as the master account, to pay for the charges of multiple AWS accounts, referred to as member accounts. The member accounts remain independent, but their charges are consolidated and paid for by the master account. This feature is particularly useful for multinational companies that have multiple AWS accounts across different regions.\nAs an AWS solutions architect, you can configure billing Cost and Usage Reports in the master account. The billing Cost and Usage Reports provide detailed information on the charges incurred by the member accounts. The reports can be generated in CSV or JSON format and can be used to track the usage and costs of AWS resources.\nThe benefits of configuring billing Cost and Usage Reports in the master account include:\nC. The JSON format billing reports are uploaded to an S3 bucket that was previously configured. You could easily download the reports from the Amazon S3 console. Configuring billing Cost and Usage Reports in the master account allows you to automatically upload the reports to an S3 bucket that you previously configured. This makes it easy to download the reports from the Amazon S3 console and analyze them using third-party tools. The reports provide detailed information on the usage and costs of AWS resources, enabling you to optimize your resource usage and reduce costs.\nB. The report data can be uploaded to Amazon Redshift, allowing you to analyze the costs and usage. Billing and Cost Management can provide the RedshiftCommands.sql file in the Billing and Cost Management Console. Configuring billing Cost and Usage Reports in the master account also allows you to upload the report data to Amazon Redshift. Amazon Redshift is a data warehousing solution that allows you to analyze large amounts of data quickly and efficiently. By uploading the report data to Amazon Redshift, you can perform complex queries on the usage and costs of AWS resources, enabling you to identify trends, patterns, and anomalies.\nD. The reports are available only to the master account and include activities for all the member accounts. The billing reports generated in the master account are only available to the master account. The reports include activities for all the member accounts, enabling the master account to monitor the usage and costs of all the member accounts. This provides a centralized view of the charges incurred by all the member accounts, allowing the master account to manage costs effectively.\nA. The billing reports for all member accounts are activated automatically and refreshed each hour. All member accounts can view their reports on the billing dashboard. This option is incorrect as the billing reports generated in the master account are only available to the master account, not to the member accounts. However, member accounts can view their usage and costs through their own AWS console, using AWS Cost Explorer and AWS Budgets.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The billing reports for all member accounts are activated automatically and refreshed each hour. All member accounts can view their reports on the billing dashboard.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The report data can be uploaded to Amazon Redshift, allowing you to analyze the costs and usage. Billing and Cost Management can provide the RedshiftCommands.sql file in the Billing and Cost Management Console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The JSON format billing reports are uploaded to an S3 bucket that was previously configured. You could easily download the reports from the Amazon S3 console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The reports are available only to the master account and include activities for all the member accounts.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 284,
  "query" : "You are moving an existing traditional system to AWS.\nDuring migration, you discover that the master server is the single point of failure.\nYou also discover that the server stores its state in the local MySQL database.\nTo minimize downtime, you select RDS to replace the local database and configure the master to use it.\nWhat steps would best allow you to create a self-healing architecture?",
  "answer" : "Answer - A.\nOption A is CORRECT because, for the database, Multi-AZ architecture provides high availability and can meet the RTO and RPO requirements in case of failures since it uses synchronous replication and maintains standby instance gets promoted to primary.\nAuto Scaling group can achieve the high availability for the master server.\nOption B is incorrect because ELB cannot ensure the minimum or the maximum number of instances running.\nOption C is incorrect because ELB cannot ensure the minimum or the maximum number of instances running.\nOption D is incorrect because the Read Replica cannot achieve the self-healing requirements.\nMulti-AZ should be used.\nMore information on Multi-AZ RDS architecture:\nMulti-AZ is used for highly available architecture.\nIf a failover happens, the secondary DB, which is a synchronous replica, will have the data, and it's just the CNAME that changes.\nFor Read replica, it's primarily used for distributing workloads.\nFor more information on Multi-AZ RDS, please refer to the below link-\nhttps://aws.amazon.com/rds/details/multi-az/\nIn the scenario described, the master server is a single point of failure, and the local MySQL database stores the system state. To create a self-healing architecture and minimize downtime, the existing local database can be replaced with Amazon RDS, which provides a managed and highly available database service. RDS offers several options for configuring high availability, which can help eliminate single points of failure.\nAnswer A suggests migrating the local database into a Multi-AZ RDS deployment and configuring the master node in an Auto Scaling group. This approach is a good choice because RDS Multi-AZ deployment provides high availability and fault tolerance for the database. In a Multi-AZ configuration, RDS automatically replicates data to a standby replica in a different availability zone, which helps ensure that the database remains available in the event of a hardware failure, software issue, or planned maintenance. With Auto Scaling group configured, the master node can be replaced automatically in case of a failure, and new nodes can be launched to maintain the desired capacity. Thus, this approach is an effective way to create a self-healing architecture.\nAnswer B suggests migrating the local database into a Multi-AZ RDS deployment and placing the master node into a Cross Zone ELB with a minimum of one and a maximum of one with health checks. This option can also provide high availability, but it does not include the automatic recovery that comes with Auto Scaling group. The ELB can help distribute traffic to the master node, and the health checks can detect when the master node is no longer available, and automatically route traffic to another instance. However, in this configuration, there is only one instance, and there is no automatic replacement, so the recovery process would need to be done manually, leading to potential downtime.\nAnswer C suggests replicating the local database into an RDS database with a Read Replica and placing the master node into a Cross Zone ELB with a minimum of one and a maximum of one with health checks. While this approach also provides some fault tolerance, it does not eliminate the single point of failure associated with the master node. In this case, the RDS Read Replica can help offload read traffic from the master node, but if the master node goes down, it will need to be replaced manually. This can lead to potential downtime, and the recovery process may take longer than the automatic recovery of Answer A.\nAnswer D suggests replicating the local database into an RDS database with a Read Replica and configuring the master node in an Auto Scaling group. This option is similar to Answer A, but it does not include the Multi-AZ configuration that provides additional fault tolerance. While Auto Scaling group can help replace failed instances automatically, the Read Replica does not provide the same level of fault tolerance as a Multi-AZ deployment. Therefore, while this approach can still provide some self-healing capability, it may not be as reliable as Answer A.\nOverall, the best approach to create a self-healing architecture in this scenario is Answer A, which suggests migrating the local database into a Multi-AZ RDS deployment and configuring the master node in an Auto Scaling group. This configuration provides high availability, fault tolerance, and automatic recovery, which can help minimize downtime and ensure that the system remains available even in the event of a failure.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Migrate the local database into the Multi-AZ database. Amazon RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments. Configure the master node in an Auto Scaling group.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Migrate the local database into the Multi-AZ database. Place the master node into a Cross Zone ELB with a minimum of one and a maximum of one with health checks.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Replicate the local database into an RDS database with a Read Replica. Place the master node into a Cross Zone ELB with a minimum of one and a maximum of one with health checks.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Replicate the local database into an RDS database with a Read Replica. Configure the master node in an Auto Scaling group.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 285,
  "query" : "An e-commerce platform has sent online order requests to a standard SQS queue.\nThe visibility timeout for the messages in the queue is set as 30 seconds by default.\nThe message retention period is 7 days.\nFrom the backend log system, it has been found that the backend processing of some messages has failed.\nAs a result, these messages were not deleted successfully from the queue.\nWhat should you do to isolate the failed messages to troubleshoot the reason why the processing doesn't succeed?",
  "answer" : "Correct Answer - D.\nWhen messages in an SQS queue can't be consumed successfully, the messages can be put into a dead-letter queue, ideal for isolating the problematic message.\nOption A is incorrect because the queue type for the original queue and dead letter queue should match.\nTherefore, in this case, the dead letter queue should also be a standard queue.\nOption B is incorrect because the visibility timer adjustment does not help to isolate messages that have issues.\nOption C is incorrect: Same reason as Option.\nB.\nWhen the visibility timer is modified to 0, visibility timeout is also terminated.\nOption D is correct: For example, if the source queue has set maxReceiveCount to 2, and the consumer of the source queue receives a message 2 times without ever deleting it, the message will be put to the dead-letter queue.\nThen those problematic messages are ready to be analyzed.\nIn this scenario, messages sent by an e-commerce platform to a standard SQS queue are not getting processed successfully. As a result, these messages are not being deleted from the queue. To troubleshoot the issue, it is important to isolate the failed messages and understand the reason behind the processing failure.\nOne solution to this problem is to create a dead-letter queue (DLQ). A dead-letter queue is a queue that receives messages that cannot be processed successfully from the main queue. It helps to isolate the problematic messages and troubleshoot the root cause of the processing failure. AWS SQS provides a dead-letter queue feature that can be configured for standard SQS queues.\nOption A suggests creating a new FIFO queue as the dead-letter queue to store failed messages. It is a valid solution as it isolates failed messages and enables troubleshooting the processing failure. However, the choice between creating a standard or a FIFO dead-letter queue depends on the use case requirements. If the order of messages is critical, FIFO queues should be used; otherwise, standard queues can be used.\nOption B suggests increasing the visibility timeout and monitoring the log system to see if there are still messages that fail to process. It is not a recommended solution as it may delay message processing and increase the chances of duplicating messages in the queue. Increasing visibility timeouts only postpones the inevitable and does not solve the underlying issue.\nOption C suggests setting the visibility timeout to 0, which mitigates the impacts when messages are deleted unsuccessfully. However, it is not a valid solution as setting the visibility timeout to 0 will result in immediate deletion of messages that are not processed successfully, making it challenging to troubleshoot the root cause of the processing failure.\nOption D suggests creating another standard SQS queue as the dead-letter queue. Although this option will isolate failed messages, it is not recommended as it creates additional complexity and management overhead. Using AWS SQS DLQ feature is more straightforward and requires fewer resources.\nIn summary, the best solution to isolate failed messages and troubleshoot the reason behind processing failure is to create a dead-letter queue using the AWS SQS DLQ feature. The option of creating a new FIFO queue as the dead-letter queue or setting the visibility timeout to 0 is not recommended.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a new FIFO queue as the dead letter queue. So the failed messages are isolated and stored in this dead letter queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enlarge the visibility timer a little bit and monitor the log system to see if there are still messages that fail to be processed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the visibility timer to 0 to mitigate the impacts when messages are deleted unsuccessfully.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create another SQS standard queue as the dead letter queue. So the problematic messages are isolated.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 286,
  "query" : "A company has an application that is hosted on an EC2 instance.\nThe code is written in .NET and connects to a MySQL RDS database.\nIf you're executing .NET code against AWS on an EC2 instance that is assigned an IAM role, which of the following is a true statement?",
  "answer" : "Answer - A.\nThe best practice for IAM is to create roles that have specific access to an AWS service and then give the user permission to the AWS service via the role.\nFor the best practices on IAM policies, please visit the links.\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\nNote:\nAs per AWS,\nWhen you launch an EC2 instance, you specify an IAM role to associate with the instance.\nApplications that run on the instance can then use the role-supplied temporary credentials to sign API requests.\nUsing roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration.\nAn application running on an EC2 instance is abstracted from AWS by the virtualized operating system.\nBecause of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications.\nThis extra step is the creation of an instance profile that is attached to the instance.\nThe instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance.\nThose temporary credentials can then be used in the application's API calls to access resources and limit access to only those resources that the role specifies.\nNote that, only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.\nThe example given here shows how the application retrieves role permissions from the instance for accessing the bucket.\nThe correct answer is A. Retrieve the temporary security credentials from Amazon EC2 instance metadata. These are then passed on to the code that assumes the role and is valid for a limited time.\nWhen an EC2 instance is assigned an IAM role, it can use the AWS SDK or CLI to retrieve temporary security credentials from the instance metadata service. These credentials include an access key, a secret access key, and a session token, and are valid for a limited time, usually one hour.\nIn this scenario, the .NET code running on the EC2 instance can use the AWS SDK for .NET to retrieve the temporary security credentials and assume the IAM role. Once the code assumes the role, it can access AWS resources as defined by the permissions of the IAM role.\nOption B is incorrect because the code does not need to have AWS access keys to execute if it is running on an EC2 instance with an assigned IAM role. The IAM role provides temporary security credentials that can be used to access AWS resources.\nOption C is incorrect because IAM roles can be assumed by any AWS SDK, not just .NET code. For example, Python, Java, and Ruby SDKs can also assume IAM roles.\nOption D is incorrect because the correct answer is A, as explained above.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Retrieve the temporary security credentials from Amazon EC2 instance metadata. These are then passed on to the code that assumes the role and is valid for a limited time.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The code must have AWS access keys to execute.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Only .NET code can assume IAM roles.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "None of the above.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 287,
  "query" : "A company is making extensive use of S3\nThey have a strict security policy and require that all artifacts are stored securely in S3\nWhen specified in an API call, which of the following request headers will cause an object to be SSE?",
  "answer" : "Answer - C.\nServer-side encryption is about protecting data at rest.\nServer-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption.\nAmazon S3 encrypts each object with a unique key.\nAs an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.\nAmazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\nThe object creation REST APIs (see Specifying Server-Side Encryption Using the REST API) provides a request header, x-amz-server-side-encryption that you can use to request server-side encryption.\nTo encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.\nThe following code example shows a Put request using SSE-S3.\nPUT /example-object HTTP/1.1\nHost: myBucket.s3.amazonaws.com.\nDate: Wed, 8 Jun 2016 17:50:00 GMT.\nAuthorization: authorization string.\nContent-Type: text/plain.\nContent-Length: 11434\nx-amz-meta-author: Janet.\nExpect: 100-continue.\nx-amz-server-side-encryption: AES256\n[11434 bytes of object data]\nIn order to enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header.\nThere are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMS-managed keys.\nFor more information on S3 encryption, please visit the links.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\nThe correct answer is C. x-amz-server-side-encryption.\nAmazon S3 provides various methods to encrypt objects at rest. Server-side encryption (SSE) is one such method that encrypts S3 objects with encryption keys managed by AWS. SSE encrypts objects before storing them in S3 and decrypts objects when you download them.\nWhen you upload an object to S3, you can specify that the object should be encrypted using SSE by adding a request header in the API call. The header to use is x-amz-server-side-encryption.\nOption A, AES256, is a valid encryption algorithm that you can use with SSE, but it is not a header. When you want to use this algorithm, you specify it as the value for the x-amz-server-side-encryption header.\nOption B, amz-server-side-encryption, is not a valid header for SSE. It should be x-amz-server-side-encryption.\nOption D, server-side-encryption, is not a valid header for SSE. It should be x-amz-server-side-encryption.\nTo summarize, when you want to encrypt an object with SSE in S3, you should use the x-amz-server-side-encryption header in the API call and specify the appropriate encryption algorithm as the header value.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AES256",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "amz-server-side-encryption",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "x-amz-server-side-encryption",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "server-side-encryption.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 288,
  "query" : "You decide to create a bucket on AWS S3 called 'mybucket' and then perform the following actions in the order that they are listed here.",
  "answer" : "Answer - B.\nObjects stored in your bucket before you set the versioning state have a version ID of null.\nWhen you enable versioning, existing objects in your bucket do not change.\nWhat changes is how Amazon S3 handles the objects in future requests.\nOption A is incorrect because the version ID for file1 would be null.\nOption B is CORRECT because the file1 was put in the bucket before the versioning was enabled.\nHence, it will have a null version ID.\nThe file2 will have two version IDs, and file3 will have a single version ID.Option C is incorrect because file2 cannot have a null version ID as the versioning was enabled before putting it in the bucket.\nOption D is incorrect because once the versioning is enabled, all the files put after that will not have a null version ID.\nBut file1 was put before versioning was enabled.\nSo it will have null as its version ID.For more information on S3 versioning, please visit the below link.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nBased on the information provided, it is not clear whether versioning has been enabled for the mybucket S3 bucket. Without versioning enabled, every object uploaded to the bucket will have a null version ID.\nTherefore, option D is a possibility, as all file version IDs will be null if versioning was not enabled before uploading objects to mybucket.\nIf versioning was enabled for mybucket before performing the actions listed, then each object uploaded to the bucket will have a unique version ID assigned to it.\nLooking at the actions listed, we can see that three files were uploaded to the mybucket bucket: file1, file2, and file3. Each file upload will create a new version of the object if versioning is enabled.\nAssuming that versioning is enabled, the following options are possible:\nOption A: There will be 1 version ID for file1, 2 version IDs for file2, and 1 version ID for file3. This is because file1 was uploaded only once, while file2 was uploaded twice, and file3 was uploaded once. Each upload creates a new version of the object, resulting in a total of 4 versions across the three files.\nOption B: The version ID for file1 will be null. There will be 2 version IDs for file2, and 1 version ID for file3. This is because file1 was uploaded once, but if versioning was not enabled, it will have a null version ID. File2 was uploaded twice, creating 2 versions, and file3 was uploaded once, creating 1 version. This results in a total of 3 versions across the three files.\nOption C: There will be 1 version ID for file1, the version ID for file2 will be null, and there will be 1 version ID for file3. This is unlikely as it assumes that only one version of file2 was uploaded and that versioning was enabled. If versioning was not enabled, file1 and file3 would have a null version ID, and file2 would have no version ID. If versioning was enabled, file2 would have two version IDs since it was uploaded twice.\nIn summary, without knowing whether versioning was enabled before the actions listed, option D is a possibility. Assuming versioning was enabled, option A or B are the most likely scenarios. Option C is unlikely as it assumes specific conditions that are not clear from the information provided.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "There will be 1 version ID for file1, 2 version IDs for file2, and 1 version ID for file3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The version ID for file1 will be null. There will be 2 version IDs for file2, and 1 version ID for file3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "There will be 1 version ID for file1, the version ID for file2 will be null, and there will be 1 version ID for file3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All file version ID`s will be null because versioning must be enabled before uploading objects to `mybucket`.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 289,
  "query" : "One of your requirements is to set up an S3 bucket to store your files like documents and images.\nHowever, those objects should not be directly accessible via the S3 URL.\nThey should only be accessible from pages on your website so that only your paying customers can see them.\nHow could you implement this?",
  "answer" : "Answer - B.\nSuppose you have a website with the domain name (www.example.com or example.com) with links to photos and videos stored in your S3 bucket, examplebucket.\nBy default, all the S3 resources are private, so only the AWS account that created the resources can access them.\nTo allow read access to these objects from your website, you can add a bucket policy that allows s3:GetObject permission with a condition, using theaws:referer key.\nThe get request must originate from specific web pages.\nOption A is incorrect because the HTTPS endpoint will not ensure that only authenticated users can get access to the content.\nOption B is CORRECT because it defines the appropriate bucket policy to give access to the S3 content to the authenticated users.\nRefer to page 390 under section \"Restricting Access to a Specific HTTP Referer\" in the below link.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf\nOption C is incorrect because you can control the access to the S3 content via bucket policy.\nOption D is incorrect because the question is not about encrypting/decrypting the data.\nThe proper bucket policy needs to be defined to give access to the S3 content to certain users.\nFor more information on S3 bucket policy examples, please visit the link-\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\nTo implement this requirement, we need to ensure that the S3 bucket can only be accessed via authorized requests from our website and not by direct requests to the S3 URL. We can achieve this by implementing a combination of encryption, access control, and web server settings.\nOption A, using HTTPS endpoints to encrypt data in the S3 bucket, is a good security practice to ensure that data in transit is secure. However, this alone does not prevent direct access to the S3 URL.\nOption B is the correct answer. We can use a bucket policy to restrict access to the S3 bucket based on the HTTP referer header in the request. The referer header is a field in the HTTP request that indicates the website that the user is coming from. By setting up a bucket policy with the aws:referer key, we can restrict access to the S3 bucket to only requests that originate from our website domain. This means that if a user tries to access the S3 URL directly, without coming from our website, they will be denied access.\nHere is an example of a bucket policy that restricts access to the S3 bucket based on the HTTP referer header:\njsonCopy code{     \"Version\": \"2012-10-17\",     \"Id\": \"PolicyForReferer\",     \"Statement\": [         {             \"Sid\": \"Allow get requests referred by mywebsite.com\",             \"Effect\": \"Allow\",             \"Principal\": \"*\",             \"Action\": \"s3:GetObject\",             \"Resource\": \"arn:aws:s3:::mybucket/*\",             \"Condition\": {                 \"StringLike\": {                     \"aws:Referer\": \"https://www.mywebsite.com/*\"                 }             }         }     ] } \nThis policy allows s3:GetObject requests for objects in the mybucket S3 bucket, but only if the aws:Referer field in the request header matches the pattern https://www.mywebsite.com/*.\nOption C is incorrect, as we can use a bucket policy to restrict access to the S3 bucket and prevent direct access to the S3 URL.\nOption D, using server-side and client-side encryption, is a good security practice to ensure that data at rest is secure. However, this does not address the requirement of restricting access to the S3 bucket only from authorized requests originating from our website.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use HTTPS endpoints to encrypt the data in the S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use a bucket policy with the aws:referer key in a condition where the key must match your domain. Browsers should include the HTTP referer header in the request.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can`t. The S3 URL must be public in order to use it on your website.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use server-side and client-side encryption. Only your application can decrypt the objects.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 290,
  "query" : "A big trading company requires various organizations such as importers, exporters, banks, shipping companies, and customs departments to work with one another.\nThe trade-related paperwork needs to go back and forth between the stakeholders, taking 5-10 days to complete, which is very time-consuming.\nThe company is considering creating a blockchain network where all parties can transact and process trade-related paperwork electronically without the need for a central trusted authority.",
  "answer" : "E.\nCorrect Answer - A, B, E.\nAmazon Managed Blockchain is a fully managed service that makes it easy to create and manage scalable blockchain networks using the popular open-source frameworks Hyperledger Fabric and Ethereum.\nAmazon Managed Blockchain has various benefits, such as that it is fully managed, scalable, and secure.\nDetails can be found in https://aws.amazon.com/managed-blockchain/.\nOption A is CORRECT because a member can quickly add a new peer node using Managed Blockchain's APIs.\nOption B is CORRECT because Amazon Managed Blockchain takes care of the hardware, software provisioning, etc.\nOption C is incorrect because Amazon Managed Blockchain does not require an upfront fee, and there is no commitment for usage.\nOption D is incorrect because, at the moment, only Hyperledger Fabric and Ethereum are supported in US East (N.\nVirginia) region.\nOption E is CORRECT because the member can configure peer nodes.\nRefer to the below picture on how AWS Managed Blockchain works.\nThe scenario presented in the question describes a need for a blockchain network to facilitate trade-related paperwork between various stakeholders. Blockchain technology is known for its ability to create a trustless environment where multiple parties can transact with each other without the need for a central trusted authority. Amazon Managed Blockchain is a fully managed service that allows customers to create and manage scalable blockchain networks using popular open-source frameworks, including Hyperledger Fabric, Ethereum, and R3 Corda.\nAnswer A states that Amazon Managed Blockchain can easily scale the blockchain network as the usage of applications on the network grows over time. This is a key feature of the service, as it allows organizations to start small and expand the network as needed without worrying about the underlying infrastructure. Amazon Managed Blockchain is designed to scale to meet the demands of large, complex applications.\nAnswer B states that Amazon Managed Blockchain eliminates the need for manually provisioning hardware, configuring software, and setting up networking and security components. This is another key feature of the service, as it removes the burden of managing infrastructure from the customer. With Amazon Managed Blockchain, customers can focus on building and deploying applications on the blockchain network instead of worrying about the underlying infrastructure.\nAnswer C is incorrect. Amazon Managed Blockchain does not charge an upfront fee or require a commitment for a 12 months usage. Customers pay only for the resources they use, and there are no upfront costs or long-term commitments.\nAnswer D is correct. Amazon Managed Blockchain supports popular blockchain frameworks, including Hyperledger Fabric, Ethereum, and R3 Corda, in most regions. This means that customers can choose the blockchain framework that best meets their needs and deploy it on the Amazon Managed Blockchain service.\nAnswer E is also correct. Once a new member is added, Managed Blockchain lets that member launch and configure multiple blockchain peer nodes. This allows organizations to quickly and easily deploy their blockchain network and start transacting with other members.\nIn summary, Amazon Managed Blockchain is a fully managed service that allows customers to create and manage scalable blockchain networks using popular open-source frameworks. It eliminates the need for managing infrastructure and allows organizations to focus on building and deploying applications on the blockchain network. It supports popular blockchain frameworks and can easily scale to meet the demands of large, complex applications.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Amazon Managed Blockchain can easily scale the blockchain network as the usage of applications on the network grow over time.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon Managed Blockchain eliminates the need for manually provisioning hardware, configuring software, and setting up networking and security components.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon Managed Blockchain charges an upfront fee and requires a commitment for a 12 months usage.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Popular blockchain frameworks, including Hyperledger Fabric, Ethereum, and R3 Corda, are supported in most regions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Once a new member is added, Managed Blockchain lets that member launch and configure multiple blockchain peer nodes.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 291,
  "query" : "You are having trouble maintaining session states on some of your applications using an Elastic Load Balancer(ELB)\nRequests from the same session should be routed to the same target in the target group.\nWhich of the following is the quickest and cost-efficient method to implement this?",
  "answer" : "Answer - C.\nOption A is incorrect because although ElastiCache can be utilized to store the session state in the cache rather than in any database, it is slow to implement and not cost-efficient if compared with option.\nC.Option C is CORRECT because Sticky sessions, also known as session affinity, allow you to route a site user to the particular web server managing that individual user's session.\nIt is also the most cost-efficient method.\nOptions B and D are incorrect because they describe the methods partially and do not clearly indicate that the sticky session feature should be used.\nReference can be found in https://aws.amazon.com/caching/session-management/.\nThe correct answer to the question is C. Use the sticky session feature (also known as session affinity), enabling the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance.\nExplanation: In a load-balanced environment, requests from a user can be distributed across multiple instances, making it challenging to maintain session state information. Elastic Load Balancer (ELB) distributes incoming traffic across multiple instances, allowing you to scale your applications quickly and efficiently. However, ELB's default behavior is to distribute incoming requests across all healthy instances evenly.\nTo maintain session state information and ensure that requests from a user during a session are sent to the same instance, you can use the sticky session feature. When you enable sticky sessions, ELB binds a user's session to a specific instance for the duration of the session. All requests from that user during the session are sent to the same instance, ensuring that session state information is maintained.\nAnswer A is incorrect because Elasticache is a caching service, and while it can be used to store session data, it is not the quickest and most cost-efficient method to implement sticky sessions. Additionally, Elasticache adds complexity to your architecture, which can make it more difficult to manage.\nAnswer B is incorrect because it requires the use of a special cookie to track the instance for each request to each listener, which adds complexity to your application architecture. Additionally, this method does not ensure that all requests from a user during a session are sent to the same instance.\nAnswer D is incorrect because it requires the configuration of a session cookie, which adds complexity to your application architecture. Additionally, this method does not ensure that all requests from a user during a session are sent to the same instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Disable Sticky session. Use Elasticache to put session data. Elasticache is easy to set up, manage, and scale a distributed in-memory cache environment in the cloud.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a special cookie to track the instance for each request to each listener. When the load balancer receives a request, it will then check to see if this cookie is present in the request.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the sticky session feature (also known as session affinity), enabling the load balancer to bind a user`s session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "If your application does not have its own session cookie, then you can configure Elastic Load Balancing to create a session cookie by specifying your own stickiness duration.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 292,
  "query" : "You are deploying your first EC2 instance in AWS and are using the AWS console to do this.\nYou have chosen your AMI and your instance type and have now come to the screen where you configure your instance details.\nOne of the things that you need to decide is whether you want to auto-assign a public IP address or not.\nYou assume that if you do not choose this option, you will be able to assign an Elastic IP address later, which happens to be a correct assumption.\nWhich of the below options best describes why an Elastic IP address would be preferable to a public IP address?",
  "answer" : "Answer - B.\nOption A is incorrect because public IP addresses are free.\nOption B is CORRECT because you can reassign the EIP to a new instance in case of an instance failure.\nThus you do not need to change any reference to the IP address in your application.\nOption C is incorrect because the number of EIPs per account per region is limited (5).\nOption D is incorrect because EIPs are accessible from the internet.\nMore information on EIPs.\nAn Elastic IP address is a static IPv4 address designed for dynamic cloud computing.\nAn Elastic IP address is associated with your AWS account.\nWith an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.\nAn Elastic IP address is a public IPv4 address, which is reachable from the internet.\nIf your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the internet; for example, to connect to your instance from your local computer.\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\nWhen you launch an Amazon Elastic Compute Cloud (Amazon EC2) instance, you have the option to assign it a public IP address automatically, or to use an Elastic IP address.\nA public IP address is assigned to your instance from Amazon's pool of public IP addresses. This IP address is associated with the instance until the instance is stopped, terminated, or replaced. You may also have to pay for a public IP address when it is not in use.\nAn Elastic IP address is a static, public IPv4 address that you can allocate to your AWS account. You can associate an Elastic IP address with your instance, and it remains associated with the instance until you choose to release it. You can also remap the Elastic IP address to another instance if the original instance fails.\nHere are some reasons why an Elastic IP address might be preferable to a public IP address:\n1.\nAvailability: Elastic IP addresses are yours until you release them, whereas public IP addresses are assigned to you temporarily. If you stop or terminate an instance with a public IP address, you lose the IP address.\n2.\nRemapping: You can remap Elastic IP addresses to other instances in your account, allowing you to quickly recover from instance or software failures.\n3.\nRouting: You can also use Elastic IP addresses with your own domain name servers (DNS) so that your instances can be accessed using a friendly DNS name.\n4.\nSecurity: Elastic IP addresses can be used to avoid exposing your instances' public IP addresses to the internet. This can be helpful for applications that require more secure network architectures.\nTherefore, option B is the best description of why an Elastic IP address is preferable to a public IP address. Elastic IP addresses provide the ability to remap the address to another instance in your account in the event of an instance or software failure.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "An Elastic IP address is free, whilst you must pay for a public IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can have an unlimited amount of Elastic IP addresses. However, public IP addresses are limited in number.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An Elastic IP address cannot be accessed from the internet like a public IP address and hence is safer from a security standpoint.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 293,
  "query" : "You have an EBS root device on /dev/sda1 on one of your EC2 instances.\nYou are having trouble with this particular instance, and you want to either Stop/Start, Reboot, or Terminate the instance.\nBut you do not want to lose any data that you have stored on /dev/sda1\nWhich of the below statements best describes the effect each change of instance state would have on the data you have stored on /dev/sda1?",
  "answer" : "Answer - D.\nSince this is an EBS backed instance, it can be stopped and later restarted without affecting data stored in the attached volumes.\nBy default, the root device volume for this instance will be deleted when the instance terminates.\nOption A is incorrect because, upon termination, the volume would get deleted, and the data would get lost (DeleteOnTermination setting is not mentioned, so this is a default case).\nOption B is incorrect because the data on EBS volume would not get lost upon stop/start or reboot.\nOption C is incorrect because the data on EBS volume would not get lost upon reboot.\nOption D is CORRECT because the data on EBS volume would not get lost upon stop/start or reboot as it is not ephemeral.\nOn the other hand, the Instance store is ephemeral storage, and the data would get lost upon starting/stopping the instance.\nMore information on this topic:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\nThe correct answer is (A) Whether you stop/start, reboot, or terminate the instance, it does not matter because data on an EBS volume is not ephemeral, and the data will not be lost regardless of what method is used.\nExplanation:\nAn Amazon Elastic Block Store (EBS) volume is a persistent block-level storage device for use with Amazon EC2 instances. EBS volumes are independent from the EC2 instance, which means that the data on the EBS volume is not lost when the EC2 instance is stopped or terminated.\nWhen an EC2 instance is launched, the root device is typically an EBS volume attached to the instance. In this case, /dev/sda1 refers to the root EBS volume.\nWhen an EC2 instance is stopped or terminated, the root EBS volume is not deleted by default, and the data stored on the volume remains intact. When you start the instance again, the same root EBS volume is reattached to the instance, and the data on it is available.\nIf you terminate an EC2 instance, the root EBS volume is deleted by default, but you can choose to keep the volume as a separate resource. In this case, you can attach the EBS volume to a new EC2 instance and access the data on it.\nIn summary, the data on an EBS volume is persistent, meaning that it is not lost when the EC2 instance is stopped or terminated. Therefore, regardless of whether you stop/start, reboot, or terminate the EC2 instance, the data on /dev/sda1 will not be lost.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Whether you stop/start, reboot, or terminate the instance, it does not matter because data on an EBS volume is not ephemeral, and the data will not be lost regardless of what method is used.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Whether you stop/start, reboot, or terminate the instance, it does not matter because data on an EBS volume is ephemeral, and it will be lost no matter what method is used.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "If you stop/start the instance, the data will not be lost. However, if you either terminate or reboot the instance, the data will be lost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The data will remain on /dev/sda1 if you reboot or stop/start the instance because data on an EBS volume is not ephemeral.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 294,
  "query" : "Someone on your team configured a Virtual Private Cloud with two public subnets in two separate AZs and two private subnets in two separate AZs.\nEach public subnet AZ has a matching private subnet AZ.\nThe VPC and its subnets are properly configured.\nThere are multiple webserver instances in the private subnet.\nYou are told to set up a public-facing Elastic Load Balancer in the public subnets to accept requests from clients and distribute those requests to the webserver instances.\nHow can you set this up?",
  "answer" : "Answer - C.\nOption A is incorrect because you need to set up the internet-facing load balancer.\nSo the public subnets need to be associated.\nOption B is incorrect because web servers need to remain in the private subnets.\nThere is no need to shifting them to the public subnet.\nOption C is CORRECT because you need to associate the public subnets with the internet-facing load balancer.\nYou would also need to ensure that the security group of the load balancer has the listener ports open and the security groups of the private instances allow traffic on the listener ports and the health check ports.\nOption D is incorrect because you can configure the internet-facing load balancer with the public subnet.\nFor more information on the AWS ELB, please refer to the below links.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/ https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/\nTo set up a public-facing Elastic Load Balancer (ELB) in the public subnets to accept requests from clients and distribute those requests to the webserver instances in the private subnets, we need to follow the steps below:\n1. Create an Elastic Load Balancer\nNavigate to the EC2 console, click on \"Load Balancers\" under the \"Load Balancing\" category.\nClick on \"Create Load Balancer\" and select \"Application Load Balancer\" or \"Network Load Balancer\" depending on your needs.\nConfigure the ELB by selecting the VPC and then selecting the two public subnets in different AZs where you want to place the ELB.\nAdd the required security groups to allow incoming traffic on the ELB.\n1. Configure the Target Group\nCreate a new target group and select the same VPC as the ELB.\nChoose the protocol and port that your webserver instances use to communicate with each other.\nSelect the two private subnets in different AZs where the webserver instances are running.\n1. Register the Instances\nRegister the webserver instances with the target group by adding their IP addresses or instance IDs.\nOnce the above steps are complete, the ELB will accept incoming requests and distribute them to the registered webserver instances in the private subnets. Clients can access the web applications through the ELB's public DNS name or IP address.\nAnswer: C. Select both of the public subnets when configuring the EL.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Select both of the private subnets which contain the webserver instances when configuring the EL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Move the webserver instances from the public subnets to the private subnets and then configure the ELB with those subnets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Select both of the public subnets when configuring the EL.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can`t. The ELB must be in the same private subnets as the webserver instances.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 295,
  "query" : "A company has hired you to assist with the migration of an interactive website that allows registered users to rate local restaurants.\nUpdates to the ratings are displayed on the home page, and ratings are updated in real-time.\nAlthough the website is not very popular today, the company anticipates that it will grow over the next few weeks.\nThey also want to ensure that the website remains highly available.\nThe current architecture consists of a single Windows server 2016 web server and a MySQL database on Linux.\nBoth reside inside on an on-premises hypervisor.\nWhat would be the most efficient and optimal way to point the application to AWS, ensuring high performance and availability?",
  "answer" : "Answer - C.\nThe main consideration in the question is that the architecture should be highly available with high performance.\nOption A is incorrect because (a) EC2 servers can communicate with S3 for the web files, and (b) Auto Scaling of webservers and the setup of Multi-AZ RDS instance, as well as the Route 53 alias record with ELB provides high availability.\nHowever, this option does not mention the use of VM Import/Export that would provide an \"optimal\" solution to the given requirement.\nOption B is incorrect because this is an interactive website and S3 is suitable for static websites.\nOption C is CORRECT because Route 53 is used to create an Alias record to point to an ELB DNS.\nUsing a VM Import/Export would be the most \"optimal\" solution as given in the requirement.\nOption D is incorrect because, in Route 53, you should create an Alias record pointing to the ELB rather than the EC2 IP addresses.\nWhen the EC2 instance in the Auto Scaling group is recreated, the IP will be changed, and the A record will not work in Route 53.\nFor more information, please refer to the below URL-\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\nOption A is the best solution among the given choices to point the application to AWS, ensuring high performance and availability. Here's why:\n1.\nConfigure Auto Scaling to launch one Windows Server 2016 instance each in us-west-1a and us-west-1b: Auto Scaling will ensure that the web server instances are launched in multiple Availability Zones (AZs) to provide high availability and fault tolerance. If one AZ goes down, the other will continue to serve the requests.\n2.\nCopy the web files from an on-premises web server to each Amazon EC2 web server, using Amazon S3 as the repository: Copying the web files to Amazon S3 will enable easy access to the files for both instances, and will also provide an additional level of durability and availability.\n3.\nLaunch a Multi-AZ MySQL Amazon RDS Instance in us-west-1a and us-west-1b: By launching a Multi-AZ RDS instance, the database will be highly available, and Amazon RDS will automatically handle synchronous data replication between the primary and standby instances.\n4.\nImport the data into Amazon RDS from the latest MySQL backup: Importing the data into Amazon RDS from the latest MySQL backup will ensure that the most up-to-date data is available in the RDS instance.\n5.\nCreate an elastic load balancer (ELB) to front your web servers: The ELB will distribute incoming traffic across multiple web server instances, improving performance and availability.\n6.\nUse Amazon Route53 and create an alias (Type: A-IP4 Address) record pointing to the ELB: Route53 will provide the ability to route traffic based on latency or geography, and the A record will provide a DNS mapping for the ELB.\nOption B is not a feasible solution because running a website directly out of Amazon S3 is not possible, and S3 is not optimized for hosting dynamic content like a web application.\nOption C is a valid solution, but it involves unnecessary steps such as creating an EC2 AMI and using VM Import/Export, which are not needed for this scenario. This would also add additional complexity to the solution.\nOption D is not recommended because it does not include an ELB, which is essential for distributing traffic across multiple web server instances, improving performance and availability. Additionally, it does not use a Route 53 Alias record for the ELB, which is required for high availability and fault tolerance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure Auto Scaling to launch one Windows Server 2016 instance each in us-west-1a and us-west-1b. Copy the web files from an on-premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. Launch a Multi-AZ MySQL Amazon RDS Instance in us-west-1a and us-west-1b. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer (ELB) to front your web servers. Use Amazon Route53 and create an alias ( Type: A-IP4 Address ) record pointing to the EL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Export web files to an Amazon S3 bucket in us-west-1. Run the website directly out of Amazon S3. Launch a Multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Use Route 53 and create an alias record pointing to the elastic load balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS VM Import/Export to create an Amazon EC2 AMI of the webserver. Configure Auto Scaling to launch one web server in us-west-1a and one in us-west-1b. Create an Elastic Load Balancer to distribute the traffic. Launch a Multi-AZ MySQL Amazon RDS instance in us-west-1. Import the data Into Amazon RDS from the latest MySQL backup. Use Amazon Route 53 and create an Alias record pointing to the Elastic Load Balancer.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS VM Import/Export to create an Amazon EC2 AMI of the webserver. Configure Auto Scaling to launch one webserver in us-west-1a and one in us-west-1b. Launch a Multi-AZ MySQL Amazon RDS instance in us-west-1. Import the data into Amazon RDS from the latest MySQL backup. Use Amazon Route 53 to create a hosted zone and point an A record to the EC2 IP addresses.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 296,
  "query" : "You have been asked to design the storage layer for an application.\nThe application requires the disk performance to be at least 100000 IOPS.\nThe data should be highly available without data loss, even when an availability zone has an outage.\nThe volume you provide must have a capacity of at least 2 TB.\nWhich of the following designs will meet these objectives?",
  "answer" : "E.\nAnswer - B.\nOption A is incorrect because this configuration is done entirely in a single AZ.\nThere will be a data loss if the entire AZ goes down.\nOption B is CORRECT because (a) it uses RAID 0 configuration that utilizes all the volumes and gives the aggregated IOPS performance, and (b) the replication across another AZ gives higher availability and fault tolerance even in case of an entire AZ becomes unavailable.\nOption C is incorrect because it uses an asynchronous backup of the data.\nThe problem scenario demands a synchronous replication to prevent any data loss.\nOption D is incorrect because RAID 5 is not recommended for Amazon EBS since the parity write operations consume some of the IOPS available to the volumes.\nSee the links below for more details.\nhttp://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/raid-config.html https://en.wikipedia.org/wiki/Standard_RAID_levels\nOption E is incorrect because, even if the snapshots are taken every 15 minutes, there are chances that there will be data loss during this time.\nThe requirement is that there should be absolutely no data loss.\nTo meet the requirement of providing a highly available storage layer with a capacity of at least 2TB and 100,000 IOPS, we need to carefully consider the design options that AWS offers.\nA. Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD disks provided with the instance. Provision 3×1 TB EBS volumes attach them to the instance and configure them as a second RAID 0 volume. Configure synchronous, block-level replication from the ephemeral backed volume to the EBS-backed volume.\nOption A uses an i2.8xlarge instance with four 800GB SSD disks to create a RAID 0 volume with a capacity of 3.2TB. Additionally, three 1TB EBS volumes are attached to the instance to create a second RAID 0 volume with a total capacity of 3TB. Synchronous, block-level replication is configured from the ephemeral backed volume to the EBS-backed volume. While this solution meets the capacity and IOPS requirements, it does not meet the highly available requirement since the instance is deployed in a single availability zone.\nB. Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD disks provided with the Instance Configure synchronous block-level replication to an identically configured instance in us-east-1b.\nOption B also uses an i2.8xlarge instance with four 800GB SSD disks to create a RAID 0 volume with a capacity of 3.2TB. However, synchronous, block-level replication is configured to an identically configured instance in a different availability zone. This solution meets the capacity and IOPS requirements and provides high availability by replicating the data to another availability zone.\nC. Instantiate a c3.8xlarge instance in us-east-1. Provision an AWS Storage Gateway and configure it for 3 TB of storage and 100,000 IOPS. Attach the volume to the instance.\nOption C uses a c3.8xlarge instance with an AWS Storage Gateway configured for 3TB of storage and 100,000 IOPS. The Storage Gateway is a hybrid cloud storage service that provides on-premises applications with low-latency access to cloud-based storage. This solution meets the capacity and IOPS requirements and provides high availability by replicating the data to AWS cloud storage.\nD. Instantiate a c3.8xlarge instance in us-east-1 provision 4x1TB EBS volumes, attach them to the instance, and configure them as a single RAID 5 volume. Ensure that EBS snapshots are performed every 15 minutes.\nOption D uses a c3.8xlarge instance with four 1TB EBS volumes configured as a single RAID 5 volume. EBS snapshots are configured to be taken every 15 minutes. While this solution meets the capacity and IOPS requirements, it does not provide high availability since the data is stored in a single availability zone.\nE. Instantiate a c3 8xlarge Instance in us-east-1 Provision 3x1TB EBS volumes, attach them to the instance, and configure them as a single RAID 0 volume. Ensure that EBS snapshots are performed every 15 minutes.\nOption E uses a c3.8xlarge instance with three 1TB EBS volumes configured as a single RAID 0 volume. EBS snapshots are configured to be taken every 15 minutes. While this solution meets the capacity and IOPS requirements, it does not provide high availability since the data is stored in a single availability zone.\nOverall, option B is the most appropriate solution as it meets all the requirements of capacity, IOPS",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD disks provided with the instance. Provision 3×1 TB EBS volumes attach them to the instance and configure them as a second RAID 0 volume. Configure synchronous, block-level replication from the ephemeral backed volume to the EBS-backed volume.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD disks provided with the Instance Configure synchronous block-level replication to an identically configured instance in us-east-1b.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Instantiate a c3.8xlarge instance in us-east-1. Provision an AWS Storage Gateway and configure it for 3 TB of storage and 100,000 IOPS. Attach the volume to the instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Instantiate a c3.8xlarge instance in us-east-1 provision 4x1TB EBS volumes, attach them to the instance, and configure them as a single RAID 5 volume. Ensure that EBS snapshots are performed every 15 minutes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Instantiate a c3 8xlarge Instance in us-east-1 Provision 3x1TB EBS volumes, attach them to the instance, and configure them as a single RAID 0 volume. Ensure that EBS snapshots are performed every 15 minutes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 297,
  "query" : "A complicated data analysis software in JAVA has used a standard SQS queue to decouple users' requests and the backend processing.\nThe visibility timeout for the queue is set as 60 seconds.\nIn most cases, the process of messages can finish within 1 minute successfully.\nHowever, it may take about 100 seconds for the backend to get the job done for certain new requests.\nThese requests are tagged with a specific JSON header by the frontend already.\nYou want to ensure that these new requests are processed properly in the backend.\nHow should you improve the queue configurations in the best way?",
  "answer" : "Correct Answer - B.\nFor the new requests, the default visibility timeout of 60 seconds for the queue would be insufficient.\nThe messages' visibility should be extended by specifying a new timeout value using the ChangeMessageVisibility action through the AWS SDK.\nOption A is incorrect: Because this change in the AWS console is a global change for all messages in the queue.\nHowever, in this case, only specific messages need to extend the timer.\nOption B is correct: Refer to https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-visibility-timeout-queue.html on how to configure Visibility Timeout for an Amazon SQS queue.\nUsing the method, those messages with particular JSON headers can get a revised timer.\nOption C is incorrect: Because the FIFO queue type does not help with the visibility timeout issue for the new messages.\nOption D is incorrect: Although the dead letter queue can help isolate the new messages, it cannot fix the problem on the visibility timeout issue.\nBesides, the timer needs to be enlarged only for these new messages.\nIn this scenario, a complicated data analysis software in Java is using a standard SQS queue to decouple user requests and backend processing. The visibility timeout for the queue is set to 60 seconds. However, for certain new requests that are tagged with a specific JSON header by the frontend, it may take about 100 seconds for the backend to complete the job.\nTo ensure that these new requests are processed properly in the backend, the queue configurations need to be improved. Let's examine each of the answer choices to determine the best course of action:\nA. In the AWS SQS console, simply change the default visibility timeout from 1 minute to 2 minutes.\nThis option may work in some cases where the backend processing time is slightly longer than the current visibility timeout. However, in this scenario, the processing time is significantly longer, and changing the default visibility timeout for the entire queue may negatively impact the overall performance of the system.\nB. Use AWS SDK to adjust the visibility timeout to 2 minutes for messages that contain the specific JSON header.\nThis option is a more targeted approach than changing the default visibility timeout for the entire queue. By using the AWS SDK to adjust the visibility timeout to 2 minutes only for messages that contain the specific JSON header, we can ensure that only the necessary messages are affected by the longer timeout. This will help improve the processing of the specific requests without negatively impacting the overall system performance.\nC. Change the queue type from standard to FIFO with the default visibility timeout configured as 2 minutes.\nChanging the queue type from standard to FIFO can provide ordering guarantees for messages, which may be useful in some scenarios. However, in this case, it does not address the specific issue of processing requests with a longer backend processing time. Additionally, changing the queue type may require significant modifications to the code that interacts with the queue.\nD. Create a new SQS queue as the dead letter queue. Route these specific requests to the dead letter queue so that the normal backend process is not influenced.\nCreating a new SQS queue as the dead letter queue can be useful for handling messages that cannot be processed by the normal backend process. However, in this case, the issue is not with the message itself but with the backend processing time. Routing these specific requests to the dead letter queue would not address the issue of longer processing times.\nBased on the above analysis, the best option is B: Use AWS SDK to adjust the visibility timeout to 2 minutes for messages that contain the specific JSON header. This option provides a targeted approach to improving the queue configurations without negatively impacting the overall system performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS SQS console, simply change the default visibility timeout from 1 minute to 2 minutes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS SDK to adjust the visibility timeout to 2 minutes for messages that contain the specific JSON header.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Change the queue type from standard to FIFO with the default visibility timeout configured as 2 minutes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new SQS queue as the dead letter queue. Route these specific requests to the dead letter queue so that the normal backend process is not influenced.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 298,
  "query" : "You have been asked to leverage Amazon VPC EC2 and SQS to implement an application that processes millions of messages per second from a message queue.\nYou want to ensure that your application in EC2 instances has sufficient bandwidth when the SQS queue grows.\nThe instances also need to access the internet.\nWhich option will provide the most scalable solution for the application?",
  "answer" : "Answer - D.\nThe Amazon EC2 instance requires access to the Internet.\nHence, it should be in a public subnet or be in a private subnet with a NAT instance/gateway in the public subnet.\nOption A is incorrect because ELB does not ensure scalability.\nOption B is incorrect because (a) the EBS-optimized option will not contribute to scalability, and (b) there should be a NAT instance/gateway in the public subnet of the VPC for accessing SQS.\nOption C is incorrect because if you remove the NAT instance, the EC2 instance cannot access the SQS service.\nOption D is CORRECT because (a) it uses Auto Scaling for ensuring scalability of the application, and (b) it has instances in the public subnet so they can access the SQS service.\nFor more information on SQS, please visit the below URL-\nhttps://aws.amazon.com/sqs/faqs/\nTo implement an application that processes millions of messages per second from a message queue using Amazon VPC, EC2, and SQS, the most scalable solution would be to launch the application instances in private subnets with the associate-public-IP-address=true option enabled. Removing any NAT instance from the public subnet, if any, would also be required.\nOption A - Ensure the application instances are properly configured with an Elastic Load Balancer: Using an Elastic Load Balancer (ELB) ensures that incoming traffic is distributed evenly across multiple EC2 instances. However, it does not address the issue of ensuring that the application in EC2 instances has sufficient bandwidth when the SQS queue grows. ELB does not provide any additional bandwidth and does not affect the private/public subnet choice.\nOption B - Ensure the application instances are launched in private subnets with the EBS-optimized option enabled: Launching the application instances in private subnets with the EBS-optimized option enabled ensures that the instances have optimized performance for EBS volumes, but it does not address the issue of ensuring that the application in EC2 instances has sufficient bandwidth when the SQS queue grows. EBS-optimized instances are recommended for applications that require high I/O operations on their EBS volumes.\nOption C - Ensure the application instances are launched in private subnets with the associate-public-IP-address=true option enabled. Remove any NAT instance from the public subnet, if any: This option is the most scalable solution. Launching the instances in private subnets ensures that they are not directly accessible from the internet, while enabling the associate-public-IP-address option allows the instances to access the internet for tasks like software updates and accessing third-party APIs. Removing the NAT instance from the public subnet ensures that all traffic flows through the private subnets, avoiding a single point of failure that can occur with NAT instances.\nOption D - Ensure the application instances are launched in public subnets with an Auto Scaling group and Auto Scaling triggers are configured to watch the SQS queue size: Launching instances in public subnets is not a recommended best practice, as it exposes them directly to the internet. It also does not address the issue of ensuring that the application in EC2 instances has sufficient bandwidth when the SQS queue grows. While using an Auto Scaling group and triggers to watch the SQS queue size is a good practice, it does not replace the need for proper subnet configurations.\nTherefore, the correct answer is C - Ensure the application instances are launched in private subnets with the associate-public-IP-address=true option enabled. Remove any NAT instance from the public subnet, if any.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Ensure the application instances are properly configured with an Elastic Load Balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure the application instances are launched in private subnets with the EBS-optimized option enabled.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure the application instances are launched in private subnets with the associate-public-IP-address=true option enabled. Remove any NAT instance from the public subnet, if any.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure the application instances are launched in public subnets with an Auto Scaling group and Auto Scaling triggers are configured to watch the SQS queue size.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 299,
  "query" : "Your company is migrating an entire project to AWS.\nHowever, as certain legacy databases are too old to be migrated, your team has established two direct connections (10 gigabits) to link the on-premises data center with various AWS services such as S3, EC2, Lambda, etc.\nYour lead asks you to aggregate the two direct connections ( that use the same AWS device ) to increase the bandwidth.\nWhich combination of steps should you take to fulfill this requirement? (Select TWO)",
  "answer" : "Correct Answer - A, C.\nAs this case needs to aggregate the two direct connections to increase the bandwidth, Direct Connect LAG should be considered.\nTo establish a LAG connection, it needs the below steps:\n1, Create a new link aggregation group (LAG).\n2, Associate existing connections or new connections with the LAG.Details can be checked in https://docs.aws.amazon.com/directconnect/latest/UserGuide/lags.html.\nOption A is correct: AWS Direct Connect supports LAG which uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections.\nOption B is incorrect: LAG supports existing connections.\nSo creating new direct connections is not required.\nOption C is correct: Refer to https://docs.aws.amazon.com/directconnect/latest/UserGuide/associate-connection-with-lag.html on how to associate connections with LAG.Option D is incorrect: AWS Direct Connect gateway is used to connect Direct Connect connection to one or more VPCs in the AWS account, which is not needed for this scenario.\nSure, I'd be happy to provide a detailed explanation.\nThe requirement here is to increase the bandwidth by aggregating two direct connections between the on-premises data center and various AWS services. To achieve this, we need to create a link aggregation group (LAG) in the AWS Direct Connect console.\nA LAG allows us to combine multiple connections into a single logical connection, providing increased bandwidth and improved reliability. In this case, we can use a LAG to combine the two existing direct connections.\nTo create a LAG and associate the existing connections, we need to follow these steps:\n1.\nIn the AWS Direct Connect console, select \"Link Aggregation Groups\" from the navigation pane.\n2.\nClick the \"Create LAG\" button to create a new LAG.\n3.\nSpecify the LAG details, such as the name, number of connections, and bandwidth capacity.\n4.\nSelect the two existing connections that you want to associate with the LAG.\n5.\nChoose the same AWS device for both connections to ensure that they can be aggregated.\n6.\nClick \"Create\" to create the LAG and associate the connections.\nOnce the LAG is created, traffic between the on-premises data center and AWS services will be evenly distributed across the two connections in the LAG, providing increased bandwidth.\nOption B, which suggests provisioning two new connections and associating them with the LAG, is not necessary since we already have two existing connections that can be associated with the LAG. Therefore, option B is not the correct answer.\nOption D, which suggests creating an AWS Direct Connect gateway to combine two existing connections over a private virtual interface, is also not necessary in this case. A Direct Connect gateway is used to connect a virtual private cloud (VPC) to on-premises resources over a Direct Connect connection. It is not used to combine multiple Direct Connect connections into a LAG. Therefore, option D is also not the correct answer.\nIn summary, the correct answers are A and C. We need to create a LAG in the AWS Direct Connect console and associate the two existing direct connections with the LAG to increase the bandwidth.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS Direct Connect console, create a link aggregation group (LAG).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Provision two new connections and associate them with the link aggregation group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Associate two existing connections with the LAG in the AWS Direct Connect console.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an AWS Direct Connect gateway to combine two existing AWS Direct Connect connections over a private virtual interface.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 300,
  "query" : "In order to keep a dedicated connection and more consistent network performance, company ABC has set up 8 active Direct Connections between one of Amazon's Direct Connect locations and the company's colocation environment.\nThe speed of the connections is 1Gbps for 6 connections and 10 Gbps for the other 2 connections.\nThe company plans to create several link aggregation groups (LAG) for all the connections.\nWhich configurations are valid? (Select TWO)",
  "answer" : "Correct Answer - B, C.\nAccording to https://docs.aws.amazon.com/directconnect/latest/UserGuide/lags.html, there are several rules to apply for LAG.\nFirstly, all connections in the LAG must use the same bandwidth.\nSecondly, in a LAG, a maximum of four connections are allowed.\nOption A is incorrect: Because only a maximum of four connections can be configured for a LAG.Option B is correct: Because it meets the conditions mentioned in the above link.\nOption C is correct: Same as Option.\nB.Option D is incorrect: Because the connections in a LAG cannot use different bandwidth.\nIn this scenario, Company ABC has set up 8 active Direct Connections with Amazon's Direct Connect location to achieve a dedicated connection and consistent network performance. The speed of the connections is 1Gbps for six connections and 10 Gbps for two connections. The company wants to create several link aggregation groups (LAGs) for all the connections.\nA link aggregation group (LAG) is a collection of physical connections that are bundled together to form a single logical connection. This approach helps to increase the aggregate bandwidth and provides redundancy in case of any link failure. AWS allows up to 8 connections per LAG.\nNow, let's evaluate each of the given options:\nOption A: One LAG with six 1Gbps connections. One LAG with two 10 Gbps connections. This option is valid because the total number of connections is eight, and it splits the 1Gbps connections and 10Gbps connections into two separate LAGs.\nOption B: Two LAGs with three 1Gbps connections each. One LAG with two 10 Gbps connections. This option is valid because it splits the 1Gbps connections into two separate LAGs and combines the 10Gbps connections into one LAG.\nOption C: One LAG with four 1Gbps connections. One LAG with the other two 1Gbps connections. One LAG with two 10 Gbps connections. This option is also valid because it splits the 1Gbps connections into two separate LAGs and combines the 10Gbps connections into one LAG.\nOption D: One LAG with all eight direct connections. This option is invalid because a single LAG can have a maximum of 8 connections, and in this scenario, there are eight connections. If all the connections are bundled together into one LAG, it may lead to oversubscription, and the benefits of link aggregation may not be fully realized.\nTherefore, options A, B, and C are valid configurations for creating link aggregation groups (LAGs) for all the connections in the scenario provided.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "One LAG with six 1Gbps connections. One LAG with two 10 Gbps connections.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Two LAGs with three 1Gbps connections each. One LAG with two 10 Gbps connections.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "One LAG with four 1Gbps connections. One LAG with the other two 1Gbps connections. One LAG with two 10 Gbps connections.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "One LAG with all eight direct connections.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 301,
  "query" : "An organization is planning to use AWS for its production rollout.\nThe organization needs a managed AWS service to create a LAMP stack automatically, download the latest PHP installable from S3, and set up the ELB.",
  "answer" : "Answer - A.\nThe Elastic Beanstalk is an easy-to-use AWS service for deploying and scaling web applications and services.\nElastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring.\nMeanwhile, if needed, we can still retain full control over the AWS resources used in the application and access the underlying resources at any time.\nHence, A is the CORRECT answer.\nFor more information on launching a LAMP stack with Elastic Beanstalk, please check the following references:\nhttps://aws.amazon.com/getting-started/projects/launch-lamp-web-app/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/php-ha-tutorial.html?icmpid=docs_tutorial_projects\nWe can deploy the resources on AWS CloudFormation as well, but it is more complicated than Elastic Beanstalk:\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/deploying.applications.html\nThe managed AWS service that would fit the requirements of the organization to create a LAMP stack automatically, download the latest PHP installable from S3, and set up the ELB is AWS Elastic Beanstalk (Option A).\nHere is a detailed explanation of why Elastic Beanstalk is the most suitable service:\nA) AWS Elastic Beanstalk: Elastic Beanstalk is a fully managed service that makes it easy to deploy and run applications in multiple languages such as PHP, Python, Ruby, Java, Node.js, and more. Elastic Beanstalk simplifies the deployment, management, and scaling of applications by handling the underlying infrastructure and resource provisioning. With Elastic Beanstalk, developers can quickly deploy their code and focus on writing the application's business logic.\nIn the case of the LAMP stack, Elastic Beanstalk provides a pre-configured environment that includes Linux, Apache, MySQL, and PHP. Elastic Beanstalk also allows developers to customize the environment by modifying the server, platform, and application settings. Elastic Beanstalk can be configured to automatically download the latest PHP installable from S3 and set up the ELB to distribute the incoming traffic across multiple instances.\nB) AWS CloudFront: CloudFront is a content delivery network (CDN) that delivers content, including images, videos, applications, and APIs, with low latency and high transfer speeds. CloudFront can be used to distribute the content to users from the edge locations closest to them. However, CloudFront is not suitable for creating a LAMP stack, downloading PHP installables, or setting up ELB.\nC) AWS CloudFormation: CloudFormation is a service that allows developers to create and manage AWS infrastructure as code. CloudFormation templates define the resources needed for an application or service, and CloudFormation provisions and configures the resources automatically. While CloudFormation is powerful and flexible, it does not provide the managed environment required for creating a LAMP stack, downloading PHP installables, or setting up ELB.\nD) AWS DevOps: DevOps is a set of practices that combines software development (Dev) and IT operations (Ops) to reduce the time between committing a change to a system and putting it into production. AWS DevOps is a suite of services that support DevOps practices such as continuous integration (CI), continuous delivery (CD), and infrastructure as code (IAC). While AWS DevOps provides automation and integration tools, it does not provide the managed environment required for creating a LAMP stack, downloading PHP installables, or setting up ELB.\nTherefore, the most suitable managed AWS service for the organization to create a LAMP stack automatically, download the latest PHP installable from S3, and set up the ELB is AWS Elastic Beanstalk.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Elastic Beanstalk",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS CloudFront",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS CloudFormation",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS DevOps.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 302,
  "query" : "You work in the integration team of a company, and your team is integrating the infrastructure with Amazon VPC.\nYou are recently assigned a task to create a VPN connection.\nYou have the AWS management console logging access.\nThe first step that you plan to do is to create a customer gateway in the AWS VPC console.\nIn order to do that, which information do you need? (Select TWO)",
  "answer" : "Correct Answer - A, D.\nThe first step of creating a VPN connection is to set up a customer gateway in the AWS VPC console according to https://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html.\nOption A is correct: AWS VPN has used BGP ASN to establish the connection for dynamic routing.\nOption B is incorrect: For static routing, no BGP ASN is needed.\nOption C is incorrect: The internet-routable IP address for the customer gateway device's external interface is required.\nAnd the value must be static.\nOption D is correct: Same reason as option.\nC.\nWhen creating a VPN connection in Amazon VPC, the first step is to create a customer gateway. A customer gateway is the VPN device that is located on-premises and establishes the VPN connection with the virtual private gateway (VGW) in Amazon VPC.\nTo create a customer gateway, the following information is required:\n1.\nA static, internet-routable IP address for the customer gateway device: This is required to establish a secure VPN tunnel between the customer gateway and virtual private gateway. The IP address must be reachable over the internet and should not be a private or non-routable IP address.\n2.\nA Border Gateway Protocol (BGP) Autonomous System Number (ASN): This is required if the routing type is dynamic. BGP is a protocol that enables the exchange of routing information between the customer gateway and the virtual private gateway. If the customer gateway is using BGP, it must have a unique ASN.\nIn addition, if the customer gateway is behind a NAT device, the dynamic public IP address of the NAT device can be used.\nIt is important to note that the ASN is only required for dynamic routing. If the routing type is static, an ASN is not required. Static routing requires the configuration of static routes on the customer gateway, whereas dynamic routing uses BGP to automatically learn and advertise routes between the customer gateway and the virtual private gateway.\nIn summary, to create a customer gateway in Amazon VPC, you need a static, internet-routable IP address for the customer gateway device, and a BGP ASN if the routing type is dynamic. If the customer gateway is behind a NAT device, the dynamic public IP address of the NAT device can be used.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "A Border Gateway Protocol (BGP) Autonomous System Number (ASN) if the routing type is Dynamic.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A BGP Autonomous System Number (ASN) if the routing type is static.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A dynamic public IP address for the customer gateway device. If the customer gateway is behind a NAT device, use the NAT device`s dynamic public IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A static, internet-routable IP address for the customer gateway device.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 303,
  "query" : "A user has launched a large EBS backed EC2 instance in the US-East-1a region.\nThe user wants to achieve Disaster Recovery (DR), for that instance, by creating another small instance in Europe.\nHow can the user achieve DR?",
  "answer" : "Answer - B.\nOptions A and C are incorrect because you cannot directly copy the instance.\nYou need to create an AMI from the instance.\nOption B is CORRECT because if you need an AMI across multiple regions, you have to copy the AMI across regions.\nNote that, by default, AMI's that you have created will not be available across all regions.\nOption D is incorrect because using \"Launch More Like This...\" enables you to use a current instance as a base for launching other instances in the same availability zone.\nIt does not clone your selected instance; it only replicates some configuration details.\nFirst, create a copy of your instance, create an AMI from it, and then launch more instances from the AMI.\nFor the entire details to copy AMI's, please visit the link -\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\nTo achieve Disaster Recovery (DR) for an EBS-backed EC2 instance launched in the US-East-1a region, the user can follow the below steps:\nOption A: Copy the running instance using the “Instance Copy” command to the EU region. This option is not possible as the \"Instance Copy\" feature is not available for copying instances across regions.\nOption B: Create an AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI. This option involves creating an Amazon Machine Image (AMI) of the instance, copying the AMI to the EU region, and then launching an instance from the copied AMI. The user can follow the below steps to achieve this:\n1.\nCreate an AMI of the running instance: The user needs to create an AMI of the running instance. To create an AMI, the user can select the running instance and click on the \"Create Image\" option. The user needs to provide a unique name and description for the AMI.\n2.\nCopy the AMI to the EU region: Once the AMI is created, the user needs to copy the AMI to the EU region. To copy the AMI, the user can select the AMI and click on the \"Copy AMI\" option. The user needs to select the destination region as EU and provide a unique name and description for the copied AMI.\n3.\nLaunch an instance from the copied AMI: Once the AMI is copied to the EU region, the user can launch an instance from the copied AMI. To launch an instance, the user can select the copied AMI and click on the \"Launch Instance\" option. The user needs to select the instance type, VPC, subnet, and other details as per the requirement.\nOption C: Copy the instance from the US East region to the EU region. This option is not possible as it is not feasible to copy a running instance across regions.\nOption D: Use the “Launch more like this” option to copy the instance from one region to another. This option is not possible as the \"Launch more like this\" feature is not available for launching instances across regions.\nTherefore, option B is the correct answer. The user needs to create an AMI of the running instance, copy the AMI to the EU region, and launch an instance from the copied AMI to achieve DR for the instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Copy the running instance using the “Instance Copy” command to the EU region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Copy the instance from the US East region to the EU region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the “Launch more like this” option to copy the instance from one region to another.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 304,
  "query" : "You are managing the AWS account of a big organization.\nThe organization already has a third-party service to perform the user authentication.\nThe organization has more than 1000+ employees, and they want to provide access to various AWS services to most of the employees.\nWhich of the below mentioned options is the best possible solution in this case?",
  "answer" : "Answer - D.\nThe best practice for IAM is to create roles that have specific access to an AWS service and then give the user permission to the AWS service via the role.\nOption A is incorrect because creating a separate IAM user is not a feasible solution here.\nInstead, creating an IAM role would be a more appropriate solution.\nOption B is incorrect because this is an invalid workflow of using IAM roles for authenticating the users.\nOption C is incorrect because creating an IAM group for each user is not a best practice.\nOption D is CORRECT because it authenticates the users with the organization's authentication service and associates an appropriate IAM Role for accessing the AWS services.\nFor the best practices on IAM policies, please visit the link.\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\nThe best possible solution in this case would be option D: Create IAM roles to work with the organization's authentication service to authorize users for various AWS services.\nExplanation:\nSince the organization already has a third-party service to perform user authentication, it would be more efficient to leverage this existing authentication service rather than creating separate IAM users or groups for each employee. Using IAM roles to work with the organization's authentication service would allow the organization to centralize its user management and access control policies.\nIAM roles allow AWS resources, such as EC2 instances or Lambda functions, to assume the permissions associated with the role. By creating IAM roles that work with the organization's existing authentication service, the organization can authorize users to access various AWS services based on their permissions in the authentication service. This also allows for easier management of user permissions, as the organization can simply update the permissions in the authentication service and those changes will be reflected in the IAM roles.\nOption A, creating a separate IAM user for each employee, would be impractical and difficult to manage for an organization with 1000+ employees. Additionally, it would not leverage the organization's existing authentication service.\nOption B, creating an IAM role and attaching STS with the role, would allow for temporary access to AWS services, but it would not necessarily integrate with the organization's existing authentication service.\nOption C, creating IAM groups for each user based on the organization's departments, could be a valid option but may not be the most efficient. It would require the organization to manage group membership for each employee, and any changes to group membership would have to be managed separately from the authentication service.\nTherefore, option D is the best solution as it leverages the existing authentication service and allows for centralized user management and access control policies.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The user should create a separate IAM user for each employee and provide access to them as per the policy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user should create an IAM role and attach STS with the role. The user should attach that role to the EC2instance and setup AWS authentication on that server.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user should create IAM groups for each user as per the organization’s departments and add each user to the group for better access control.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create IAM roles to work with the organization’s authentication service to authorize users for various AWS services.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 305,
  "query" : "A user uses CloudFormation to launch an EC2 instance and plans to configure an application after the instance is launched.\nThe user wants to coordinate stack resource creation with configuration actions that are external to the stack creation.\nHow can the user configure this?",
  "answer" : "Answer - D.\nYou can use a wait condition for situations like the following.\nTo coordinate stack resource creation with configuration actions that are external to the stack creation.\nTo track the status of a configuration process.\nFor more information on Cloudformation Wait condition, please visit the link.\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-waitcondition.html\nWhen launching an EC2 instance using CloudFormation, the user may need to coordinate the stack resource creation with configuration actions that are external to the stack creation. For example, the user may want to run a script or install software on the instance after it is launched.\nIn this scenario, the user can use a CreationPolicy attribute to ensure that the instance is fully configured before the stack creation is considered complete. The CreationPolicy attribute is used to specify a wait condition that CloudFormation monitors during stack creation. CloudFormation will not consider the stack creation complete until the wait condition is satisfied.\nTo use a CreationPolicy attribute, the user can define a WaitCondition resource in the CloudFormation template. The WaitCondition resource specifies a URL that the user's configuration actions can send a signal to when they are complete. The signal can include a status value that indicates whether the configuration actions succeeded or failed.\nThe user can then associate the WaitCondition resource with the EC2 instance using a CreationPolicy attribute. This tells CloudFormation to monitor the wait condition specified by the WaitCondition resource and wait until it receives a signal indicating that the configuration actions are complete.\nOption A is incorrect because it is possible to wait for the creation and launch of other dependent resources. Option B is incorrect because there is no HoldCondition resource in CloudFormation. Option C is incorrect because there is no DependentCondition resource in CloudFormation. Option D is incorrect because there is no WaitCondition attribute in CloudFormation, but there is a CreationPolicy attribute that can be used to achieve the desired result.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It is not possible that the stack creation will wait until one service is created and launched.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user can use the HoldCondition resource to wait for the creation of the other dependent resources.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user can use the DependentCondition resource to hold the creation of the other dependent resources.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Associate a CreationPolicy attribute with the wait condition.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 306,
  "query" : "A marketing research company has developed a tracking system that collects user behavior during web marketing campaigns on behalf of customers worldwide.\nThe tracking system consists of an Auto Scaling group of EC2 instances behind an ELB.\nAnd the collected data is stored in DynamoDB.\nAfter the campaign is terminated, the tracking system is torn down, and the data is moved to Amazon Redshift, where it is aggregated and used to generate detailed reports.",
  "answer" : "E.\nAnswer - C and E.\nOption A is incorrect because you need to retain or keep the snapshots of the EBS volumes to launch similar instances in the new region.\nOption B is incorrect because a DynamoDB table with the same name can be created in different regions.\nThey have to be unique in a single region.\nOption C is CORRECT because you need to get the name of the Availability Zone based on the region in which the template would be used.\nOption D is incorrect because you do not need to define IAM users per region as they are global.\nOption E is CORRECT because the AMI ID would be needed to launch similar instances in the new region where the template would be used.\nMore information on CloudFormation intrinsic functions:\nYou can use the Fn::GetAZs function of CloudFormation to get the AZ of the region and assign it to the ELB.An example of the Fn::GetAZs function is given below.\n{ \"Fn::GetAZs\" : \"\" }\n{ \"Fn::GetAZs\" : { \"Ref\" : \"AWS::Region\" } }\n{ \"Fn::GetAZs\" : \"us-east-1\" }\nAn example of the FindInMap is shown below.\nThis is useful when you want to get particular values region wise which can be used as parameters.\nSince the Launch configuration contains the AMI ID information and the AMI ID is different in different regions, you need to recreate the Launch Configurations based on the AMI ID.{\n...\n\"Mappings\" : {\n\"RegionMap\" : {\n\"us-east-1\" : { \"32\" : \"ami-6411e20d\", \"64\" : \"ami-7a11e213\" },\n\"us-west-1\" : { \"32\" : \"ami-c9c7978c\", \"64\" : \"ami-cfc7978a\" },\n\"eu-west-1\" : { \"32\" : \"ami-37c2f643\", \"64\" : \"ami-31c2f645\" },\n\"ap-southeast-1\" : { \"32\" : \"ami-66f28c34\", \"64\" : \"ami-60f28c32\" },\n\"ap-northeast-1\" : { \"32\" : \"ami-9c03a89d\", \"64\" : \"ami-a003a8a1\" }\n}\n},\n\"Resources\" : {\n\"myEC2Instance\" : {\n\"Type\" : \"AWS::EC2::Instance\",\n\"Properties\" : {\n\"ImageId\" : { \"Fn::FindInMap\" : [ \"RegionMap\", { \"Ref\" : \"AWS::Region\" }, \"32\"]},\n\"InstanceType\" : \"m1.small\"\n}\n}\n}\n}\nFor more information on the Fn::FindInMap function, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html\nOut of the given answers, the most relevant and correct answer for the scenario is option E, which suggests using AWS CloudFormation's built-in Mappings and FindInMap functions to refer to the AMI ID set in the ImageID attribute of the Autoscaling::LaunchConfiguration resource.\nExplanation:\nThe given scenario describes a marketing research company that has developed a tracking system for collecting user behavior data during web marketing campaigns on behalf of customers worldwide. The tracking system is hosted on an Auto Scaling group of EC2 instances behind an ELB, and the collected data is stored in DynamoDB. After the campaign is terminated, the tracking system is torn down, and the data is moved to Amazon Redshift for generating detailed reports.\nIn this context, the use of AWS CloudFormation can help automate and manage the infrastructure as code. AWS CloudFormation enables creating and managing a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. AWS CloudFormation templates are text files written in JSON or YAML format that describe the infrastructure and resources required for an application.\nThe answer option E suggests using AWS CloudFormation's built-in Mappings and FindInMap functions to refer to the AMI ID set in the ImageID attribute of the Autoscaling::LaunchConfiguration resource. This is a good practice to make the CloudFormation template more modular and reusable, as it allows separating the AMI IDs from the main template code.\nMappings are key-value pairs that can be used to specify values based on a key. Mappings can be used to specify different values for different regions or environments. FindInMap is a function that enables finding a value based on a key and a subkey in a mapping.\nFor example, the CloudFormation template can define a mapping that associates an AMI ID with an AWS region as follows:\nyamlCopy codeMappings:   AMIMap:     us-east-1:       AMI: ami-12345678     us-west-2:       AMI: ami-87654321 \nThen, in the Autoscaling::LaunchConfiguration resource, the ImageId attribute can refer to the AMI ID by using the FindInMap function as follows:\nyamlCopy codeResources:   MyLaunchConfig:     Type: AWS::AutoScaling::LaunchConfiguration     Properties:       ImageId: !FindInMap [AMIMap, !Ref 'AWS::Region', AMI]       # other properties \nThis way, the AMI ID can be easily changed based on the region or environment without modifying the main template code.\nAnswer option A, suggesting avoiding deletion policies for EBS snapshots, is not relevant to the given scenario as it is not related to CloudFormation templates.\nAnswer option B, suggesting using different DynamoDB table names in every target region, is not necessary as DynamoDB tables can have the same name in different regions as long as they are in different AWS accounts.\nAnswer option C, suggesting using the built-in function of CloudFormation to set the AZ attribute of the ELB resource, is not relevant to the given scenario as the AZ attribute of the ELB resource is set automatically by AWS.\nAnswer option D, suggesting defining IAM users with the right to start CloudFormation stacks for every target region, is not necessary as IAM users can be defined in a centralized AWS account and used for managing CloudFormation stacks in different regions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Avoid using Deletion Policies for the EBS snapshots.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The names of the DynamoDB tables must be different in every target region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the built-in function of Cloudformation to set the AZ attribute of the ELB resource.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "IAM users with the right to start Cloudformation stacks must be defined for every target region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the built-in Mappings and FindInMap functions of AWS Cloudformation to refer to the AMI ID set in the ImageID attribute of the Autoscaling::LaunchConfiguration resource.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 307,
  "query" : "A user has created a VPC with the public and private subnets using the VPC wizard.\nThe VPC has CIDR 10.0.0.0/16\nThe public subnet uses CIDR 10.0.1.0/24\nThe user plans to host a web server in the public subnet with port 80 and a Database server in the private subnet with port 3306\nThe user is configuring a security group for the public subnet (WebSecGrp) and the private subnet (DBSecGrp)\nWhich of the below mentioned entries is required in the private subnet database security group DBSecGrp?",
  "answer" : "Answer - A.\nThe important point in this question is to allow the incoming traffic to the private subnet on port 3306 only for the instances in the private subnet.\nOption A is CORRECT because (a) it allows the inbound traffic only for the required port 3306, and (b) it allows only the traffic from the instances in the public subnet (WebSecGrp).\nOption B is incorrect because it allows the inbound traffic to all the instances in the VPC which is not the requirement.\nOption C is incorrect because defining outbound traffic will not ensure the incoming traffic from the public subnet.\nAlso, since the security groups are stateful, you need to define the inbound traffic for the public subnet only (WebSecGrp)\nThe outbound traffic would be automatically allowed.\nOption D is incorrect because you do not need to open the port 80 in this case.\nMore information on Web Server and DB Server Security Group settings:\nSince the Web server needs to talk to the database server on port 3306, the database server should allow incoming traffic on port 3306\nThe below table from the AWS documentation shows how the security groups should be set up.\nFor more information on security groups, please visit the below link.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\nTo allow communication between the web server in the public subnet and the database server in the private subnet, the user needs to create two security groups: one for the web server (WebSecGrp) and another for the database server (DBSecGrp). The security group rules should be configured to allow the necessary traffic to flow between the two servers.\nOption A: Allow Inbound on port 3306 in the DBSecGrp with source as WebSecGrp.\nThis option is correct. The user needs to allow inbound traffic on port 3306 (MySQL) in the DBSecGrp security group from the WebSecGrp security group. This will allow the web server to communicate with the database server. By specifying the source as WebSecGrp, the user ensures that only traffic from the web server is allowed, and traffic from other sources is blocked.\nOption B: Allow Inbound on port 3306 from source 10.0.0.0/16.\nThis option is not recommended because it allows traffic from any source within the VPC. While this may work, it is not a good security practice because it allows any instance within the VPC to connect to the database server.\nOption C: Allow Outbound on port 3306 in the DBSecGrp with destination as WebSecGrp.\nThis option is not required. Outbound traffic is allowed by default in a security group, so the user does not need to create a rule to allow outbound traffic from the database server to the web server.\nOption D: Allow Outbound on port 80 for destination NAT instance IP.\nThis option is not related to the communication between the web server and the database server. It is a rule that would be used to allow outbound traffic from the web server to a NAT instance, which is used for instances in a private subnet to communicate with the internet.\nIn summary, the correct answer is A. Allow Inbound on port 3306 in the DBSecGrp with source as WebSecGrp.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Allow Inbound on port 3306 in the DBSecGrp with source as WebSecGrp.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Allow Inbound on port 3306 from source 10.0.0.0/16.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Allow Outbound on port 3306 in the DBSecGrp with destination as WebSecGrp.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Allow Outbound on port 80 for destination NAT instance IP.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 308,
  "query" : "Your customer is implementing a video-on-demand streaming platform on AWS.\nThe requirement is to be able to support multiple devices such as iOS, Android, and Windows as client devices, using a standard client player, using streaming technology and scalable architecture with cost-effectiveness.",
  "answer" : "Answer - B.\nOption A is incorrect because it uses CloudFront distribution with the streaming option which does not work on all platforms; whereas, it should use the download option.\nOption B is CORRECT because (a) it uses CloudFront distribution (b) It uses Elemental MediaConvert for streaming the on-demand videos on any mobile and (c) it uses S3 as origin, so it keeps the costs low.\nOption C is incorrect because (a) provisioning streaming EC2 instances is a costly solution, (b) the videos are to be delivered on-demand, not live streaming.\nOption D is incorrect because the videos are to be delivered on-demand, not live streaming.\nSo, the streaming server is not required.\nFor more information on live and on-demand streaming using CloudFront, please visit the below URL:\nhttps://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/\nNote:\nIn the on-demand streaming case, your video content is stored in Amazon S3\nViewers can choose to watch it at any desired time.\nA complete on-demand streaming solution typically uses Amazon S3 for storage, AWS Elemental MediaConvert for file-based video processing, and Amazon CloudFront for delivery.\nOnce uploaded, you may need to convert your video into the size, resolution, or format needed by a particular television or connected device.\nAWS Elemental MediaConvert will take care of this for you.\nMediaConvert takes content from S3, transcodes it as requested, and stores the result back in S3\nTranscoding processes video files, creating compressed versions of the original content to reduce its size, change its format, or increase playback device compatibility.\nYou can also create assets that vary in resolution and bitrate for adaptive bitrate streaming, which adjusts the viewing quality depending on the viewer's available bandwidth.\nAWS Elemental MediaConvert outputs the transcoded video to an S3 bucket.\nThe next step is global delivery with Amazon CloudFront.\nCloudFront caches content at the edges for low latency and high throughput video delivery.This delivery can be made in two different ways.\nYou can deliver the entire video file to the device before playing it, or you can stream it to the device.\nMore information is available at:\nhttps://aws.amazon.com/cloudfront/streaming/ https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-video.html\nVideo streaming requires a scalable and cost-effective architecture to deliver content to multiple devices across different platforms. AWS provides a variety of services that can be used to meet the requirements of the client.\nOption A: Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents. Amazon S3 is a highly scalable, reliable, and durable object storage service. It can be used as an origin server to store the video content. Amazon CloudFront is a content delivery network (CDN) that can be used to deliver the video content to end-users. With the streaming option enabled, CloudFront can deliver video content to a variety of devices, including iOS, Android, and Windows. This option is cost-effective, scalable, and easy to manage.\nOption B: Store the video contents to Amazon S3 as the origin server. Configure the Amazon CloudFront distribution. Use the AWS Elemental MediaConvert to easily create video-on-demand (VOD) content for broadcast and multiscreen delivery (multi-devices) at scale. This option is similar to Option A, but it uses AWS Elemental MediaConvert to create video-on-demand content for broadcast and multiscreen delivery. MediaConvert is a file-based video transcoding service that can be used to convert video content into various formats and resolutions. This option provides additional flexibility to create VOD content for broadcast and multiscreen delivery, but it may be more expensive than Option A.\nOption C: Launch a streaming server on Amazon EC2 (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents. This option uses Amazon EC2 to launch a streaming server (e.g. Adobe Media Server) and store the video content as an origin server. Amazon CloudFront is used to deliver the video content to end-users with the download option enabled. This option may be more expensive than Option A or B since it requires the deployment and management of a streaming server.\nOption D: Launch a streaming server on Amazon EC2 (for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents. This option is similar to Option C, but it uses multiple streaming servers as edge servers to deliver the video content to end-users. This option provides additional scalability and reliability, but it may be more expensive than Option A or B.\nIn conclusion, the best option for implementing a video-on-demand streaming platform on AWS is Option A. It provides a cost-effective, scalable, and easy-to-manage solution using Amazon S3 as the origin server and Amazon CloudFront as the CDN with the streaming option enabled. Option B may be more flexible, but it may also be more expensive. Options C and D may provide additional scalability and reliability, but they may also be more expensive and complex to manage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the video contents to Amazon S3 as the origin server. Configure the Amazon CloudFront distribution. Use the AWS Elemental MediaConvert to easily create video-on-demand (VOD) content for broadcast and multiscreen delivery ( multi- devices) at scale.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Launch a streaming server on Amazon EC2 (for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 309,
  "query" : "A document storage company deploys its application to AWS and changes its business model to support both Free Tier and Premium Tier users.\nThe premium Tier users will be allowed to store up to 300GB of data and Free Tier customers will be allowed to store only 100GB.\nThe customer expects that billions of files will be stored.\nAll users need to be alerted when approaching 75 percent quota utilization and again at 90 percent quota use.",
  "answer" : "E.\nAnswer - A.\nOption A is CORRECT because DynamoDB which is a highly scalable service is the most suitable in this scenario.\nOption B is incorrect because RDS would not be a suitable solution for storing billions of files since this exceeds the manageability thresholds of RDS.\nOptions C and D are both incorrect because it uses object-level storage and iterating over billions of objects for each operation is performance-wise not a good option at all.\nThe best option for this scenario is D. The company should write both the content length and the username of the file's owner as S3 metadata for the object. They should then create a file watcher to iterate over each object and aggregate the size for each user and send a notification via Amazon Simple Queue Service to an emailing service if the storage threshold is exceeded.\nOption A is not the best choice because it introduces additional complexity with Simple Workflow Service (SWF) which is not necessary for this scenario. It also involves updating a counter in DynamoDB which may not be the best option as it will result in many write operations to the database which can become expensive at scale.\nOption B is a good approach for sending email notifications, but the activity worker should not be responsible for updating the used data counter in DynamoDB.\nOption C involves deploying a relational database which adds additional complexity and overhead to the solution. It also requires the upload server to query the stored objects table for every user which can become inefficient at scale.\nOption E involves creating separate S3 buckets for each user tier, which is not necessary for this scenario. It also requires the activity worker to query all objects for a given user based on the bucket that the data is stored in which can be inefficient at scale.\nOption D is the best approach because it writes both the content length and the username of the file's owner as S3 metadata for each object. This allows a file watcher to iterate over each object and aggregate the size for each user. When a user's storage threshold is exceeded, a notification is sent via Amazon Simple Queue Service to an email service. This approach avoids the overhead of updating a counter in DynamoDB or deploying a relational database. It also avoids the complexity of creating separate S3 buckets for each user tier.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use an S3 Event notification published to Lambda and the lambda task ( lambda functions ) would be used by the Simple Workflow Service. The SWF`s activity worker updates the user`s used data counter in DynamoD.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The activity worker would then use Simple Email Service to send an email if the counter increases above the appropriate thresholds.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The company should deploy an Amazon Relational Database Service (RDS) relational database with a stored objects table that has a row for each stored object along with the size of each object. The upload server will query the aggregate consumption of the user in question (by first determining the files stored by the user, and then querying the stored objects table for respective file sizes) and send an email via Amazon Simple Email Service if the thresholds are breached.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The company should write both the content length and the username of the file`s owner as S3 metadata for the object. They should then create a file watcher to iterate over each object and aggregate the size for each user and send a notification via Amazon Simple Queue Service to an emailing service if the storage threshold is exceeded.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The company should create two separate Amazon Simple Storage Service buckets, one for data storage for Free Tier Users, and another for data storage for Premium Tier users. An Amazon Simple Workflow Service activity worker will query all objects for a given user based on the bucket that the data is stored in and aggregate storage. The activity worker will notify the user via Amazon Simple Notification Service when necessary.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 310,
  "query" : "You are designing a VPC with different subnets and instances in them.\nYou want the instances in the subnets to communicate with each other and also ensure that traffic flows between different subnets seamlessly.\nHow can you accomplish these requirements?",
  "answer" : "Correct Answer: B.\nOption A is INCORRECT because the Security Group should be applied on an instance instead of an availability zone, and also \"Configure explicit deny rules\" is incorrect.\nIt is implicit deny and not explicit deny.\nOption B is CORRECT because NACLs should be configured as explicit allow at the subnet level.\nSecurity Groups should also be configured to allow the communication at the instance level.\nOption C is INCORRECT because for all Route tables, either main or custom: the default route is at target local, and this cannot be either deleted or modified.\nOption D is INCORRECT because Security Group can be applied on an instance and not on the Zone level.\nThe Source in a security group rule can be either a range of IP, any security group within a given VPC or single IP.\nSure, I'd be happy to provide a detailed explanation of each answer option.\nA. Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldn't be able to communicate with one another.\nThis answer option suggests creating a security group for each zone and configuring a default allow all rule. This would allow instances in each zone to communicate with each other without any restrictions. However, to prevent communication between zones that shouldn't be able to communicate with each other, explicit deny rules need to be configured for those zones. This approach may work, but it could become difficult to manage as the number of subnets and security groups increases.\nB. Use NACLs to explicitly allow communication between subnets and Security Groups to allow communication between different instances.\nThis answer option recommends using Network Access Control Lists (NACLs) to explicitly allow communication between subnets and Security Groups to allow communication between different instances. NACLs are stateless and operate at the subnet level. They can be used to allow or deny traffic based on the source and destination IP address, port number, and protocol. Security Groups are stateful and operate at the instance level. They control inbound and outbound traffic for instances and can be used to allow traffic from specific IP addresses or Security Groups. This approach provides granular control over the traffic flow in the VPC.\nC. Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesn't have routes to subnets with which it shouldn't be able to communicate.\nThis answer option recommends creating multiple subnets in the VPC, one for each zone, and configuring routing within the VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate. This approach is based on the principle of least privilege, which means that each subnet only has access to the resources it needs and nothing more. By configuring routing tables in this way, you can ensure that traffic flows only between subnets that need to communicate with each other.\nD. Configure a security group for every availability zone. Configure allow rules only between the availability zones that need to be able to communicate with one another. Use the implicit deny all rule to block any other traffic.\nThis answer option suggests creating a security group for every availability zone and configuring allow rules only between the availability zones that need to communicate with each other. This approach is similar to Option A but is more specific to availability zones instead of zones. Using the implicit deny all rule ensures that any traffic that doesn't match the allow rules is blocked. However, this approach could be difficult to manage as the number of availability zones and security groups increases.\nIn conclusion, Option B, which suggests using NACLs to explicitly allow communication between subnets and Security Groups to allow communication between different instances, is the best answer option as it provides granular control over the traffic flow in the VPC while still being manageable.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldn’t be able to communicate with one another.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use NACLs to explicitly allow communication between subnets and Security Groups to allow communication between different instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesn’t have routes to subnets with which it shouldn’t be able to communicate.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a security group for every availability zone. Configure allow rules only between the availability zones that need to be able to communicate with one another. Use the implicit deny all rule to block any other traffic.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 311,
  "query" : "You've been tasked with moving an e-commerce web application from a customer's data center into a VPC.\nThe application must be fault-tolerant and well as highly scalable.\nMoreover, the customer is adamant that service interruptions do not affect the user experience.\nAs you near launch, you discover that the application currently uses multicast to share session state between web servers.\nIn order to handle session state within the VPC, you choose the following option.",
  "answer" : "Answer - B.\nOption A is incorrect because ELB sticky sessions only cache user data locally for better performance.\nIf the EC2 instance fails, the session data will still be lost.\nOption B is CORRECT because Redis is a fast, open-source, in-memory data store and caching service.\nIt is highly available, reliable, and with high performance suitable for the most demanding applications such as this one.\nOption C is incorrect because Mesh VPN is not fault-tolerant or highly scalable - the client's real priorities.\nIts failure would impact users.\nThe supernode that handles the registration is a single point of failure.\nIn case of failure, new VPN nodes would not be able to register.\nAlso, the nodes wouldn't register across multiple AZs.\nEven if it is possible, it is very cumbersome.\nOption D is incorrect because storing the session state in RDS is not a good option.\nFor more information on Elastic Cache, please visit the below URL:\nhttps://aws.amazon.com/elasticache/\nNote:\nOur main requirement is to provide fault tolerance and high scalability.\nRedis data resides in-memory, in contrast to databases that store data on disk or SSDs.\nBy eliminating the need to access disks, in-memory data stores such as Redis avoid seeking time delays and access microseconds data.\nRedis is a popular choice for caching, session management, real-time analytics, geospatial, chat/messaging, media streaming, and gaming leaderboards.\nElastiCache Redis can provide high scalability and is fault-tolerant.\nWhen moving an e-commerce web application from a customer's data center to a VPC, it is important to ensure that the application is fault-tolerant, scalable, and that service interruptions do not affect the user experience. One of the key challenges in this scenario is handling session state between web servers. The current application uses multicast to share session state between web servers, which is not supported within the VPC.\nTo handle session state within the VPC, there are several options available, but the best one will depend on the specific requirements of the application. The four options listed in the question are:\nA. Enable session stickiness via Elastic Load Balancing: This option involves using Elastic Load Balancing (ELB) to distribute incoming traffic to the web servers in the VPC. With session stickiness, ELB can ensure that each user's requests are always routed to the same web server. This can help maintain session state and prevent data loss. However, this option may not be suitable for applications with very high traffic volumes or complex session state requirements.\nB. Store session state in Amazon ElastiCache for Redis: This option involves using Amazon ElastiCache for Redis to store session state data. Redis is a popular in-memory data store that can support complex data structures and can be highly scalable. By using ElastiCache, the application can store session data in a highly available and fault-tolerant manner, ensuring that service interruptions do not affect the user experience.\nC. Create a mesh VPN between instances and allow multicast on it: This option involves creating a mesh VPN between instances in the VPC and allowing multicast traffic over the VPN. This can enable the application to continue using multicast for session state sharing. However, this option may not be suitable for all applications and may be complex to set up and manage.\nD. Store session state in Amazon Relational Database Service: This option involves using Amazon Relational Database Service (RDS) to store session state data. RDS is a managed database service that can support complex data structures and can be highly scalable. By using RDS, the application can store session data in a highly available and fault-tolerant manner, ensuring that service interruptions do not affect the user experience.\nIn conclusion, option B (Store session state in Amazon ElastiCache for Redis) is likely the best option for handling session state within the VPC. This option provides a highly scalable and fault-tolerant solution that can support complex session state requirements. Option A (Enable session stickiness via Elastic Load Balancing) may be a good option for simpler applications with lower traffic volumes, while options C (Create a mesh VPN between instances and allow multicast on it) and D (Store session state in Amazon Relational Database Service) may be more suitable for specific use cases.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable session stickiness via Elastic Load Balancing.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store session state in Amazon ElastiCache for Redis.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a mesh VPN between instances and allow multicast on it.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store session state in Amazon Relational Database Service.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 312,
  "query" : "Your company is migrating infrastructure to AWS.\nA large number of developers and administrators will need to control this infrastructure using the AWS Management Console.\nThe Identity Management team is objecting to creating an entirely new directory of IAM users for all employees, and the employees are reluctant to commit yet another password to memory.",
  "answer" : "Answer - D.\nOption A is incorrect because, although it is a workable solution, the users need not use the OpenID IdP (such as Facebook, Google, SalesForce, etc.)in this scenario as they can use the on-premises 2.0 SAML compliant IdP and get the federated access to the AWS.\nAccess via OpenID IdP is most suitable for mobile apps.\nOption B is incorrect because you cannot log in to AWS using the IdP provided credentials.\nYou need temporary credentials provided by Security Token Service (STS) for that.\nOption C is INCORRECT as the user cannot log in to the AWS console using AWS CLI.\nOption D is CORRECT because it uses the on-premises 2.0 SAML compliant IdP and gets the federated access to the AWS, thus avoiding creating an IAM User for the employees in the organization.\nFor more information on SAML Authentication in AWS, please visit the below URL:\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html\nThe question is about finding a solution for providing secure and easy access to the AWS Management Console for a large number of developers and administrators, without creating a new directory of IAM users.\nOption A suggests using an OpenID Connect (OIDC) compatible identity provider (IdP) to authenticate users. OpenID Connect is a protocol built on top of OAuth 2.0 that allows clients to verify the identity of end-users based on the authentication performed by an authorization server, as well as to obtain basic profile information about the end-user. In this option, users would sign in to the identity provider, which would issue an authentication token that the user could then use to log in to the AWS Management Console. This approach would allow users to use their existing login credentials, as long as their identity provider is OIDC-compatible.\nOption B proposes that users log in directly to the AWS Management Console using the credentials from their on-premises Kerberos compliant Identity provider. Kerberos is a network authentication protocol that provides strong authentication for client/server applications by using secret-key cryptography. In this option, users would use their existing credentials from their on-premises Kerberos identity provider to log in to the AWS Management Console. This approach would allow users to use their existing login credentials, but their identity provider would need to be Kerberos-compliant.\nOption C suggests that users log in to the AWS Management Console using the AWS Command Line Interface (CLI). The AWS CLI is a command-line tool that allows users to interact with AWS services and resources from the command line. While this approach would not require creating a new directory of IAM users, it would not provide an easy-to-use graphical user interface, and users would need to be familiar with the AWS CLI commands to use it effectively.\nOption D proposes that users request a Security Assertion Markup Language (SAML) assertion from their on-premises SAML 2.0-compliant identity provider (IdP) and use that assertion to obtain federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint. SAML is an XML-based standard for exchanging authentication and authorization data between parties, in particular, between an identity provider and a service provider. In this option, users would sign in to their on-premises SAML 2.0-compliant identity provider, which would issue a SAML assertion that the user could then use to log in to the AWS Management Console via the AWS SSO endpoint. This approach would allow users to use their existing login credentials, as long as their identity provider is SAML 2.0-compliant, and would provide an easy-to-use graphical user interface.\nIn conclusion, the best option for providing secure and easy access to the AWS Management Console for a large number of developers and administrators without creating a new directory of IAM users would be Option D, where users request a SAML assertion from their on-premises SAML 2.0-compliant identity provider and use that assertion to obtain federated access to the AWS Management Console via the AWS SSO endpoint.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Users sign in using an OpenID Connect (OIDC) compatible IdP, receive an authentication token, then use that token to log in to the AWS Management Console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users log in directly to the AWS Management Console using the credentials from your on-premises Kerberos compliant Identity provider.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users log in to the AWS Management Console using the AWS Command Line Interface.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users request a SAML assertion from your on-premises SAML 2.0-compliant identity provider (IdP) and use that assertion to obtain federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 313,
  "query" : "A gaming company adopted AWS Cloud Formation to automate load-testing of their games.\nThey have created an AWS Cloud Formation template for each gaming environment, including one for the load-testing stack.\nThe load-testing stack creates an Amazon Relational Database Service (RDS) Postgres database and two web servers running on Amazon Elastic Compute Cloud (EC2) that send HTTP requests, measure response times, and write the results into the database.\nA test run usually takes between 15 and 30 minutes.\nOnce the tests are done, the AWS Cloud Formation stacks are torn down immediately.\nThe test results written to the Amazon RDS database must be preserved for visualization and analysis.\n Select possible solutions that allow access to the test results after the AWS Cloud Formation load -testing stack is deleted.\n(Select TWO)",
  "answer" : "E.\nAnswer - C and D.\nOption A is incorrect because (a) the creation of read replicas is not needed in this scenario, and (b) they would anyways be deleted after the stacks get deleted.\nSo there is no need to define any dependency in the template.\nOption B is incorrect because the UpdatePolicy attribute is only applicable to certain resources like AutoScalingGroup, AWS Lambda Alias.\nIt does not apply to RDS.\nOption C is CORRECT because the RDS resources would be preserved for the visualization and analysis after the stack gets deleted with the Retain deletion policy.\nOption D is CORRECT because, with the Snapshot deletion policy, a snapshot of the RDS instance would be created for visualization and analysis later after the stack is deleted.\nOption E is incorrect because automated snapshots are not needed in this case.\nAll that is needed is a single snapshot of the RDS instance after the test is finished, which can be taken via the Snapshot deletion policy.\nNOTE: This question is asking for two possible answers.\nIt does not say that both need to be used at the same time.\nHence both C and D are valid options.\nFor more information on deletion policy, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\nSure, I'll provide a detailed explanation for each of the possible solutions that allow access to the test results after the AWS Cloud Formation load-testing stack is deleted.\nA. Define an Amazon RDS Read-Replica in the load-testing AWS CloudFormation stack and define a dependency relation between master and replica via the DependsOn attribute. This solution involves creating a read-replica of the RDS Postgres database in the load-testing AWS CloudFormation stack. A read-replica is a copy of the database that can be used for read operations. By defining a dependency relation between the master and replica databases using the DependsOn attribute, the replica database will be created before the master database, and any read operations performed on the replica will be consistent with the master. This solution allows for access to the test results after the AWS CloudFormation stack is deleted because the read-replica database will still exist and can be used to query the test results.\nB. Define an update policy to prevent the deletion of the Amazon RDS database after the AWS CloudFormation stack is deleted. This solution involves defining an update policy in the AWS CloudFormation stack to prevent the deletion of the RDS Postgres database after the stack is deleted. This will ensure that the database is not deleted inadvertently and that the test results are preserved. However, it's important to note that this solution does not guarantee access to the test results after the AWS CloudFormation stack is deleted.\nC. Define a deletion policy of type Retain for the Amazon RDS resource to assure that the RDS database is not deleted with the AWS CloudFormation stack. This solution involves defining a deletion policy of type Retain for the RDS Postgres database in the AWS CloudFormation stack. This will ensure that the database is not deleted when the stack is deleted, and the test results are preserved. However, it's important to note that this solution does not guarantee access to the test results after the AWS CloudFormation stack is deleted.\nD. Define a deletion policy of type Snapshot for the Amazon RDS resource to assure that the RDS database can be restored after the AWS CloudFormation stack is deleted. This solution involves defining a deletion policy of type Snapshot for the RDS Postgres database in the AWS CloudFormation stack. This will create a snapshot of the database before it's deleted with the stack. The snapshot can then be used to restore the database, including the test results, after the stack is deleted. This solution guarantees access to the test results after the AWS CloudFormation stack is deleted.\nE. Define automated backups with a backup retention period of 30 days for the Amazon RDS database and perform point-in-time recovery of the database after the AWS CloudFormation stack is deleted. This solution involves defining automated backups for the RDS Postgres database in the AWS CloudFormation stack with a backup retention period of 30 days. This will create backups of the database at regular intervals, including the test results. After the AWS CloudFormation stack is deleted, point-in-time recovery can be performed to restore the database to a specific point in time, including the test results. This solution guarantees access to the test results after the AWS CloudFormation stack is deleted.\nIn conclusion, the possible solutions that allow access to the test results after the AWS Cloud Formation load-testing stack is deleted are A and D. Solution A involves creating a read-replica of the RDS Postgres database, while solution D involves defining a deletion policy of type Snapshot for the RDS Postgres database. Both solutions guarantee access to the test results after the AWS CloudFormation stack is deleted.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Define an Amazon RDS Read-Replica in the load-testing AWS CloudFormation stack and define a dependency relation between master and replica via the DependsOn attribute.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Define an update policy to prevent the deletion of the Amazon RDS database after the AWS CloudFormation stack is deleted.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Define a deletion policy of type Retain for the Amazon RDS resource to assure that the RDS database is not deleted with the AWS CloudFormation stack.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Define a deletion policy of type Snapshot for the Amazon RDS resource to assure that the RDS database can be restored after the AWS CloudFormation stack is deleted.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Define automated backups with a backup retention period of 30 days for the Amazon RDS database and perform point-in-time recovery of the database after the AWS CloudFormation stack is deleted.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 314,
  "query" : "A large enterprise wants to adopt CloudFormation to automate administrative tasks and implement the security principles of least privilege and separation of duties.\nThey have identified the following roles with the corresponding tasks in the company.",
  "answer" : "E.\nAnswer - A and B.\nOption A is CORRECT because subnets cannot be deleted with instances in them.\nOption B is CORRECT because to launch instances explicitly, we need IAM permissions.\nOption C is incorrect because the stacks are created using the application group's IAM policy when nesting network stacks within application stacks.\nAnd the policy should require network-level permissions.\nOption D is incorrect because the application stack can be deleted before the network stack.\nOption E is incorrect because network administrators need resource-level permission to delete the application stack.\nFor more information, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\nSure, I'd be happy to help!\nThe enterprise in question wants to use CloudFormation to automate administrative tasks and implement the security principles of least privilege and separation of duties. CloudFormation is a service provided by Amazon Web Services (AWS) that allows users to define and manage AWS resources as code.\nThe enterprise has identified various roles within the company, each with specific tasks related to CloudFormation:\nA. Network stack updates will fail upon attempts to delete a subnet with EC2 instances.\nThis means that if there are EC2 instances running in a subnet that is being deleted, the update to the network stack will fail. This is because the subnet cannot be deleted until the instances are terminated or moved to another subnet. To avoid this issue, the network administrators should ensure that the instances are either terminated or moved before attempting to delete the subnet.\nB. Restricting the launch of EC2 instances into VPCs requires resource level permissions in the IAM policy of the application group.\nThis means that the ability to launch EC2 instances in a particular VPC is controlled by the IAM policy of the application group. The policy should be configured to allow only the necessary permissions required by the application to launch instances in that VPC. This is an example of implementing the security principle of least privilege, which restricts access to only what is necessary to perform a task.\nC. Nesting network stacks within application stacks simplifies management and debugging, but requires resource-level permissions in the network group's IAM policy.\nNesting network stacks within application stacks can help simplify management and debugging, but it also requires resource-level permissions in the network group's IAM policy. This is because the application stack needs to be able to access and manage resources in the network stack. By granting resource-level permissions, the application stack can manage the resources it needs without giving it access to resources it doesn't need.\nD. The application stack cannot be deleted before all network stacks are deleted.\nThis means that the network stacks must be deleted before the application stack can be deleted. This is because the application stack may depend on resources in the network stacks, and deleting the network stacks first ensures that the dependencies are removed before the application stack is deleted.\nE. Unless account level permissions are used on the CloudFormation: Delete Stack action, network administrators could tear down application stacks.\nThis means that network administrators could potentially delete application stacks unless account-level permissions are used to restrict access to the Delete Stack action in CloudFormation. To implement the security principle of separation of duties, network administrators should not have the ability to delete application stacks unless it is absolutely necessary. By restricting access to the Delete Stack action, the risk of accidental or intentional deletion of application stacks is reduced.\nOverall, by adopting CloudFormation and implementing these security principles, the enterprise can automate administrative tasks while maintaining security and reducing the risk of human error.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Network stack updates will fail upon attempts to delete a subnet with EC2 instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Restricting the launch of EC2 instances into VPCs requires resource level permissions in the IAM policy of the application group.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Nesting network stacks within application stacks simplifies management and debugging, but requires resource-level permissions in the network group`s IAM policy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The application stack cannot be deleted before all network stacks are deleted.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Unless account level permissions are used on the cloud formation: Delete Stack action, network administrators could tear down application stacks.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 315,
  "query" : "An Enterprise customer is starting its migration to the cloud for agility.\nThey want to make their internal Microsoft Active Directory available to any applications running on AWS as a central point of user control for leavers and joiners.\nInternal users only have to remember one set of credentials.\nHow could they make their Active Directory secure, highly available, and with minimal on-premises infrastructure changes in the most cost and time-efficient way?",
  "answer" : "Answer - B.\nOption A incorrect because it is just a complicated environment to set up and does not meet the purpose of the requirement.\nOption B is CORRECT because using an IPSec tunnel can help decrypt all the traffic from the on-premise to AWS.\nThe domain controllers in separate AZ's can address high availability.\nOption C is incorrect because the question mentions that they want minimal changes to the on-premise environment.\nOption D is incorrect because it does not address the secure communication part from on-premise to AWS.\nFor more information on creating VPN tunnels using Hardware VPN and Virtual private gateways, please refer to the below link-\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\nSure, I'd be happy to provide a detailed explanation of each answer option.\nOption A suggests using Amazon EC2 to create a DMZ (Demilitarized Zone) using a security group. Within the security group, the customer could provision two smaller Amazon EC2 instances running Openswan for resilient IPSec tunnels, and two larger instances that are domain controllers. They would use multiple Availability Zones for high availability. This approach involves creating a VPN between the customer's on-premises network and AWS using the IPSec protocol. The two domain controllers in the DMZ would replicate with the customer's on-premises domain controllers. This option minimizes on-premises infrastructure changes and provides high availability, but it involves managing the VPN connection and maintaining the domain controller instances.\nOption B suggests using VPN to create an extension to the customer's data center and using resilient hardware IPSec tunnels. The customer could then have two domain controller instances that are joined to their existing domain and reside within different subnets in different Availability Zones. This option involves creating a VPN connection between the customer's on-premises network and AWS, but instead of using domain controllers in a DMZ, the domain controllers would reside within AWS subnets. This option provides high availability and minimal on-premises infrastructure changes, but it also involves managing the VPN connection and maintaining the domain controller instances.\nOption C suggests provisioning new hardware within the customer's existing infrastructure to run Active Directory Federation Services (ADFS). ADFS would present Active Directory as a SAML2 endpoint on the internet. Any new application on AWS could be written to authenticate using SAML2. This option involves creating a SAML2-based federation between the customer's on-premises Active Directory and AWS. This approach provides a central point of user control and minimizes on-premises infrastructure changes, but it requires managing and maintaining the ADFS infrastructure.\nOption D suggests creating a stand-alone VPC with its own Active Directory Domain Controllers. Two domain controller instances could be configured, one in each Availability Zone, and new applications would authenticate with those domain controllers. This option involves creating a new VPC and deploying Active Directory Domain Controllers within it. The VPC would be isolated from the customer's on-premises network, and applications would need to be specifically designed to use the AWS-hosted Active Directory. This approach provides high availability and minimal on-premises infrastructure changes, but it requires creating a new VPC and deploying and maintaining the domain controller instances.\nIn summary, each option provides a way to make the customer's internal Microsoft Active Directory available to any applications running on AWS as a central point of user control. Each option has its own trade-offs in terms of high availability, security, and infrastructure changes. The best option will depend on the customer's specific requirements and constraints.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Using Amazon Elastic Compute Cloud (EC2), they could create a DMZ using a security group. Within the security group, they could provision two smaller Amazon EC2 instances that are running Openswan for resilient IPSec tunnels, and two larger instances that are domain controllers. They would use multiple Availability Zones.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Using VPN, they could create an extension to their data center and use resilient hardware IPSec tunnels; they could then have two domain controller instances that are joined to their existing domain and reside within different subnets in different Availability Zones.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Within the customer’s existing infrastructure, they could provision new hardware to run Active Directory Federation Services. This would present Active Directory as a SAML2 endpoint on the internet. Any new application on AWS could be written to authenticate using SAML2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The customer could create a stand-alone VPC with its own Active Directory Domain Controllers. Two domain controller instances could be configured, one in each Availability Zone, and new applications would authenticate with those domain controllers.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 316,
  "query" : "An AWS customer is deploying a web application composed of a front end running on Amazon EC2 and confidential data stored on Amazon S3.",
  "answer" : "Answer - A.\nOption A is CORRECT because the access to the sensitive data on Amazon S3 is only given to the authenticated users.\nOption B is incorrect because S3 doesn't integrate directly with CloudHSM.\nAlso, there is no centralized access management system control.\nOption C is incorrect because this is an incorrect workflow of the use of SAML.\nIt does not mention if the centralized access management system is a SAML complaint.\nOption D is incorrect because, with this configuration, the web team would have access to the sensitive data on S3.\nFor more information on STS, please refer to the URL-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\nThe scenario described in this question involves a web application consisting of a front end on Amazon EC2 and confidential data stored on Amazon S3. The task is to secure the system architecture while allowing only authorized users to access the data stored on Amazon S3.\nAnswer A suggests configuring the web application to authenticate end-users against the centralized access management system. Trusted users are then given temporary STS tokens that allow them to download approved data directly from Amazon S3. This solution uses the AWS Security Token Service (STS) to grant temporary, limited-privilege credentials to trusted users. This ensures that only authorized users have access to the confidential data on Amazon S3.\nAnswer B suggests encrypting the data stored on Amazon S3 using a CloudHSM operated by a separate security team. The web application is configured to integrate with the CloudHSM for decrypting approved data access operations for trusted end-users. CloudHSM provides secure and centralized key management for encrypting and decrypting data. In this solution, the security team manages the keys used for encryption and decryption, and the web application team can only access the data with authorized decryption operations.\nAnswer C suggests configuring the web application to authenticate end-users against the centralized access management system using SAML. End-users authenticate to IAM using their SAML token and can download approved data directly from Amazon S3. This solution uses Security Assertion Markup Language (SAML) to enable users to authenticate with AWS Identity and Access Management (IAM). SAML provides a secure and standardized way of exchanging authentication and authorization data between parties.\nAnswer D suggests having a separate security team create an IAM Role entitled to access the data on Amazon S3. The web application team provisions their instances with this Role, while their IAM users are denied access to the data on Amazon S3. This solution uses IAM Roles to grant permissions to access the data on Amazon S3. By denying access to the IAM users, this solution ensures that only authorized instances can access the data on Amazon S3.\nIn summary, all the answers provide solutions for securing the architecture while allowing only authorized users to access the confidential data stored on Amazon S3. The best solution depends on the specific requirements and constraints of the scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure the web application to authenticate end-users against the centralized access management system. Have the web application provision trusted users STS tokens entitling the download of approved data directly from Amazon S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Encrypt the data on Amazon S3 using a CloudHSM that is operated by a separate security team. Configure the web application to integrate with the CloudHSM for decrypting approved data access operations for trusted end users.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the web application to authenticate end-users against the centralized access management system using SAML. Have the end-users authenticate to IAM using their SAML token and download the approved data directly from Amazon S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Have the separate security team create an IAM Role entitled to access the data on Amazon S3. Have the web application team provision their instances with this Role while denying their IAM users access to the data on Amazon S3.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 317,
  "query" : "A customer runs an application in the US-West region in one of the availability zones and wants to set up the failover to another availability zone.\nThe customer is also interested in implementing RDS Multi-AZ for high availability.\nHowever, you are worried that the synchronous replication between the primary DB and backup DB may impact the performance and the write and commit latency may increase.\nWhich of the following actions can help you to address this concern?",
  "answer" : "Answer - B.\nOption A is incorrect because RDS read-replicas will not improve the performance for the write and commit latency.\nRead-replicas can help to share the read traffic.\nOption B is CORRECT because this method can improve the RDS instances' performance with the Multi-AZ deployments.\nOption C is incorrect because VPC peering connection can establish the communication between two different VPCs.\nBut it does not improve the performance of the Multi-AZ DB instances.\nOption D is incorrect.\nPlease check the explanations of option.\nB.The reference can be found in https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html.\nThe correct answer is B. Use Provisioned IOPS and DB instance classes that are optimized for Provisioned IOPS.\nRDS Multi-AZ is a feature that automatically provisions and maintains a synchronous standby replica of the primary database in a different availability zone. This is done to ensure high availability and minimize downtime in case of a failure in the primary availability zone. However, synchronous replication can impact the performance of the database, especially during write operations, as the write and commit latency may increase.\nTo address this concern, you can use Provisioned IOPS and DB instance classes that are optimized for Provisioned IOPS. Provisioned IOPS allow you to provision a specific level of I/O operations per second (IOPS) for your database, ensuring consistent performance. By using Provisioned IOPS, you can ensure that the database can handle the increased write and commit latency without compromising performance.\nOption A, configuring several RDS read-replicas in each availability zone, does not address the concern about the impact of synchronous replication on performance. Read-replicas are used for scaling read-heavy workloads and do not provide the same level of high availability as Multi-AZ.\nOption C, configuring a VPC peering connection between two availability zones for the RDS instances, does not address the concern about the impact of synchronous replication on performance. VPC peering is a way to connect two VPCs so that they can communicate with each other as if they were in the same network. It does not affect the performance of the database.\nOption D, stating that no actions are required as RDS Multi-AZ synchronous replication does not impact the performance, is incorrect. As mentioned earlier, synchronous replication can impact the performance of the database, especially during write operations.\nTherefore, the correct answer is B. Use Provisioned IOPS and DB instance classes that are optimized for Provisioned IOPS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure several RDS read-replicas in each availability zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Provisioned IOPS and DB instance classes that are optimized for Provisioned IOPS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure a VPC peering connection between two availability zones for the RDS instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No actions are required as RDS Multi-AZ synchronous replication does not impact the performance.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 318,
  "query" : "A public archives organization is about to move a pilot application.\nThey are running on AWS into production.\nYou have been hired to analyze their application architecture and give high-availability and cost-saving recommendations.\nThe application displays scanned historical documents.",
  "answer" : "E.\nAnswer - A, B, and C.\nOption A is CORRECT because CloudFront does the caching via the edge locations, reducing the load on the origin.\nOption B is CORRECT because increasing the size of the images would help reduce the cost of the number of GET/PUT requests on the origin server.\nOption C is CORRECT, INTELLIGENT_TIERING storage class is designed to optimize storage costs by automatically moving data to the most cost-effective storage access tier, without performance impact or operational overhead.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf\nOption D is incorrect because decreasing the size would require more requests and will increase the overall cost.\nOption E is incorrect because Glacier is an archival solution and will not be suitable for frequent access to the tiles.\nThe application displays scanned historical documents and needs to be highly available and cost-effective in production on AWS. The following recommendations can be considered:\nA. Deploy an Amazon CloudFront distribution in front of the Amazon S3 tiles bucket: CloudFront is a content delivery network (CDN) that caches content at edge locations close to the user to improve application performance. Deploying CloudFront in front of an S3 tiles bucket can improve application performance by caching content and reducing the load on the S3 bucket. This can also reduce costs as the user will be accessing content from the nearest edge location rather than directly from the S3 bucket.\nB. Increase the size (width/height) of the individual tiles at the maximum zoom level: Increasing the size of the individual tiles at the maximum zoom level can reduce the number of requests made to the S3 bucket and improve application performance. However, it can also increase the storage requirements and costs as larger tiles will require more storage space.\nC. Use the S3 INTELLIGENT_TIERING storage class: The S3 INTELLIGENT_TIERING storage class automatically moves objects between two access tiers based on changing access patterns. This can help reduce costs as objects that are infrequently accessed are automatically moved to a lower cost tier. This can be beneficial in scenarios where there are periods of high activity followed by long periods of inactivity.\nD. Decrease the size (width/height) of the individual tiles at the maximum zoom level: Decreasing the size of the individual tiles at the maximum zoom level can increase the number of requests made to the S3 bucket and improve application performance. However, it can also reduce storage requirements and costs as smaller tiles will require less storage space.\nE. Store the maximum zoom level in the low-cost Amazon S3 Glacier option and only retrieve the most frequently accessed tiles as they are requested by users: Storing the maximum zoom level in the low-cost Amazon S3 Glacier option can reduce costs but can also increase the retrieval time. The most frequently accessed tiles can be stored in S3 standard or S3 INTELLIGENT_TIERING storage classes for faster retrieval. This can be beneficial if there is a large amount of infrequently accessed data that needs to be stored for compliance or regulatory purposes.\nIn summary, recommendations A, C, and E can help reduce costs while improving application performance and availability. Recommendations B and D can improve application performance but may increase storage requirements and costs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy an Amazon CloudFront distribution in front of the Amazon S3 tiles bucket.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Increase the size (width/height) of the individual tiles at the maximum zoom level.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the S3 INTELLIGENT_TIERING storage class.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Decrease the size (width/height) of the individual tiles at the maximum zoom level.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the maximum zoom level in the low-cost Amazon S3 Glacier option and only retrieve the most frequently access tiles as they are requested by users.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 319,
  "query" : "A user has created a VPC with CIDR 20.0.0.0/16 using the wizard.\nThe user has created a public subnet CIDR (20.0.0.0/24) and VPN only subnets CIDR (20.0.1.0/24) along with the VPN gateway (vgw-12345) to connect to the user's data center.\nThe user's data center has CIDR 172.28.0.0/12\nThe user also has set up a NAT instance (i-12345) to allow traffic to the internet from the VPN subnet.\nWhich of the below-mentioned routes should NOT be configured in the main route table in this scenario?",
  "answer" : "Answer - A.\nOption A is CORRECT because the private subnet destination with NAT instance as the target is not needed in the route table.\nOption B is incorrect because you would need this entry to communicate with the internet via NAT instance (e.g.\nfor patch updates).\nOption C is incorrect because you need this entry for communicating with the customer network via the virtual private gateway.\nOption D is incorrect because this entry is present by default to allow the resources in the VPC to communicate with each other.\nThe below diagram shows how a typical setup for a VPC with a VPN and Internet gateway would look like.\nThe only routing option which should have access to the internet gateway should be the 0.0.0.0/0 address.\nSo.\nOption A is the right answer.\nFor more information on VPC with the option of VPN, please visit the links.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario3.html https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nThe main route table is automatically created by AWS when a VPC is created. It is associated with all subnets that are created in the VPC. When an instance in a subnet wants to communicate with another instance or resource outside of the VPC, the main route table is consulted to determine how to route the traffic.\nIn this scenario, the user has created a VPC with CIDR 20.0.0.0/16, a public subnet with CIDR 20.0.0.0/24, and VPN-only subnets with CIDR 20.0.1.0/24. The user has also set up a VPN gateway (vgw-12345) to connect to the user's data center, which has CIDR 172.28.0.0/12. Additionally, a NAT instance (i-12345) has been set up to allow traffic to the internet from the VPN subnet.\nNow let's go through each answer option and see which one should not be configured in the main route table.\nA. Destination: 20.0.1.0/24 and Target: i-12345 This route should be configured in the main route table because it specifies that traffic destined for the VPN-only subnets should be routed to the NAT instance (i-12345) for internet access.\nB. Destination: 0.0.0.0/0 and Target: i-12345 This route should also be configured in the main route table because it specifies that all traffic not destined for the VPC should be routed to the NAT instance (i-12345) for internet access.\nC. Destination: 172.28.0.0/12 and Target: vgw-12345 This route should be configured in the main route table because it specifies that traffic destined for the user's data center (CIDR 172.28.0.0/12) should be routed through the VPN gateway (vgw-12345).\nD. Destination: 20.0.0.0/16 and Target: local This route should not be configured in the main route table because it specifies that traffic destined for the entire VPC (CIDR 20.0.0.0/16) should be routed locally. However, this is the default behavior of the main route table and is automatically configured by AWS. Therefore, there is no need to configure this route explicitly in the main route table.\nIn conclusion, the route that should NOT be configured in the main route table in this scenario is D, i.e., Destination: 20.0.0.0/16 and Target: local.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Destination: 20.0.1.0/24 and Target: i-12345",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Destination: 0.0.0.0/0 and Target: i-12345",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Destination: 172.28.0.0/12 and Target: vgw-12345",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Destination: 20.0.0.0/16 and Target: local.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 320,
  "query" : "A user has created a mobile application that makes calls to DynamoDB to fetch certain data.\nThe application uses the DynamoDB SDK and root account access/secret access key to connect to DynamoDB from mobile.\nWhich of the below-mentioned user's authentication is true concerning the best practice for security in this scenario?",
  "answer" : "Answer - C.\nOption A is incorrect because creating a separate user for each application user is not a feasible, secure, and recommended solution.\nOption B is incorrect because the mobile users may not be AWS users.\nYou need to give access to the mobile application via a federated identity provider.\nOption C is CORRECT because it creates a role for Federated Users, enabling the users to sign in to the app using their Amazon, Facebook, or Google identity and authorize them to access DynamoDB seamlessly.\nOption D is incorrect because creating an IAM Role is not sufficient.\nYou need to authenticate the application users via a web identity provider, get the temporary credentials via a Security Token Service (STS) and then access DynamoDB.More information on Web Identity Federation:\nWith Web Identity Federation, you don't need to create custom sign-in code or manage your own user identities.\nInstead, users of your app can sign in using a well-known identity provider (IdP) -such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account.\nFor more information on Web Identity Federation, please visit the link.\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\nThe scenario described in the question involves a mobile application that accesses DynamoDB using root account access/secret access key. This approach is not recommended as it poses a security risk. Best practices for security suggest that root account access/secret access key should not be used in mobile applications or any other client-side applications.\nTo improve security, we need to consider the following options:\nOption A: The user should create a separate IAM user for each mobile application and provide DynamoDB access with it. This option is a better approach than using root account access/secret access key. By creating an IAM user for each mobile application, we can limit the permissions of each user to only the resources they need to access. This approach helps to reduce the attack surface and prevent unauthorized access.\nOption B: The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2. This option involves creating an IAM role that grants access to both DynamoDB and EC2. The mobile application would then route all calls through the EC2 instance, which would assume the IAM role and make requests to DynamoDB. While this approach may be useful in some scenarios, it is not necessary for accessing DynamoDB from a mobile application.\nOption C: The application should use an IAM role with web identity federation, which validates calls to DynamoDB with identity providers, such as Google, Amazon, and Facebook. This option involves using web identity federation to authenticate users of the mobile application with identity providers such as Google, Amazon, and Facebook. The mobile application can then assume an IAM role that grants access to DynamoDB. This approach provides a secure way to authenticate users of the mobile application and access DynamoDB.\nOption D: Create an IAM Role with DynamoDB access and attach it with the mobile application. This option involves creating an IAM role that grants access to DynamoDB and attaching it to the mobile application. This approach is better than using root account access/secret access key, but it still does not provide a secure way to authenticate users of the mobile application.\nIn summary, the best option for securing access to DynamoDB from a mobile application is to create a separate IAM user for each mobile application and provide DynamoDB access with it. Option C is also a valid option, but it may not be necessary in all scenarios. Option B is not required for accessing DynamoDB from a mobile application, and Option D is better than using root account access/secret access key, but it does not provide a secure way to authenticate users of the mobile application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The user should create a separate IAM user for each mobile application and provide DynamoDB access with it.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The application should use an IAM role with web identity federation, which validates calls to DynamoDB with identity providers, such as Google, Amazon, and Facebook.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an IAM Role with DynamoDB access and attach it with the mobile application.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 321,
  "query" : "You work as a Solution Architect for a firm, and your client has a multi-AZ infrastructure running on a VPC on AWS cloud.\nThey are planning to implement a centralized custom dashboard on the client's data center.\nThe dashboard will need to interact with the multi-AZ infrastructure.\nData from the multi-AZ will be pulled from the data center.\nThe solution should ensure low latency and good performance.\nWhich of the following provides the best solution?",
  "answer" : "Answer - A.\nExplanation.\nDirect connect connection will satisfy both the requirements of the scenario since it provides good bandwidth and low latency.\nOption B is incorrect - Since VPN uses the internet, it does not ensure high bandwidth.\nOption C is incorrect - It is not true.\nOption D is incorrect - You don't need a Direct.\nConnect connection to each AZ.\nhttps://aws.amazon.com/directconnect/faqs/\nThe scenario described in the question involves a multi-AZ infrastructure running on a VPC on AWS cloud. The client wants to implement a custom dashboard on their data center that will interact with the infrastructure, and data will be pulled from the infrastructure to the data center. The solution should ensure low latency and good performance.\nOption A suggests using a direct connect connection to the VPC. Direct Connect is a service that allows customers to establish dedicated network connections between their data centers and AWS. This option may provide access to all AZs, but it does not provide a way to direct traffic to a specific AZ. It also may not be the most cost-effective solution as it requires a dedicated network connection.\nOption B suggests using VPN connections to two VGW routers in the region. This option is also feasible but may not be the best solution as it requires VPN configuration and management for each VGW, and may not be scalable for more than two VGWs. It also does not guarantee low latency as it depends on the quality of the VPN connection.\nOption C suggests that it is not possible to connect to multiple AZs from a single location, which is not true. It is possible to connect to multiple AZs using various connectivity options, including VPN and Direct Connect.\nOption D suggests using one direct connect connection from the data center to each AZ in the region. This option may provide the best solution as it allows for a dedicated network connection to each AZ, ensuring low latency and good performance. However, it may not be the most cost-effective solution, as it requires a dedicated network connection to each AZ.\nIn summary, option D may provide the best solution for the scenario described in the question, but the most appropriate solution would depend on factors such as budget, scalability requirements, and other business considerations.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use direct connect connection to the VPC as this will provide access to all AZs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use VPN connections to 2 VGW routers in the region, as this should give you access to the infrastructure in all AZs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You cannot connect to multiple AZ’s from a single location.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use one direct connect connection from the data center to each AZ in the region.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 322,
  "query" : "A user has launched an EC2 instance store-backed instance in the us-east-1a zone.\nThe user created AMI #1 and copied it to the eu-west-1 region.\nAfter that, the user made a few updates to the application running in the us-east-1a zone.\nThe user makes an AMI #2 after the changes.\nIf the user launches a new instance in Europe from the AMI #1 copy, which statement is true?",
  "answer" : "Answer - D.\nOption A is incorrect because (a) the changes made to the instance will not automatically get updated in the AMI in US-East-1, and (b) the already copied AMI will not have any reference to the AMI in the US-East-1 region.\nOption B is incorrect because AWS does not automatically update the AMIs.\nIt needs to be done manually.\nOption C is incorrect because you can copy the instance store AMI between different regions.\nOption D is CORRECT because the instance in the EU region will not make any changes after copying the AMI.\nYou will need to copy the AMI#2 to eu-west-1 and then relaunch the instance to have all the changes.\nFor the entire details to copy AMI's, please visit the link -\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\nWhen an Amazon Machine Image (AMI) is created from an EC2 instance, it captures the state of the instance at the time the AMI was created, including the operating system, application server, and application code. An AMI can be used to launch new EC2 instances with the same configuration as the original instance.\nIn this scenario, the user has launched an EC2 instance in the us-east-1a zone, created AMI #1, and copied it to the eu-west-1 region. After that, the user made some updates to the application running on the EC2 instance in the us-east-1a zone and created AMI #2 after the changes.\nIf the user launches a new instance in the eu-west-1 region from the copied AMI #1, the new instance will have the state of the original instance at the time the AMI was created, which means it will not have the changes made after AMI #1 was created.\nOption A is incorrect because when an AMI is copied to another region, a new copy of the AMI is created in the destination region, and the new copy has no connection with the original AMI. So, any changes made to the original instance after the AMI was copied will not be reflected in the copied AMI.\nOption B is incorrect because AWS does not automatically update the AMI when changes are made to the original instance. The user must create a new AMI after making changes to the instance.\nOption C is incorrect because instance store-backed AMIs can be copied from one region to another. However, instance store-backed AMIs can only be launched in the same availability zone where they were created.\nTherefore, the correct answer is Option D - the new instance in the eu-west-1 region will not have the changes made after the AMI copy.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The new instance will have the changes made after the AMI copy as AWS copies the original AMI reference during the copying. Thus, the copied AMI will have all the updated data.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The new instance will have the changes made after the AMI copy since AWS keeps updating the AMI.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It is not possible to copy the instance store-backed AMI from one region to another.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The new instance in the eu-west-1 region will not have the changes made after the AMI copy.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 323,
  "query" : "Company B has created an e-commerce site using DynamoDB and is designing a table named Products that includes items purchased by users.\nThere are thousands of users and dozens of products.\nThe table only has the partition key and does not have the sort key.\nWhen creating the table, which of the following is the most suitable to be selected as the partition key?",
  "answer" : "Answer - A.\nWhen defining primary keys, you should always use the \"many to few\" principle.\nThere are thousands of users but only a few products and categories.\nHence, option A is the best answer.\nFor a table item, you can fill in all the products purchased for a user.\nFor more information on DynamoDB, please visit the link.\nhttps://aws.amazon.com/dynamodb/faqs/\nWhen designing a DynamoDB table, choosing the right partition key is crucial for optimal performance and scalability.\nIn this scenario, the table is intended to store items purchased by users, and there are thousands of users and dozens of products. Since the table doesn't have a sort key, it means that the items will be unordered.\nOption A: user_id The user_id may seem like a good choice, but it would likely result in a hotspotting issue. This means that a small set of partitions will receive a disproportionate amount of read/write traffic, which can lead to performance issues and increased costs. In this case, a small number of popular users could end up overwhelming the capacity of a single partition.\nOption B: product_id Similarly, using product_id as the partition key can lead to hotspotting if certain products are much more popular than others. Additionally, if there are multiple sellers for a given product, their sales data would be spread across multiple partitions, making it harder to aggregate and analyze.\nOption C: category_id Using category_id as the partition key is a better option than user_id or product_id, as it can help distribute the data more evenly across partitions. However, it may still lead to hotspotting if certain categories are more popular than others.\nOption D: None of the above Based on the information given, it's difficult to determine the optimal partition key. One option would be to use a combination of user_id and product_id as a composite partition key, which would help distribute the data more evenly. Another option would be to use a time-based partition key, such as a purchase date, which would allow for efficient querying of recent purchases.\nIn summary, the most suitable option for a partition key would depend on the specific requirements and access patterns of the application. However, based on the information given, using category_id as the partition key would be a better choice than user_id or product_id, as it can help distribute the data more evenly across partitions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "user_id",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "product_id",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "category_id",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "None of the above.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 324,
  "query" : "You are writing an AWS CloudFormation template, and you want to assign values to properties that will not be available until runtime.\nYou know that you can use intrinsic functions to do this but are unsure which part of the template they can use.\nWhich of the following is correct in describing how you can currently use intrinsic functions in an AWS CloudFormation template?",
  "answer" : "Answer - B.\nAs per AWS documentation:\nYou can use intrinsic functions only in specific parts of a template.\nCurrently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes.\nYou can also use intrinsic functions to create stack resources conditionally.\nHence, B is the correct answer.\nFor more information on intrinsic function, please refer to the below link.\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\nIntrinsic functions can be used in an AWS CloudFormation template to assign dynamic values to properties that are not available until runtime. These functions are evaluated by CloudFormation during stack creation or update, and they enable the definition of more dynamic templates that can adapt to changing requirements.\nRegarding the parts of the template where intrinsic functions can be used, the correct answer is B: you can use intrinsic functions only in specific parts of a template. Specifically, intrinsic functions can be used in:\nResource properties: to assign dynamic values to a resource's properties, such as the EC2 instance type, the name of an S3 bucket, or the IP address of an Elastic Load Balancer.\nOutputs: to return values from the stack, such as the URL of a website hosted on an EC2 instance, or the ARN of an SNS topic.\nMetadata attributes: to provide additional information about a resource, such as tags or user data.\nUpdate policy attributes: to specify how CloudFormation handles updates to a resource, such as whether to replace or retain the resource during an update.\nIntrinsic functions cannot be used in other parts of the template, such as the AWSTemplateFormatVersion or Description sections, as these are static metadata that do not depend on runtime values.\nSome examples of intrinsic functions are:\nFn::Ref: returns the value of the specified parameter or resource.\nFn::Join: concatenates a list of strings with a specified delimiter.\nFn::ImportValue: returns the value of an output exported by another stack in the same region.\nFn::Sub: substitutes variables in a string with their values.\nOverall, the use of intrinsic functions in AWS CloudFormation templates provides a powerful way to create more flexible and dynamic infrastructure as code, enabling teams to manage their resources with more agility and efficiency.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You can use intrinsic functions in any part of a template.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use intrinsic functions only in specific parts of a template. You can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can use intrinsic functions only in the resource properties part of a template.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use intrinsic functions in any part of a template, except AWSTemplateFormatVersion and Description.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 325,
  "query" : "Your company's office was just reallocated to another site.\nA Site-to-Site VPN was set up to connect the local server in the new site and the company's AWS VPC in the AWS region ap-south-1\nThe VPN is working properly.\nHowever, the operation team lead is worried about the robustness of the connection and has consulted you if it is possible to provide more redundancy to the VPN.\nWhich suggestion should you give to him?",
  "answer" : "Correct Answer - D.\nCheck out https://docs.aws.amazon.com/vpn/latest/s2svpn/VPNConnections.html on how to provide redundant Site-to-Site VPN connections to provide failover.\nOption A is incorrect because the customer gateway is a single point of failure in this case.\nIf it fails, the whole VPN connection is influenced.\nOption B is incorrect because there are already two lines between the customer gateway and the virtual private gateway.\nThere is no need to add another tunnel.\nOption C is incorrect because the customer gateway is a single point of failure instead of a virtual private gateway.\nOption D is correct because adding another Site-to-Site VPN connection is a good redundancy plan.\nEven if one customer gateway does not work, there is still another path that works.\nThe operation team lead is concerned about the robustness of the VPN connection between the local server and the AWS VPC, and is looking for a way to add more redundancy to the connection. Here are the options to consider:\nA. No redundancy is required as the VPN connection is robust enough to provide auto failover ability. No single point failure exists for the existing solution.\nThis option suggests that the existing VPN connection is already robust enough and no redundancy is needed. It states that the VPN connection can provide auto failover ability, which means that if the primary VPN connection fails, the secondary connection will take over automatically. However, this option does not provide any further details on how the VPN connection is set up to provide this failover ability. It is essential to confirm if the current VPN connection is indeed designed to provide automatic failover.\nB. Add 1 more tunnel between the customer gateway and virtual private gateway. So if the existing tunnel fails, the traffic can failover to the new one.\nThis option suggests adding one more tunnel between the customer gateway and virtual private gateway to provide redundancy. If the existing tunnel fails, traffic can failover to the new tunnel, ensuring that the connection remains operational. This option is a reasonable solution as it provides redundancy and is a relatively simple and cost-effective solution to implement.\nC. Add another virtual private gateway as it is a single point without redundancy.\nThis option suggests adding another virtual private gateway to the AWS VPC to provide redundancy. This solution would eliminate the single point of failure of the existing virtual private gateway. However, this option is a more complex and expensive solution than adding another tunnel, as it requires additional configuration and resources.\nD. Set up a second Site-to-Site VPN connection to the virtual private gateway by using a second customer gateway.\nThis option suggests setting up a second Site-to-Site VPN connection to the virtual private gateway, using a second customer gateway. This option would provide redundancy by having two separate VPN connections, but it is also a more complex and expensive solution than adding another tunnel. This option requires additional configuration and resources, and the customer would have to maintain two separate VPN connections.\nIn summary, option B is a reasonable solution, providing redundancy by adding one more tunnel between the customer gateway and virtual private gateway. This option is a simple and cost-effective solution to implement, and it addresses the concern of the operation team lead about the robustness of the VPN connection.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "No redundancy is required as the VPN connection is robust enough to provide auto failover ability. No single point failure exists for the existing solution.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add 1 more tunnel between the customer gateway and virtual private gateway. So if the existing tunnel fails, the traffic can failover to the new one.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add another virtual private gateway as it is a single point without redundancy.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up a second Site-to-Site VPN connection to the virtual private gateway by using a second customer gateway.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 326,
  "query" : "A company has used Amazon Elastic MapReduce (Amazon EMR) clusters to capture data about user actions and push it to Amazon Simple Storage Service (S3)\nThe database grows up to 50GB per day.\nThen it uses Apache Hive for querying user-activity data.\nHowever, the DevOps lead is unsatisfied with its performance, cost, and management complexity.\nYou have proposed to use AWS Athena to query the data instead.\nWhich benefits does this new solution bring? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, D.\nAmazon Athena is a query service that uses standard SQL to analyze data in Amazon S3.\nCheck https://aws.amazon.com/athena/ for its various features.\nOption A is incorrect because, with Amazon Athena, you only pay for the queries.\nAnd you are charged based on the amount of data scanned by each query.\nIt does not charge every hour.\nOption B is CORRECT because Amazon Athena is totally serverless, meaning that users only need to consider how to query the data without managing the infrastructure.\nOption C is incorrect because various standard data formats are supported, including CSV, JSON, ORC, Avro, and Parquet.\nOption D is CORRECT because Amazon Athena can automatically allocate resources for queries.\nAs a result, performance has been improved if compared with traditional solutions.\nOption E is incorrect because the source data should be located in S3 only.\nAWS RDS, EFS and Glacier are not valid.\nCheck out the Amazon Athena workflow as below.\nThe company has been using Amazon EMR clusters to collect data from user actions, which is stored in Amazon S3. The database grows by 50GB per day, and Apache Hive is used for querying user-activity data. However, the DevOps lead is not satisfied with the performance, cost, and management complexity of the current solution.\nYou have proposed using AWS Athena to query the data instead, which offers the following benefits:\nA. Cost-effective: AWS Athena charges per query and every hour, making it a cost-effective option. Unlike Amazon EMR, which requires upfront infrastructure costs, AWS Athena offers a pay-as-you-go model, where you only pay for the queries you run.\nB. Serverless: AWS Athena is a serverless service that does not require any infrastructure setup or compute capability considerations. You can focus on querying data and analyzing results without worrying about underlying infrastructure.\nC. Support for standard SQL and various formats: Amazon Athena provides support for standard SQL, which can be used to query data in S3. Additionally, it supports various file formats, such as CSV, JSON, and Parquet, making it easy to query data in multiple formats.\nD. Parallel execution of queries: AWS Athena can automatically execute queries in parallel, improving query performance and reducing response times. Most query results come back within seconds, making it easy to analyze data and make informed decisions quickly.\nE. Support for various data sources: In addition to S3, AWS Athena can query data from various data sources, such as AWS RDS, Amazon EFS, and Amazon Glacier. This makes it easy to query data from different sources and analyze it all in one place.\nIn summary, AWS Athena is a cost-effective, serverless solution that supports standard SQL and various file formats. It can execute queries in parallel, making it a fast and efficient way to analyze large datasets. Additionally, it supports various data sources, making it easy to analyze data from different sources in one place.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Athena charges every hour at an extremely low cost. It is more cost-effective than the previous solution.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Athena is serverless. So you do not need to set up the instance or consider if the compute capability is enough or not.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon Athena can query data in S3 easily using standard SQL only when the data format is CSV. Standard SQL cannot be used for other formats.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Athena can automatically execute queries in parallel. So most results come back within seconds.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon Athena can query data from any data sources, including S3, AWS RDS, EFS and Glacier.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 327,
  "query" : "Your application uses an ELB in front of an Auto Scaling group of web/application servers deployed across two AZs and a Multi-AZ RDS Instance for data persistence.\nThe database CPU is often above 80% usage, and 90% of I/O operations on the database are reads.\nTo improve the performance, you recently added a single-node Memcached ElastiCache Cluster to cache frequent DB query results.\nIn the next weeks, the overall workload is expected to grow by 30%\nDo you need to change anything in the architecture to maintain the high availability of the application with the anticipated additional load, and why?",
  "answer" : "Answer - A.\nOption A is CORRECT because having two clusters in different AZs provides high availability of the cache nodes, removing the single point of failure.\nIt will help caching the data; hence, reducing the overload on the database, maintaining the availability, and reducing the impact.\nOption B is incorrect because, even though AWS will automatically recover the failed node, there are no other nodes in the cluster once the failure happens.\nSo, the data from the cluster would be lost once that single node fails.\nFor higher availability, there should be multiple nodes.\nAlso, once the cache node fails, all the cached read load will go to the database, which will not be able to handle the load with a 30% increase to current levels.\nThis means there will be an availability impact.\nOption C is incorrect because provisioning the nodes in the same AZ does not tolerate an AZ failure.\nFor higher availability, the nodes should be spread across multiple AZs.\nOption D is incorrect because the very purpose of the cache node was to reduce the impact on the database by not overloading it.\nIf the cache node fails, the database will not be able to handle the 30% increase in the load; so, it will have an availability impact.\nMore information on this topic from AWS Documentation:\nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/BestPractices.html\nMitigating Node Failures.\nTo mitigate the impact of a node failure, spread your cached data over more nodes.\nBecause Memcached does not support replication, a node failure will always result in some data loss from your cluster.\nWhen you create your Memcached cluster, you can create it with 1 to 20 nodes or more by special request.\nPartitioning your data across a greater number of nodes means you'll lose less data if a node fails.\nFor example, if you partition your data across 10 nodes, any single node stores approximately 10% of your cached data.\nIn this case, a node failure loses approximately 10% of your cache which needs to be replaced when a replacement node is created and provisioned.\nMitigating Availability Zone Failures.\nTo mitigate the impact of an availability zone failure, locate your nodes in as many availability zones as possible.\nIn the unlikely event of an AZ failure, you will lose only the data cached in that AZ, not the data cached in the other AZs.\nBased on the information provided, the application is using an ELB in front of an Auto Scaling group of web/application servers deployed across two AZs, and a Multi-AZ RDS Instance for data persistence. The database CPU usage is high, and 90% of I/O operations on the database are reads, which can lead to performance issues.\nTo address the performance issues, a single-node Memcached ElastiCache Cluster has been added to cache frequent DB query results. However, the workload is expected to grow by 30% in the next few weeks, which may impact the overall availability of the application.\nAnswering the question:\nA. Yes. You should deploy two Memcached ElastiCache clusters in different AZ's with a change in application logic to support both clusters because the RDS instance will not be able to handle the load if the cache node fails.\nExplanation: Deploying two Memcached ElastiCache clusters in different AZs would help ensure high availability and avoid a single point of failure. By deploying two clusters, the application can continue to function even if one of the cache nodes fails. However, it is important to ensure that the application logic can support both clusters to take advantage of the increased availability.\nB. No. If the cache node fails, the automated ElastiCache node recovery feature will prevent any availability impact.\nExplanation: ElastiCache provides automated node recovery features, which can help avoid availability impact in case of a cache node failure. However, it is important to note that a single-node cache cluster may not provide the required performance and availability for the application, especially with the anticipated workload growth.\nC. Yes. You should deploy the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS DB master instance to handle the load if one cache node fails.\nExplanation: Deploying a two-node Memcached ElastiCache cluster in the same AZ as the RDS DB master instance can help ensure high availability and reduce network latency. In this scenario, the application can continue to function even if one of the cache nodes fails. However, it is important to note that this approach may not provide optimal performance and availability if the AZ experiences a failure.\nD. No. If the cache node fails, you can always get the same data from the DB without having any availability impact.\nExplanation: If the cache node fails, the application can still retrieve the required data from the RDS instance, but it may impact performance and availability. Additionally, the RDS instance is already experiencing high CPU usage, which may worsen with the anticipated workload growth. Hence, relying solely on the RDS instance may not provide optimal performance and availability for the application.\nIn conclusion, option A is the most appropriate answer as it provides high availability by deploying two Memcached ElastiCache clusters in different AZs, and ensures the application logic can support both clusters.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Yes. You should deploy two Memcached ElastiCache clusters in different AZ`s with a change in application logic to support both clusters because the RDS instance will not be able to handle the load if the cache node fails.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "No. If the cache node fails, the automated ElastiCache node recovery feature will prevent any availability impact.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Yes. You should deploy the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS DB master instance to handle the load if one cache node fails.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No. If the cache node fails, you can always get the same data from the DB without having any availability impact.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 328,
  "query" : "You are an architect for a new sharing mobile application.\nAnywhere in the world, your users can see local news on topics they choose.\nThey can post pictures and videos from inside the application.\nSince the application is being used on a mobile phone, connection stability is required for uploading content, and delivery should be quick.",
  "answer" : "Answer - D.\nOption A is CORRECT because Route53 is not required in this case.\nOptions B and C are both incorrect because you do not need to upload the content to the source that is closer to the user.\nCloudFront will take care of that.\nOption D is CORRECT because it uses S3 Transfer Acceleration to take advantage of the globally distributed edge locations in Amazon CloudFront.\nIt enables fast, easy, and secure transfers of files over long distances.\nFor more information on Cloudfront, please refer to the below URLs.\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.html https://stackoverflow.com/questions/24014596/upload-files-via-cloudfront-distribution https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AddingObjects.html\nThe best solution for this scenario is D - Use S3 Transfer Acceleration that uses CloudFront global edge locations for uploading the content to the S3 bucket and for content delivery.\nExplanation:\nS3 Transfer Acceleration is a service provided by AWS that uses the CloudFront global network of edge locations to accelerate the transfer of files over the internet to S3 buckets. This means that data can be uploaded and delivered to users quickly and reliably, regardless of their location.\nOption A suggests uploading and storing files in S3 in several regions, and using Route53 to choose the region with the minimal latency. While this can work, it requires a lot of manual setup and management, and does not guarantee the fastest possible transfer speeds.\nOption B suggests uploading and storing files in S3 in the region closest to the user, and then using multiple distributions of CloudFront. While this can work well for content delivery, it does not address the issue of unstable connections when uploading content.\nOption C suggests uploading to EC2 instances in regions closer to the user, sending content to S3, and using CloudFront for content delivery. This option adds unnecessary complexity and management overhead, and may not provide the fastest transfer speeds.\nOption D is the best solution because it uses S3 Transfer Acceleration to take advantage of the CloudFront global network of edge locations for both uploading and delivering content. This ensures fast and reliable transfers regardless of user location and connection stability. S3 Transfer Acceleration uses optimized network paths and Amazon's backbone network to provide accelerated transfer speeds and is highly scalable and reliable.\nIn summary, Option D provides the best solution for a mobile application where fast and reliable content upload and delivery is required.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Upload and store the files in S3 in several regions, and use Route53 to choose the region that has the minimal latency.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Upload and store in S3 in the region closest to the user and then use multiple distributions of CloudFront.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Upload to EC2 in regions closer to the user, send content to S3, use CloudFront.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use S3 Transfer Acceleration that uses the Cloudfront global edge locations for uploading the content to the S3 bucket and for content delivery.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 329,
  "query" : "You are maintaining an application that is spread across multiple web servers and has incoming traffic balanced by ELB.\nThe application allows users to upload pictures.\nCurrently, each web server stores the image, and a background task synchronizes the data between servers.\nHowever, the synchronization task can no longer keep up with the number of images uploaded.",
  "answer" : "Answer - A.\nOption A is CORRECT because S3 provides a durable, secure, cost-effective, and highly available storage service for the uploaded pictures.\nOption B is incorrect because the application needs just a storage solution, not a global content distribution service.\nCloudFront is also a costlier solution compared to S3.\nOption C is incorrect because you cannot share EBS volumes ( can be done only for \"io1\" EBS volumes) among multiple EC2 instances.\nOption D is incorrect because ELB cannot be used as a storage service.\nFor more information on AWS S3, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\nThe most appropriate option for storing uploaded images from multiple web servers that are load-balanced by an Elastic Load Balancer (ELB) is to store the images in Amazon S3 (Simple Storage Service). Option A is the correct answer.\nExplanation:\nOption A: Store the images in Amazon S3. Amazon S3 is a highly durable, scalable, and secure object storage service offered by AWS. Storing images in S3 allows for easy access and retrieval of images from all web servers, as each server can access the same S3 bucket. Additionally, S3 can be configured to automatically replicate data across multiple Availability Zones for high availability, and versioning can be enabled to track changes to the images. This option eliminates the need for the synchronization task and ensures that all web servers have access to the same images.\nOption B: Store the images on Amazon CloudFront. Amazon CloudFront is a content delivery network (CDN) service that caches content from the origin server to edge locations for faster content delivery. While CloudFront can be used to cache images, it is not a storage solution, and images would still need to be stored in a durable storage service such as S3. Therefore, this option is not appropriate.\nOption C: Store the images on Amazon EBS. Amazon EBS (Elastic Block Store) provides block-level storage volumes that can be attached to EC2 instances. However, EBS is not a suitable solution for storing images since it is designed for high-performance transactional processing workloads rather than long-term storage of large files like images.\nOption D: Store the images on the ELB. Elastic Load Balancer (ELB) is a service that distributes incoming traffic across multiple instances. ELB is not designed for storing data and is only responsible for load balancing traffic. Therefore, this option is not appropriate for storing uploaded images.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Store the images in Amazon S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Store the images on Amazon CloudFront.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the images on Amazon EBS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the images on the EL.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 330,
  "query" : "The company runs a complex customer system and consists of 10 different software components.\nAll backed up by RDS.\nYou adopted Opswork to simplify the management and deployment of that application and created a stack and layers for each component.",
  "answer" : "E.\nAnswer - D and E.\nOption A is incorrect because you would have to re-launch new instances to change the AMI, and you can't do that with chef recipes only.\nOption B is incorrect because the AMI replacements should be done without incurring application downtime or capacity problems.\nSo if you shutdown the stack, all applications will be stopped.\nOption C is incorrect because the application could face the problem of insufficient capacity.\nOption D is CORRECT because it represents a common practice of Blue-Green Deployment which is carried out for reducing the downtime and risk by running two identical production environments called Blue and Green.\nPlease see the \"More information..\" section for additional details.\nOption E is CORRECT because you can only add new instances at the layer level by specifying to use Custom AMI at the stack level.\nMore information on Blue-Green Deployment:\nBlue-green deployment is a technique that reduces downtime and risk by running two identical production environments called Blue and Green.\nAt any time, only one of the environments is live, with the live environment serving all production traffic.\nFor this example, Blue is currently live, and Green is idle.\nAs you prepare a new version of your software, deployment and the final stage of testing takes place in an environment that is not live: in this example, Green.\nOnce you have deployed and fully tested the software in Green, you switch the router.\nSo all incoming requests now go to Green instead of Blue.\nGreen is now live, and Blue is idle.\nThis technique can eliminate downtime due to application deployment.\nIn addition, blue-green deployment reduces risk: if something unexpected happens with your new version on Green, you can immediately roll back to the last version by switching back to Blue.\nPlease refer to the below URL for more details.\nhttps://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\nSure, I'd be happy to provide a detailed explanation of each answer choice and how it relates to the scenario described.\nA. Assign a custom recipe to each layer that replaces the underlying AMI. Use OpsWorks life-cycle events to incrementally execute this custom recipe and update the instances with the new AMI.\nThis answer suggests using a custom recipe to replace the underlying Amazon Machine Image (AMI) for each layer of the OpsWorks stack. The OpsWorks life-cycle events would be used to incrementally execute this custom recipe, meaning that each instance in the layer would be updated with the new AMI one at a time. This approach would minimize the risk of downtime, as only one instance would be updated at a time. Additionally, by using a custom recipe, you can ensure that each instance is updated in a consistent manner, which can help prevent configuration drift.\nB. Specify the latest AMI in the custom AMI at the stack level. Terminate instances of the stack and let OpsWork launch new instances with the new AMI.\nThis answer suggests specifying the latest AMI in the custom AMI at the stack level. Once this is done, you would terminate the instances of the stack and let OpsWorks launch new instances with the new AMI. This approach would result in downtime, as all instances would need to be terminated and replaced with new instances. However, this approach would be simpler than some of the other options, as you would not need to worry about updating instances one at a time.\nC. Identify all EC2 instances of your OpsWork stack, stop each instance, replace the AMI ID property with the latest AMI ID, and restart the instance. To avoid downtime, make sure no more than one instance is stopped at the same time.\nThis answer suggests identifying all EC2 instances of your OpsWorks stack and updating each instance's AMI ID property with the latest AMI ID. To avoid downtime, you would need to stop each instance and restart it after the update. To minimize the risk of downtime, you would need to make sure that no more than one instance is stopped at the same time. This approach would be more labor-intensive than some of the other options, as you would need to manually update each instance's AMI ID property.\nD. Create a new stack and layers with identical configuration, add instances with the latest AMI specified as a custom AMI to the new layers, switch DNS to the new stack, and tear down the old stack.\nThis answer suggests creating a new stack and layers with identical configuration to the existing stack. Once the new stack is created, you would add instances with the latest AMI specified as a custom AMI to the new layers. Once the new instances are up and running, you would switch DNS to the new stack and tear down the old stack. This approach would result in downtime during the DNS switch, but it would also ensure that all instances are updated to the latest AMI in a consistent manner.\nE. Add new instances with the latest AMI as a custom AMI to all OpsWork layers of your stack and terminate the old ones.\nThis answer suggests adding new instances with the latest AMI as a custom AMI to all OpsWork layers of your stack. Once the new instances are up and running, you would terminate the old ones. This approach would result in downtime during the instance termination, but it would ensure that all instances are updated to the latest AMI in a consistent manner.\nIn summary, each answer choice represents a different approach to updating the Amazon Machine Images (AMIs) for an OpsWorks stack. Option A and E involve updating instances one at a time to minimize downtime, while options B, C, and D involve more drastic measures that may result in downtime but are simpler to execute. The",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Assign a custom recipe to each layer that replaces the underlying AMI. Use OpsWorks life-cycle events to incrementally execute this custom recipe and update the instances with the new AMI.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Specify the latest AMI in the custom AMI at the stack level. Terminate instances of the stack and let OpsWork launch new instances with the new AMI.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Identify all EC2 instances of your OpsWork stack, stop each instance, replace the AMI ID property with the latest AMI ID, and restart the instance. To avoid downtime, make sure no more than one instance is stopped at the same time.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new stack and layers with identical configuration, add instances with the latest AMI specified as a custom AMI to the new layers, switch DNS to the new stack, and tear down the old stack.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Add new instances with the latest AMI as a custom AMI to all OpsWork layers of your stack and terminate the old ones.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 331,
  "query" : "As a solution architect professional, you have been requested to launch 20 Large EC2 instances which will all be used to process huge amounts of data.\nThere is also a requirement that these instances need to transfer data back and forth among each other and need a low-latency network performance.\nWhich of the following would be the most efficient setup to achieve this?",
  "answer" : "Answer - C.\nOption A is incorrect because being in the same region would not mean that the data transfer between the instances would be faster.\nIn fact, the instances would experience network latency.\nOption B is incorrect because just being in the same AZ is not sufficient.\nThey should be added to a Placement Group to benefit from the low network latency.\nOption C is CORRECT because Placement Group enables applications to get the low-latency network performance necessary for tightly-coupled node-to-node communication typical of many high-performance computing applications.\nOption D is incorrect because despite being of the largest size, the EC2 instances would still experience network latency if they are not part of a Placement Group.\nMore information on Placement Groups:\nA placement group is a logical grouping of instances within a single Availability Zone.\nPlacement groups are recommended for applications that benefit from low network latency, high network throughput, or both.\nTo provide the lowest latency, and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.\nFor more information on Placement Groups, please visit the URL.\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nThe most efficient setup to achieve low-latency network performance while transferring data among the 20 large EC2 instances would be to use a cluster Placement Group and ensure that all instances are launched in it.\nA cluster placement group is a logical grouping of instances within a single Availability Zone that provides low-latency network performance by enabling network traffic to flow directly between the instances instead of traversing the underlying network infrastructure. When instances are launched in a cluster placement group, they are placed in a way that provides a high-bandwidth, low-latency network connection between them.\nOption A, to place all the instances in the same region, does not provide any specific benefits for low-latency network performance. Regions are geographical areas where AWS has multiple Availability Zones.\nOption B, to place all the instances in the same Availability Zone, does not guarantee low-latency network performance as it depends on how the instances are placed within the Availability Zone. Additionally, placing all the instances in a single Availability Zone may not provide sufficient redundancy and fault tolerance.\nOption D, to use the largest EC2 instances and spread them across multiple Availability Zones, does not guarantee low-latency network performance. Also, spreading instances across multiple Availability Zones may increase network latency as instances may need to communicate over a public network.\nTherefore, option C, to use a cluster Placement Group and ensure that all instances are launched in it, is the most efficient setup to achieve low-latency network performance while transferring data among the 20 large EC2 instances.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Ensure that all the instances are placed in the same region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure that all instances are placed in the same Availability Zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a cluster Placement Group and ensure that all instances are launched in it.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the largest EC2 instances currently available on AWS and make sure they are spread across multiple Availability Zones.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 332,
  "query" : "You are an AWS solutions architect in an IT company.\nYour company has a big data product that analyzes data from various sources like transactions, web servers, surveys, and social media.\nYou need to design a new solution in AWS that can extract data from the source in S3 and transform the data to match the target schema automatically.\nMoreover, the transformed data can be analyzed using standard SQL.\nWhich combination of solutions would meet these requirements in the best way? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - C, E.\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.\nAWS Athena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a metadata repository across various services.\nAWS Glue and AWS Athena, when used together, can meet the requirements for this case.\nOption A is incorrect:Although the ECS cluster may work, it is not fully managed by AWS.\nBy AWS Glue, you can create and run an ETL job with a few clicks in the AWS Management Console.\nOption B is incorrect: Similar reason as.\nOption A.\nAWS Glue should be considered as the first choice when an extract, transform, and load (ETL) service is required in AWS.\nOption C is correct because AWS Glue generates Scala or PySpark (the Python API for Apache Spark) scripts with AWS Glue extensions.\nYou can use and modify these scripts to perform various ETL operations.\nYou can extract, clean, and transform raw data, and then store the result in a different repository, where it can be queried and analyzed.\nOption D is incorrect: Because AWS QuickSight is a service to deliver insights rather than perform SQL queries.\nOption E is CORRECT: Because AWS Athena can directly query the data in AWS Glue Data Catalog with SQL commands.\nNo additional step is needed.\nTo meet the requirements of extracting and transforming data from various sources to S3 and enabling analysis using standard SQL, the best solution would be a combination of AWS Glue and Athena.\nAWS Glue is a fully managed ETL service that makes it easy to move data between data stores. It automatically discovers and profiles your data via a Glue data catalog, and then creates ETL scripts to transform your data from the source schema to the target schema. This makes it easy to build and maintain ETL pipelines, reducing the amount of manual work required to transform data.\nUsing AWS Glue, you can create a job that extracts the data from S3, transforms it according to the target schema, and loads it back into S3. You can schedule the job to run at a specific time, or trigger it based on events, such as when new data is added to the source.\nOnce the data is transformed, you can use AWS Athena to query the data using standard SQL commands. Athena is a serverless, interactive query service that makes it easy to analyze data in S3 using SQL. It is highly scalable and can handle large datasets, making it a good fit for big data workloads.\nOption A is not the best choice because AWS ECS is a container management service and is not specifically designed for ETL workloads. While it is possible to run ETL jobs on ECS, it requires more setup and management than using a dedicated ETL service like AWS Glue.\nOption B is not the best choice because while AWS EMR is a good choice for processing big data workloads, it is not as well-suited for ETL workloads as AWS Glue. EMR requires more setup and management than Glue and is better suited for complex data processing tasks that require distributed computing.\nOption D is not the best choice because AWS QuickSight is a business intelligence service that enables visualization and reporting of data, but it is not designed for data transformation or ETL workloads.\nTherefore, options C and E are not the best choice either, as Glue and Athena would provide a more cost-effective and streamlined approach to transforming and analyzing data.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up an AWS ECS cluster to manage the extract, transform, and load (ETL) service to prepare and load the data in S3 for analytics.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an AWS EMR cluster to transform the data to a target format for downstream. Execute the EMR tasks on a schedule.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an AWS Glue ETL to run on schedule to scan the data in S3 and populate the Glue data catalog accordingly.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure AWS QuickSight to analyze the data using standard SQL commands.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Athena to query the data using standard SQL.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 333,
  "query" : "A big SaaS provider migrated one of its on-premises products to AWS since many customers (over 300) already have their own Virtual Private Clouds (VPCs) in AWS.\nDue to certain product features, one key requirement is that all traffic between the service provider and customers should be private without being exposed to the internet.\nThe company's operation team already configured a VPC.\nWhat is the best solution that can be recommended here?",
  "answer" : "E.\nF.\nG.\nCorrect Answer - C.\nOne major requirement for this question is that traffic data should not be exposed to the public Internet.\nPrivate connectivity can be provided by AWS PrivateLink between the SaaS VPC and customers' VPCs from other AWS accounts.\nOption A is incorrect: Because the virtual private gateway is for VPN connection which does not work between AWS VPCs.\nOption B is incorrect: This may work, but it is not as good as option.\nC.\nVPC peering is a relatively old solution to connect two VPCs privately.\nHowever, it also has several limitations.\nFor example, the maximum limit is 125 peering connections per VPC, and VPC peering requires non-overlapping IP addresses.\nOption C is CORRECT: Because by using AWS PrivateLink, it is very easy to connect services across different accounts and VPCs through the Amazon private network.\nOption D is incorrect: Because NAT Gateway is used to route traffic from private subnets to the internet, which cannot be used between VPCs.\nThe best solution for the SaaS provider to ensure private communication with its customers' VPCs would be to set up a private VPN connection. This would provide a highly available and secure link between the SaaS provider and its customers.\nOption A, setting up a virtual private gateway, is not the best solution because this only provides a secure connection between the VPC and the provider's own data center, not with the customers' VPCs.\nOption C, creating a VPC peering between the provider's VPC and each customer's VPC, would work in theory but could become cumbersome and difficult to manage as the number of customers increases.\nOption D, configuring an AWS endpoint service (PrivateLink), is a good option but would require the customers to create a connection from their VPC to the endpoint service using an interface VPC endpoint, which may not be feasible for all customers.\nOption F, creating a NAT Gateway in the VPC, is not relevant to this particular requirement as it is used for providing Internet connectivity to resources within a private subnet.\nOption G, modifying the main route table to allow traffic to other VPCs only through a specific gateway, would not be sufficient on its own to ensure private communication between the SaaS provider and its customers' VPCs.\nTherefore, the best option is to set up a private VPN connection, which would provide a secure and private link between the SaaS provider and its customers' VPCs, allowing them to communicate without being exposed to the internet.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up a virtual private gateway in this VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "To communicate with customer VPCs, set up a private VPN connection. Therefore, a highly available and private link is created between customers and the service provider.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a VPC peering between this VPC and each customer’s VPC so that all connections are secure and private.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure an AWS endpoint service (PrivateLink) in the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Other AWS customer accounts can create a connection from their VPC to the endpoint service using an interface VPC endpoint.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a NAT Gateway in the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the main route table to allow traffic to other VPCs only through this gateway to ensure that the traffic is not public.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 334,
  "query" : "An online gaming server, which you have recently increased its IOPS performance by creating a RAID 0 configuration, has now started to have bottleneck problems due to your instance bandwidth.\nWhich of the following would be the best solution for this to increase throughput?",
  "answer" : "Answer - A.\nOption A is CORRECT because SR-IOV helps achieve higher network throughput, lower CPU utilization, and lower network latency which can translate into supporting more VMs per host, delivering increased network bandwidth utilization on the host, and providing greater performance predictability to the instances.\nOption B is incorrect because having all the instances in the same AZ does not necessarily increase the throughput.\nOption C is incorrect because the RAID 1 configuration which has data mirroring, provides redundancy of data for high availability.\nIt does not increase the throughput.\nOption D is incorrect because this option will help in achieving high availability, not increased throughput.\nMore information on SR-IOV:\nEnhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types.\nSR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces.\nEnhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latency.\nReference Link:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\nNote:\nThis is a very challenging question.\nThe answers are very similar.\nBut there is still a very clear answer.\nLet me dive deeply into each option.\n------------------------\nA.\nSingle Root I/O Virtualization (SR-IOV) on all the instances.\nHere, we are accessing NIC directly and exempting the virtualization layer.\nIt increases the speed by emulating the vNIC and \"Visualizing the network adapter on the physical host\".\nB.\nMove all your EC2 instances to the same availability Zone.\nThis is not the correct option as it will lose high availability.\nIt also does not solve the bottleneck problems.\nC.\nUse a RAID 1 configuration instead of RAID 0\nThis is not the correct option because there is no I/O performance enhancement.\nD.\nUse instance store backed instances and strip the attached ephemeral storage and use DRBD Asynchronous Replication.\nDRBD produces two mirror copies of data.\nWhen the primary failed, the standby has whatever data the primary has just before the crash.\nWhen the standby starts up, the RDBMS goes through the normal recovery and boot up.\nIt's the same as the primary crashed and being started up again.\nIt asks for \"DRBD Asynchronous Replication\"\nHere, you should know the following points.\nAsynchronous: in case of forced failover: Data loss.\nin our case, even we compromise all other factors like doubling the billing, we can not ensure data.\nSemi-synchronous: No write is lost in case of forced failover.\nSynchronous: No loss of data in all cases.\nHere you should know 2 factors.\nYou need to create a backup of each instance.\nYou need to double the number of instances and the size of volumes.\nDouble billing: It will have primary and secondary EC2s.\nIf the primary fails, the traffic will automatically be diverted to the secondary.\nEach EC2 primary and secondary will have its own EBS.\n------------------------\nKeep in mind our challenge:\n\"Started to have bottleneck problem due to instance bandwidth.\"\nThe question is not asking you to change the RAID 0 configuration.\nIn brief, to answer this question:\nIf we go for option D: Even we are ready to pay double-billing, our data is not safe.\nAlso, it is asking to change the RAID 0, which created a problem for us, and the question starts after RAID 0\nOption D is fully wrong.\nif we go for option A: This helps us increase I/O by taking leverage of \"Visualizing the network adapter on the physical host.\"\nHence answer A is correct.\nThe best solution for increasing throughput on an online gaming server that is experiencing bottleneck problems due to instance bandwidth will depend on various factors such as the type of workload, the size of data transfer, and the specific resources available on the AWS infrastructure.\nOption A: Using Single Root I/O Virtualization (SR-IOV) on all instances can help in improving network performance by reducing the hypervisor overhead of the virtualization layer. However, it may not be the best solution in this case as it focuses on network performance and not necessarily on increasing throughput.\nOption B: Moving all EC2 instances to the same availability zone can help in reducing network latency and improving network performance, but it may not address the bottleneck issue as it is related to instance bandwidth.\nOption C: Using a RAID 1 configuration instead of RAID 0 may not be the best solution in this case as it reduces the overall disk space by mirroring the data and may not provide significant improvement in throughput.\nOption D: Using instance store backed instances and stripe the attached ephemeral storage devices and use DRBD Asynchronous Replication can help in improving throughput by leveraging the ephemeral storage devices available on the instance. DRBD Asynchronous Replication is a software-based data replication technology that can help in replicating the data between two instances asynchronously, thereby improving the fault tolerance and scalability of the system.\nTherefore, option D would be the best solution to increase throughput in this scenario. However, it's worth noting that this solution may have additional considerations such as backup and recovery strategies, data consistency, and performance monitoring.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Single Root I/O Virtualization (SR-IOV) on all the instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Move all your EC2 instances to the same availability zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a RAID 1 configuration instead of RAID 0.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use instance store backed instances and stripe the attached ephemeral storage devices and use DRBD Asynchronous Replication.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 335,
  "query" : "You are designing multi-region architecture, and you want to send users to a geographic location based on latency-based routing, which seems simple enough.\nHowever, you also want to use weighted-based routing among resources within that region.\nWhich of the below setups would best accomplish this?",
  "answer" : "Answer - B.\nOption A is incorrect because you need to define the recordset to be pointed to (in this case, weighted) before creating the top level recordset (in this case, latency).\nOption B is CORRECT because you need to create the weighted policies first because you will use those record sets as the alias pointing to in the latency record sets.\nOption C is incorrect because you can create the nested recordsets to accomplish this.\nOption D is incorrect because the use of IPv6 is not mandatory in this configuration (and it does not mention any latency based routing - which is one of the requirements).\nPlease refer to the below documentation from AWS for an example where you can define complex routing.\nPlease find the below link for complex-based routing.\nhttp://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html\nTo accomplish geographic location-based routing with weighted-based routing among resources within the region in a multi-region architecture, you will need to use complex routing (nested record sets) and ensure that you define the latency-based records first.\nLatency-based routing directs traffic to the region with the lowest latency, which results in faster response times and better user experience. Weighted routing directs traffic to specific resources within a region based on defined weights.\nBy using nested record sets, you can combine these two types of routing to achieve the desired setup. You can set up a top-level latency-based record set that directs traffic to the appropriate region based on latency. Within each region, you can set up a weighted resource record set that directs traffic to specific resources based on the defined weights.\nIt's important to define the latency-based record sets first because they will determine which region the traffic goes to, and then the weighted resource record sets can be defined for resources within that region.\nOption A is the correct answer as it correctly explains the setup required for achieving the desired outcome. Option B is incorrect because it suggests defining the weighted resource record sets first, which would not work as expected. Option C is incorrect because different routing records can be used together with the use of nested record sets. Option D is irrelevant because AAAA-IPv6 addresses are not necessary for this setup.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You will need to use complex routing (nested recordsets) and ensure that you define the latency based records first.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You will need to use complex routing (nested recordsets) and ensure that you define the weighted resource record sets first.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "This cannot be done. You can`t use different routing records together.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You will need to use AAAA - IPv6 addresses when you define your weighted based record sets.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 336,
  "query" : "An Amazon EMR cluster with four nodes is running 24/7/365 and expects potentially to add one extra node for several days only during the entire year that can be terminated.\nWhich architecture would have the lowest possible cost for the cluster requirement?",
  "answer" : "Answer - C.\nOption A is INCORRECT because purchasing On-demand instances would involve more cost than purchasing a spot instance.\nOption B is incorrect because reserving the 5th node is unnecessary.\nOption C is CORRECT because (a) the application requires 4 nodes throughout the year and reserved instances would save the cost and (b) since the extra node needs only to run for several days in a year and it can be terminated, purchasing a spot instance can provide a lowest cost option in addition to purchasing RI.\nOption D is incorrect because reserving only 2 instances would not be sufficient.\nPlease find the below link for Reserved Instances.\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\nTo determine the most cost-effective architecture for the EMR cluster, we need to consider the cluster's expected usage and the pricing models for AWS EC2 instances.\nOption A: Purchase 4 reserved nodes and rely on on-demand instances for the fifth node if required.\nIn this architecture, we would purchase reserved instances for the four nodes that run continuously throughout the year. The fifth node, which is only needed for several days during the year, would be launched as an on-demand instance. This approach would provide a predictable cost for the majority of the year but could result in higher costs during the time when the fifth node is needed.\nOption B: Purchase 5 reserved nodes to cover all possible usage during the year.\nIn this architecture, we would purchase reserved instances for all five nodes, which would provide a fixed cost for the entire year. However, since the fifth node is only needed for several days during the year, this approach would result in higher costs for the majority of the year when only four nodes are required.\nOption C: Purchase 4 reserved nodes and launch a spot instance for the extra node if required.\nIn this architecture, we would purchase reserved instances for the four nodes that run continuously throughout the year. The fifth node would be launched as a spot instance, which is a type of instance that is available at a lower price than on-demand instances but can be terminated at any time. This approach would provide a predictable cost for the majority of the year, and the use of a spot instance for the fifth node would result in lower costs during the time when it is needed.\nOption D: Purchase 2 reserved nodes and utilize 3 on-demand nodes only for peak usage times.\nIn this architecture, we would purchase reserved instances for two nodes and use on-demand instances for the remaining three nodes. The on-demand instances would be used only during peak usage times, providing a predictable cost for the majority of the year. However, this approach could result in higher costs during peak usage times, and the use of on-demand instances would not be cost-effective for long-term usage.\nConclusion:\nAmong the four options, Option C is the most cost-effective architecture for the EMR cluster. It provides a predictable cost for the majority of the year, and the use of a spot instance for the fifth node would result in lower costs during the time when it is needed.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Purchase 4 reserved nodes and rely on on-demand instances for the fifth node, if required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Purchase 5 reserved nodes to cover all possible usage during the year.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Purchase 4 reserved nodes and launch a spot instance for the extra node if required.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Purchase 2 reserved nodes and utilize 3 on-demand nodes only for peak usage times.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 337,
  "query" : "A company has placed a set of on-premises resources with an AWS Direct Connect provider.\nAfter establishing connections to a local AWS region in the US, the company needs to establish a low latency dedicated connection to an S3 public endpoint over the Direct Connect dedicated low latency connection.\nWhat steps need to be taken to accomplish configuring a direct connection to a public S3 endpoint?",
  "answer" : "The correct answer is C: Configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection.\nExplanation:\nAWS Direct Connect allows customers to establish dedicated, low-latency connections between their on-premises data centers and AWS. Customers can establish private virtual interfaces to connect to their Amazon VPCs or public virtual interfaces to connect to AWS services over the Direct Connect connection.\nIn this scenario, the company needs to establish a low latency dedicated connection to an S3 public endpoint over the Direct Connect dedicated low latency connection. This means that the company wants to use Direct Connect to connect to the public S3 endpoint directly, without going through the Internet.\nTo accomplish this, the company needs to configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection. The private virtual interface will allow the company to access the S3 public endpoint as if it were part of its VPC, but without going through the Internet.\nTo configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection, the company needs to follow these steps:\n1. Create a Direct Connect private virtual interface.\n2. Associate the private virtual interface with the Direct Connect connection.\n3. Configure the on-premises router to advertise the route to the S3 public endpoint via the Direct Connect private virtual interface.\n4. Verify connectivity to the S3 public endpoint.\nOption A is incorrect because configuring a public virtual interface is not necessary to connect to a public S3 endpoint over Direct Connect. Additionally, configuring on-premise routing to utilize the Direct Connect for AWS S3 is not sufficient to establish a low latency dedicated connection to a public S3 endpoint.\nOption B is incorrect because establishing a VPN connection from the VPC to the public S3 endpoint would route traffic through the Internet, which is not what the company wants to do.\nOption D is incorrect because adding a BGP route as part of the on-premises router would not be sufficient to establish a private virtual interface to connect to a public S3 endpoint via Direct Connect.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure a Public Virtual Interface AND configure on-premise routing to utilize the direct connect for AWS S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Establish a VPN connection from the VPC to the public S3 endpoint.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add a BGP route as part of the on-premises router. This will route S3 related traffic to the public S3 endpoint to the dedicated AWS region.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 338,
  "query" : "A fintech startup company is developing a product on the AWS platform.\nTo speed up the development, the company plans to use a SaaS provided by AWS Marketplace.\nThe SaaS provider already configured an AWS PrivateLink.\nIn the company's VPC, which configuration is required to utilize this private connection so that traffic flows to the service provider over private AWS networking rather than over the Internet?",
  "answer" : "Answer - A.\nAWS Direct Connect enables you to directly interface your on-premises network with a device at an AWS Direct Connect location.\nYou need to configure a Public Virtual Interface AND configure on-premises routing to utilize the direct connect for AWS public-facing services.\nEach virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key.\nSee the image below.\nOption A is CORRECT because, as mentioned above, it creates a public virtual interface to connect to the S3 endpoint.\nOption B is incorrect because to connect to the S3 endpoint a public virtual interface needs to be created, not VPN.\nOption C is incorrect because to connect to the S3 endpoint a public virtual interface needs to be created, not private.\nOption D is incorrect because this setup will not help to connect to the S3 endpoint.\nFor more information on virtual interfaces, please visit the below URL-\nhttp://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html\nSure, I'd be happy to provide a detailed explanation.\nAWS PrivateLink is a service that enables private connectivity between VPCs, AWS services, and on-premises applications using the AWS network. It allows you to access services over private AWS networking instead of the public internet, which enhances security and reduces data transfer costs.\nIn this scenario, the fintech startup company plans to use a SaaS provided by AWS Marketplace, which has already configured an AWS PrivateLink. To utilize this private connection so that traffic flows over private AWS networking rather than over the internet, the company needs to configure an interface VPC endpoint for the SaaS in its VPC. Therefore, answer A is correct.\nAn interface VPC endpoint is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a service. When you create an interface VPC endpoint, AWS creates an elastic network interface in the specified subnet with a private IP address that serves as an entry point for traffic destined to the service.\nBy configuring an interface VPC endpoint for the SaaS in the company's VPC, the SaaS can be accessed securely and privately over the AWS PrivateLink. This means that the traffic flows over private AWS networking instead of over the internet, enhancing security and reducing data transfer costs.\nAnswer B is incorrect because it suggests configuring a site-to-site VPN connection in the customer VPC for the SaaS to use the AWS PrivateLink connection. This is not necessary as AWS PrivateLink provides a private, secure, and scalable way to access services over the AWS network without the need for a VPN.\nAnswer C is also incorrect because it suggests setting up a gateway VPC endpoint for the SaaS, which creates an elastic network interface in the subnet with an elastic IP address. This is not necessary as interface VPC endpoints are the recommended way to connect to AWS services over AWS PrivateLink.\nAnswer D is also incorrect because it suggests creating an AWS Direct Connect connection for the SaaS to securely connect with the AWS PrivateLink. This is not necessary as AWS Direct Connect is used to establish a dedicated network connection between the customer's datacenter and AWS, and it is not relevant in this scenario.\nIn conclusion, to utilize the private connection already configured by the SaaS provider over AWS PrivateLink, the fintech startup company needs to configure an interface VPC endpoint for the SaaS in its VPC.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the VPC, configure an interface VPC endpoint for the SaaS which creates an elastic network interface in the subnet with a private IP address.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure a site-to-site VPN connection in customer VPC for the SaaS to use the AWS private link connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the VPC, set up a gateway VPC endpoint for the SaaS which creates an elastic network interface in the subnet with an elastic IP address.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the VPC, create an AWS Direct Connect connection for the SaaS to securely connect with the AWS PrivateLink.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 339,
  "query" : "You have launched an EC2 instance with four (4) 500 GB EBS Provisioned IOPS volumes attached.\nThe EC2 instance is EBS-Optimized and supports 500 Mbps throughput between EC2 and EBS.\nThe EBS volumes are configured as a single RAID 0 device, and each Provisioned IOPS volume is provisioned with 4,000 IOPS (4,000 16KB reads or writes) for a total of 16,000 random IOPS on the instance.\nThe EC2 instance initially delivers the expected 16,000 IOPS random read and write performance.\nSometime later, in order to increase the total random I/O performance of the instance, you add an additional two 500 GB EBS Provisioned IOPS volumes to the RAID.\nEach volume is provisioned to 4,000 lOPs like the original four for a total of 24,000 IOPS on the EC2 instance.\nMonitoring shows that the EC2 instance CPU utilization increased from 50% to 70%\nBut the total random IOPS measured at the instance level did not increase at all.\nWhat is the problem and a valid solution?",
  "answer" : "E.\nCorrect Answer - A.\nTo use AWS PrivateLink, an interface VPC endpoint for a service in the VPC is required.\nDetails can be checked in AWS document https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html#what-is-privatelink.\nOption A is CORRECT: Because interface VPC endpoint is essential to establish a secure connection to the private link.\nOption B is incorrect: Because a site to site VPN connection is used for the VPN connection between AWS and on-premise data center which is not suitable for this case.\nOption C is incorrect: Because it should be an interface VPC endpoint rather than a gateway VPC endpoint.\nSecondly, the IP address in the elastic network interface should be private instead of elastic.\nOption D is incorrect: Because an AWS Direct Connect connection is for the private connection between an on-premise server and AWS VPC.\nHowever, this question asks for the solution to communicate between the SaaS and AWS VPC.\nThe issue here is that even though two more EBS volumes have been added with the same Provisioned IOPS rate, the total random IOPS measured at the instance level did not increase at all, and the EC2 instance CPU utilization increased from 50% to 70%. This indicates that the bottleneck is not the storage but the EBS-optimized throughput.\nOption B is the correct solution. The EBS-Optimized throughput limits the total IOPS that can be utilized. Therefore, choosing a larger EBS-optimized instance that provides more dedicated Amazon EBS throughput will increase the total IOPS measured at the instance level.\nOption A is not a valid solution because larger storage volumes do not necessarily support higher Provisioned IOPS rates, and increasing the provisioned volume storage of each of the 6 EBS volumes to 1T would not solve the EBS-optimized throughput bottleneck.\nOption C is not a valid solution because while configuring the instance device driver and file system to use 64KB blocks may increase throughput, it will not solve the EBS-optimized throughput bottleneck.\nOption D is not a valid solution because even though RAID 0 only scales linearly to about 4 devices, increasing each Provisioned IOPS EBS volume to 6,000 IOPS will not solve the EBS-optimized throughput bottleneck.\nOption E is not a valid solution because changing the instance root volume to be a 500GB 4,000 Provisioned IOPS volume will not solve the EBS-optimized throughput bottleneck.\nIn summary, the solution is to choose a larger EBS-optimized instance that provides more dedicated Amazon EBS throughput to increase the total IOPS measured at the instance level.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Larger storage volumes support higher Provisioned IOPS rates, increasing the provisioned volume storage of each of the 6 EBS volumes to 1T.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The EBS-Optimized throughput limits the total IOPS that can be utilized. Therefore consider choosing a larger EBS–optimized instance that provides more dedicated Amazon EBS throughput.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Small block sizes cause performance degradation, limiting the I/O throughput. Hence, configure the instance device driver and file system to use 64KB blocks to increase throughput.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "RAID 0 only scales linearly to about 4 devices. Use RAID 0 with 4 EBS Provisioned IOPS volumes but increase each Provisioned IOPS EBS volume to 6,000 IOPS.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The standard EBS instance root volume limits the total IOPS rate. Hence, it can change the instant root volume to be a 500GB 4,000 Provisioned IOPS volume.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 340,
  "query" : "You work for an e-commerce retailer as an AWS Solutions Architect.\nYour company is looking to improve customer loyalty programs by partnering with other third parties to offer a more comprehensive selection of customer rewards.\nYou plan to use Amazon Managed Blockchain to implement a blockchain network that allows your company and third parties to share and validate rewards information quickly and transparently.\nHow do you add members to this blockchain?",
  "answer" : "Correct Answer - A.\nBy using Amazon Managed Blockchain, it is very convenient to manage members.\nNew members can be added to your own account without sending an invitation to yourself, or you can create a network invitation for a member in a different AWS account.\nRefer to https://docs.aws.amazon.com/managed-blockchain/latest/managementguide/get-started-joint-channel.html on how to “Invite Another AWS Account to be a Member and Create a Joint Channel”.\nOption A is CORRECT: Because for members in other AWS accounts, you need to send out an invitation.\nOption B is incorrect: Because you can invite members from other AWS accounts.\nOption C is incorrect: Because there is already an initial account while the blockchain is set up.\nOption D is incorrect: Because there is already an initial account while the blockchain is set up.\nAlso, an invitation is required for members in other AWS accounts.\nThe correct answer is A: When Amazon Managed Blockchain is set up, there is an initial member in the AWS account. Then new members can be added to this AWS account by sending an invitation or a network invitation can be created for a member in a different AWS account.\nAmazon Managed Blockchain is a fully managed service that makes it easy to create and manage scalable blockchain networks. It allows you to create blockchain networks using popular open source frameworks like Ethereum and Hyperledger Fabric.\nWhen you create a blockchain network using Amazon Managed Blockchain, there is an initial member in the AWS account. This member is responsible for creating the network and adding other members to the network. To add new members to the network, you can send them an invitation to join the network. The invitation can be sent to an email address or to an AWS account.\nIf you want to add a member from a different AWS account, you can create a network invitation for them. This creates a cross-account trust relationship between the two AWS accounts, allowing members from both accounts to join the network.\nIn summary, Amazon Managed Blockchain allows you to add new members to your blockchain network by sending invitations or creating network invitations for members in different AWS accounts.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "When Amazon Managed Blockchain is set up, there is an initial member in the AWS account. Then new members can be added to this AWS account by sending an invitation or a network invitation can be created for a member in a different AWS account.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "While Amazon Managed Blockchain is configured, there is an initial member in the AWS account. Then new members can be added to this AWS account without having to send an invitation. You cannot add new members from other AWS accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "When Amazon Managed Blockchain is created, there would be no member in the AWS account. Then new members can be added to this AWS account or other accounts by sending out an invitation.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "When Amazon Managed Blockchain is first created, there would be no member in the AWS account. Then new members can be added to this AWS account. For other accounts, they can join this net blockchain network by using the network I.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 341,
  "query" : "You are recently hired as an AWS architect in a startup company.\nThe company just developed an online photo-sharing product.\nAfter the product was deployed for some time, you have found that the instances in the Auto Scaling group have reached the maximum value from time to time due to the high CPU rate and traffic.",
  "answer" : "E.\nCorrect Answer - A, D.\nFor AWS WAF, an ACL can be configured to control the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront, or an Application Load Balancer.\nOption A is CORRECT: Because CloudFront distribution is a global resource that WAF ACL can be associated with.\nOption B is incorrect: Because the Auto Scaling group is not a valid resource that ACL can be linked with.\nOption C is incorrect: Same as Option.\nB.Option D is CORRECT: Because the application load balancer in a region can be selected for the ACL as below.\nOption E is incorrect: Because the network load balancer is not supported.\nFor the regions where the application load balancer is supported for AWS WAF ACL, please check the AWS document in.\nhttps://docs.aws.amazon.com/general/latest/gr/rande.html#waf_region.\nThe correct answer is B. The Auto Scaling group that the product has used.\nExplanation:\nAuto Scaling groups in AWS are used to automatically increase or decrease the number of instances running based on demand. The Auto Scaling group allows for setting thresholds for CPU utilization, memory usage, network traffic, and other metrics. When these thresholds are exceeded, new instances are launched, and when the thresholds are no longer met, instances are terminated.\nIn the case of the online photo-sharing product, it is likely that the high CPU rate and traffic are causing the instances in the Auto Scaling group to reach the maximum value. This means that the Auto Scaling group is not able to meet the demands of the users, and therefore, the company should consider adjusting the threshold settings in the Auto Scaling group to handle the increased load.\nOther options such as CloudFront distribution, load balancers, and individual EC2 instances are not likely to be the root cause of the issue. A global CloudFront distribution can help to distribute traffic and improve performance, but it will not directly address the issue of high CPU utilization in the Auto Scaling group. Load balancers can help distribute traffic, but they will not directly address the issue of the Auto Scaling group reaching its maximum value. Adding each instance ID to associate with the ACL is not a solution for this issue but rather a security measure. Finally, the network load balancer for the product in a specific region is not likely to be the root cause of the issue as it only manages traffic within that region.\nTherefore, the correct answer is B. The Auto Scaling group that the product has used.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The global CloudFront distribution that the product is using.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The Auto Scaling group that the product has used.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All EC2 instances created for the product. Add each instance ID to associate with the ACL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The application load balancer for the product.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The network load balancer for the product, however, only in US East (N. Virginia) region.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 342,
  "query" : "You work in a financial company as an AWS architect.\nThe security team has informed you that the company's AWS web product has recently been attacked by SQL injection.\nSeveral attackers tried to insert certain malicious SQL code into web requests to extract data from the MySQL database.\nThe database is deployed in several EC2 instances under an application load balancer.\nAlthough the attack was unsuccessful, you are expected to provide a better solution to protect the product.\nWhich action should you perform?",
  "answer" : "Correct Answer - B.\nThere are several firewall services that AWS has provided, including AWS WAF, AWS Shield, and AWS Firewall Manager.\nThe differences among them can be found in https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html.\nIn this case, AWS WAF should be used as it can help to prevent SQL injection attacks.\nOption A is incorrect: AWS Firewall Manager is a tool to simplify the AWS WAF and AWS Shield Advanced administration.\nHowever, an ACL is still needed in AWS WAF.\nOption B is more accurate.\nOption B is CORRECT: Because AWS WAF can protect against SQL injection attacks for an application load balancer.\nOption C is incorrect: Because AWS Shield Advanced provides expanded DDoS attack protection rather than SQL injection attack protection.\nOption D is incorrect: Because after a WAF Access Control List (ACL) is created, the application load balancer should be associated instead of all EC2 instances.\nThe best solution to protect the AWS web product against SQL injection attacks is to use a Web Application Firewall (WAF). A WAF is a security service that provides protection for web applications from common web exploits that could affect the availability, integrity, and confidentiality of the data. WAF is designed to inspect incoming traffic to web applications, and it can block malicious traffic based on predefined rules.\nAnswer B is the correct solution to this problem. Creating a WAF Access Control List (ACL) with a rule to block malicious SQL injection requests and associating the application load balancer with the new ACL will prevent SQL injection attacks on the web product. This solution has the following steps:\n1.\nCreate a new WAF ACL. In the AWS WAF console, create a new ACL, and add a rule to block SQL injection attacks. AWS provides pre-configured rule sets for common web application vulnerabilities, including SQL injection attacks. You can use these rule sets as a starting point and customize them as per the specific requirements of your web application.\n2.\nAssociate the new WAF ACL with the application load balancer. In the AWS console, associate the new WAF ACL with the application load balancer that is fronting the web product. This will enable the WAF to inspect all incoming traffic to the web product and block SQL injection attacks before they reach the EC2 instances running the MySQL database.\n3.\nTest the solution. Once the new WAF ACL is associated with the application load balancer, test the solution to ensure that the web product is protected against SQL injection attacks.\nOption A is incorrect because it suggests configuring a rule in AWS Firewall Manager to block all malicious SQL injection requests for the EC2 instances. Although Firewall Manager is a useful tool for managing network security policies, it is not designed to provide application-level security.\nOption C is also incorrect because AWS Shield Advanced service is designed to protect against Distributed Denial of Service (DDoS) attacks, not SQL injection attacks.\nOption D is incorrect because it suggests configuring a WAF ACL with a rule to allow all requests except malicious SQL injection requests and associating each EC2 instance with the new ACL. This approach is not scalable and can lead to inconsistent security policies across different instances. It is also not recommended to rely on the application-level security of individual instances as they may have different configurations and software stacks. It is more effective to use a centralized security service like a WAF to ensure consistent protection across all instances.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure a rule in AWS Firewall Manager to block all malicious SQL injection requests for the EC2 instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a WAF Access Control List (ACL) with a rule to block the malicious SQL injection requests. Associate the application load balancer with this new ACL.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS Shield Advanced service to block the malicious SQL injection requests that go to the application load balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a WAF Access Control List (ACL) with a rule to allow all requests except the malicious SQL injection requests. Associate each EC2 instance with the new ACL.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 343,
  "query" : "Which of the following must be done while generating a pre-signed URL in S3 to ensure that the user who is given the pre-signed URL has the permission to upload the object?",
  "answer" : "Answer - C.\nOption A is INCORRECT because this is not a required step.\nWith a pre-signed URL, the user itself does not need to have the write permission to S3.\nOption B is INCORRECT because this is not a required step.\nWith a pre-signed URL, the user itself does not need to have the read permission to S3.\nOption C is CORRECT because to upload an object to S3 successfully, the pre-signed URL must be created by someone who has permission to perform the operation that the pre-signed URL is based upon.\nOption D is INCORRECT because CloudFront distribution is not needed in this scenario.\nFor more information on pre-signed URL's, please visit the below URL.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\nWhen generating a pre-signed URL in S3, it is important to ensure that the user who is given the pre-signed URL has the permission to upload the object. To accomplish this, you need to follow these steps:\n1.\nEnsure the user has write permission to S3: The user who is given the pre-signed URL must have write permission to S3. You can grant this permission by using AWS Identity and Access Management (IAM) to create a policy that allows the user to write to the appropriate S3 bucket.\n2.\nEnsure the user has read permission to S3: Additionally, the user may also need read permission to S3 to download the object after it has been uploaded. You can grant this permission by using IAM to create a policy that allows the user to read from the appropriate S3 bucket.\n3.\nEnsure that the person who has created the pre-signed URL has the permission to upload the object to the appropriate S3 bucket: The person who creates the pre-signed URL must have permission to upload the object to the appropriate S3 bucket. This permission can be granted by using IAM to create a policy that allows the person to upload to the appropriate S3 bucket.\n4.\nCreate a CloudFront distribution (optional): Creating a CloudFront distribution is not necessary to ensure that the user who is given the pre-signed URL has the permission to upload the object. However, using CloudFront can help improve the performance and security of your S3 objects.\nIn summary, the correct answer to the question is A. Ensure the user has write permission to S3, and C. Ensure that the person who has created the pre-signed URL has the permission to upload the object to the appropriate S3 bucket. Answer B is not necessary to ensure that the user who is given the pre-signed URL has the permission to upload the object, and answer D is optional and not directly related to the permissions required for pre-signed URLs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Ensure the user has write permission to S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure the user has read permission to S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure that the person who has created the pre-signed URL has the permission to upload the object to the appropriate S3 bucket.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a CloudFront distribution.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 344,
  "query" : "A customer needs corporate IT governance and cost oversight of all AWS resources consumed by its divisions.\nThe divisions want to maintain administrative control of the discrete AWS resources they consume and keep those resources separate from the resources of other divisions.\nHow would you achieve this requirement?",
  "answer" : "Answers - A\nWe need to satisfy 2 requirements here.\n1\nTo provide either autonomy or control of divisions while maintaining IT Governance.\n2\nTo evaluate the overall cost.\nAWS Organizations enables you to consolidate multiple AWS accounts into an organization that you create and manage centrally.\nAWS Organizations also includes account management and consolidated billing capabilities.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html\nOptions B, C, and D do not satisfy the requirements of the scenario mentioned in the question.\nFor more details, please check below AWS Docs.\nhttps://aws.amazon.com/blogs/aws/aws-organizations-policy-based-management-for-multiple-aws-accounts/\nTo achieve the requirement of corporate IT governance and cost oversight of all AWS resources consumed by its divisions while maintaining administrative control of the discrete AWS resources they consume, the best solution is to use AWS Organizations. AWS Organizations is a service that allows you to consolidate multiple AWS accounts into an organization that you create and centrally manage.\nAnswer A:\nUsing AWS Organizations, you can manage AWS accounts, group the accounts into organizational units (OUs), and use the consolidated billing feature to consolidate billing and payment. This approach allows for centralized management of all AWS resources consumed by the divisions while also providing each division with administrative control over their own resources.\nBy grouping the accounts into OUs, you can apply policies to specific groups of accounts, such as security policies or cost management policies. This allows for more granular control over the resources consumed by each division. Using consolidated billing, you can consolidate the charges for all accounts in the organization into a single bill, making it easier to manage and track costs.\nAnswer B:\nCreating separate VPCs for each division within the AWS account is not the best solution for this requirement, as it does not provide centralized management of AWS resources across all divisions. Each VPC would need to be managed separately, making it difficult to implement consistent policies and track costs across all divisions.\nAnswer C:\nWriting all child AWS CloudTrail and Cloudwatch logs to each child account's Amazon S3 is not an effective solution for this requirement, as it does not provide centralized management of AWS resources across all divisions. It also does not provide the ability to implement consistent policies and track costs across all divisions.\nAnswer D:\nWriting all child AWS CloudTrail and Amazon CloudWatch logs to each child account's Amazon S3 'Log' bucket is also not an effective solution for this requirement, as it does not provide centralized management of AWS resources across all divisions. It also does not provide the ability to implement consistent policies and track costs across all divisions.\nIn summary, the best solution to achieve this requirement is to use AWS Organizations to manage AWS accounts, group the accounts into organizational units (OUs), and use the consolidated billing feature to consolidate billing and payment. This approach allows for centralized management of all AWS resources consumed by the divisions while also providing each division with administrative control over their own resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use AWS Organizations to manage AWS accounts, group the accounts into organizational units (OUs), and use the consolidated billing feature to consolidate billing and payment.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create separate VPC`s for each division within the AWS account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Write all child AWS CloudTrail and Cloudwatch logs to each child account`s Amazon S3 I.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Write all child AWS CloudTrail and Amazon CloudWatch logs to each child account`s Amazon S3 `Log` bucket.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 345,
  "query" : "A startup company just hired you as an AWS solutions architect.\nYour manager has assigned you a task to lower down AWS resources' costs due to a limited budget.\nIn the meantime, the service should not be impacted when the cost reduction activities are performed.\nFor example, idle load balancers can be safely deleted to save costs without service impact.\nWhich way can quickly help you to identify ways to optimize the cost?",
  "answer" : "Correct Answer - D.\nThere are various tools that are billing or cost-related.\nHowever, the quickest way to identify the possibilities to optimize costs is to use the Trusted Advisor Cost Optimization dashboard.\nOption A is incorrect: Because Cost Explorer does not directly tell which services can be adjusted to save cost.\nOption B is incorrect: Although AWS Cost and Usage Reports can provide the cost report in a detailed way, you still need to dig into the resources to differentiate which ones can help save cost.\nOption C is incorrect: This can help if a few resources are used.\nHowever, more analysis is required to understand how to optimize the spend if compared with Option.\nD.Option D is CORRECT: Because the Cost Optimization dashboard quickly checks several cost-related items, including Low Utilization Amazon EC2 Instances, Underutilized Amazon EBS Volumes, Idle Load Balancers, Unassociated Elastic IP Addresses, etc.\nAs an AWS Solutions Architect, your primary responsibility is to design and implement cost-effective solutions for your client. In this scenario, your client is a startup with limited resources, and your task is to lower AWS costs without impacting the service. To achieve this, you need to identify areas where you can optimize costs without affecting the service.\nThere are several ways to identify areas for cost optimization in AWS, but the most efficient ones are:\nA. Launch Cost Explorer which can graph, visualize, and analyze the spend and then identify the services to be optimized. Cost Explorer is an AWS service that provides a graphical view of your AWS usage and costs. It allows you to analyze your costs using custom reports, visualize your costs and usage trends, and identify areas for cost optimization. Cost Explorer provides detailed reports on your AWS usage, including usage by service, region, instance type, and more. With this information, you can identify areas where you can reduce costs without impacting the service.\nB. Configure AWS Cost and Usage Reports and then analyze the AWS costs as well as the specific product offerings and usage amounts underlying those costs. AWS Cost and Usage Reports provide a detailed breakdown of your AWS usage and costs. It enables you to analyze your costs and usage on a granular level and identify areas where you can optimize costs. AWS Cost and Usage Reports are available in CSV and JSON format, and you can configure them to receive regular reports via email or S3.\nC. Check the Billing details which list all AWS service charges. Identify and optimize the services that have an unreasonable cost. The billing details page provides a comprehensive breakdown of your AWS charges. It enables you to view your costs by service, region, and resource type, among others. By analyzing your billing details, you can identify areas where you can optimize costs. For example, you can identify unused resources, delete idle load balancers, or resize instances to match the workload.\nD. Search for the recommended actions in the Trusted Advisor Cost Optimization dashboard. Trusted Advisor is an AWS service that provides best practices and recommendations to optimize your AWS resources. The Cost Optimization dashboard provides recommendations to optimize your AWS usage and reduce costs. It provides recommendations such as deleting unused EBS volumes, resizing EC2 instances, and purchasing Reserved Instances. By following these recommendations, you can optimize costs without impacting the service.\nIn conclusion, AWS provides several services and tools to help you identify areas for cost optimization. By leveraging these tools, you can identify cost optimization opportunities, reduce your AWS spend, and ensure that your service continues to operate smoothly.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Launch Cost Explorer which can graph, visualize, and analyze the spend and then identify the services to be optimized.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure AWS Cost and Usage Reports and then analyze the AWS costs as well as the specific product offerings and usage amounts underlying those costs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check the Billing details which list all AWS service charges. Identify and optimize the services that have an unreasonable cost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Search for the recommended actions in the Trusted Advisor Cost Optimization dashboard.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 346,
  "query" : "A user has set up Auto Scaling with ELB on the EC2 instances.\nThe user wants to configure that whenever the CPU utilization is below 10%, Auto Scaling should remove one instance.\nHow can the user configure this?",
  "answer" : "Answer - D.\nOption A is incorrect because for the user to get the notification, they have to configure CloudWatch which triggers a notification to Auto Scaling Group to terminate the instance.\nUpdating the desired capacity will not work in this case.\nOption B is incorrect because scheduled scaling is used to scale your application in response to predictable load changes, not upon any notification.\nOption C is incorrect because the notification should be sent to Auto Scaling Group, not the launch configuration.\nOption D is CORRECT because the notification is sent to Auto Scaling Group, removing an instance from the running instances.\nMore information on Auto Scaling, Scheduled Actions:\nAuto Scaling helps you maintain application availability and allows you to scale your Amazon EC2 capacity up or down automatically according to conditions you define.\nYou can use Auto Scaling to help ensure that you are running your desired number of Amazon EC2 instances.\nAuto Scaling can also automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs.\nFor more information on AutoScaling, please visit the link -\nhttps://aws.amazon.com/autoscaling/ https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nThe correct answer is D. Configure a CloudWatch alarm in the execute policy that notifies the Auto Scaling group when the CPU Utilization is less than 10%, and configure the Auto Scaling policy to remove the instance.\nHere's why:\nAuto Scaling is a feature of Amazon Web Services (AWS) that allows users to automatically adjust the capacity of their EC2 instances based on demand. Auto Scaling can be configured to launch new instances when demand is high and terminate instances when demand is low. This helps to ensure that users are only paying for the resources they need at any given time.\nTo set up Auto Scaling with ELB, the user must create an Auto Scaling group and configure the launch configuration. The launch configuration defines the settings for the instances that will be launched by the Auto Scaling group. The user must also configure the Auto Scaling group to use ELB for load balancing.\nTo configure Auto Scaling to remove instances when the CPU utilization is below 10%, the user must use CloudWatch alarms. CloudWatch is a monitoring service that provides metrics and alarms for AWS resources. The user can create a CloudWatch alarm that monitors the CPU utilization of the EC2 instances in the Auto Scaling group. When the CPU utilization falls below 10%, the CloudWatch alarm will be triggered.\nThe user can then configure an Auto Scaling policy to remove an instance from the Auto Scaling group when the CloudWatch alarm is triggered. The policy should be set up to remove only one instance at a time. This will ensure that the Auto Scaling group scales down gradually and avoids any sudden drops in capacity.\nOption A is incorrect because it suggests using SNS to send an email when the CPU utilization is less than 10%. While SNS can be used to send notifications, it does not provide the functionality required to remove instances from an Auto Scaling group.\nOption B is incorrect because it suggests using scheduled actions to remove instances. Scheduled actions can be used to scale an Auto Scaling group up or down at specific times, but they cannot be used to respond to changes in demand in real-time.\nOption C is incorrect because it suggests configuring the CloudWatch alarm to notify the Auto Scaling launch configuration. The launch configuration defines the settings for the instances that will be launched by the Auto Scaling group, but it cannot be used to remove instances from the group.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The user can get an email using SNS when the CPU utilization is less than 10%. The user can use the desired capacity of Auto Scaling to remove the instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use CloudWatch to monitor the data and Auto Scaling to remove the instances using scheduled actions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a CloudWatch alarm in the execute policy that notifies the Auto Scaling Launch configuration when the CPU utilization is less than 10%, and configure the Auto Scaling policy to remove the instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a CloudWatch alarm in the execute policy that notifies the Auto Scaling group when the CPU Utilization is less than 10%, and configure the Auto Scaling policy to remove the instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 347,
  "query" : "An organization has added 3 of its AWS accounts to consolidated billing.\nOne of the AWS accounts has purchased a Reserved Instance (RI) of a small instance size in the US-East-1a zone.\nAll other AWS accounts are running instances of a small size in the same zone.\nWhat will happen in this case for the RI pricing?",
  "answer" : "Answer - C.\nOption A is incorrect because the price benefit of the reserved instances is applicable to all the accounts that are part of the consolidated billing group, not just the payer account (or the account that has reserved the instance) - for the total number of instances reserved.\nOption B is incorrect because, since only a single instance is reserved, any one instance across all the accounts will receive the reserved instance price benefit.\nOption C is CORRECT because the reserved price benefit will be applied to a single EC2 instance across all the accounts.\nOption D is incorrect because the total number of accounts that will receive the cost-benefit will be the same as the total number of reserved instances (in this case, it's one).\nMore information on Consolidated Billing:\nAs per the AWS documentation for billing purposes, AWS treats all the accounts on the consolidated bill as if they were one account.\nSome services, such as Amazon EC2 and Amazon S3, have volume pricing tiers across certain usage dimensions that give you lower prices when you use the service more.\nWith consolidated billing, AWS combines the usage from all accounts to determine which volume pricing tiers to apply, giving you a lower overall price whenever possible.\nFor more information on Consolidated billing, please visit the URL:\nhttp://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\nWhen an organization adds multiple AWS accounts to consolidated billing, the billing is consolidated into a single bill, and the accounts are treated as a single entity for billing purposes.\nIn the given scenario, one of the accounts has purchased a Reserved Instance (RI) of a small instance size in the US-East-1a zone. Reserved Instances provide a significant discount compared to On-Demand instances, but the discount applies only to instances that match the RI's specifications.\nThe other AWS accounts are running instances of a small size in the same zone. The question is about what will happen to the RI pricing in this case.\nOption A states that only the account that has purchased the RI will get the advantage of RI pricing. This option is incorrect because RI pricing benefits can be shared among accounts within consolidated billing.\nOption B states that one instance of a small size and running in the US-East-1a zone of each AWS account will benefit RI pricing. This option is also incorrect because the RI pricing benefit applies to instances that match the RI's specifications, and only one RI has been purchased.\nOption C states that any single instance from any of the three accounts can benefit AWS RI pricing if they are running in the same zone and are of the same size, but only one instance at a time since only 1 RI was purchased. This option is correct. Since the three accounts are under consolidated billing, any instance that matches the RI's specifications in the US-East-1a zone will get the RI pricing benefit, but only one instance at a time, as only one RI has been purchased.\nOption D states that all of the accounts will be able to benefit from the RI at the same time. This option is incorrect because the RI pricing benefit applies to instances that match the RI's specifications, and only one RI has been purchased.\nTherefore, the correct answer is option C. Any single instance from any of the three accounts can benefit AWS RI pricing if they are running in the same zone and are of the same size, but only one instance at a time since only 1 RI was purchased.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Only the account that has purchased the RI will get the advantage of RI pricing.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One instance of a small size and running in the US-East-1a zone of each AWS account will benefit RI pricing.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Any single instance from any of the three accounts can benefit AWS RI pricing if they are running in the same zone and are of the same size, but only one instance at a time since only 1 RI was purchased.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "All of the accounts will be able to benefit from the RI at the same time.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 348,
  "query" : "A user has configured an SSL(secure TCP) listener at a Classic Load Balancer for the back-end instances.\nWhich of the below mentioned statements helps the user understand ELB traffic handling with respect to the SSL listener?",
  "answer" : "Answer - D.\nOption A is invalid because if the front-end connection uses TCP or SSL, your back-end connections can use either TCP or SSL.\nIf the front-end connection uses HTTP or HTTPS, then your back-end connections can use either HTTP or HTTPS.\nOption B is invalid because when you use TCP/SSL for both front-end and back-end connections, your load balancer forwards the request to the back-end instances without modifying the headers.\nOption C is invalid because with this configuration you do not receive cookies for session stickiness.\nBut, when you use HTTP/HTTPS, you can enable sticky sessions on your load balancer.\nOption D is CORRECT because with SSL configuration the load balancer will forward the request to the back-end instances without modifying the headers.\nFor more information on ELB, please visit the link:\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html\nThe correct answer is D: ELB will not modify the headers.\nWhen a user configures an SSL listener at a Classic Load Balancer, the ELB acts as a pass-through for the SSL traffic, meaning the SSL traffic is terminated at the back-end instances rather than at the ELB. The SSL listener at the ELB is responsible for decrypting the SSL traffic and passing it along to the back-end instances in an unencrypted form.\nWhen the ELB passes traffic to the back-end instances, it does not modify any headers in the requests. This means that any headers that were present in the original request, such as user agent or referrer, will be passed through to the back-end instances unchanged.\nFurthermore, when sticky sessions are enabled, ELB will add a cookie to the request to ensure that subsequent requests from the same client are sent to the same back-end instance. This cookie is added as a separate header and does not modify any existing headers.\nTo summarize, when an SSL listener is configured at a Classic Load Balancer, ELB acts as a pass-through for the SSL traffic and does not modify any headers in the requests. If sticky sessions are enabled, ELB adds a cookie as a separate header to the request to ensure that subsequent requests from the same client are sent to the same back-end instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It is not possible to have the SSL listener both at ELB and back-end instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "ELB will modify headers to add requestor details.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "ELB will intercept the request to add the cookie details if sticky session is enabled.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "ELB will not modify the headers.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 349,
  "query" : "In a large company, you work as an AWS administrator.\nFor a Windows SQL server instance, there is already a daily task to regularly transfer the database backup files to an S3 bucket (DB_Backup_1) in AWS account 111111111111\nOne data scientist asks you if it is possible to copy the latest backup file to another S3 bucket (DB_Backup_2) in his AWS account 222222222222\nYou plan to use AWS S3 CLI to do this.\nWhich combinations of methods can accomplish this mission? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A, C.\nTwo items are required to enable the copy from an S3 bucket in the source account to another S3 bucket in the target account.\n1, In the source AWS account, there is a bucket policy to allow the target account to ListBucket and GetObject.\n2, In the target AWS account, there is an IAM policy for the IAM user (or group) to allow the user to copy the file from the source bucket (in source AWS account) to the target bucket (in target AWS account).\nOption A is CORRECT: One example of this bucket policy is as below.\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"DelegateS3Access\",\n\"Effect\": \"Allow\",\n\"Principal\": {\"AWS\": \"222222222222\"},\n\"Action\": [\"s3:ListBucket\",\"s3:GetObject\"],\n\"Resource\": [\n\"arn:aws:s3:::sourcebucket/*\",\n\"arn:aws:s3:::sourcebucket\"\n]\n}\n]\n}\nSome more details can be found in https://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-objects-account/.\nOption B is incorrect: Because it should be a bucket policy rather than an ACL policy.\nBesides, s3:GetObject is needed instead of s3:PutObject.\nOption C is CORRECT: Because this allows the data scientist user to copy files (GetObject ) from the source bucket and put files (PutObject) in the target bucket.\nOption D is incorrect: Because there is no need in the target account to put a bucket policy.\nOption E is incorrect: Because in the target account, no Access Control List (ACL) policy is required.\nInstead, an IAM policy should exist to allow the file copy.\nTo copy the latest backup file from the DB_Backup_1 bucket in account 111111111111 to the DB_Backup_2 bucket in account 222222222222, we need to ensure that the destination account has the necessary permissions to access and copy the objects.\nOption A: In account 111111111111, attach a bucket policy to DB_Backup_1 that allows the destination account 222222222222 to do the actions of \"s3:ListBucket\" and \"s3:GetObject\". This option will allow the destination account to list the contents of the DB_Backup_1 bucket and retrieve the latest backup file. However, it does not provide permissions to copy the file to another bucket. Therefore, this option alone cannot accomplish the mission.\nOption B: In account 111111111111, attach an Access Control List (ACL) policy to DB_Backup_1 that allows the destination account 222222222222 to do the actions of \"s3:ListBucket\" and \"s3:PutObject\". This option will allow the destination account to list the contents of the DB_Backup_1 bucket, retrieve the latest backup file, and copy it to the DB_Backup_2 bucket. Therefore, this option, in combination with option C or D, can accomplish the mission.\nOption C: In account 222222222222, attach an IAM policy to the IAM user or group that allows the data scientist user to copy objects from source bucket DB_Backup_1 to bucket DB_Backup_2. This option will grant the IAM user or group in the destination account permissions to copy objects from the source bucket DB_Backup_1 to the destination bucket DB_Backup_2. Therefore, this option, in combination with option B or D, can accomplish the mission.\nOption D: In account 222222222222, attach a bucket policy to DB_Backup_2 that allows the destination account 222222222222 to do the actions of \"s3:ListBucket\" and \"s3:PutObject\". This option will allow the destination account to list the contents of the DB_Backup_2 bucket and copy the latest backup file to it. Therefore, this option, in combination with option B or C, can accomplish the mission.\nOption E: In account 222222222222, attach an Access Control List (ACL) policy to DB_Backup_2 that allows the source account 111111111111 to do the actions of \"s3:ListBucket\", \"s3:GetObject\" and \"s3:ListObject\". This option grants permissions to the source account to list the contents of the destination bucket and retrieve the latest backup file. However, it does not provide permissions to copy the file to another bucket. Therefore, this option alone cannot accomplish the mission.\nTherefore, the correct combinations of methods that can accomplish this mission are B and C, or B and D.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In account 111111111111, attach a bucket policy to DB_Backup_1 that allows the destination account 222222222222 to do the actions of \"s3:ListBucket\" and \"s3:GetObject\".",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In account 111111111111, attach an Access Control List (ACL) policy to DB_Backup_1 that allows the destination account 222222222222 to do the actions of \"s3:ListBucket\" and \"s3:PutObject\".",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In account 222222222222, attach an IAM policy to the IAM user or group that allows the data scientist user to copy objects from source bucket DB_Backup_1 to bucket DB_Backup_2.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In account 222222222222, attach a bucket policy to DB_Backup_2 that allows the destination account 222222222222 to do the actions of \"s3:ListBucket\" and \"s3:PutObject\".",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In account 222222222222, attach an Access Control List (ACL) policy to DB_Backup_2 that allows the source account 111111111111 to do the actions of \"s3:ListBucket\", \"s3:GetObject\" and \"s3:ListObject\".",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 350,
  "query" : "An organization has created one IAM user and applied the below mentioned policy to the user.\nWhat entitlements do the IAM users avail with this policy?",
  "answer" : "Answer - D.\nAWS Identity and Access Management is a web service that allows organizations to manage users and user permissions for various AWS services.\nIf an organization wants to set up read-only access to EC2 for a particular user, they should mention the action in the IAM policy which entitles the user for Describe rights for EC2, CloudWatch, Auto Scaling, and ELB.\nThe user will have read-only access for EC2, CloudWatch, and Auto Scaling in the policy shown below.\nSince ELB is not mentioned as a part of the list, the user will not have access to ELB.The above policy will allow the user to view EC2 instances (look at Auto Scaling and CloudWatch).\nOptions A, B and C are incorrect because the policy does not include ELB permissions.\nFor more information on IAM policy, please visit the URL:\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\nThe policy mentioned in the question is not provided. Without the policy, it's not possible to provide a definitive answer. However, I can provide an explanation of IAM policies and how they work in AWS.\nIAM (Identity and Access Management) is a service that allows you to manage users, groups, and permissions in AWS. IAM policies are JSON documents that define permissions for IAM users, groups, and roles. Policies can be attached to users, groups, and roles to grant or deny access to AWS resources.\nIAM policies consist of a set of permissions called \"actions\" and a set of resources to which those actions can be performed. Actions represent individual operations that can be performed on a resource. Resources represent AWS resources such as EC2 instances, S3 buckets, or RDS databases.\nThere are two types of IAM policies: managed policies and inline policies. Managed policies are standalone policies that can be attached to multiple users, groups, and roles. Inline policies are policies that are created and embedded directly into a user, group, or role.\nIn order to determine what entitlements an IAM user has with a particular policy, you need to look at the actions and resources defined in the policy. The actions in the policy will determine what the user is allowed to do, and the resources will determine which AWS resources the user is allowed to perform those actions on.\nIn summary, without the policy that was mentioned in the question, it is not possible to provide a definitive answer to what entitlements the IAM user will have.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The policy will allow the user to perform all read-only activities on the EC2 services including EC2 instances, Auto Scaling groups and ELBs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The policy will allow the user to list all the EC2 resources including EC2 instances, Auto Scaling groups and ELBs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The policy will allow the user to perform all read and write activities on the EC2 services including EC2 instances, Auto Scaling groups and ELBs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The policy will allow the user to perform the read-only activities on the EC2 instances and Auto Scaling groups.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 351,
  "query" : "A company is creating a new recipe blog sharing application in AWS.\nThe static content has been put into an S3 bucket.\nIn the meantime, a database is needed to store the data for various blog posts.\nThe application retrieves data from the database and dynamically renders a recipe blog page.",
  "answer" : "E.\nF.\nCorrect Answer - B, D, E.\nThe main requirement for this question is that no extra efforts are needed for origin infrastructure, automatic scaling, backups, and database redundancy.\nProper AWS services should be chosen to align with this rule.\nHence, options having EC2 with ELB and Auto Scaling can be safely eliminated.\nOption A is incorrect: Because ELB and Auto Scaling do not help to route traffic to users globally.\nOption B is CORRECT: Because CloudFront has over 100 Edge Locations and is a proper CDN to deliver content to end-users with lower latency globally.\nOption C is incorrect: Same reason as.\nOption A as EC2 is eliminated.\nOption D is CORRECT: Because Lambda@Edge can work together with CloudFront to build dynamic web applications that automatically scale up and down.\nThere is also no need to build up servers to hold the application code.\nOption E is CORRECT: Because DynamoDB database is a fully managed and high-performance database.\nDynamoDB automatically scales tables to adjust for capacity and maintain performance.\nOption F is incorrect: RDS needs Read Replica and Multi-AZ to be scalable, which brings extra efforts.\nThere is also no need to use a relational database for this case.\nOption E (DynamoDB) is better and is a pure serverless solution together with other valid options.\nThe company is creating a new recipe blog sharing application in AWS, and they have already put their static content into an S3 bucket. Now they need a database to store the data for various blog posts, and the application will retrieve data from the database and dynamically render a recipe blog page. There are several options available to address this requirement, which are listed below:\nA. Configure an Elastic Load Balancer and Auto Scaling group to balance the traffic dynamically.\nAn Elastic Load Balancer (ELB) can be used to distribute traffic across multiple instances of an application, and an Auto Scaling group can automatically scale up or down the number of instances based on demand. This solution can be useful in handling variable traffic loads, but it doesn't address the requirement for a database.\nB. Configure an AWS CloudFront to serve end-users from various Edge Locations.\nAWS CloudFront is a content delivery network that can cache content at Edge Locations, reducing the time it takes for content to reach end-users. This solution can help with performance and scalability, but it doesn't address the requirement for a database.\nC. Create an Auto Scaling configuration for EC2 instances to automatically scale up and down according to the load.\nThis solution is similar to Option A, but instead of using an ELB, it involves directly managing EC2 instances. While this approach can be effective in handling variable traffic loads, it doesn't address the requirement for a database.\nD. Run the application code in Lambda@Edge to retrieve data from the database, and the response is also cached and served by CloudFront.\nLambda@Edge is a serverless computing service that runs code in response to events at AWS Edge locations. This solution involves running the application code in Lambda@Edge to retrieve data from the database, and the response is cached and served by CloudFront. This solution can provide low-latency access to data and can be useful in handling variable traffic loads, but it may not be suitable for applications that require a significant amount of processing or have long-running tasks.\nE. Use DynamoDB to store data for blog posts that the application is using.\nDynamoDB is a fully managed NoSQL database service that can be used to store and retrieve data. This solution involves using DynamoDB to store data for blog posts that the application is using. This solution can provide fast and scalable access to data, but it may not be suitable for applications that require complex queries or relational data.\nF. For the dynamic data including blog posts for customers, configure an RDS instance as the database.\nAmazon RDS is a managed database service that can be used to deploy and operate relational databases. This solution involves configuring an RDS instance as the database for the dynamic data, including blog posts for customers. This solution can provide the benefits of a relational database, including complex queries and relational data, but it may not be suitable for applications that require high scalability.\nIn summary, there are several options available to address the requirement for a database to store the data for various blog posts in a recipe blog sharing application in AWS. The best solution will depend on the specific needs of the application, including performance, scalability, and data requirements. Options D, E, and F are the most likely candidates, depending on the specific needs of the application.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an elastic load balancer and Auto Scaling group to balance the traffic dynamically.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an AWS CloudFront to serve end users from various Edge Locations.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an Auto Scaling configuration for EC2 instances to automatically scale up and down according to the load.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Run the application code in Lambda@Edge to retrieve data from the database, and the response is also cached and served by CloudFront.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use DynamoDB to store data for blog posts that the application is using.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "For the dynamic data including blog posts for customers, configure an RDS instance as the database.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 352,
  "query" : "An organization has configured Auto Scaling with ELB.\nThere is a memory issue in the application which is causing CPU utilization to go above 90%\nThe higher CPU usage triggers an event for Auto Scaling as per the scaling policy.\nIf the user wants to find the root cause inside the application without triggering a scaling activity, how can he achieve this?",
  "answer" : "Answer - D.\nIn this scenario, the user wants to investigate the problem during the Auto Scaling process without triggering the scaling activity.\nFor this, the user can leverage the suspend and resume option available on Auto Scaling.\nOption A is incorrect because the scaling process needs not to be stopped.\nIt can be suspended so that it can be resumed.\nOption B is incorrect because scaling can be momentarily suspended until the investigation is completed.\nOption C is incorrect because the Auto Scaling group is totally unnecessary in this scenario.\nOption D is CORRECT because you can suspend and then resume one or more of the scaling processes for your Auto Scaling group, if you want to investigate a configuration problem or other issue with your web application and then make changes to your application without triggering the scaling processes.\nFor more information on suspending AutoScaling processes, please visit the link.\nhttp://docs.aws.amazon.com/autoscaling/latest/userguide/as-suspend-resume-processes.html\nIn this scenario, the user wants to investigate the root cause of the high CPU utilization in the application without triggering a scaling activity. To achieve this, the user can suspend the scaling process until the research is completed.\nOption A, which suggests stopping the scaling process, is not the best option as it could affect the availability of the application. Stopping the scaling process means that instances will not be launched or terminated in response to changes in demand. This could result in insufficient resources to handle traffic spikes, leading to application downtime.\nOption B, which states that it is not possible to find the root cause from that instance without triggering scaling, is incorrect. It is possible to investigate the root cause of the high CPU utilization by analyzing logs and metrics collected from the instance.\nOption C, which suggests deleting the Auto Scaling group until research is completed, is also not the best option. Deleting the Auto Scaling group means that all instances will be terminated, resulting in downtime for the application. Moreover, if the user needs to launch new instances later, they will have to recreate the Auto Scaling group from scratch.\nOption D, which suggests suspending the scaling process until research is completed, is the best option. Suspending the scaling process means that Auto Scaling will not launch or terminate instances in response to changes in demand. This allows the user to investigate the root cause of the high CPU utilization without any interference from the scaling process. Once the research is completed, the user can resume the scaling process if needed.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Stop the scaling process until research is completed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It is not possible to find the root cause from that instance without triggering scaling.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Delete the Auto Scaling group until research is completed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Suspend the scaling process until research is completed.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 353,
  "query" : "A company has 2 accounts- one is a development account, and the other is a production account.\nThere are 20 people on the development account who now need various access levels provided to them on the production account.\n10 of them need read-only access to all resources on the production account, 5 of them need read/write access to EC2 resources, and the remaining 5 only need read-only access to S3 buckets.\nWhich of the following options would be the best way for both practical and security-wise to accomplish this task?",
  "answer" : "Answer - A.\nOption A is CORRECT because it creates 3 roles according to the need in the production account and adds the permissions to each of the IAM Users in the development account to assume those roles accordingly in the production account.\nOption B is incorrect because you should be creating IAM Roles in the production account, and the development users should assume those roles.\nThis option is suggesting creating 3 separate users in the production account which is incorrect.\nOption C is incorrect because encryption keys are unnecessary and will not work in this scenario.\nOption D is incorrect because the creation of the IAM user accounts in the production account is unnecessary.\nYou should be creating IAM Roles instead.\nFor more information on \"Delegating Access Across AWS Accounts Using IAM Role\" - please refer to the below link.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nThe best way to provide the required access levels to the 20 users on the development account for the production account would be option A: Create 3 roles in the production account with a different policy for each of the access levels needed. Add permissions to each IAM User in the development account to assume a role on the production account based on the type of access needed.\nExplanation:\nOption A involves creating roles in the production account with specific policies for each level of access required by the 20 users. The roles will be associated with an AWS Identity and Access Management (IAM) policy, which grants permissions to the users. The IAM policy can be used to define the specific actions, resources, and conditions that the user is allowed or denied to access.\nOnce the roles are created, permissions to assume the roles need to be added to the IAM users in the development account. The IAM users in the development account will use temporary security credentials to access the resources in the production account. This approach ensures that access to resources in the production account is strictly controlled and monitored, enhancing the security of the production account.\nOption B involves creating new users on the production account with varying levels of access, and then providing login credentials to the 20 users based on the access level required. This approach is not scalable and requires additional management overhead to manage the users and their access levels. Furthermore, this approach does not provide fine-grained control over permissions and may result in users being granted excessive access.\nOption C involves creating encryption keys for each resource that needs access and providing these keys to the users based on the access level required. This approach is impractical as it requires creating and managing encryption keys for each resource, and the keys may need to be frequently rotated. Additionally, this approach does not provide fine-grained control over permissions, and it may be difficult to revoke access once granted.\nOption D involves copying the IAM accounts of the 20 users from the development account to the production account and then changing the access levels for each user on the production account. This approach is not recommended as it may result in users being granted excessive access to resources in the production account. Additionally, it does not provide a separation of duties between the development and production accounts, which is important for security and compliance reasons.\nIn summary, option A is the best approach as it provides fine-grained control over permissions, is scalable, and enhances the security of the production account by strictly controlling access to resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create 3 roles in the production account with a different policy for each of the access levels needed. Add permissions to each IAM User in the development account to assume a role on the production account based on the type of access needed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create 3 new users on the production account with the various levels of permissions needed. Give each of the 20 users the login for whichever one of the 3 users they need depending on the level of access required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create encryption keys for each of the resources that need access and provide those keys to each user depending on the access required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Copy the 20 users IAM accounts from the development account to the production account. Then change the access levels for each user on the production account.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 354,
  "query" : "A traditional IT company is actively migrating its existing on-premises servers to AWS and making big progress.\nRecently, to meet the internal regulatory requirements, backup policies and strategies are needed for its various AWS services.\nThe company plans to use the AWS Backup service as the central place to manage and automate backups.\nWhich AWS services can be managed by AWS Backup? (Select FOUR.)",
  "answer" : "E.\nF.\nCorrect Answer -A, D, E and F.\nAWS Backup is a central place to set up backup policies and monitor backup activities for AWS resources, including Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems, and AWS Storage Gateway volumes.\nThe details can be found in https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html.\nOption A is CORRECT: Because all AWS RDS types are supported except Amazon Aurora.\nOption B is incorrect: Because S3 is a highly scalable, reliable service which does not need backups from AWS Backup service.\nOption C is incorrect: Because Amazon Redshift is not a supported service for AWS Backup, according to the above.\nOption D is CORRECT: Because AWS Backup does support Amazon EFS.\nOption E is CORRECT: Because EBS volumes can be saved via snapshots which are manageable by AWS Backup.\nOption F is CORRECT: AWS Storage Gateway is a storage service that enables the on-premises applications to use AWS cloud storage.\nAWS Backup can integrate with AWS Storage Gateway and help design a hybrid backup plan for the services both in the cloud and on-premises.\nAWS Backup is a fully managed backup service by AWS that centralizes and automates the backup of data across AWS services. It provides a simple and cost-effective way to backup and restore data from various AWS services, ensuring data protection and compliance.\nThe AWS services that can be managed by AWS Backup are as follows:\nA. An AWS RDS MariaDB instance (db.r5.large): AWS Backup can backup and restore Amazon RDS instances, including Amazon RDS for MariaDB. With AWS Backup, you can create backup plans to specify how often and for how long you want to backup the RDS instance.\nB. S3 buckets: AWS Backup can backup Amazon S3 buckets, including all objects stored in the bucket. AWS Backup can also restore the entire bucket or individual objects from a specific backup.\nC. An AWS Redshift cluster: AWS Backup can backup and restore Amazon Redshift clusters, including all data stored in the cluster. With AWS Backup, you can create backup plans to specify how often and for how long you want to backup the Redshift cluster.\nD. Amazon Elastic File System (EFS) volumes: AWS Backup can backup and restore Amazon EFS volumes, including all data stored in the EFS volume. With AWS Backup, you can create backup plans to specify how often and for how long you want to backup the EFS volume.\nE. Several EBS volumes which are not encrypted: AWS Backup can backup and restore Amazon EBS volumes, including all data stored in the EBS volume. With AWS Backup, you can create backup plans to specify how often and for how long you want to backup the EBS volume. However, it's important to note that EBS volumes must be encrypted for AWS Backup to manage them.\nF. AWS Storage Gateway service (Volume Gateway): AWS Backup can backup and restore the data stored on AWS Storage Gateway volumes, including tape, file, and volume gateway. With AWS Backup, you can create backup plans to specify how often and for how long you want to backup the Storage Gateway volumes.\nIn conclusion, the AWS services that can be managed by AWS Backup are RDS MariaDB instance, S3 buckets, Redshift cluster, EFS volumes, and Storage Gateway service (Volume Gateway).",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "An AWS RDS MariaDB instance (db.r5.large).",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "S3 buckets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An AWS Redshift cluster.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon Elastic File System (EFS) volumes.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Several EBS volumes which are not encrypted.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Storage Gateway service (Volume Gateway).",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 355,
  "query" : "Your company has configured an AWS organization with a master account and several organization units (OU) for its various R&D departments.\nThere is an S3 bucket owned by AWS account A that needs to be accessed by one IAM user that belongs to another AWS account B.\nAccount B is outside the organization.\nThe S3 bucket policy already granted access to this account B user.\nIn account B, the user has the IAM permissions to read the bucket.\nHowever, AWS account A has a Service Control Policy (SCP) attached to allow the bucket access only from account A users.\nIs the IAM user in account B able to read the files in the bucket successfully?",
  "answer" : "Correct Answer - D.\nWhen the Service Control Policy (SCP) is configured, only those IAM users that are within the organization are affected.\nIn other words, SCP does not affect users or roles if they are outside the organization.\nFor various SCP effects on permissions, refer to https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html.\nOption A is incorrect: Because SCP does not affect the user in account B since it does not belong to the Organization.\nOption B is incorrect: Because S3 cross-accounts access is possible as long as enough IAM permissions are granted.\nOption C is incorrect: Because there is no need to allocate a new OU for account.\nB.Option D is CORRECT: Because the user in account B can read the files in the bucket as SCP policy does not influence it.\nAbout the details on how to copy S3 objects from another AWS account, check the AWS tutorial in.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-objects-account/.\nThe correct answer is A. No, because the SCP policy takes priority and disallows the bucket access from the user in account B.\nExplanation:\nAn AWS organization is a collection of AWS accounts that are linked together. It allows for centralized management of multiple AWS accounts. AWS Organizations can help to simplify billing, security, and compliance. AWS Organizations provides a hierarchical structure for the accounts, with a master account at the top and organizational units (OUs) beneath it.\nIn this scenario, there is an S3 bucket owned by AWS account A that needs to be accessed by an IAM user that belongs to another AWS account B. The S3 bucket policy already granted access to this account B user. In account B, the user has the IAM permissions to read the bucket. However, AWS account A has a Service Control Policy (SCP) attached to allow the bucket access only from account A users.\nAn SCP is a type of policy that you can use to manage permissions in an AWS organization. It is a policy that is attached to an organizational unit (OU) or the root of the organization. SCPs allow you to define which AWS services and actions are available to accounts in an organization. SCPs do not grant permissions, but rather limit permissions.\nIn this scenario, the SCP attached to the AWS account A takes priority over the S3 bucket policy. This means that even though the S3 bucket policy granted access to the IAM user in account B, the SCP attached to AWS account A disallows the bucket access from the user in account B.\nTherefore, the correct answer is A. No, because the SCP policy takes priority and disallows the bucket access from the user in account B.\nOption B is incorrect because S3 cross-account access can be allowed for IAM users outside an AWS organization.\nOption C is incorrect because adding a new OU for account B and then configuring an SCP in the new OU is not required in this scenario.\nOption D is incorrect because even though the SCP doesn't apply to users outside the AWS organization, the S3 bucket policy is overruled by the SCP policy in this scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "No, because the SCP policy takes priority and disallows the bucket access from the user in account",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No, because S3 cross-accounts access can only be allowed for AWS IAM users within an AWS Organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "No, the AWS organization should add a new OU for account B and then configure an SCP in the new OU to allow access to the S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Yes. Since SCP doesn`t apply to those outside users, the user in account B has the permission to access the files in the bucket.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 356,
  "query" : "An IT company has owned several AWS accounts that belong to an AWS Organization.\nThe root account and all children accounts have configured Service Control Policies (SCPs) to help manage the organization.\nRecently, an IAM user in a child account needs the permissions to enable its Amazon VPC Flow Logs.\nUnder which configurations can the user operate the VPC Flow Logs successfully? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, D.\nAn SCP policy can limit the permissions for all entities in its member accounts, which means the root account policy affects all the children's accounts.\nAnd the child account has only those permissions permitted by every parent above it.\nIn this case, to have the permissions to enable VPC Flow Logs, the root account SCP, the child account SCP, and IAM policy should all allow the action.\nOption A is incorrect: Because the child account SCP blocks the action.\nOption B is CORRECT: Because all parties allow the action according to the above.\nOption C is incorrect: Because users or roles must be granted permissions using IAM permission policies even if SCP policies allow the actions.\nOption D is CORRECT: Because the FullAWSAccess policy is a default one that allows everything including VPC Flow Logs.\nOption E is incorrect: Because the action is blocked by the root account SCP even if the child SCP allows it.\nThe correct answers are A and D.\nExplanation: In an AWS Organization, Service Control Policies (SCPs) can be used to manage and restrict access to AWS services and actions across the organization. SCPs are applied at the organizational unit (OU) level, and they can be attached to the root of the organization, as well as individual OUs.\nIn this scenario, an IAM user in a child account needs permissions to enable its Amazon VPC Flow Logs. To enable VPC Flow Logs, the user needs permissions at two levels: an SCP that permits enabling VPC Flow Logs and an IAM policy that allows the user to perform this action.\nAnswer A is correct because the SCP for the root account permits enabling VPC Flow Logs, but the SCP for the child account does NOT permit it. Therefore, the user needs the IAM permission policy to enable VPC Flow Logs to perform this action. With these two configurations, the user will be able to operate VPC Flow Logs successfully.\nAnswer B is incorrect because both the SCP for the root account and the child account permit enabling VPC Flow Logs, but it doesn't mention anything about the user's IAM permission policy. Therefore, we cannot assume that the user has the required permissions to enable VPC Flow Logs.\nAnswer C is incorrect because the SCP for the root account permits all actions with the default FullAWSAccess policy. However, the user does NOT have the IAM permission policy to enable VPC Flow Logs, which is a required permission to perform this action.\nAnswer D is correct because the SCP for the root account permits all actions with the default FullAWSAccess policy, and the child account permits enabling VPC Flow Logs. Therefore, the user needs only the IAM permission policy to enable VPC Flow Logs to perform this action.\nAnswer E is incorrect because the SCP for the root account does NOT permit enabling VPC Flow Logs, and the user does NOT have the IAM permission policy to perform this action. Although the SCP for the child account permits enabling VPC Flow Logs, the user still needs the IAM permission policy to perform this action.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The SCP for the root account permits enabling VPC Flow Logs. The SCP for the child account does NOT permit enabling VPC Flow Logs. The user has the IAM permission policy to enable VPC Flow Logs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The SCPs for both root account and the child account permit enabling VPC Flow Logs. The user has the IAM permission policy to enable VPC Flow Logs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The SCP for the root account permits all actions with default FullAWSAccess policy. The user does NOT have the IAM permission policy to enable VPC Flow Logs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The SCP for the root account permits all actions with default FullAWSAccess policy. The child account permits enabling VPC Flow Logs. The user has the IAM permission policy to enable VPC Flow Logs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The SCP for the root account does NOT permit enabling VPC Flow Logs. The SCP for the child account permits enabling VPC Flow Logs. The user does NOT have the IAM permission policy to enable VPC Flow Logs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 357,
  "query" : "An organization (Account ID 123412341234)\nhas attached the below-mentioned IAM policy to a user.\nWhat does this policy statement entitle the user to perform?",
  "answer" : "Answer - D.\nFirst, to give a user a certain set of policies, you need to mention the following line.\nThe aws:username will apply to the AWS logged in user.\nResource\": \"arn:aws:iam::account-id-without-hyphens:user/${aws:username}\nNext, the policies will give the permissions to modify the IAM user's password, sign in certificates, and access keys.\n“iam:*LoginProfile”,\n“iam:*AccessKey*”,\n“iam:*SigningCertificate*”\nFor information on IAM security policies, please visit the link:\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\nThe IAM policy statement attached to the user in question is not provided in the question. Without the policy statement, it is not possible to determine what actions the user is entitled to perform.\nHowever, if we assume that the policy statement is provided and looks like the following:\njsonCopy code{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"iam:ChangePassword\",                 \"iam:CreateLoginProfile\",                 \"iam:DeleteLoginProfile\",                 \"iam:GetLoginProfile\",                 \"iam:ListMFADevices\",                 \"iam:CreateVirtualMFADevice\",                 \"iam:DeleteVirtualMFADevice\",                 \"iam:EnableMFADevice\",                 \"iam:ResyncMFADevice\",                 \"iam:UpdateLoginProfile\"             ],             \"Resource\": \"*\"         }     ] } \nThen, the policy statement allows the user to perform certain IAM actions related to password and multi-factor authentication (MFA) management. These actions include changing the password, creating, deleting, and getting login profiles, listing and creating virtual MFA devices, enabling and resyncing MFA devices, and updating login profiles.\nHowever, the user is not allowed to modify all IAM user credentials using the console, SDK, CLI, or APIs as option A suggests. Similarly, option B is incorrect as it implies that the policy statement is invalid, which is not the case. Option C is incorrect as it limits the actions to only be performed through the console, while the policy statement includes actions that can be performed using the CLI and APIs as well. Therefore, option D is the closest to the correct answer, but it is important to note that the policy statement also allows the user to manage virtual MFA devices.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The policy allows the IAM user to modify all IAM user’s credentials using the console, SDK, CLI, or APIs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The policy will give an invalid resource error.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The policy allows the IAM user to modify all credentials using only the console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The policy allows the user to modify the IAM user’s password, signing certificates and access keys only.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 358,
  "query" : "You are working on a proof of concept serverless project and presenting it to the shareholders in a week.\nThis project needs an API gateway, a Lambda function, and a DynamoDB table to store user data.\nTo save time, you plan to use AWS Serverless Application Model (AWS SAM) as it provides templates to deploy all required resources quickly.\nYou have found that SAM templates are very similar to CloudFormation templates.\nHowever, which resource types are specially introduced by the SAM template? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, D.\nAWS SAM is an extension of AWS CloudFormation.\nA SAM serverless application is defined in a CloudFormation template and deployed as a CloudFormation stack.\nThe AWS SAM template can be regarded as a CloudFormation template.\nHowever, it has its own special resources.\nCheck https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessapplication for details.\nNote: to include objects defined by AWS SAM, the template must include a Transform section in the document with a value of AWS::Serverless-2016-10-31.\nOption A is incorrect: Because AWS::DynamoDB::Table is not SAM special and belongs to the CloudFormation resource type.\nOption B is CORRECT: Because AWS::Serverless::Api is designed for API gateway resource in the SAM framework.\nOption C is incorrect: Similar to.\nOption A.Option D is CORRECT: Because AWS::Serverless::Function is the SAM resource that creates a Lambda function, IAM execution role, and event source mappings.\nOption E is incorrect: Because AWS::ApiGateway::RestApi also belongs to the CloudFormation resource type.\nRefer to https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-apigateway-restapi.html.\nThe AWS Serverless Application Model (SAM) is an open-source framework used for building serverless applications. It provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed for your serverless application. SAM templates are very similar to AWS CloudFormation templates, but they include some additional resource types that are specifically designed for serverless applications.\nThe SAM template includes the following resource types that are specifically designed for serverless applications:\n1.\nAWS::Serverless::Function: This resource type is used to define a serverless function, which is executed by AWS Lambda. This resource type includes properties such as the function code location, runtime environment, and event triggers.\n2.\nAWS::Serverless::Api: This resource type is used to define an Amazon API Gateway REST API that integrates with a serverless function. This resource type includes properties such as the API Gateway stage name, endpoint configuration, and authorization settings.\nIn contrast, AWS::ApiGateway::RestApi is a CloudFormation resource type used to define an Amazon API Gateway REST API, but it is not specifically designed for serverless applications. AWS::Lambda::Api is not a valid resource type.\nAWS::DynamoDB::Table is a CloudFormation resource type used to define an Amazon DynamoDB table, but it is not specifically designed for serverless applications. However, the SAM template includes a Serverless::SimpleTable resource type that simplifies the creation of DynamoDB tables in serverless applications.\nTherefore, the correct answer is: B. AWS::Serverless::Api D. AWS::Serverless::Function",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS::DynamoDB::Table",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS::Serverless::Api",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS::Lambda::Api",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS::Serverless::Function",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS::ApiGateway::RestApi.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 359,
  "query" : "You are a software engineer and creating a serverless application in AWS to process photos.\nWhen each image is uploaded to an S3 bucket, a Lambda function is invoked.\nThis Lambda function then calls Amazon Rekognition to detect text in the image.\nThe returned results are saved in a DynamoDB table.\nYou have used a template provided by AWS Serverless Application Model (AWS SAM) to build and deploy the whole application.\nFor the above mentioned AWS services, which one do you still need to pay even if the service is not used?",
  "answer" : "E.\nCorrect Answer - E.\nAWS has provided a variety of services that customers only pay for the amount that they use (Pay-as-you-go model), which means that customers are not charged if services are unused.\nThe document https://aws.amazon.com/pricing/?nc2=h_ql_pr is an introduction of AWS pricing features.\nOption A is incorrect: Because the Lambda function created by SAM has used a Pay-as-you-go model, and there is no charge if it is not used.\nOption B is incorrect: Because SAM Template is an extension to the CloudFormation template.\nNo charge for the SAM service.\nOption C is incorrect: Because users pay only for the API calls received and the amount of data transferred out for API Gateway.\nOption D is incorrect: Because AWS Rekognition Image Analysis is charged based on the number of images analyzed.\nOption E is CORRECT: Because all the above options do not have an upfront or monthly fee, and customers are not charged if the services are not used.\nThe AWS services used in the serverless application are:\nAmazon S3: Object storage service used for storing images.\nAWS Lambda: Serverless compute service that executes the code in response to events, like an image upload to S3 in this case.\nAmazon Rekognition: A service that can identify objects, people, text, scenes, and activities in images and videos.\nAmazon DynamoDB: A NoSQL database that provides low-latency and consistent performance at scale.\nNow, to answer the question, which service needs to be paid even if it is not used?\nAWS SAM Template: It is a framework for building serverless applications using AWS CloudFormation. It is not a service that runs in the cloud, and hence there is no cost associated with it. So option B is not correct.\nAPI Gateway: The application does not use API Gateway, so there is no cost associated with it. Option C is not correct.\nLambda Function: Lambda function is invoked only when an image is uploaded to S3. Hence, if no image is uploaded, there will be no Lambda function execution, and hence there will be no cost associated with Lambda function when it is not used. So, option A is not correct.\nRekognition Image Analysis: As the Lambda function calls Amazon Rekognition to detect text in the image, it incurs cost per image processed. So, option D is the correct answer.\nTherefore, the answer to the question is D. Rekognition Image Analysis is the service that needs to be paid even if it is not used.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Lambda function",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "SAM Template",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "API Gateway",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Rekognition Image Analysis",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "None of the above.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 360,
  "query" : "You have owned a two-tier LAMP stack in a local data center.\nThe stack has a frontend of Apache and PHP and a backend database running on MySQL.\nThe whole system is hosted on a virtualized platform, and you need to migrate the system to AWS in two weeks.\nYou plan to use AWS provided migration services to migrate MySQL database to RDS Aurora and on-premise virtual machines (managed by VMware vSphere) to EC2\nWhich AWS services should be used? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A, D.\nThis case aims to find out the proper AWS tools to help with the migrations for its two tiers application and database.\nFor the database, AWS Database Migration Service should be chosen to easily replicate the database to an RDS instance of Amazon Aurora which is a MySQL compliant database.\nCheck https://docs.aws.amazon.com/dms/latest/sbs/CHAP_MySQL2Aurora.html on how to migrate a MySQL-compatible database to Amazon Aurora MySQL.\nFor the application, AWS Server Migration Service should be used because it is designed to automate the migration of on-premises VMware vSphere or Microsoft Hyper-V/SCVMM virtual machines to the AWS Cloud.\nThe service can replicate the VM to an AMI for deployment on Amazon EC2.\nOption A is CORRECT: Because it is used to migrate the database to RDS Aurora.\nOption B is incorrect: The service can be used to discover the existing servers, plan migrations, and track the status.\nHowever, the migration tools of Database Migration Service and Server Migration Service are still needed.\nAWS Migration Hub is not necessary for this case.\nOption C is incorrect: Because AWS Snowball is a service to copy a large amount of data rather than migrate applications or databases.\nOption D is CORRECT: Because AWS Server Migration Service is the correct AWS service to migrate applications.\nOption E is incorrect: AWS Application Discovery Service can collect data in local data centers, including system specification, performance, dependencies, and other useful information.\nIt is not mentioned or required for this case.\nThe correct answers are A (AWS Database Migration Service) and D (AWS Server Migration Service).\nAWS Database Migration Service (AWS DMS) is a fully managed service that helps migrate databases to AWS quickly and securely. In this scenario, the MySQL database can be migrated to Amazon RDS Aurora using AWS DMS. AWS DMS can also migrate other types of databases, such as Oracle, Microsoft SQL Server, PostgreSQL, and MongoDB.\nAWS Server Migration Service (AWS SMS) is a service that helps migrate on-premises virtual machines to AWS. In this scenario, the on-premises virtual machines running Apache and PHP can be migrated to Amazon EC2 instances using AWS SMS. AWS SMS can also migrate other types of virtual machines, such as those managed by VMware vSphere, Microsoft Hyper-V, and other virtualization platforms.\nAWS Migration Hub (B) is a service that provides a single place to track the status of application migrations across multiple AWS and partner solutions. It can be useful for managing large-scale migrations but is not required for this specific scenario.\nAWS Snowball (C) is a service that is used to transfer large amounts of data into and out of AWS using physical storage devices. It is not needed for this scenario since the migration can be accomplished over the network.\nAWS Application Discovery Service (E) is a service that helps to discover and inventory on-premises applications and infrastructure dependencies. It is not needed for this scenario since the application components to be migrated are already known.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "AWS Database Migration Service",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Migration Hub",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Snowball",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS Server Migration Service",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Application Discovery Service.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 361,
  "query" : "A small trading business consults you to implement a blockchain across its supply chain network, providing greater transparency, real-time recording, and tracking of goods.\nEach supplier or distributor can be a member of the blockchain network.\nThe customers are looking to manage their own blockchain network and need an easy way to set up and get started.\nThey prefer to deploy the blockchain framework of Hyperledger Fabric as containers on an Amazon Elastic Container Service (ECS) cluster, or directly on an EC2 instance running Docker.\nWhat way should you recommend?",
  "answer" : "Correct Answer - D.\nThere are several Amazon Blockchain services as in https://aws.amazon.com/blockchain/:\nAmazon QLDB - a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority.\nAmazon Managed Blockchain - a fully managed service that makes it easy to create and manage scalable blockchain networks using popular open-source frameworks Hyperledger Fabric and Ethereum.\nAWS Blockchain Templates - a fast and easy way to create and deploy secure blockchain networks using popular open-source frameworks.\nAWS Blockchain Templates deploys the blockchain framework you choose as containers on an Amazon Elastic Container Service (ECS) cluster or directly on an EC2 instance running Docker.\nIt is essentially a CloudFormation template where users can still fully manage the instances:\nOption A is incorrect: Because Amazon Quantum Ledger Database is a ledger database that is not used to deploy the Blockchain network.\nOption B is incorrect: Because Amazon Managed Blockchain does not allow you to manage your ECS or EC2 instances.\nAlso, QLDB is not used to manage the Blockchain network fully.\nOption C is incorrect: Same reason as Option.\nB.Option D is CORRECT: Because AWS Blockchain Templates allows you to create and deploy a Hyperledger Fabric network fast and easily in ECS cluster or EC2 instances.\nThe best option for this scenario is option D: Use AWS Blockchain Templates to deploy the blockchain Hyperledger Fabric framework on an Amazon Elastic Container Service (ECS) cluster or directly on an EC2 instance.\nHere's why:\nFirst, let's understand the requirements. The business wants to implement a blockchain across its supply chain network, providing greater transparency, real-time recording, and tracking of goods. Each supplier or distributor can be a member of the blockchain network. The customers want to manage their own blockchain network and need an easy way to set up and get started. They prefer to deploy the blockchain framework of Hyperledger Fabric as containers on an Amazon Elastic Container Service (ECS) cluster or directly on an EC2 instance running Docker.\nOption A suggests using Amazon Quantum Ledger Database (QLDB) to deploy the AWS BlockChain network in ECS and EC2 depending on specific requirements. However, QLDB is not a blockchain platform but a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority.\nOption B suggests using Amazon Managed Blockchain to deploy the network and replicate an immutable copy of the blockchain network activity into Amazon Quantum Ledger Database (QLDB), where customers can fully manage the blockchain network. While this is a good option, it requires additional setup and maintenance of QLDB, and it may not meet the customer's preference for managing their own network.\nOption C suggests using Amazon Managed Blockchain to deploy the framework and access ECS or EC2 instances to deploy decentralized applications. However, this option does not align with the customer's preference to manage their own network, as they would still need to rely on Amazon Managed Blockchain for the deployment.\nOption D suggests using AWS Blockchain Templates to deploy the blockchain Hyperledger Fabric framework on an Amazon Elastic Container Service (ECS) cluster or directly on an EC2 instance. AWS Blockchain Templates provide a fast and easy way to deploy blockchain networks, as they automate the process of setting up and configuring the necessary infrastructure components for the blockchain network. Hyperledger Fabric is a popular blockchain framework for enterprise use cases, and it supports containerization and deployment on ECS or EC2 instances.\nIn summary, option D aligns with the customer's preference for managing their own network and provides a fast and easy way to deploy the blockchain Hyperledger Fabric framework on an Amazon Elastic Container Service (ECS) cluster or directly on an EC2 instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Amazon Quantum Ledger Database (QLDB) to deploy the AWS BlockChain network in ECS and EC2 depending on specific requirements.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon Managed Blockchain to deploy the network. Replicate an immutable copy of the blockchain network activity into Amazon Quantum Ledger Database (QLDB), where customers can fully manage the blockchain network.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon Managed Blockchain to deploy the framework. By using Amazon Managed Blockchain, customers can access ECS or EC2 instances and deploy decentralized applications.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use AWS Blockchain Templates to deploy the blockchain Hyperledger Fabric framework on an Amazon Elastic Container Service (ECS) cluster or directly on an EC2 instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 362,
  "query" : "Which of the following are best practices that need to be followed when updating Opswork stack instances with the latest security patches?",
  "answer" : "Answers: A and B.\nThe best practices for updating your OpsWork stacks instances with the latest security patches.\nCreate and start new instances to replace your current online instances.\nThen delete the current instances.\nThe new instances will have the latest set of security patches installed during setup.\nOn Linux-based instances in Chef 11.10 or older stacks, run the Update Dependencies stack command, installing the current set of security patches and other updates on the specified instances.\nFor more information on OpsWork Linux security updates best practices, please visit the link -\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingsecurity-updates.html\nWhen updating Opswork stack instances with the latest security patches, the following are best practices that need to be followed:\nA. Create and start new instances to replace your current online instances: This practice involves creating new instances with the latest security patches and replacing the current online instances with the new ones. This approach ensures that the updated instances are running with the latest security patches and also provides an opportunity to test the new instances before switching traffic to them.\nB. Run the Update Dependencies stack command for Linux based instances: This practice involves running the Update Dependencies stack command on the Linux-based instances to ensure that all the dependencies are updated to the latest version. This command updates the packages, libraries, and other dependencies that the instance relies on, ensuring that it is running on the latest version of the software.\nC. Delete the entire stack and create a new one: This practice involves deleting the entire stack and creating a new one with the latest security patches. This approach ensures that the entire stack is updated with the latest security patches and eliminates any potential issues that may arise from patching individual instances.\nD. Use CloudFormation to deploy the security patches: This practice involves using CloudFormation to automate the deployment of security patches across multiple instances. This approach can help simplify the patching process and ensure that all instances are updated consistently.\nIn summary, the best practice when updating Opswork stack instances with the latest security patches is to create and start new instances to replace the current online instances. Additionally, it is recommended to run the Update Dependencies stack command for Linux-based instances and use CloudFormation to automate the deployment of security patches. Deleting the entire stack and creating a new one is also a viable option but should only be used if necessary.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create and start new instances to replace your current online instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "run the Update Dependencies stack command for Linux based instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Delete the entire stack and create a new one.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Cloudformation to deploy the security patches.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 363,
  "query" : "While managing your instances in the current OpsWorks stack, you suddenly started getting the following error.",
  "answer" : "Answer: A and.\nB.This can occur if a resource outside AWS OpsWorks on which the instance depends was edited or deleted.\nThe following are examples of resource changes that can break communications with an instance.\nAn IAM user or role associated with the instance has been deleted accidentally, outside of AWS OpsWorks Stacks.\nThis causes a communication failure between the AWS OpsWorks agent that is installed on the instance and the AWS OpsWorks Stacks service.\nThe IAM user that is associated with an instance is required throughout the life of the instance.\nEditing volume or storage configurations while an instance is offline can make an instance unmanageable.\nAdding EC2 instances to an ELB manually.\nAWS OpsWorks reconfigures an assigned Elastic Load Balancing load balancer when an instance enters or leaves the online state.\nAWS OpsWorks only considers instances it knows about to be valid members.\nInstances that are added outside of AWS OpsWorks, or by some other process, are removed.\nEvery other instance is removed.\nFor more information on troubleshooting OpsWorks, please visit the link:\nhttp://docs.aws.amazon.com/opsworks/latest/userguide/common-issues-troubleshoot.html\nThe error could be related to any number of factors, so it's important to first try to identify the root cause. Here's a more detailed explanation of each possible answer:\nA. Check the IAM role which was attached to the instance. If an instance is not properly configured with the right IAM role, it may not have the permissions required to perform the necessary operations in the OpsWorks stack. Therefore, it's important to check if the IAM role is properly configured and has the necessary permissions.\nB. Check if the EC2 instances were manually added to the EL. If EC2 instances were manually added to the Elastic Load Balancer (EL), they may not have been added properly or may be using incorrect settings. As a result, it's important to check if the instances were added properly to the EL and that the settings are correct.\nC. Check if the stack is configured properly. If the OpsWorks stack is not configured properly, it may result in errors when managing instances. This could be due to a variety of factors, such as incorrect settings or configurations, misconfigured permissions, or other issues.\nD. Check if the OpsWorks client is configured properly. The OpsWorks client must be installed and properly configured on the instance to ensure that it can communicate with the OpsWorks stack. If the client is not properly configured or installed, it may result in errors when managing instances.\nIn summary, it's important to carefully examine each possible answer to identify the root cause of the error. This may require further investigation and troubleshooting to resolve the issue.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Check the IAM role which was attached to the instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Check if the EC2 instances were manually added to the EL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check if the stack is configured properly.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check if the OpsWorks client is configured properly.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 364,
  "query" : "The company you are working for has an on-premises blog web application, which is built on VMware vSphere virtual machines.\nAs an AWS solutions architect, you need to evaluate the proper methods to migrate the application to AWS.\nAfter the initial analysis, you have suggested using AWS Server Migration Service (SMS) to migrate.\nDuring this migration process, which AWS services will NOT bring extra costs? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A, D.\nAWS SMS is a free service to use for the server migration which means that there is no additional fee.\nHowever, the storage resources used during the migration process, such as Amazon EBS snapshots and Amazon S3, can generate standard fees.\nRefer to https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html for the introductions to AWS SMS.\nOption A is CORRECT: Because the Server Migration Connector is a pre-configured FreeBSD virtual machine available for deployment in the VMware environment.\nOption B is incorrect: Because during the migration, there is a step to convert the VMDK to an Amazon Elastic Block Store (Amazon EBS) snapshot, which generates cost.\nOption C is incorrect: Because the S3 usage is charged when VMDK files are uploaded to Amazon S3.\nOption D is CORRECT: Because the replication job or task itself is free to use.\nOption E is incorrect: Because EC2 instances are charged at a standard rate.\nSure, I'd be happy to help you with that!\nIn this scenario, the company's on-premises blog web application is built on VMware vSphere virtual machines, and you've suggested using AWS Server Migration Service (SMS) to migrate it to AWS.\nDuring this migration process, there are certain AWS services that will NOT bring extra costs. Let's discuss each of the options:\nA. The Server Migration Connector downloaded from AWS: When you use AWS Server Migration Service (SMS), you'll need to install a connector on your on-premises servers, which will communicate with the SMS service. This connector is downloaded from AWS, but it won't incur any additional costs.\nB. Amazon EBS snapshots generated during the migration: Amazon Elastic Block Store (EBS) provides block-level storage volumes for use with Amazon EC2 instances. During the migration process, AWS Server Migration Service creates Amazon EBS snapshots of your on-premises servers. However, these snapshots will incur additional costs.\nC. Amazon S3 which is used to store the uploaded VMDK: During the migration process, you'll need to upload the VMDK files to AWS. AWS Server Migration Service uses Amazon S3 to store the uploaded VMDK files. However, standard Amazon S3 pricing applies to this storage, so it will incur additional costs.\nD. The replication job that is created in Server Migration Service: When you create a replication job in AWS Server Migration Service, you're basically defining the settings for how your on-premises servers will be migrated to AWS. Creating a replication job won't incur any additional costs.\nE. The EC2 instances that run based on the new AMI: After the migration is complete, you'll have new Amazon EC2 instances running in AWS, based on the new Amazon Machine Image (AMI) that was created during the migration process. These instances will incur additional costs, based on the instance type, region, and usage.\nSo, based on the above discussion, options A and D are the correct answers. The Server Migration Connector downloaded from AWS and the replication job created in Server Migration Service won't incur any additional costs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The Server Migration Connector downloaded from AWS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Amazon EBS snapshots generated during the migration.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon S3 which is used to store the uploaded VMDK.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The replication job that is created in Server Migration Service.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The EC2 instances that run based on the new AMI.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 365,
  "query" : "You have deployed a Windows Server instance (x86_64) in AWS EC2\nAfter the instance has run for a week, you realized that you needed to run a script in PowerShell.\nYou were logged in the AWS EC2 console and started using Systems Manager to run a command.\nYou chose the command as \"AWS-RunPowerShellScript\"\nHowever, your instance cannot be selected as the target.\nHow should you troubleshoot the issue so that the command can run in the current Windows instance successfully? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - C, E.\nAWS Systems Manager Run Command is a service to run a command for Windows or Linux instance:\nHowever, if the instance cannot be seen as the target for the command to run, some items must be checked according to https://docs.aws.amazon.com/systems-manager/latest/userguide/troubleshooting-remote-commands.html?icmpid=docs_ec2_console.\nOption A is incorrect: Because “AWS-RunPowerShellScript” is correct while “AWS-RunShellScript” is for Linux instance.\nOption B is incorrect: Because the Windows instance is supported.\nCheck https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-prereqs.html for the supported Operating Systems.\nOption C is CORRECT: Because only Amazon EC2 Windows Amazon Machine Images (AMIs) and certain Linux AMIs are pre-configured with the SSM Agent.\nYou need to check if the SSM Agent is properly installed.\nOption D is incorrect: Because system patches do not impact the SSM connection.\nOption E is CORRECT: Because a proper IAM instance role is required; otherwise, EC2 cannot communicate with SSM API.\nThe scenario presented describes an issue where the user cannot select their Windows Server instance as a target to run a PowerShell script using Systems Manager's \"AWS-RunPowerShellScript\" command. To troubleshoot the issue, the following steps can be taken:\n1.\nVerify that the instance is a Windows Server instance (x86_64) - This is important to ensure that the correct type of instance is being targeted. If the instance is not a Windows Server instance, then the \"AWS-RunPowerShellScript\" command will not work.\n2.\nCheck if the instance is running - It is important to ensure that the instance is running before trying to run any commands on it. If the instance is not running, start it before proceeding.\n3.\nCheck if the latest version of SSM (AWS Systems Manager) Agent is installed on the Windows instance - AWS Systems Manager agent must be installed on the instance to enable communication with the Systems Manager service. The agent must be running, and its version must be compatible with the Systems Manager version in use.\n4.\nCheck if the Windows instance has the latest system patches installed - Before attempting to run any scripts, it is essential to ensure that the instance is up-to-date with the latest system patches. This will help prevent any compatibility issues that may arise due to outdated patches.\n5.\nVerify that the instance has configured with the IAM role that enables it to communicate with the Systems Manager API - An IAM role is required to enable communication between the instance and Systems Manager API. Ensure that the instance has been configured with the appropriate IAM role that grants it the required permissions to use the \"AWS-RunPowerShellScript\" command.\nBased on the above steps, the following options can be eliminated:\nOption B is incorrect because Systems Manager supports both Linux and Windows instances, so the Windows instance should be available as a target.\nOption A is incorrect because \"AWS-RunShellScript\" is used to run shell scripts on Linux instances, and not PowerShell scripts on Windows instances.\nTherefore, the correct answers are C and D. By ensuring that the SSM agent is up-to-date and that the instance has the latest system patches installed, any compatibility issues with the Systems Manager service should be resolved.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Change the command “AWS-RunPowerShellScript” to “AWS-RunShellScript”.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Systems Manager Run Command only works for Linux instances so that the Windows instance is unavailable.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check if the latest version of SSM (AWS Systems Manager) Agent is installed on the Windows instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Check if the Windows instance has the latest system patches installed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Verify that the instance has configured with the IAM role that enables it to communicate with the Systems Manager API.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 366,
  "query" : "You are an AWS administrator.\nRecently, you started to use various AWS services in AWS Systems Manager to maintain over 20 EC2 and on-premises instances.\nIn the past month, the AWS bill has increased by about 10% than before.\nThe company's accountant asked you the potential cause of this.\nFor the AWS Systems Manager services, which ones may bring additional charges? (Select 3.)",
  "answer" : "E.\nCorrect Answer - A, D, and E.\nFor the services provided by AWS Systems Manager, most of them are free without extra cost.\nHowever, there are still some priced features when used.\nFor the details, please refer to https://aws.amazon.com/systems-manager/pricing/ on AWS Systems Manager Pricing.\nOption A is CORRECT: Because for on-premises Instance Management, the advanced instances are priced on a pay-as-you-go basis:\nOption B is incorrect: Because Run Command is free.\nCheck the above link.\nOption C is incorrect: Because Patch Manager automates the process of patching managed instances, which is a free service.\nOption D is CORRECT: Parameter Store is a secure place to store parameters or secrets.\nWhen you create advanced parameters, you are charged based on the number of advanced parameters stored each month and per API interaction.\nPlease refer the following link.\nhttps://aws.amazon.com/systems-manager/pricing/\nOption E is CORRECT: The Automation pricing is as below.\nAs an AWS administrator, when using AWS Systems Manager to maintain instances, it is essential to be aware of the services that can bring additional charges. The services that can potentially bring additional charges are:\nA. On-Premises Instance Management - This service is used to manage on-premises servers or virtual machines using Systems Manager. This service can bring additional charges if you use AWS Site-to-Site VPN or AWS Direct Connect to connect to on-premises resources.\nB. Run Command in Systems Manager - This service is used to execute commands remotely on EC2 instances, on-premises instances, and virtual machines. This service can bring additional charges based on the number of commands executed and the number of instances targeted.\nC. Patch Manager - This service is used to patch instances in AWS and on-premises environments. This service can bring additional charges based on the number of instances you are patching and the number of patches you apply.\nD. Advanced Parameter Store - This service is used to manage parameters and secrets across multiple regions and accounts. This service can bring additional charges based on the number of parameters stored and the number of API requests.\nE. Systems Manager Automation - This service is used to automate maintenance and deployment tasks. This service can bring additional charges based on the number of automation executions and the number of instances targeted.\nTherefore, the services that can potentially bring additional charges are On-Premises Instance Management, Run Command in Systems Manager, and Patch Manager.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "On-Premises Instance Management",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Run Command in Systems Manager",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Patch Manager",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Advanced Parameter Store",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Systems Manager Automation.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 367,
  "query" : "You are maintaining over a dozen EC2 Ubuntu instances.\nThe application installed in the instances had an issue last week, and the development team already knew the root cause.\nTo prevent the issue from happening again, some debug logs have been added to the application.\nYour manager asked you to use AWS Systems Manager to send the logs to an S3 bucket every 2:00 AM for all the EC2 instances.\nWhich AWS Systems Manager service should you use to meet this requirement? Choose 2 Options.",
  "answer" : "Correct Answer - C and D.\nFor this case, the State Manager should be used.\nThe “AWS-RunPowerShellScript” document can be created, and its target is all the EC2 instances.\nAWS Systems Manager State Manager is a service that automates the process of keeping the Amazon EC2 or hybrid infrastructure in the state that you desire.\nOption A is incorrect: Because Session Manager is a service to provide secure and auditable instance management, which is not a tool to execute a script or command.\nOption B is incorrect: Because Distributor packages the software to install on AWS Systems Manager managed instances.\nIt is not designed to run a script or command as a schedule.\nOption C is CORRECT: Because in State Manager, the user can design an association that contains a proper document to accomplish this mission.\nOption D is CORRECT: Configure instance permissions with IAM needed to run maintenance window tasks on your instances and Configure user permissions in your account who assigns tasks to maintenance windows.\nTo meet the requirement of sending logs from multiple EC2 instances to an S3 bucket every day at 2:00 AM, AWS Systems Manager provides different services that can be used. Two possible options that could meet the requirement are:\nOption A: Use the Session Manager to send the required logs to the S3 bucket every 2:00 AM. Option D: Create a schedule in AWS Systems Manager Maintenance Windows to move the logs to the S3 bucket every 2:00 AM.\nOption B (Use Systems Manager Distributor to transfer the logs every 2:00 AM on all the AWS Systems Manager managed instances) does not meet the requirement as it is not clear if the EC2 instances are managed by AWS Systems Manager or not. Additionally, using Distributor to transfer logs to S3 requires an additional setup process of creating packages, versions, and associations which may not be necessary.\nOption C (Use State Manager to run a shell script every 2:00 AM for all the EC2 instances) does not meet the requirement as State Manager is used for managing the configuration of EC2 instances and not for transferring logs.\nOption A: Use the Session Manager to send the required logs to the S3 bucket every 2:00 AM.\nAWS Systems Manager Session Manager allows users to securely manage instances without the need for SSH or RDP connections. It provides a simple and secure way to execute commands on EC2 instances and provides features like session history, audit logging, and access control. However, it does not have built-in functionality to send logs to S3.\nTherefore, using the Session Manager for transferring logs would require additional setup, such as writing a script that extracts the logs and sends them to the S3 bucket using AWS CLI or SDK. This approach may require additional IAM permissions and access control setup to ensure secure transfer of logs.\nOption D: Create a schedule in AWS Systems Manager Maintenance Windows to move the logs to the S3 bucket every 2:00 AM.\nAWS Systems Manager Maintenance Windows is a service that enables users to schedule AWS Systems Manager automation tasks across fleets of instances. It provides a simple and automated way to manage patching, maintenance, and other administrative tasks on EC2 instances.\nUsing Maintenance Windows to schedule the transfer of logs to S3 bucket would require creating an SSM automation document that includes a script to extract logs from the EC2 instances and copy them to the S3 bucket. This can be done using the \"AWS-RunShellScript\" document type or using a custom document that is tailored to meet the specific requirements.\nOnce the automation document is created, it can be scheduled to run at 2:00 AM using Maintenance Windows. Maintenance Windows provide a centralized and automated way to run the automation tasks across all EC2 instances in the fleet, including tagging and filtering options that can help to target specific instances.\nIn summary, both options A and D can be used to transfer logs from multiple EC2 instances to an S3 bucket every 2:00 AM. Option A requires additional setup to extract logs and send them to S3, while option D requires creating an SSM automation document that can be scheduled using Maintenance Windows. The best option would depend on the specific requirements, such as the level of automation, security, and access control required.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the Session manager to send the required logs to the S3 bucket every 2:00 AM.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Systems Manager Distributor to transfer the logs every 2:00 AM on all the AWS Systems Manager managed instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use State Manager to run a shell script every 2:00 AM for all the EC2 instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a schedule in AWS Systems Manager Maintenance Windows to move the logs to the S3 bucket every 2:00 AM.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 368,
  "query" : "You were just hired by an IT company as an AWS solutions architect.\nYour company has several Windows EC2 instances for multiple projects.\nYou found that these EC2 instances did not have a plan to install system patches.\nYou start using Patch Manager in AWS Systems Manager to configure when and how the patches should be installed.\nWhen patching activity is being configured, which method is NOT a valid one for instances to be selected?",
  "answer" : "Correct Answer - D.\nAWS Systems Manager Patch Manager is a native AWS solution to automate patching.\nThe below diagram explains how it works.\nStep 2 is optional, and there are several methods for the instances to be chosen as the target.\nOption A is incorrect because one or several tag key/value pairs can be used to select instances.\nOption B is incorrect because a patch group is a tag with the key as “Patch Group”, which is also a supported method to organize patching activities.\nOption C is incorrect because selecting instances manually is supported.\nOption D is CORRECT because security group identities/names are not used to choose instances for patching activities in Patch Manager.\nAs an AWS solutions architect, you have identified that your company's Windows EC2 instances are not being patched regularly. To address this, you have decided to use Patch Manager in AWS Systems Manager to configure when and how the patches should be installed.\nWhen configuring the patching activity, you will need to select the instances that you want to patch. There are several methods available for doing so, but one of them is not valid. Let's examine each of the methods listed in the question:\nA. Specify one or more instance tag key/value pairs to identify the instances you want to patch. This method allows you to select instances based on the tags associated with them. For example, you could select all instances that have the tag \"Environment\" with a value of \"Production\". This is a commonly used method for selecting instances for patching.\nB. Use one or more patch groups to identify the instances you want to patch. Patch groups require the use of the tag key “Patch Group”. Patch groups are a way to group instances together based on the tag key \"Patch Group\". This is useful if you have multiple groups of instances that need to be patched at different times or with different patch baselines. For example, you could have a patch group for \"Development\" instances and a separate patch group for \"Production\" instances.\nC. Manually select the required instances. This method allows you to manually select the instances that you want to patch. This may be useful if you have only a small number of instances to patch, but it can be time-consuming if you have a large number of instances.\nD. Select the required EC2 instances based on security group identities/names. This method is not a valid option for selecting instances for patching. While security groups can be used to control network access to EC2 instances, they do not provide a way to identify specific instances for patching.\nIn summary, the method that is NOT a valid one for selecting instances for patching is D. Select the required EC2 instances based on security group identities/names. The other methods (A, B, and C) are all valid options for selecting instances for patching using Patch Manager in AWS Systems Manager.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Specify one or more instance tag key/value pairs to identify the instances you want to patch.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use one or more patch groups to identify the instances you want to patch. Patch groups require the use of the tag key “Patch Group”.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Manually select the required instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Select the required EC2 instances based on security group identities/names.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 369,
  "query" : "Your team has owned a web application which is an online ticket booking system.\nYou are trying to use another m4.xlarge EC2 instance as a backup.\nYou want to stop the instance to save some cost unless it needs to bring up.\nHowever, it takes a long time for the bootstrap process to bring the system up, which is unacceptable.\nThe company's cloud architect has suggested hibernating the instance so that it can start up very quickly.\nTo hibernate the instance, which below prerequisites are needed?",
  "answer" : "E.\nCorrect Answer - B, C, E.\nRefer to the below for the hibernating process:\nIt is a process of transferring data inside and outside RAM to Amazon EBS root volume.\nHowever, some prerequisites have to be met.\nRefer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html for the details.\nOption A is incorrect: Because only Windows and Linux are supported for EC2 hibernation.\nPlease read the description given under 'Important' in the page under the below link.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Hibernate.html\nOption B is CORRECT: Because instance store volume will lose data when stopped and is not supported for EC2 hibernation.\nOption C is CORRECT: Because when the instance is in an Auto Scaling group, and you try to hibernate it, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy and may terminate it and launch a replacement instance.\nOption D is incorrect: Because in order to use hibernation, the root volume must be encrypted to ensure the protection of sensitive content that is in memory at the time of hibernation.\nOption E is CORRECT: If the root volume is not enough to store the data from RAM, the following message may appear: hibinit-agent: Insufficient disk space.\nCannot create setup for hibernation.\nPlease allocate a larger root device.\nTo hibernate an EC2 instance, the following prerequisites must be met:\nA. The AMI of Mac OS EC2 instances must support hibernation. Hibernation is not supported for macOS instances, so this option is not relevant to the scenario.\nB. The instance root volume must be an Amazon EBS volume, not an instance store volume. Hibernation requires an EBS-backed instance, not an instance store-backed instance. EBS volumes persist independently of the lifecycle of the instance, allowing the instance to be stopped and started again while preserving its state.\nC. The instance cannot be in an Auto Scaling group or used by Amazon ECS. An instance in an Auto Scaling group or used by Amazon ECS cannot be hibernated because these services are designed to scale up or down dynamically, and hibernation is not supported in this scenario.\nD. The root volume must not be encrypted; otherwise, it will cause an issue to transfer data from RAM. If the root volume is encrypted, hibernation will not be successful because the instance cannot write the contents of RAM to the encrypted disk. Therefore, the root volume must not be encrypted.\nE. The root volume must be large enough so that the RAM contents can be stored. During hibernation, the instance saves its memory contents to the root volume, and when it starts up again, it restores the memory contents from the root volume. Therefore, the root volume must be large enough to store the contents of the instance's RAM.\nIn summary, to hibernate an EC2 instance, it must be EBS-backed, not in an Auto Scaling group or used by Amazon ECS, the root volume must not be encrypted, and the root volume must be large enough to store the contents of the instance's RAM.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The AMI of Mac OS EC2 instances must support hibernation.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The instance root volume must be an Amazon EBS volume, not an instance store volume.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The instance cannot be in an Auto Scaling group or used by Amazon ECS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The root volume must not be encrypted; otherwise, it will cause an issue to transfer data from RAM.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The root volume must be large enough so that the RAM contents can be stored.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 370,
  "query" : "What are some of the common types of content that are supported by a web distribution via CloudFront?",
  "answer" : "Answer - A, B, and C.\nYou can use web distributions to serve the following content over HTTP or HTTPS.\nStatic and dynamic download content, for example, .html, .css, .php, and image files, using HTTP or HTTPS.\nMultimedia content on-demand using progressive download and Apple HTTP Live Streaming (HLS)\nA live event, such as a meeting, conference, or concert, in real-time.\nFor live streaming, you create the distribution automatically by using an AWS CloudFormation Stack.\nHence, options A, B, and C are CORRECT.\nFor more information on CloudFront distribution, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html\nCloudFront is a Content Delivery Network (CDN) service offered by Amazon Web Services (AWS) that helps to deliver static, dynamic, and streaming content to users from edge locations that are geographically closer to the users.\nThe common types of content that are supported by a web distribution via CloudFront are:\nA. Static content: CloudFront can deliver static content such as HTML, CSS, JavaScript, images, and videos from edge locations. Static content does not change frequently, and CloudFront can cache it at edge locations to deliver it faster to end-users.\nB. Live events: CloudFront can deliver live events such as sports events, concerts, and webinars to viewers in real-time. CloudFront can use either RTMP (Real-Time Messaging Protocol) or HTTP Live Streaming (HLS) to deliver live events to viewers.\nC. Multimedia content: CloudFront can deliver multimedia content such as audio and video files in various formats such as MP3, MP4, and FLV. CloudFront can use HTTP or RTMP to deliver multimedia content to viewers.\nD. Peer to peer networking: CloudFront does not support peer to peer networking as it is a CDN service that primarily focuses on delivering content from edge locations to end-users. Peer to peer networking is a different technology that allows users to share files directly with other users without using a central server.\nIn summary, CloudFront is a CDN service that can deliver static, dynamic, and streaming content to users from edge locations. It supports static content, live events, and multimedia content, but it does not support peer to peer networking.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Static content",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Live events",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Multimedia content",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Peer to peer networking.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 371,
  "query" : "A client is using CloudFront with a source that normally serves dynamic content.\nThere is a requirement that as soon the content is changed in the source, it is delivered to the client.\nWhich of the following configuration can be made to fulfill this requirement?",
  "answer" : "E.\nAnswer - C.\nIn CloudFront, to enforce content delivery to the user as soon as it gets changed by the origin, the time to live (TTL) should be set to 0.\nOption A is incorrect because invalidate is used to remove the content from CloudFront edge locations cache before it expires.\nThe next time a viewer requests the object, CloudFront fetches the content from the origin; whereas, setting TTL to 0 enforces CloudFront to deliver the latest content as soon as the origin updates it.\nOption B is incorrect because setting TTL to 10 will keep the content in the cache for some time even though the origin updates it.\nOption C is CORRECT because setting TTL to 0 will enforce content delivery to the user as soon as it gets changed by the origin.\nOption D is incorrect as CloudFront surely serves dynamic content.\nOption E is incorrect as you do not have to contact the AWS support center for this scenario.\nMore information on TTL in CloudFront:\nYou can control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin.\nReducing the duration allows you to serve dynamic content.\nThe low TTL is also given in the AWS documentation.\nFor more information on CloudFront dynamic content, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html\nThe requirement is to deliver the updated content to the client as soon as it is changed in the source. This can be achieved by configuring the Time-To-Live (TTL) value of the CloudFront distribution.\nTTL is the amount of time that CloudFront caches an object before requesting the object again from the origin. By default, the TTL value for dynamic content is set to 24 hours, which means that CloudFront caches the content for 24 hours before requesting it again from the origin. However, this default behavior can be overridden by setting a different TTL value.\nOption A suggests using the fast invalidate feature provided in CloudFront. This feature allows you to invalidate the cached objects before their TTL expires. This would achieve the requirement of delivering updated content to the client as soon as it is changed in the source. However, this feature is not instantaneous and may take several minutes to propagate through the CloudFront network. Also, it incurs additional costs for invalidation requests.\nOption B suggests setting the TTL to 10 seconds. This means that CloudFront caches the content for only 10 seconds before requesting it again from the origin. This would achieve the requirement of delivering updated content to the client as soon as it is changed in the source. However, it also means that CloudFront will request the content from the origin every 10 seconds, which may increase the load on the origin server.\nOption C suggests setting the TTL to 0 seconds. This means that CloudFront does not cache the content at all and requests it from the origin every time it is requested by a client. This would achieve the requirement of delivering updated content to the client as soon as it is changed in the source. However, it also means that CloudFront does not provide any caching benefits, which may increase the load on the origin server.\nOption D suggests that dynamic content cannot be served from CloudFront, which is incorrect. CloudFront can serve dynamic content, and the TTL value can be configured to achieve the requirement.\nOption E suggests that you have to contact the AWS support center to enable this feature, which is incorrect. The TTL value can be configured through the CloudFront console or API.\nTherefore, the best option to fulfill the requirement of delivering updated content to the client as soon as it is changed in the source is option B, setting the TTL to 10 seconds. However, the trade-off is that it may increase the load on the origin server due to frequent requests. Option A can also be used, but it incurs additional costs for invalidation requests and may take some time to propagate through the CloudFront network. Option C should be avoided as it does not provide any caching benefits. Option D is incorrect as CloudFront can serve dynamic content. Option E is also incorrect as no support center intervention is needed to configure the TTL value.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the fast invalidate feature provided in CloudFront.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set TTL to 10 seconds.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set TTL to 0 seconds.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Dynamic content cannot be served from the CloudFront.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You have to contact the AWS support center to enable this feature.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 372,
  "query" : "You are responsible for a web application that consists of an Elastic Load Balancer (ELB) in front of an Auto Scaling group of EC2 instances.\nBefore deploying a new version of the application, a new AMI was created and the Auto Scaling Group was updated with a new launch configuration that refers to this new AMI.\nDuring the deployment, you received complaints from users that the website was responding with errors.\nAll instances passed the ELB health checks.\nWhat should you do in order to avoid errors for future deployments? (Choose 2 answers)",
  "answer" : "E.\nAnswers - C &amp; D.\nIn this scenario, the ELB health check was passed which implies that the instances were successfully deployed using the new AMIs by the launch configuration and Auto Scaling group.\nThe deployment was successful, but as the users started using the application, they started receiving the error.\nSo, it implies that the errors are related to the application itself, not the setup.\nOption A is incorrect because setting the short period of health check will not be useful in this scenario.\nOption B is incorrect because you cannot change the launch configuration based on the CloudWatch alert.\nOption C is CORRECT because the current health check might be just checking if the application/website is reachable or not.\ni.e.\nIt may not be currently checking whether the application is fully functioning.\nIf the health check is configured to test the application fully, it will stop deploying the instances with the faulty application.\nOption D is CORRECT because doubling the Auto Scaling size will give some lead time for instances to become healthy before the old instances get terminated (kind of Blue/Green Deployment).\nOption E is incorrect because increasing the unhealthy threshold will not help this scenario since it does not prevent unhealthy instances from being deployed.\nIn this scenario, the Auto Scaling group was updated with a new launch configuration that refers to a new AMI, but users are experiencing errors during deployment. All instances have passed the Elastic Load Balancer health checks. To prevent such errors from happening during future deployments, we should take the following steps:\nA. Add an Elastic Load Balancing health check to the Auto Scaling group. Set a short period for the health checks to operate as soon as possible to prevent premature registration of the instance to the load balancer.\nAdding an Elastic Load Balancing (ELB) health check to the Auto Scaling group can help ensure that instances that are not ready to serve traffic are not added to the ELB. By setting a short period for the health checks to operate as soon as possible, we can prevent premature registration of the instance to the load balancer. This way, we can avoid adding instances that are still initializing or not fully functional to the load balancer and causing errors for users.\nC. Set the Elastic Load Balancing health check that fully tests application health and returns an error if the tests fail.\nThe Elastic Load Balancer health check should fully test the application health to ensure that instances that are not fully functional are not added to the load balancer. If the tests fail, the health check should return an error to prevent the instance from serving traffic. By doing so, we can avoid adding instances that are not fully functional or have issues to the load balancer and causing errors for users.\nNote: Options B, D, and E are incorrect for this scenario.\nB. Enable EC2 instance CloudWatch alerts to change the launch configuration AMI to the previous one. Gradually terminate instances that are using the new AMI.\nThis option does not address the root cause of the issue, which is why users are experiencing errors during deployment. It also involves changing the launch configuration AMI to the previous one and gradually terminating instances that are using the new AMI, which may cause service disruptions and downtime for users.\nD. Create a new launch configuration that refers to the new AMI, and associate it with the Auto Scaling group. Double the size of the Auto Scaling group, wait for the new instances to become healthy, and reduce back to the original size. If new instances do not become healthy, associate the previous launch configuration.\nThis option involves creating a new launch configuration that refers to the new AMI and doubling the size of the Auto Scaling group. While waiting for the new instances to become healthy, the original size of the Auto Scaling group should be reduced. If the new instances do not become healthy, the previous launch configuration should be associated. This approach is not recommended as it involves creating unnecessary instances and may cause service disruptions and downtime for users.\nE. Increase the Elastic Load Balancing Unhealthy Threshold to a higher value to prevent an unhealthy instance from going into service behind the load balancer.\nIncreasing the Elastic Load Balancer Unhealthy Threshold to a higher value is not recommended as it may cause instances that are not fully functional or have issues to serve traffic, resulting in errors for users. Instead, we should focus on improving the Elastic Load Balancer health check to ensure that instances that are not fully functional are not added to the load balancer.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Add an Elastic Load Balancing health check to the Auto Scaling group. Set a short period for the health checks to operate as soon as possible to prevent premature registration of the instance to the load balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enable EC2 instance CloudWatch alerts to change the launch configuration AMI to the previous one. Gradually terminate instances that are using the new AMI.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set the Elastic Load Balancing health check that fully tests application health and returns an error if the tests fail.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a new launch configuration that refers to the new AMI, and associate it with the Auto Scaling group. Double the size of the Auto Scaling group, wait for the new instances to become healthy, and reduce back to the original size. If new instances do not become healthy, associate the previous launch configuration.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Increase the Elastic Load Balancing Unhealthy Threshold to a higher value to prevent an unhealthy instance from going into service behind the load balancer.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 373,
  "query" : "You have deployed a web application targeting a global audience across multiple AWS Regions under the domain name example.com.\nYou decide to use Route53 Latency-Based Routing to serve web requests to the users from the region closest to them.\nTo provide business continuity in the event of server downtime, you configure weighted record sets associated with two web servers in separate Availability Zones per region.",
  "answer" : "E.\nAnswer - B &amp; E.\nOption A is incorrect because you can set up weighted record sets as the failover or secondary recordset.\nOption B is CORRECT because if the HTTP health check is not set with the weighted resource record sets of the disabled web servers, Route 53 will consider them healthy and continue to forward the traffic.\nOnce the health check is enabled, the DNS queries will get a response indicating that the web servers are disabled, and then the requests would get routed to the other region.\nOption C is incorrect because even if the weight is lower for the region with disabled web servers, Route 53 will continue forwarding the users' requests closest to that region because it will evaluate the latency record set first.\nOption D is incorrect because, even if one of the servers fails, the other server will still work, and the region should get the traffic.\nOption E is CORRECT because if the “Evaluate Target Health” is not set to “Yes” for the region containing the disabled web servers, Route 53 will consider the health of the recordset as healthy and continue to route the traffic to it.\nFor more information on How Amazon Route 53 chooses records when Health Checking is configured, please visit the link below:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-how-route-53-chooses-records.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html#dns-failover-complex-configs-eth-no\nThe correct answer is E. You did not set “Evaluate Target Health” to “Yes” on the latency alias resource record set associated with example.com in the region where you disabled the servers.\nExplanation:\nThe scenario described in the question involves the deployment of a web application across multiple AWS regions under the domain name example.com. The objective is to use Route53 Latency-Based Routing to serve web requests to users from the region closest to them. To ensure business continuity in case of server downtime, the configuration involves weighted record sets associated with two web servers in separate Availability Zones per region.\nIn such a setup, each region has a latency alias resource record set associated with example.com. Each alias resource record set contains the weighted record sets of the two web servers in the region. The weight distribution across the two record sets ensures that traffic is distributed between them in a specified proportion, which can be used to balance traffic load or provide failover capabilities.\nThe question asks for the reason for the failure of business continuity in case of server downtime. There can be several reasons why a server in a region can fail, but the configuration should ensure that traffic is redirected to the other server. The options provided in the question are as follows:\nA. Latency resource record sets cannot be used in combination with weighted resource record sets.\nThis statement is incorrect. Latency alias resource record sets can be combined with weighted record sets to provide failover capabilities based on latency.\nB. You did not set up an HTTP health check to one or more of the weighted resource record sets associated with the disabled web servers.\nThis statement is incorrect. The scenario mentions that weighted record sets are used to provide business continuity in case of server downtime. Health checks can be configured to monitor the health of the web servers associated with the weighted record sets. If a server is found to be unhealthy, Route53 can automatically route traffic to the other server.\nC. The value of the weight associated with the latency alias resource record set in the region with the disabled servers is higher than the weight for the other region.\nThis statement is incorrect. The weights associated with the latency alias resource record sets determine the proportion of traffic to be sent to each region. The weight distribution across the two weighted record sets within each latency alias resource record set determines the proportion of traffic to be sent to each web server within the region.\nD. One of the two working web servers in the other region did not pass its HTTP health check.\nThis statement is incorrect. The scenario mentions that weighted record sets are used to provide business continuity in case of server downtime. Health checks can be configured to monitor the health of the web servers associated with the weighted record sets. If a server is found to be unhealthy, Route53 can automatically route traffic to the other server.\nE. You did not set “Evaluate Target Health” to “Yes” on the latency alias resource record set associated with example.com in the region where you disabled the servers.\nThis statement is correct. The option \"Evaluate Target Health\" should be set to \"Yes\" on the latency alias resource record set associated with example.com in the region where servers are disabled. This setting ensures that Route53 checks the health of the web servers associated with the weighted record sets within the latency alias resource record set before redirecting traffic to them. If Route53 finds that a web server is unhealthy, it routes traffic to the other web server associated with the weighted record set.\nTherefore, the correct answer is option E: You did not set “Evaluate Target Health” to “Yes” on the latency alias resource record set associated with example.com in the region where you disabled the servers.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Latency resource record sets cannot be used in combination with weighted resource record sets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You did not set up an HTTP health check to one or more of the weighted resource record sets associated with the disabled web servers.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The value of the weight associated with the latency alias resource record set in the region with the disabled servers is higher than the weight for the other region.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One of the two working web servers in the other region did not pass its HTTP health check.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You did not set “Evaluate Target Health” to “Yes” on the latency alias resource record set associated with example.com in the region where you disabled the servers.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 374,
  "query" : "Using an Amazon Linux AMI, you have created an m5.large EC2 instance for one of your research programs.\nDue to a limited budget, you prefer to put the instance into a stopped state when it is not used.\nAnother requirement is that when you start the instance, the instance does not need to reboot, and you can continue from where it left off.\nYou want to hibernate the EC2 instance to make this happen.\nWhich description is correct to implement this hibernation option?",
  "answer" : "Correct Answer - C.\nFor the hibernation feature, one limitation is that you cannot enable hibernation on an existing instance (running or stopped)\nFor details, please refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html.\nOption A is incorrect: Because instance store-backed instances cannot be hibernated.\nOption B is incorrect: Because a new instance has to be launched to enable hibernation function as below.\nOption C is CORRECT: This is the suggested way in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html.\nOption D is incorrect: Same reason as Option.\nB.\nOne more thing is that the CLI command is something like “aws ec2 stop-instances --instance-ids i-1234567890abcdef0 --hibernate”\nAlso, AWS Console is supported for the hibernation option.\nThe correct answer is option C: You cannot enable hibernation on an existing instance. You have to launch a new instance with an HVM AMI. Enable the hibernation feature on the Configure Instance Details page.\nHibernation is a feature that allows you to pause and resume an Amazon EC2 instance's state, preserving its RAM contents. This feature is only available for certain instance types and AMIs.\nIn this scenario, the instance type used is m5.large, which supports hibernation. However, hibernation cannot be enabled on an existing instance, and the instance needs to be launched from an HVM AMI that supports hibernation.\nTo enable hibernation on the instance, you need to launch a new instance from an HVM AMI that supports hibernation. During the launch process, you need to select the instance type that supports hibernation and enable the hibernation feature on the Configure Instance Details page. Once the instance is launched with hibernation enabled, you can stop and start the instance, and it will hibernate and resume from where it left off without rebooting.\nOption A is incorrect because you do not need to relaunch a new instance from an instance store volume to support the hibernation option. Instead, you need to launch a new instance from an HVM AMI that supports hibernation.\nOption B is incorrect because although you can use the AWS console to stop the instance and select the Hibernate option, hibernation cannot be enabled on an existing instance. You need to launch a new instance from an HVM AMI that supports hibernation.\nOption D is incorrect because although you can use the AWS CLI to enable the hibernation option via “aws ec2 hibernate-instances --instance-ids i-1234567890abcdef0,” hibernation cannot be enabled on an existing instance. You need to launch a new instance from an HVM AMI that supports hibernation.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You cannot enable hibernation on an existing instance. You have to relaunch a new instance from an instance store volume to support the hibernation option.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You can use the AWS console to select the existing instance and choose Actions -> Instance State -> Stop - Hibernate.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "You cannot enable hibernation on an existing instance. You have to launch a new instance with an HVM AMI. Enable the hibernation feature on the Configure Instance Details page.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "You can only use AWS CLI to enable the hibernation option via “aws ec2 hibernate-instances --instance-ids i-1234567890abcdef0”. AWS Console is not supported for the hibernation option.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 375,
  "query" : "An organization needs a plan to store a large number of files.",
  "answer" : "Answer - C.\nOption A is incorrect because it can help maintain data, but is not low on cost and is a high-cost option since you need to maintain a Multi-AZ environment.\nHence we need to count this option out.\nOption B is incorrect because it does not talk about data loss avoidance and is more of network avoidance.\nOption C is CORRECT because S3 provides a durable, highly available, low-cost, and more secure storage solution.\nOption D is incorrect because it talks about AMIs, but not about the underlying data on EBS storage which will need to be backed up.\nMore information about Amazon S3:\nAmazon S3 is storage for the Internet.\nIt's a simple storage service that offers software developers a highly scalable, reliable, and low-latency data storage infrastructure at very low costs.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\nOption A: This option is not suitable for storing a large number of files because RDS is a relational database service, which is designed for structured data rather than unstructured data such as files. Multi-AZ deployment is used for disaster recovery and high availability, but it does not address the storage requirements for a large number of files.\nOption B: This option is also not suitable for storing a large number of files because instances are designed for running applications and services, not for storing files. Auto Scaling and Elastic Load Balancing are used for scaling applications horizontally, not for scaling storage vertically. Additionally, spot instances are not recommended for storing data because they can be terminated at any time, resulting in data loss.\nOption C: This option is the most suitable for storing a large number of files because S3 is a highly scalable and durable object storage service that is designed for unstructured data such as files. S3 provides low-cost storage, and cross-region replication can be enabled to improve availability and durability. This option also allows for easy integration with other AWS services such as Lambda and CloudFront.\nOption D: This option is also not suitable for storing a large number of files because pre-configured servers using Amazon Machine Images are designed for running applications and services, not for storing files. Elastic IP and Route 53 are used for managing IP addresses and DNS records, not for managing storage. This option also requires manual intervention to switch over to new infrastructure, which may result in downtime and data loss.\nIn conclusion, Option C is the most suitable plan for storing a large number of files due to the scalability, durability, and low cost of S3 storage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Ensure that you have RDS set up as an asynchronous Multi-AZ deployment, which automatically provisions and maintains an asynchronous “standby” replica in a different Availability Zone.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up a number of smaller instances in a different region, which all have Auto Scaling and Elastic Load Balancing enabled. If there is a network outage, these instances will auto scale up. As long as spot instances are used and the instances are small, this should remain a cost-effective solution.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "S3 should be considered due to the low cost of S3 storage. Enable cross-region replication for the S3 buckets.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up pre-configured servers using Amazon Machine Images. Use an Elastic IP and Route 53 to quickly switch over to your new infrastructure if there are any problems when you run your health checks.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 376,
  "query" : "Your company has landed a contract to build a search engine of public legal documents.\nThe dataset is around 150TB in size and is available at the customer's data center in various formats.\nPart of the dataset is stored in tapes, and the other is stored in disks.\nSome of the dataset is very old, dated to nearly 15 years back, and stored in the compressed format to save on the disk space.\nThe management has assigned you the task to come up with a flexible and cost-efficient design to ingest the data and make it available for the front-end application to search through efficiently.\nWhich two sequential steps should you choose to accomplish this final task?",
  "answer" : "E.\nCorrect Answer: A and B.\nOption A is CORRECT because Snowball Edge Storage Optimized devices can be used to transfer a large amount of data to AWS Data Centres for one-time transfer or periodic transfer.\nOption B is CORRECT because AWS Batch will allow you to run custom code to decompress the data and finally save the output to OpenSearch via the Kinesis Firehose.\nOption C is INCORRECT because it will use the standard internet line available to the company and take up a huge amount of time to migrate the data.\nOption D is INCORRECT because RDS might not be the right database for this data size and the requirement to search through it.\nOption E is INCORRECT because the Direct Connect setup will not be cost-effective for a one-time transfer.\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html https://aws.amazon.com/blogs/compute/orchestrating-an-application-process-with-aws-batch-using-aws-cloudformation/ https://aws.amazon.com/blogs/developer/orchestrating-an-application-process-with-aws-batch-using-aws-cdk/\nTo accomplish the task of ingesting the legal documents data and making it available for the front-end application to search through efficiently, the following sequential steps can be followed:\nStep 1: Transfer data to AWS S3\nThe first step is to transfer the dataset to AWS S3. Since the dataset is available at the customer's data center in various formats and part of it is stored in tapes, the best way to transfer the data to AWS S3 is by setting up a VPN connection and transferring the data over the weekend. This ensures that the data is securely transferred to AWS S3 without affecting the customer's daily operations.\nOption C is the correct answer: Set up a VPN connection and transfer the data to AWS S3 over the weekend.\nStep 2: Process and analyze data using AWS Batch and Amazon OpenSearch Service\nThe next step is to process and analyze the data using AWS Batch and Amazon OpenSearch Service. AWS Batch can be used to process the data from S3 and send it to Kinesis Firehose. Kinesis Firehose can then be used to save the processed data to Amazon OpenSearch Service. This will allow the front-end application to search through the data efficiently.\nOption B is the correct answer: Configure the AWS Batch to process the data from S3, send it to Kinesis Firehose, and save it to Amazon OpenSearch Service.\nStep 3: Alternative option - Use Snowball Edge Storage Optimized devices\nAn alternative to transferring the data over a VPN connection is to use Snowball Edge Storage Optimized devices to migrate the data to S3. This option is useful if the customer's data center has limited bandwidth and cannot transfer the data over a VPN connection efficiently. Snowball Edge Storage Optimized devices can be used to transfer the data securely to S3.\nOption A is an alternative option: Set up the Snowball Edge Storage Optimized devices to migrate the data to S3.\nStep 4: Alternative option - Use EFS and RDS\nAnother alternative is to load the data to EFS and create Auto Scaling EC2 instances to read through the data and save it into the AWS RDS for querying. This option is useful if the data needs to be accessed frequently and requires low latency. However, this option may not be cost-effective as the dataset is around 150TB in size.\nOption D is not the best answer as it is not cost-efficient: Load the data to EFS and create Auto Scaling EC2 instances to read through the data and save it into the AWS RDS for querying.\nStep 5: Alternative option - Use Direct Connect\nAnother alternative is to set up a Direct Connect connection to transfer the data from on-premise servers to S3. Direct Connect provides a dedicated network connection from the customer's data center to AWS, which ensures high-speed data transfer and reduces network costs. However, this option may not be feasible if the customer's data center is located far from an AWS Direct Connect location.\nOption E is not the best answer as it may not be feasible for all customers: Set up a Direct Connect connection to transfer the data from on-premise servers to S3.\nIn conclusion, the best sequence of steps to accomplish the task of ingesting the legal documents data and making it available for the front-end application to search through efficiently is:\nStep 1: Set up a VPN connection and transfer the data to AWS S3 over the weekend. Step 2: Configure the AWS Batch to process the data from S3, send it to Kinesis Firehose, and save it to Amazon OpenSearch Service.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up the Snowball Edge Storage Optimized devices to migrate the data to S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure the AWS Batch to process the data from S3, send it to Kinesis Firehose, and save it to Amazon OpenSearch Service.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up a VPN connection and transfer the data to AWS S3 over the weekend.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Load the data to EFS and create Auto Scaling EC2 instances to read through the data and save it into the AWS RDS for querying.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up a Direct Connect connection to transfer the data from on-premise servers to the S3.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 377,
  "query" : "A company has set up a Direct Connect connection between its on-premises location and its AWS VPC.\nIt wants to set up redundancy in case the Direct Connect connection fails.\nWhat can the company do in this regard?",
  "answer" : "Answer - A and B.\nOptions A and B are CORRECT because with A you can have a redundant Direct Connect setup as a backup if the main Direct Connect connection fails (even though it is an expensive solution, it will work)\nWith B, VPN is an alternate way for the connection between AWS and on-premises infrastructure (even though the connectivity is slow, it will work).\nMore information on Direct Connect:\nIf you have established a second AWS Direct Connect connection, traffic will failover to the second link automatically.\nWe recommend enabling Bidirectional Forwarding Detection (BFD) when configuring your connections to ensure fast detection and failover.\nIf you have configured a backup IPsec VPN connection instead, all VPC traffic will failover to the VPN connection automatically.\nTraffic to/from public resources such as Amazon S3 will be routed over the Internet.\nIf you do not have a backup AWS Direct Connect link or an IPSec VPN link, then Amazon VPC traffic will be dropped in the event of a failure.\nTraffic to/from public resources will be routed over the Internet.\nFor more information on Direct Connect FAQ's, please visit the below URL.\nhttps://aws.amazon.com/directconnect/faqs/\nTo set up redundancy for a Direct Connect connection, a company can follow the below approaches:\nA. Set up another Direct Connect connection: Setting up another Direct Connect connection with a different provider, location, or both can provide redundancy for the primary connection. The two connections can be configured with different virtual interfaces in the same VPC, each with its own router, or they can be configured in different regions to provide redundancy across regions. In case the primary connection fails, traffic can be automatically rerouted to the secondary connection.\nB. Set up an IPSec VPN Connection: Setting up an IPSec VPN connection can provide redundancy for the Direct Connect connection. The VPN connection can be set up between the on-premises location and the VPC, and traffic can be routed through the VPN if the Direct Connect connection fails. This approach requires additional setup and configuration, and the VPN connection may have lower performance compared to the Direct Connect connection.\nC. Set up S3 Connection: Setting up an S3 connection is not a solution for providing redundancy for a Direct Connect connection. S3 is an object storage service in AWS, and it does not provide connectivity between on-premises and AWS.\nD. Set up a connection via EC2 instances: Setting up a connection via EC2 instances can provide redundancy for the Direct Connect connection. The company can set up EC2 instances in different availability zones and configure them as routers between the on-premises location and the VPC. Traffic can be routed through the EC2 instances if the Direct Connect connection fails. This approach requires additional setup and configuration, and the EC2 instances may have higher latency compared to the Direct Connect connection.\nTherefore, the correct answers for providing redundancy for a Direct Connect connection are A (Set up another Direct Connect connection) and B (Set up an IPSec VPN Connection).",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Set up another Direct Connect connection.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up an IPSec VPN Connection.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Set up S3 connection.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up a connection via EC2 instances.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 378,
  "query" : "Your company stores millions of sensitive transactions across thousands of 100-GB files that must be encrypted in-transit and at-rest.\nAnalysts concurrently depend on subsets of files, which can consume up to 35 TB of space, to generate simulations that can be used to steer business decisions.\nYou are required to design an AWS solution that can cost-effectively accommodate the long-term storage and in-flight subsets of data.\nWhich one would you choose?",
  "answer" : "E.\nAnswer - D.\nThe main considerations of this scenario are: (1) the solution must be cost-effective, (2) provide long-term storage, and (3) encrypt in-transit as well as at-rest data.\nOption A is incorrect because (a) server-side encryption does not apply to in-transit data, and (b) ephemeral volumes are not encrypted at rest.\nOption B is incorrect.\nYes, Amazon S3 supports server-side encryption, but 35TB in memory not possible because the largest instances have only 25 TB of capacity.\nDue to this, we have to mark this option wrong.\nOption C is incorrect because the ephemeral drive is not long term storage.\nOption D is CORRECT because (a) EMR File system supports both in-transit and at-rest data encryption, and (b) S3 provides the long term storage.\nOption E is incorrect because this is not a cost-effective solution.\nFor more information on EMR, please visit the link below:\nhttps://aws.amazon.com/blogs/aws/new-at-rest-and-in-transit-encryption-for-amazon-emr/ https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html\nThis question requires designing an AWS solution that can accommodate the long-term storage and in-flight subsets of data that are both sensitive and large in size. The solution must also be cost-effective. Let's examine each answer option in detail and see which one is the best fit for the requirements:\nA. Use Amazon Simple Storage Service (S3) with server-side encryption, and run simulations on subsets in ephemeral drives on Amazon EC2.\nAmazon S3 is an object storage service that provides industry-leading scalability, durability, and performance. It is designed for long-term data storage, and its server-side encryption ensures that the data is encrypted at rest. However, running simulations on subsets in ephemeral drives on Amazon EC2 can lead to data loss if the instance fails or terminates. Therefore, this option is not ideal.\nB. Use Amazon S3 with server-side encryption, and run simulations on subsets in-memory on Amazon EC2.\nRunning simulations in-memory on Amazon EC2 can be fast and efficient, but it can also be expensive and may not be cost-effective for large datasets. Moreover, storing data in-memory can lead to data loss if the instance fails or terminates. Therefore, this option is also not ideal.\nC. Use HDFS on Amazon EMR, and run simulations on subsets in ephemeral drives on Amazon EC2.\nAmazon EMR is a managed Hadoop framework that can run big data processing workloads on scalable EC2 instances. Hadoop Distributed File System (HDFS) is a distributed file system that can store and manage large datasets across multiple nodes. Running simulations on subsets in ephemeral drives on Amazon EC2 can be fast and efficient, but it can also lead to data loss if the instance fails or terminates. Therefore, this option is not ideal.\nD. Use HDFS on Amazon Elastic MapReduce File System (EMRFS) in conjunction with AWS S3.\nAmazon EMRFS is a distributed file system that enables Hadoop jobs to directly access data stored in Amazon S3. HDFS on Amazon EMRFS can provide a cost-effective solution for long-term storage of large datasets. Running simulations on subsets in ephemeral drives on Amazon EC2 can be fast and efficient, but it can also lead to data loss if the instance fails or terminates. Therefore, this option is also not ideal.\nE. Store the full data set in encrypted Amazon Elastic Block Store (EBS) volumes and regularly capture snapshots that can be cloned to EC2 workstations.\nAmazon EBS provides persistent block-level storage volumes for EC2 instances. Storing the full dataset in encrypted Amazon EBS volumes ensures that the data is encrypted at rest. Regularly capturing snapshots of the data allows analysts to create clones of the data on EC2 workstations for simulations. This option provides a cost-effective solution for long-term storage of large datasets, and cloning snapshots for simulations ensures that the original data remains intact. Therefore, this option is the best fit for the requirements.\nIn conclusion, option E is the best solution for storing sensitive, large datasets in AWS cost-effectively, with both in-transit and at-rest encryption, and accommodating concurrent simulation workloads.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use Amazon Simple Storage Service (S3) with server-side encryption, and run simulations on subsets in ephemeral drives on Amazon EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon S3 with server-side encryption, and run simulations on subsets in-memory on Amazon EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use HDFS on Amazon EMR, and run simulations on subsets in ephemeral drives on Amazon EC2.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use HDFS on Amazon Elastic MapReduce File System (EMRFS) in conjunction with AWS S3.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Store the full data set in encrypted Amazon Elastic Block Store (EBS) volumes and regularly capture snapshots that can be cloned to EC2 workstations.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 379,
  "query" : "Your company has built a Therapist Finder service.\nSince the launch last year, over 150K therapists have registered from around the country, and the service is growing rapidly.\nThe management has decided to add a new, much-needed feature to showcase Verified Therapists based on a complex search context and other parameters on their website.\nThus, when users search for related therapists, the service can show verified therapists.\nTheir current database is in DynamoDB.\nThe management is ready to do some reengineering if the solution can be cost-effective as well.",
  "answer" : "Correct Answer: A.\nOption A is CORRECT because moving the data to Amazon OpenSearch will give better search options without breaking the Read Capacity of the DynamoDB.\nUse the OpenSearch queries to boost the search result efficiently.\nOption B is INCORRECT because moving the data to RDS will not solve the issue and will need significant engineering effort as well.\nOption C is INCORRECT because ElastiCache is to cache the data and not run the complex search queries.\nOption D is INCORRECT because DAX is not a service to help to search content and does not help to implement the feature.\nDAX and ElastiCache are used for different purposes than Amazon OpenSearch services.\nDAX is better than ElastiCache but cannot be compared to Amazon OpenSearch Service.\nReference:\nhttps://aws.amazon.com/blogs/compute/indexing-amazon-dynamodb-content-with-amazon-elasticsearch-service-using-aws-lambda/\nTo add a new feature to showcase verified therapists based on a complex search context and other parameters on their website, there are multiple solutions available. Four possible solutions are discussed below.\nA. Stream the DynamoDB data to Amazon OpenSearch using AWS Lambda and use it for the search: This solution involves using AWS Lambda to stream the data from DynamoDB to Amazon OpenSearch. Once the data is indexed in Amazon OpenSearch, it can be used to search for verified therapists. This solution provides a scalable and cost-effective way to perform complex searches on large datasets. AWS Lambda can be used to trigger the indexing process whenever new data is added to DynamoDB. The downside of this solution is that it requires additional setup and maintenance to stream the data to Amazon OpenSearch.\nB. Migrate the DynamoDB data to the AWS RDS database and use it for the search: This solution involves migrating the data from DynamoDB to the AWS RDS database. Once the data is in the RDS database, it can be used for the search queries. AWS RDS provides a managed database service that is scalable and reliable. However, it may not be as cost-effective as DynamoDB for large datasets. The downside of this solution is that it requires a migration process, and the RDS database may need to be optimized for search queries.\nC. Use the AWS ElastiCache in front of the DynamoDB for the search queries: This solution involves using AWS ElastiCache in front of DynamoDB to cache the search queries. ElastiCache is a fully managed in-memory data store that can be used to cache frequently accessed data. By caching the search queries in ElastiCache, the response times for the search queries can be improved. However, this solution may not be as effective for complex search queries that require multiple DynamoDB queries.\nD. Use the DynamoDB Accelerator for faster response times and save the read capacity: This solution involves using DynamoDB Accelerator (DAX) to improve the response times for the search queries. DAX is a fully managed, in-memory cache for DynamoDB that provides up to 10 times faster read performance. By using DAX, the read capacity required for the search queries can be reduced, resulting in cost savings. However, DAX may not be as effective for complex search queries that require multiple DynamoDB queries.\nIn conclusion, each solution has its pros and cons. A cost-effective solution that can be implemented with minimal reengineering and maintenance is using ElastiCache. However, if the search queries are complex and require multiple DynamoDB queries, then using DAX or migrating the data to RDS may be more suitable solutions. Finally, if scalability and performance are critical, then streaming the data to Amazon OpenSearch using AWS Lambda is the most appropriate solution.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Stream the DynamoDB data to Amazon OpenSearch using AWS Lambda and use it for the search.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Migrate the DynamoDB data to the AWS RDS database and use it for the search.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the AWS ElastiCache in front of the DynamoDB for the search queries",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the DynamoDB Accelerator for faster response times and save the read capacity.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 380,
  "query" : "As an IT administrator, you have been requested to manage the CloudFormation stacks for a set of developers in your company.\nA set of web and database developers will be working on the application.\nHow would you design the CloudFormation stacks in the best way possible?",
  "answer" : "Answer - C.\nOption A is incorrect because CloudFormation is best for creating and maintaining all the infrastructure resources in the cloud environment.\nOption B is incorrect because when your stack grows in scale and broadens in scope, managing a single stack can be cumbersome and time-consuming.\nAlso, coordinating and communicating updates can become difficult.\nOption C is CORRECT because (a) having multiple (or sub) stacks is easier to maintain, (b) there is a clear separation of ownership and concerns, (c) better chances of you staying within the limit for 'Template body size' which happens to be 460,800 bytes, and (d) you can reuse common template patterns.\nSee the \"More information...\" section for more details.\nOption D is incorrect because you can provision and maintain the infrastructure if the CloudFormation templates are created correctly.\nMore information on CloudFormation Best Practices:\nThe following use case scenario is given in the AWS documentation to support the answer:\nFor example, imagine a team of developers and engineers who own a website that is hosted on autoscaling instances behind a load balancer.\nBecause the website has its own lifecycle and is maintained by the website team, you can create a stack for the website and its resources.\nNow imagine that the website also uses back-end databases, where the databases are in a separate stack that is owned and maintained by database administrators.\nWhenever the website team or database team needs to update their resources, they can do so without affecting each other's stack.\nIf all resources were in a single stack, coordinating and communicating updates can be difficult.\nFor more information on Cloudformation best practices, please visit the below URL.\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\nAs an IT administrator managing CloudFormation stacks for developers in your company, the best approach would be to create separate CloudFormation stacks for the web and database developers (option C).\nThis approach allows for better isolation of resources, enabling more granular control and management. For instance, separate stacks enable the web and database developers to work independently without affecting each other. They can each deploy their own resources and update them as necessary, without worrying about conflicts with other resources.\nMoreover, separate stacks enable better control over security and access management. By defining separate stacks, you can assign appropriate permissions to each group of developers, ensuring that they can only access the resources that are relevant to their work. This approach helps in maintaining security and minimizing the risk of unauthorized access or changes to the resources.\nIn contrast, creating one stack for both web and database developers (option B) can lead to increased complexity and make it difficult to manage and troubleshoot issues. Additionally, this approach could lead to resource conflicts and cause delays in the deployment process.\nOption A suggests using OpsWorks instead of CloudFormation, but this may not be the best approach if your organization is already using CloudFormation. Moving to a different technology may require additional training and result in additional costs.\nOption D of defining separate EC2 instances instead of using CloudFormation is not a recommended approach as it would require manual configuration and would not enable version control, infrastructure as code, and the other benefits provided by CloudFormation.\nIn summary, the best approach would be to create separate CloudFormation stacks for the web and database developers, providing better isolation, control, and security.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "CloudFormation is not the right fit. Use OpsWork instead.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create one stack for the web and database developers.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create separate CloudFormation stacks for the web and database developers.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Define separate EC2 instances since defining CloudFormation can get cumbersome.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 381,
  "query" : "You work for an AWS consulting company and are required to provide cost control for a customer's AWS resources.\nThis customer has owned various applications which have used over 100 DynamoDB tables.\nFor the below cases, which ones should you consider using On-Demand capacity for DynamoDB? Select 3.",
  "answer" : "E.\nCorrect Answer - A, B, E.\nAmazon DynamoDB On-Demand has no capacity planning and a pay-per-request pricing model.\nYou only pay for what you use, making it easy to balance costs and performance.\nOption A is CORRECT: Because DynamoDB on-demand is useful if the application traffic is difficult to predict and control.\nOption B is CORRECT: Same reason as.\nOption A.Option C is incorrect: Because if the traffic is stable, it is better to use Provisioned capacity.\nOption D is incorrect: Because as the traffic is predictable, a Provisioned capacity with Auto Scaling can be considered.\nOption E is CORRECT: Because DynamoDB on-demand is using a pay per request model and fits into this pricing model of the serverless stacks.\nWhen it comes to cost control for a customer's AWS resources, it's important to consider DynamoDB capacity usage. DynamoDB is a NoSQL database service provided by AWS that offers On-Demand capacity and Provisioned capacity modes. The On-Demand mode offers a pay-per-request pricing model while Provisioned capacity mode provides a pre-paid model where capacity is reserved for the customer's application.\nTo decide which mode to use for DynamoDB, several factors should be considered. Here are the explanations for each answer choice:\nA. New applications whose DynamoDB database workload is very complex to forecast.\nFor new applications, it can be difficult to forecast the workload on the database. In this case, On-Demand capacity may be the better option, as it allows the customer to pay for only what they use. This way, they can avoid over-provisioning their database and paying for unused capacity.\nB. An application has large spikes sometimes, however with very short duration.\nApplications that experience occasional large spikes in traffic but for a very short duration of time can benefit from On-Demand capacity as well. This way, the customer only pays for the spikes when they occur, rather than provisioning capacity that remains unused most of the time.\nC. A long term monitor program that has stable read/write traffic for a DynamoDB table.\nFor a long-term monitor program that has a stable read/write traffic, it may be more cost-effective to use Provisioned capacity mode. This way, the customer can reserve capacity for the expected traffic, and not pay for unused capacity.\nD. The read/write throughput for a DynamoDB database is quite steady on weekdays and getting a 20% increase in weekends.\nIn this scenario, the customer can also benefit from using Provisioned capacity mode. By reserving the capacity for the steady weekday traffic, they can avoid paying for unused capacity. Meanwhile, they can use DynamoDB's auto-scaling feature to handle the 20% increase in traffic during the weekends.\nE. An application with serverless stacks with pay-per-use pricing model.\nApplications with serverless stacks, such as AWS Lambda, can also benefit from using On-Demand capacity mode. In this case, the customer only pays for the capacity that is used by the Lambda functions when they execute. This can lead to cost savings, especially when the Lambda functions are not executing frequently.\nIn summary, for cases where the workload on DynamoDB is unpredictable or sporadic, using On-Demand capacity mode is recommended. Meanwhile, for cases where the workload is more predictable and stable, using Provisioned capacity mode is more cost-effective.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "New applications whose DynamoDB database workload is very complex to forecast.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "An application has large spikes sometimes however with very short duration.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A long term monitor program that has stable read/write traffic for a DynamoDB table.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The read/write throughput for a DynamoDB database is quite steady in weekdays and getting a 20% increase in weekends.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An application with serverless stacks with pay-per-use pricing model.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 382,
  "query" : "A customer is hosting the company website on a cluster of web servers behind a public-facing Elastic Load Balancer.\nThe customer also uses Amazon Route 53 to manage their public DNS.\nHow should the customer configure the DNS zone apex record to point to the ELB?",
  "answer" : "Answer - D.\nOption A is incorrect because it suggests creating a record pointing to the IP address of the ELB; but, ELBs don't have predefined IP addresses.\nOptions B and C are incorrect because you should preferably create an ALIAS record rather than a CNAME record.\nSee the \"More information...\" section for more details.\nOption D is CORRECT - To route domain traffic to an Elastic Load Balancer, use Amazon Route 53 to create an alias record that points to your load balancer.\nAn alias record is a Route 53 extension to DNS.\nIt's similar to a CNAME record, but you can create an alias record for the root domain and subdomains.\nPlease refer to page 386 in the below link:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/route53-dg.pdf\nMore information on ALIAS Record:\nAlias resource record sets are virtual records that work like CNAME records.\nBut they differ from CNAME records in that they are not visible to resolvers.\nResolvers only see the A record and the resulting IP address of the target record.\nUnlike CNAME records, alias resource record sets are available to configure a zone apex (also known as a root domain or naked domain) in a dynamic environment.\nFor more information on the zone apex, please refer to the link below.\nhttp://docs.aws.amazon.com/govcloud-us/latest/UserGuide/setting-up-route53-zoneapex-elb.html\nFor more information on choosing between ALIAS and Non-ALIAS records, please refer to the link below.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html?console_help=true\nThe correct answer to this question is D. Create an alias record that points to your Load Balancer.\nExplanation: When a customer wants to point their domain name to an AWS resource like an Elastic Load Balancer (ELB), they can use Amazon Route 53, a DNS web service provided by AWS. In this case, the customer has already configured their website to be hosted on a cluster of web servers behind a public-facing ELB, and they want to use Route 53 to manage their public DNS.\nThe DNS zone apex record (also known as the root domain) is the record that maps the customer's domain name (e.g., example.com) to an IP address or another DNS name. In this case, the customer wants to point the DNS zone apex record to the ELB so that all incoming requests for their website are routed through the ELB.\nTo accomplish this, the customer should create an alias record that points to the ELB. An alias record is a special type of DNS record that allows the customer to map their domain name to an AWS resource like an ELB, an Amazon S3 bucket, or an Amazon CloudFront distribution.\nCreating an A record (option A) pointing to the IP address of the Load Balancer is not recommended because the IP address of the ELB can change over time, which means that the customer would need to update their DNS record manually each time the IP address changes.\nCreating a CNAME record (option B) pointing to the Load Balancer DNS name is also not recommended because the DNS specification does not allow a CNAME record to coexist with any other record type at the same DNS name. This means that the customer would not be able to create any other record types (such as MX or TXT records) at the DNS zone apex record.\nCreating a CNAME record aliased to the Load Balancer DNS name (option C) is also not recommended for the same reason as option B.\nTherefore, the correct answer is option D - Create an alias record that points to your Load Balancer.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an A record pointing to the IP address of the Load Balancer.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CNAME record pointing to the Load Balancer DNS name.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CNAME record aliased to the Load Balancer DNS name.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an alias record that points to your Load Balancer.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 383,
  "query" : "By default, when an EBS volume is attached to a Windows instance, it may show up as any drive letter on the instance.\nWhich of the following services can be used to change the settings of the drive letters of the EBS volumes as per your specifications?",
  "answer" : "Answer - C.\nOption C is correct because the EC2Config service (EC2Config.exe) is an application that enables users to make changes to windows VMs, such as changing the drive letters for any attached Amazon EBS volumes.\nEC2Config starts when the instance boots and performs tasks during startup and each time you stop or start the instance.\nPlease refer to the below link.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\nOption A is incorrect because there is no such service as EBSConfig service.\nOption B is incorrect because there is no such service as AMIConfig service.\nOption D is incorrect because there is no such service as EC2-AMIConfig service.\nThe correct answer is C. EC2Config.\nEC2Config is a Windows service provided by Amazon Web Services (AWS) that is used to manage instances running on Amazon Elastic Compute Cloud (EC2). The EC2Config service can be used to customize the configuration of instances running on EC2.\nWhen an EBS volume is attached to a Windows instance, the drive letter assigned to the volume by default may not match the desired configuration of the user. The EC2Config service can be used to change the settings of the drive letters of the EBS volumes as per the user's specifications.\nOption A, EBSConfig Service, and Option B, AMIConfig Service, are not valid AWS services. Option D, EC2-AMIConfig Service, is also not a valid AWS service.\nIn summary, EC2Config is the correct AWS service that can be used to change the drive letter settings of EBS volumes attached to Windows instances.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "EBSConfig Service",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AMIConfig Service",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "EC2Config",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "EC2-AMIConfig Service.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 384,
  "query" : "You are designing the network infrastructure for an application server in Amazon VPC.\nUsers will access all the application instances from the Internet as well as from an on-premises network.\nThe on-premises network is connected to your VPC over an AWS Direct Connect link.\nYou want to simplify the AWS routes from your on-premises network to your VPC to reduce the number of routes in the table.\nYou do not foresee any additional external routing requirements in the future.\nHow would you design routing to meet the above requirements?",
  "answer" : "Answer - B.\nOption A is INCORRECT because we would have conflicts when using the default route.\nOption C is INCORRECT because we cannot propagate two default routes.\nInstead, a single default route should be used.\nOption D is incorrect because the subnet where the instances are placed can have a single routing table associated with them.\nMore information on Route Tables and Directconnect:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html https://docs.aws.amazon.com/directconnect/latest/UserGuide/dc-ug.pdf\nThe correct answer is B. Configure a single routing table with a default route via the Virtual Private Gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets.\nExplanation:\nIn this scenario, the goal is to simplify the AWS routes from the on-premises network to the VPC and reduce the number of routes in the table. To achieve this, we need to design the routing to ensure that traffic can flow from the on-premises network to the VPC, and from the Internet to the VPC, using the most direct and efficient routes possible.\nOption A, which involves configuring a single routing table with a default route via the Virtual Private Gateway and propagating a default route via BGP on the AWS Direct Connect customer router, does not meet the requirements of the scenario. This option will result in all traffic from the on-premises network being routed via the Virtual Private Gateway, including traffic that is destined for specific subnets within the VPC.\nOption C involves configuring a single routing table with two default routes, one to the Internet via a Virtual Private Gateway and the other to the on-premises network via the VPN gateway. While this option would simplify the routing table, it does not provide any specific routes for traffic from the on-premises network to reach specific subnets within the VPC.\nOption D involves configuring two routing tables, one with a default route via the Virtual Private Gateway and another with a default route via the VPN gateway. While this option allows for more granular control over the routing of traffic from the on-premises network to specific subnets within the VPC, it requires additional configuration and management overhead.\nOption B provides the best solution. By configuring a single routing table with a default route via the Virtual Private Gateway and propagating specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router, we can ensure that traffic from the on-premises network is routed directly to the specific subnets within the VPC that it needs to reach. This option simplifies the routing table while still providing specific routes for the on-premises network to reach specific subnets within the VPC.\nTherefore, option B is the correct answer.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure a single routing table with a default route via the Virtual Private Gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a single routing table with a default route via the Virtual Private Gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure a single routing table with two default routes: one to the Internet via a Virtual Private Gateway the other to the on-premises network via the VPN gateway. Use this routing table across all subnets in your VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure two routing tables: one that has a default route via the Virtual Private Gateway and another that has a default route via the VPN gateway. Associate both routing tables with each VPC subnet.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 385,
  "query" : "Which of the following can be done by Auto Scaling?",
  "answer" : "Answer - A and B.\nOptions A and B are CORRECT because Auto Scaling can launch or terminate instances based on CPU utilization.\nOptions C and D are incorrect because Auto Scaling cannot increase or decrease the instance size based on CPU utilization.\nIt will launch the instances based on the launch configuration.\nAs per the AWS documentation, below is what can be done with Auto Scaling.\nYou can only scale horizontally and not vertically.\nScale-out Amazon EC2 instances seamlessly and automatically when demand increases.\nShed unneeded Amazon EC2 instances automatically and save money when demand subsides.\nScale dynamically based on your Amazon CloudWatch metrics or predictably according to a schedule that you define.\nReplace unhealthy or unreachable instances to maintain the higher availability of your applications.\nReceive notifications via Amazon Simple Notification Service (Amazon SNS) to be alerted when you use Amazon CloudWatch alarms to initiate Auto Scaling actions or when Auto Scaling completes an action.\nRun On-Demand or Spot Instances, including those inside your virtual private cloud (VPC) or high-performance computing (HPC) clusters.\nIf you're signed up for the Amazon EC2 service, you're already registered to use Auto Scaling and can begin using the feature via the API or command-line interface.\nFor more information on Auto Scaling, please visit the link.\nhttps://aws.amazon.com/autoscaling/\nAuto Scaling is a service offered by AWS that helps in automatically scaling resources in response to the changing demand of the application. The service allows you to ensure that the desired number of EC2 instances are available to handle the incoming traffic, while also maintaining the cost efficiency of the infrastructure. It does so by automatically launching or terminating EC2 instances based on predefined scaling policies.\nIn response to the given question, the following are the options that can be performed by Auto Scaling:\nA. Launch EC2 instances when CPU utilization is above the threshold: This option allows Auto Scaling to launch new EC2 instances when the CPU utilization of existing instances exceeds a predefined threshold. This is useful when the application's traffic increases and requires more resources to handle the increased load.\nB. Terminate EC2 instances when CPU utilization is below the threshold: This option allows Auto Scaling to terminate EC2 instances when the CPU utilization falls below a predefined threshold. This is useful when the application's traffic decreases, and there are unused resources that can be terminated to save costs.\nC. Increase the instance size when utilization is above the threshold: This option allows Auto Scaling to increase the instance size of the EC2 instances when the CPU utilization exceeds a predefined threshold. This is useful when the application's traffic requires more processing power to handle the increased load.\nD. Decrease the instance size when utilization is below the threshold: This option allows Auto Scaling to decrease the instance size of the EC2 instances when the CPU utilization falls below a predefined threshold. This is useful when the application's traffic decreases, and there are unused resources that can be terminated to save costs.\nIn conclusion, Auto Scaling can launch or terminate EC2 instances, and increase or decrease the instance size based on predefined scaling policies. These policies can be set based on the application's traffic and demand, ensuring optimal resource allocation and cost efficiency.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Launch EC2 instances when CPU utilization is above the threshold.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Terminate EC2 instances when CPU utilization is below the threshold.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Increase the instance size when utilization is above the threshold.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Decrease the instance size when utilization is below the threshold.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 386,
  "query" : "There is a requirement to create EMR jobs that sift through all web server logs and error logs to pull statistics on clickstream and errors based on client IP address.\nThe web application uses HTTPS and is behind a Classic Load Balancer.\nHow can the application be configured based on the above requirements?",
  "answer" : "Answer - C.\nOption A is incorrect because the X-Forwarded-Proto header is used to determine the protocol used between the client and the load balancer.\nAnd X-Forwarded-Port header is used to identify the destination port that the client used to connect to the load balancer.\nOption B is incorrect because it does not specify how error logs would be configured and analyzed.\nELB access logs do not tell the errors in the application layer.\nOption C is CORRECT because the web server can get the client IP address from the x-forwarded-for header for HTTP/HTTPS traffic.\nOption D is incorrect because it does not specify how access logs would be configured and analyzed.\nELB error logs do not contain the required information.\nFor more information on HTTP Headers and Classic ELB, please refer to the links below:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\nThe requirement is to create EMR jobs that can analyze web server logs and error logs to gather statistics on clickstream and errors based on the client IP address. The web application is using HTTPS and is behind a Classic Load Balancer.\nOption A suggests modifying the application code to get the client IP from either the X-Forwarded-Proto header or the X-Forwarded-Port header. However, the headers mentioned in this option are used to retrieve the protocol and port numbers used by the client to connect to the load balancer. These headers do not contain the client IP address. Therefore, option A is incorrect.\nOption B suggests configuring ELB access logs to store the logs in an S3 bucket. Then, a Data Pipeline job can be created to import the logs from S3 into EMR for analysis. The analyzed data can be output into a new S3 bucket. This option is correct as it uses the ELB access logs to analyze the client IP addresses from the web server logs and error logs. The ELB access logs contain the client IP address, and hence, they can be used for analyzing statistics based on the client IP address. Therefore, option B is the correct answer.\nOption C suggests modifying the application code to get the client IP from the x-forwarded-for header. The x-forwarded-for header is commonly used to retrieve the client IP address when the application is behind a load balancer. However, the Classic Load Balancer does not support the x-forwarded-for header. Therefore, option C is incorrect.\nOption D suggests configuring ELB error logs to store the logs in an S3 bucket. Then, a Data Pipeline job can be created to import the logs from S3 into EMR for analysis. The analyzed data can be output into a new S3 bucket. However, this option is incorrect as error logs do not contain client IP addresses. Therefore, option D is incorrect.\nIn conclusion, the correct answer is option B. Configure ELB access logs. Then create a Data Pipeline job that imports the logs from an S3 bucket into EMR for analyzing and output the EMR data into a new S3 bucket.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Modify the application code to get the client IP from the X-Forwarded-Proto header or the X-Forwarded-Port header.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure ELB access logs. Then create a Data Pipeline job that imports the logs from an S3 bucket into EMR for analyzing and output the EMR data into a new S3 bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the application code to get the client IP from the x-forwarded-for header.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure ELB error logs and create a Data Pipeline job that imports the logs from an S3 bucket into EMR for analyzing and outputs the EMR data into a new S3 bucket.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 387,
  "query" : "Which of the following benefits does adding Multi-AZ deployment in RDS provide?",
  "answer" : "Answer - A and D.\nOption A is CORRECT because in Multi-AZ deployment, if an availability zone (AZ) goes down, the automatic failover occurs.\nThe DB instance CNAME gets pointed to the synchronously updated secondary instance in another AZ.\nOption B is incorrect because Multi-AZ deployment does not affect the latency of the application's DB access.\nOption C is incorrect because DB access time does not get affected by Multi-AZ deployment.\nOption D is CORRECT because, during the maintenance tasks, the DB instance CNAME can point to the second instance in another AZ to carry out the DB tasks.\nSome of the advantages of Multi-AZ rds deployments are given below.\nIf an Availability Zone failure or DB Instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.\nThe availability benefits of Multi-AZ deployments also extend to planned maintenance and backups.\nIn the case of system upgrades like OS patching or DB Instance scaling, these operations are applied first on the standby, before the automatic failover.\nAs a result, your availability impact is, again, only the time required for automatic failover to complete.\nIf a storage volume on your primary fails in a Multi-AZ deployment, Amazon RDS automatically initiates a failover to the up-to-date standby.\nFor more information on Multi-AZ rds deployments, please visit the link.\nhttps://aws.amazon.com/rds/details/multi-az/\nMulti-AZ (Availability Zone) deployment in Amazon RDS (Relational Database Service) provides high availability and resilience for your database. When you enable Multi-AZ deployment, a secondary database instance is automatically created in a different Availability Zone. Both the primary and secondary instances are kept in sync through synchronous replication.\nThe benefits of adding Multi-AZ deployment in RDS are as follows:\nA. Multi-AZ deployed database can tolerate an Availability Zone failure: The primary database instance is located in one Availability Zone, while the secondary instance is located in another Availability Zone. In case of a planned or unplanned outage of the primary instance or its Availability Zone, Amazon RDS automatically promotes the secondary instance to the primary instance, providing uninterrupted database availability. This helps to prevent data loss and minimizes downtime.\nB. Decrease latencies if app servers accessing the database are in multiple Availability zones: With Multi-AZ deployment, if your application servers are also distributed across multiple Availability Zones, they can access the secondary database instance that is located in the same Availability Zone as the app server, reducing the network latency and improving the application's response time.\nC. Make database access times faster for all app servers: While Multi-AZ deployment may not necessarily improve the performance of your database, it can help ensure that database access times are consistent across all app servers, regardless of their location. This is because all app servers can access the same endpoint to connect to the database, which always points to the primary instance.\nD. Make database more available during maintenance tasks: When Amazon RDS performs maintenance tasks on your database, such as patching the database engine or upgrading the database instance class, it performs these tasks on the secondary instance first. Once the secondary instance is up-to-date, it's promoted to the primary instance, and the maintenance tasks are then performed on the old primary instance. This helps to minimize downtime and ensure that your database remains available during maintenance tasks.\nTherefore, the correct answer is A. Multi-AZ deployed database can tolerate an Availability Zone failure.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Multi-AZ deployed database can tolerate an Availability Zone failure.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Decrease latencies if app servers accessing the database are in multiple Availability zones.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Make database access times faster for all app servers.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Make database more available during maintenance tasks.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 388,
  "query" : "What of the following is true about the features Lambda@Edge in AWS?",
  "answer" : "Answer - C.\nOption A is incorrect as it is not used for Edge-based programming.\nOption B is incorrect because edge locations are part of the CloudFront setup, not S3.\nOption C is CORRECT because Lambda@Edge allows you to run Lambda functions at the AWS edge locations in response to CloudFront events.\nWithout Lambda@Edge, customized processing requires requests to be forwarded back to compute resources at the centralized servers.\nThis slows down the user experience.\nOption D is incorrect because Lambda@Edge supports only Node.js, which is a server-side JavaScript framework.\nFor more information on Lambda@Edge, please visit the link:\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/what-is-lambda-at-edge.html\nLambda@Edge is a service provided by AWS that allows developers to run AWS Lambda functions at AWS Edge locations, specifically at the edge locations of the Amazon CloudFront content delivery network.\nThe correct answer is C. It is used for running Lambda functions at edge locations used by CloudFront.\nWhen a user requests content from an AWS CloudFront distribution, the content is served from the CloudFront edge location closest to the user. With Lambda@Edge, developers can add custom logic to CloudFront by writing and deploying AWS Lambda functions that are triggered by CloudFront events such as viewer requests, origin requests, and response generation.\nLambda@Edge provides a range of benefits, including reducing network latency by moving computation closer to the user, improving application security, and reducing infrastructure costs by only running the code when needed.\nLambda@Edge supports any programming language that can be run within an AWS Lambda function, including Node.js, Python, Java, Go, and C#. However, the developer must package their code and dependencies into a zip file and upload it to AWS Lambda.\nIn summary, Lambda@Edge is a powerful tool that allows developers to customize and enhance the performance and security of their CloudFront distributions by running AWS Lambda functions at CloudFront edge locations.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It is used specifically for the Edge-based programing language.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It is used for running Lambda functions at edge locations defined by S3.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It is used for running Lambda functions at edge locations used by CloudFront.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "It can support any type of programming language.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 389,
  "query" : "A SAAS provider has an online training service that is deployed in AWS.\nFor each subscriber, there is a new DynamoDB table created for it.\nThe company needs the deployment in AWS to be simple.\nIt is known that the read throughput is much more than the write throughput.\nDuring the day, the read traffic is very unbalanced and spiky among various DynamoDB tables.\nWhat kind of capacity should the DynamoDB tables be configured?",
  "answer" : "Correct Answer - C.\nDynamoDB on-demand is normally used if the application traffic is difficult to predict and control, the workload has large spikes of short duration, or if the average table utilization is well below the peak.\nRefer to https://aws.amazon.com/blogs/aws/amazon-dynamodb-on-demand-no-capacity-planning-and-pay-per-request-pricing/ for its use cases.\nOption A is incorrect: Because as the traffic is very different from each other, it is improper to use a provisioned capacity.\nOption B is incorrect: Although this may work, it is not as simple as On-Demand capacity.\nThis SAAS provider needs the deployment to be simple.\nOption C is CORRECT: Because by setting up On-Demand capacity, the service provider does not need to be worried about the traffic fluctuation.\nOption D is incorrect: Because you cannot configure On-Demand for read and Provisioned for write.\nYou have to use one for both read and write.\nRefer to below:\nIn this scenario, the SAAS provider needs a simple deployment of their online training service in AWS. They have multiple subscribers and a new DynamoDB table is created for each subscriber. The read throughput is much higher than the write throughput, and the read traffic is very unbalanced and spiky during the day.\nOption A suggests configuring provisioned capacity based on the type of subscriber. However, this approach may not be feasible as the read traffic is unbalanced and spiky among various DynamoDB tables.\nOption B suggests configuring a median provisioned capacity for all tables and enabling auto-scaling when target utilization is over 80%. While this approach can handle the spikes in read traffic, it may lead to over-provisioning of resources and higher costs during periods of low utilization.\nOption C suggests setting up On-Demand capacity for individual DynamoDB tables. This approach is suitable when the workload is unpredictable, and the company wants to pay only for the resources consumed. However, On-Demand capacity may lead to higher costs if the workload is consistently high.\nOption D suggests configuring an On-Demand capacity for read and a provisioned capacity for write. This approach is suitable for the scenario described as the read traffic is much higher than the write traffic, and the read traffic is unbalanced and spiky during the day. With this approach, the SAAS provider can pay for the resources consumed by read traffic and provision sufficient capacity for write traffic.\nIn conclusion, option D is the best option for the SAAS provider to configure the capacity of DynamoDB tables in AWS for their online training service.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Depending on which type of subscriber it has, configure a provisioned capacity accordingly.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a median provisioned capacity for all tables. Enable Auto Scaling when target utilization is over 80%.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Set up On-Demand capacity for individual DynamoDB tables.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure an On-Demand capacity for read, and configure a provisioned capacity for write using a reasonable value.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 390,
  "query" : "Your company recently launched a mobile application that allows users to take pictures of what they are eating daily, share with others, and get feedback to maintain a healthy diet routine.\nThese pictures are stored in S3 and served via the CloudFront.\nThe CEO of the company thinks the storage is not well optimized by looking at the bill.\nHe has asked to generate the access log reports and shift the images to different storage if they are not frequently used.\nIt is not very clear which images will be accessed frequently, but as per the past data analysis, the usage becomes low after the initial 3-4 weeks.\nPlease select the best option to optimize the cost and also maintain accessibility.",
  "answer" : "E.\nCorrect Answer: D.\nOption A is INCORRECT because the CloudFront is used to deliver the images to end users to reduce the latency lag.\nEdge Locations work as caching servers, and it saves the images based on the frequency of use.\nHowever, CloudFront still loads the content from the origin S3 Bucket.\nOption B is INCORRECT because migrating the images to Glacier will make it almost inaccessible after 3 weeks.\nGlacier retrieval time is between 3-5 hours, and per the requirement, the application still needs to maintain accessibility.\nOption C is INCORRECT because the CloudFront already geo-distributes the load and putting an elastic load balancer in front of it is not viable.\nOption D is CORRECT because S3 Intelligent Tier automatically shifts content between Standard and Infrequent Access tier based on the usage pattern.\nIf the content is not used for 30 consecutive days, it will automatically be moved to the Infrequent Access tier.\nUpon use, it will be transferred back to the Standard tier without any additional cost.\nOption E is INCORRECT because serving images from EBS is not very scalable as compared to S3 storage.\nThere is no lifecycle event for EBS volumes.\nSource: https://aws.amazon.com/s3/storage-classes/\nThe best option to optimize the cost and maintain accessibility for the mobile application's images stored in S3 is to use S3 Intelligent Tiering. This storage class automatically moves objects between two access tiers, one for frequent access and another for infrequent access, based on changing access patterns. It is cost-effective and ensures that frequently accessed objects are readily available, while infrequently accessed objects are stored in a more cost-effective storage class.\nOption A is incorrect because while CloudFront Edge Locations and access logs can optimize the storage cache based on the access pattern, it will not shift images to a different storage class as requested by the CEO. Additionally, it may not be clear which images will be accessed frequently and which will be infrequently accessed.\nOption B is incorrect because Glacier is a long-term storage solution, and while it is cost-effective, it may not be optimal for images that users may want to access frequently.\nOption C is also incorrect because it involves creating an elastic load balancer, which adds additional complexity and cost to the solution. It also requires setting up Kinesis to process logs, which adds additional setup and management overhead. This option may work in some cases, but it is not the best solution for this specific use case.\nOption E is incorrect because it suggests storing the images on EC2 EBS, which is a block storage solution intended for use with EC2 instances. It would be more expensive to store the images on EC2 EBS, and it would require additional setup and management overhead. Moreover, it would not optimize the storage cost effectively as requested.\nTherefore, the best option for optimizing the cost and maintaining accessibility of the images stored in S3 is to use S3 Intelligent Tiering, which automatically moves objects between two access tiers, based on changing access patterns, to provide cost-effective storage for the images.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use CloudFront Edge Locations and enable access logs. It will automatically optimize the storage cache based on the access pattern.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Apply Lifecycle event to migrate from S3 Standard to Glacier after 3 weeks.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an elastic load balancer, point to the CloudFront distribution, and enable the Access Logs. Use Kinesis to process the logs and shift the images to S3 Infrequent Access based on low usage.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Store the images with S3 Intelligent Tier. It will automatically select the best storage class depending on the access pattern.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Store the images on EC2 EBS and serve it from there. EBS is highly optimized and cheap compared to S3. Put a lifecycle event to shift the unused images to S3 after 30 days.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 391,
  "query" : "Your company runs a high-end and a long-running analytics pipeline on your on-premises data centers.\nThe solution uses clusters of high configuration machines that are connected via a high throughput, low latency fiber network.\nDue to the periodic hardware and networking issues, the setup uses a replica of clusters for redundancy and failover purposes.\nThe setup is now due for a major hardware upgrade and requires a considerable budget increase as well.\nThe management has decided to evaluate if AWS can be used and check whether it is possible to replicate the same setup with low latency.\nSelect two valid options to include in your suggestion.",
  "answer" : "E.\nCorrect Answer: B and E.\nOption A is INCORRECT because using the On-demand instances might not be a cost-efficient option for running a long-running application.\nOption B is CORRECT because this option achieves the low latency network necessary for tightly-coupled node-to-node communication between the virtual machines in the same Availability Zone.\nOption C is INCORRECT because as such Partitioned Groups do span over Availability Zones, which will defeat the requirement of the application of low latency communication.\nOption D is INCORRECT because the spot instances may not be ideal for long-running applications within the partition groups.\nOption E is CORRECT because using the RI would be the best option to get some upfront discount for a long-running application.\nSure, I can provide a detailed explanation of the options.\nOption A: Use On-demand instances to minimize the cost. On-demand instances are instances that you can launch and pay for by the hour, with no upfront payment or long-term commitment. They are the most expensive type of EC2 instances, as they provide the most flexibility and convenience. However, they may not be the best option for cost optimization in this scenario since the high-end, long-running analytics pipeline requires a large number of instances that could quickly add up in cost.\nOption B: Cluster Placement Groups Cluster Placement Groups are a feature that helps you to logically group instances within a single Availability Zone to work closely together. Instances within a placement group benefit from low network latency and high network throughput as they are placed in close proximity to each other. This option could be a good fit for the high-end, long-running analytics pipeline because it requires low latency and high throughput, which the placement group can provide. However, this option only provides redundancy within a single Availability Zone, so it does not provide geographic redundancy.\nOption C: Placement Groups spread across two availability zones. This option provides a similar benefit to Option B, but with geographic redundancy. With placement groups spread across two Availability Zones, you can distribute your instances across multiple data centers, thereby increasing the resiliency of your application. This option could be a good fit for the high-end, long-running analytics pipeline, as it provides both low latency and high throughput and geographic redundancy. However, keep in mind that there will be additional cost implications associated with running instances across multiple Availability Zones.\nOption D: Use Spot instances to minimize the cost. Spot instances are instances that are available at a significantly lower cost than on-demand instances. However, their pricing is variable and determined by supply and demand, so they can be terminated with just two minutes' notice. This option may not be suitable for the high-end, long-running analytics pipeline, as the risk of instance termination could lead to data loss or pipeline disruptions.\nOption E: Use Reserved instances to minimize the cost. Reserved instances are instances that you purchase with a one- or three-year commitment upfront. They offer significant discounts compared to on-demand instances and are best suited for predictable workloads that require long-term infrastructure support. This option could be a good fit for the high-end, long-running analytics pipeline since it is a long-running workload, but it may not be the best fit for a dynamic environment that requires the flexibility to adjust instance types or regions.\nIn summary, options B and C may be the most appropriate choices for the high-end, long-running analytics pipeline, depending on the level of redundancy and cost requirements.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the On-demand instances to minimize the cost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Cluster Placement Groups",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Placement Groups spread across two availability zones.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the Spot instances to minimize the cost.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the Reserved instances to minimize the cost.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 392,
  "query" : "Your company recently got a contract to help optimize an IoT based application.\nThe application consists of thousands of on-field monitoring devices that send the bulk of data daily via Kinesis.\nOnce the data is processed, the snapshot is saved into S3 for later processing.\nThere is a deep learning application that runs once a day and consumes these snapshots of data from S3 and generates the forecast reports for the management.\nThe snapshots of data in S3 are no longer used after a month.\nThe original team optimizes the overall process well and has selected different storage options for different kinds of data.\nAn expansion plan put forward by the management will increase the devices by 100 folds and thus has suggested pinpointing the areas which can be optimized for such heavy load.\nPlease select the cost-effective option to support the scalability and maintain the durability of the data.",
  "answer" : "E.\nCorrect Answer: C.\nOption A is INCORRECT because the Redshift is used for data warehousing.\nMaintaining a cluster for such a large load will be way more costly compared to managing the snapshots to S3.\nOption B is INCORRECT because the standard object storage is comparatively costly (when compared to S3-IA) to store the snapshots.\nOption C is CORRECT because the infrequent storage would be comparatively cheaper (when compared to the S3-Standard) to store the snapshots, and it will be billed for a minimum of 30 days.\nThen, we can move the snapshots to the Glacier immediately after the processing to save the unnecessary billing days.\nOption D is INCORRECT because while saving directly to the Glacier is highly cost-effective, it is mostly used for long-term archival.\nEven with Bulk Retrieval, it would take a few hours to retrieve those snapshots and not effectively use the resources.\nOption E is INCORRECT because maintaining such a large amount of data on EFS will be an additional overhead in itself.\nSimultaneously, EFS is scalable and durable, but not cost-effective compared to the S3 Standard storage class.\nPlus, the migration has to be done to move the snapshots from the S3 to the EFS to the Glacier after the processing.\nOption A: Shift the snapshots from S3 to the Redshift cluster. This will help to scale based on the additional load and take periodic backups of Redshift to Glacier.\nRedshift is a data warehousing service that is optimized for large scale data analytics. It can handle petabyte-scale data and is designed for high throughput and parallel query processing. Moving the snapshots to Redshift can help with scalability and performance. However, Redshift is optimized for analytical workloads and is not suitable for transactional workloads. Additionally, Redshift has a higher cost compared to S3. The periodic backups to Glacier will ensure durability and reduce the cost of storage.\nOption B: Use the S3 standard option to store snapshots. After the processing is complete, change the storage class to Glacier after a month.\nS3 standard is a highly durable and available storage class that is suitable for frequently accessed data. Changing the storage class to Glacier after a month can reduce the storage cost while still maintaining durability. However, this approach may not be suitable for the increased workload as S3 may not be able to handle the increased traffic and may cause performance issues.\nOption C: Use the S3 Infrequent Access storage class to save the snapshots. Use a lifecycle rule to migrate the snapshots to Glacier.\nS3 Infrequent Access is a storage class that is designed for infrequently accessed data. It has a lower cost compared to S3 standard but with a slightly lower availability. Using a lifecycle rule to migrate the snapshots to Glacier after a month can further reduce the storage cost. This option is cost-effective and suitable for the given use case.\nOption D: Save the snapshots with the Glacier class and use the low-cost bulk retrieval option to fetch required snapshots for the deep learning program.\nGlacier is a low-cost storage service designed for long-term data archiving. It has a high latency and is not suitable for frequently accessed data. Using the low-cost bulk retrieval option can reduce the retrieval cost, but it may not be suitable for the daily deep learning program as it requires frequent access to the snapshots.\nOption E: Migrate the snapshots to EFS attached to the machine where the deep learning program is running. Once the program completes the processing, migrate the snapshots to the Glacier.\nEFS is a file system service designed for high-performance computing workloads. It is optimized for low-latency access and high throughput. However, it has a higher cost compared to S3 and is not suitable for long-term data archiving. Additionally, moving data between EFS and Glacier can be time-consuming and can add to the overall processing time.\nConclusion:\nOption C, using the S3 Infrequent Access storage class and a lifecycle rule to migrate snapshots to Glacier, is the most cost-effective option for the given use case. It provides a balance between cost and durability, and can handle the increased workload while maintaining performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Shift the snapshots from S3 to the Redshift cluster. This will help to scale based on the additional load and take periodic backups of Redshift to Glacier.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the S3 standard option to store snapshots. After the processing is complete, change the storage class to Glacier after a month.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the S3 Infrequent Access storage class to save the snapshots. Use a lifecycle rule to migrate the snapshots to Glacier",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Save the snapshots with the Glacier class and use the low-cost bulk retrieval option to fetch required snapshots for the deep learning program.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Migrate the snapshots to EFS attached to the machine where the deep learning program is running. Once the program completes the processing, migrate the snapshots to the Glacier.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 393,
  "query" : "What are the steps that get carried out by OpsWork when you attach a load balancer to a layer in OpsWork?",
  "answer" : "Answer- B, C, and D.\nFor the exam, remember that, after you attach a load balancer to a layer, AWS OpsWorks Stacks does the following.\nDeregisters any currently registered instances.\nAutomatically registers the layer's instances when they come online and deregisters instances when they leave the online state, including load-based and time-based instances.\nAutomatically activates and deactivates the instances' Availability Zones.\nHence, options B, C, and D are CORRECT.\nFor more information on working with Opswork layer ELB's, please refer to the below link.\nhttp://docs.aws.amazon.com/opsworks/latest/userguide/layers-elb.html\nPlease refer to page 218 on the below link.\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/opsworks-ug.pdf\nWhen you attach a load balancer to a layer in OpsWorks, the following steps are carried out:\nB. De-registers any currently registered instances: OpsWorks will de-register any instances that are currently registered with the load balancer, which means that the load balancer will stop sending traffic to these instances.\nC. Automatically registers the layer's instances when they come online and de-registers instances when they go offline, including load-based and time-based instances: OpsWorks will automatically register any instances that are launched within the layer and are available for use. This includes instances launched based on load and time-based scaling policies. Similarly, when instances are terminated or become unavailable, OpsWorks will automatically de-register them from the load balancer.\nD. Automatically starts routing requests to registered instances in their Availability Zones: Once instances are registered with the load balancer, OpsWorks will start routing traffic to them based on their Availability Zones. This ensures that traffic is distributed evenly across all available instances, and that instances in different Availability Zones are used as part of a high-availability configuration.\nA. Terminates the EC2 Instances: This is not a step that is carried out when you attach a load balancer to a layer in OpsWorks. Instead, instances will be terminated based on the scaling policies that are configured for the layer, which may include load-based and time-based scaling policies.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Terminates the EC2 Instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "De-registers any currently registered instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Automatically registers the layer`s instances when they come online and de-registers instances when they go offline, including load-based and time-based instances.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Automatically starts routing requests to registered instances in their Availability Zones.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 394,
  "query" : "Your company has developed a complex numerical calculation system used by some scientific and medical organizations in your country.\nThe whole application is using the Hadoop engine with an HBase database.\nBecause of the complex nature of the business, the overall setup runs in a private data center spread into two facilities for redundancy.\nThe application needs high I/O bandwidth for the workload.\nAlthough they are not running the workloads with full capacity at the moment, there are some new developments in the business, and they are extending their services to a few other organizations in a different country.\nDue to compliance reasons, they plan to host their application in the cloud instead of setting up data center facilities in the new country.\nThey want the same setup to be migrated over the cloud and still achieve the low latency, high I/O, and redundant cluster setup.\nPlease select a valid option that can help replicate the setup on AWS.",
  "answer" : "Correct Answer: C.\nOptions A and B are INCORRECT because RAID 0 is used for the disk splitting, and RAID 1 is used for the disk level mirroring.\nBecause the instances might be placed on different hardware, the high-speed I/O may not be achieved as per the requirement.\nOption C is CORRECT because the I3 instance supports the high-speed networking capability, and the Partition Placement Group can span across the multiple availability zones to achieve the required redundancy.\nOption D is INCORRECT because the Cluster Placement Groups cannot span multiple availability zones or underlying hardware.\nTo replicate the existing Hadoop setup with HBase database in AWS, we need to consider factors like low latency, high I/O, and redundant cluster setup. Based on the provided options, the best approach would be to use I3 instances with Enhanced Networking enabled, as they offer high I/O performance and low latency.\nThe I3 instances are optimized for high I/O operations, and they come with locally attached NVMe-based SSD storage. Enhanced Networking is a feature that enables higher bandwidth, lower latency, and lower jitter network performance. It can provide up to 25 Gbps of network bandwidth and has improved packet-per-second (PPS) performance compared to the standard network interface.\nOption A suggests using I3 instances with RAID 0 configuration to achieve disk mirroring. RAID 0 does not provide any redundancy, so if one disk fails, all data will be lost. Therefore, RAID 0 is not a suitable option for achieving redundant cluster setup.\nOption B suggests using I3 instances with RAID 1 configuration. RAID 1 provides mirroring of data between two disks, which offers redundancy, but it does not distribute the data across multiple disks, resulting in lower I/O performance. Therefore, RAID 1 is not a suitable option for achieving high I/O performance.\nOption C suggests using I3 instances with Enhanced Networking enabled and placing them in a Placement Group with Partitioned option to spread the clusters across multiple availability zones. Placement Groups are a feature that enables low latency, high-bandwidth networking between instances within a single availability zone. However, the Partitioned option does not spread the instances across multiple availability zones, making it a single point of failure. Therefore, Option C is not a suitable option for achieving redundant cluster setup.\nOption D suggests using I3 instances with Enhanced Networking enabled and placing them in a Cluster Placement Group with an option to spread the clusters across multiple availability zones. Cluster Placement Groups are a feature that enables low latency, high-bandwidth networking between instances across multiple availability zones. It also ensures that instances are placed on different hardware to provide additional redundancy. Therefore, Option D is a suitable option for achieving low latency, high I/O, and redundant cluster setup.\nIn conclusion, Option D is the most suitable option to replicate the existing Hadoop setup with HBase database in AWS. It involves using I3 instances with Enhanced Networking enabled and placing them in a Cluster Placement Group with an option to spread the clusters across multiple availability zones.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the I3 instances with Enhanced Networking enabled and RAID 0 configuration to achieve the disk mirroring.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the I3 instances with Enhanced Networking enabled and RAID 1 configuration to achieve the disk mirroring.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the I3 instances with Enhanced Networking enabled and use the Placement Group with Partitioned option to spread the clusters across multiple availability zones.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the I3 instances with Enhanced Networking enabled and use the Cluster Placement Group with an option to spread the clusters across multiple availability zones.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 395,
  "query" : "An IT company is managing a list of its customers' domain names.\nThe company wants to optimize its processes so that the domain names and their DNS records can be managed somewhere centralized in AWS.\nWhich option do you recommend?",
  "answer" : "E.\nCorrect Answer: B.\nOption B is CORRECT because the Route53 is a scalable and highly available Domain Name System service within AWS.\nExisting domain names can be transferred to Route53.\nOptions A, C, D, E are incorrect because they cannot be used to manage domain names in AWS.\nAmong the given options, the most suitable solution for managing a list of customer domain names and their DNS records centrally in AWS is to transfer the domains to AWS Route53 (option B).\nAWS Route53 is a highly scalable and reliable Domain Name System (DNS) service that offers various features for managing domain names, routing traffic to AWS services, and other resources. By transferring the domains to AWS Route53, the company can have centralized control over its domain names and their DNS records, and can manage them easily using the AWS Management Console, AWS CLI, or API.\nS3 Bucket folders (option A) are object storage services that are commonly used to store and retrieve data such as documents, images, videos, and other types of files. While S3 can be used to host static websites and can serve DNS records, it is not recommended for managing domain names as it lacks many features and capabilities provided by Route53.\nSQS queues (option C) are message queues used for decoupling and scaling applications. While they can be used to manage messages, notifications, and events, they are not relevant to managing domain names and their DNS records.\nVPC Peering (option D) is a networking solution used to connect two or more virtual private clouds (VPCs) together using a secure and private connection. While VPC peering can be used for interconnecting resources in different VPCs, it is not relevant to managing domain names and their DNS records.\nThe AL (option E) is not clear, and it is not possible to provide an answer without more information.\nIn conclusion, transferring the domains to AWS Route53 is the most appropriate solution for the company to manage its domain names and their DNS records centrally in AWS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use S3 Bucket folders.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Transfer existing domains to AWS route53.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure an SQS queue.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use VPC Peering.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the AL.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 396,
  "query" : "A user has created a VPC with CIDR 20.0.0.0/24\nThe user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25\nThe user has launched one instance each in the private and public subnet.\nWhich of the below mentioned options cannot be the correct IP address (private IP) assigned to an instance in the public or private subnet?",
  "answer" : "Answer - A.\nIn Amazon VPC, the first four IP addresses and the last IP address in each subnet CIDR block are not available for the user to assign to an instance.\nFor example: In this VPC, the following five IP addresses are reserved for a subnet with CIDR block 20.0.0.0/25:\n20.0.0.0: Network address.\n20.0.0.1: Reserved by AWS for the VPC router.\n20.0.0.2: Reserved by AWS.\nThe IP address of the DNS server is always the base of the VPC network range plus two; however, we also reserve the base of each subnet range plus two.\n20.0.0.3: Reserved by AWS for future use.\n20.0.0.3:Reserved by AWS for future use.\n20.0.0.255: Network broadcast address.\nWe do not support broadcast in a VPC.\nTherefore we reserve this address.\nFor more information on IP Reservation, please visit the link.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\nAs per the given scenario, the user has created a VPC with CIDR 20.0.0.0/24, which provides 256 IP addresses for use. The user has created a public subnet with CIDR 20.0.0.0/25, which gives 128 IP addresses (20.0.0.0 - 20.0.0.127), and a private subnet with CIDR 20.0.0.128/25, which also gives 128 IP addresses (20.0.0.128 - 20.0.0.255). The user has launched one instance each in the private and public subnet.\nNow let's check each option one by one:\nA. 20.0.0.255 - This IP address is not valid for any instance as it's the broadcast address of the subnet. In the given scenario, the public subnet's broadcast address is 20.0.0.127 and the private subnet's broadcast address is 20.0.0.255. Therefore, Option A cannot be the correct IP address for any instance.\nB. 20.0.0.132 - This IP address is a valid IP address for an instance in the private subnet because it falls within the CIDR range of 20.0.0.128/25.\nC. 20.0.0.122 - This IP address is a valid IP address for an instance in the public subnet because it falls within the CIDR range of 20.0.0.0/25.\nD. 20.0.0.55 - This IP address is a valid IP address for an instance in the public subnet because it falls within the CIDR range of 20.0.0.0/25.\nTherefore, the answer is option A (20.0.0.255) because it is not a valid IP address for any instance in the given scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "20.0.0.255",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "20.0.0.132",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "20.0.0.122",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "20.0.0.55",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 397,
  "query" : "You've been working on a CloudFront whole site CDN.\nAfter configuring the whole site CDN with a custom CNAME and supported HTTPS custom domain (i.e., https://domain.com), you open domain.com and receive the HTTP 502 status code (Bad Gateway).",
  "answer" : "Answer - A.\nOptions B, C, and D are INCORRECT because these options do not result in the 502 status code (Bad Gateway).\nFor more information on CloudFront CDN, please see the below links.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-502-bad-gateway.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AmazonCloudFront_DevGuide.pdf http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-502-bad-gateway.html#ssl-certificate-expired\nBased on the information provided, it is likely that the HTTP 502 Bad Gateway error is related to a problem with the CloudFront distribution or its origin server. Let's examine each answer option to determine which one is the most likely cause of the issue:\nA. The SSL/TLS certificate on the Origin has expired or missing a third-party signer. To resolve this purchase, add a new SSL certificate.\nThis option is a valid possibility because CloudFront requires a valid SSL/TLS certificate on the origin server to establish an encrypted connection. If the SSL/TLS certificate on the origin server is expired or missing a third-party signer, it can cause CloudFront to fail to establish a secure connection with the origin server. In this case, purchasing and adding a new SSL certificate to the origin server can resolve the issue.\nB. HTTPS isn't configured on the CloudFront distribution, but it is configured on the CloudFront origin.\nThis option is less likely to be the cause of the issue because CloudFront requires that HTTPS be configured on both the CloudFront distribution and the origin server. If HTTPS is not configured on the CloudFront distribution, it would typically result in a different error, such as a redirect loop or a security warning.\nC. The user does not have enough permissions to log in to the website.\nThis option is unlikely to be the cause of the issue because the HTTP 502 Bad Gateway error is related to a failure to establish a connection between CloudFront and the origin server, rather than a user authentication issue.\nD. The validation Lambda has failed to validate the request.\nThis option is also unlikely to be the cause of the issue because the HTTP 502 Bad Gateway error is related to a failure to establish a connection between CloudFront and the origin server, rather than a problem with Lambda validation.\nOverall, based on the information provided, option A seems to be the most likely cause of the issue. It is recommended to check the SSL/TLS certificate on the origin server to ensure it is valid and properly configured. If the issue persists, additional troubleshooting steps may be necessary, such as checking the CloudFront distribution configuration, verifying DNS settings, or checking the health of the origin server.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The SSL/TLS certificate on the Origin has expired or missing a third-party signer. To resolve this purchase, add a new SSL certificate.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "HTTPS isn`t configured on the CloudFront distribution, but it is configured on the CloudFront origin.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The user does not have enough permissions to login to the website.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The validation Lambda has failed to validate the request.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 398,
  "query" : "A user is using a small MySQL RDS DB.\nThe user is experiencing high latency due to the Multi-AZ feature.\nWhich of the below mentioned options may NOT help the user in this situation?",
  "answer" : "Answer - D.\nOption A is INCORRECT since scheduling automatic back-ups in non-working hours will help reduce the high latency that the user is experiencing.\nThe question is asking for a NOT option.\nTherefore this option is incorrect.\nOption B is INCORRECT because using a larger or a higher instance size will reduce the latency due to the multi-AZ feature.\nOption C is INCORRECT because using the provisioned IOPS, the users would get high throughput from the DB instance.\nThe question is asking for a NOT option.\nTherefore this option is incorrect.\nOption D is CORRECT because taking a snapshot from a standby replica would not help to reduce the latency.\n###########################\nAmazon RDS Basic Operational Guidelines.\nThe following are basic operational guidelines that everyone should follow when working with Amazon RDS.\nNote that the Amazon RDS Service Level Agreement requires that you follow these guidelines.\nMonitor your memory, CPU, and storage usage.\nAmazon CloudWatch can be set up to notify you when usage patterns change or when you approach your deployment capacity so that you can maintain system performance and availability.\nScale up your DB instance when you are approaching storage capacity limits.\nYou should have some buffer in storage and memory to accommodate unforeseen increases in demand from your applications.\nEnable automatic backups and set the backup window to occur during the daily low in write IOPS.\nIf your database workload requires more I/O than you have provisioned, recovery after a failover or database failure will be slow.\nTo increase the I/O capacity of a DB instance, do any or all of the following:\nMigrate to a DB instance class with High I/O capacity.\nConvert from standard storage to either General Purpose or Provisioned IOPS storage, depending on how much of an increase you need.\nFor information on available storage types, see Amazon RDS Storage Types.\nIf you convert to Provisioned IOPS storage, make sure you also use a DB instance class that is optimized for Provisioned IOPS.\nFor information on Provisioned IOPS, see Provisioned IOPS SSD Storage.\nIf you are already using Provisioned IOPS storage, provision additional throughput capacity.\n###########################\nFor more information on Multi-AZ RDS, please visit the link:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html\nThe Multi-AZ feature in Amazon RDS (Relational Database Service) is a feature that automatically replicates a user's primary database in a different availability zone to provide high availability and failover support. When a user's primary database becomes unavailable, the failover process is triggered and the secondary replica is promoted to become the new primary database.\nIf the user is experiencing high latency due to the Multi-AZ feature, there could be several reasons for it. For example, the user's application may be configured to connect to the primary database, and if there is a failover event, the application will need to re-establish a connection to the new primary database, which could cause latency. Additionally, the replication process between the primary and secondary database could also cause latency.\nLet's go through the options and see which of them may not help the user in this situation:\nA. Schedule the automated backup in non-working hours: This option may help the user in reducing the load on the database during working hours, but it may not necessarily help with the latency issue caused by the Multi-AZ feature.\nB. Use a large or higher size instance: This option may help the user in improving the performance of the database, but it may not necessarily help with the latency issue caused by the Multi-AZ feature.\nC. Use Provisioned IOPS storage: This option may help the user in improving the I/O performance of the database, but it may not necessarily help with the latency issue caused by the Multi-AZ feature.\nD. Take a snapshot from standby Replica: This option may also help the user in reducing the load on the primary database during working hours, but it may not necessarily help with the latency issue caused by the Multi-AZ feature.\nTherefore, the answer is that all options may help the user to some extent, but none of them may completely solve the latency issue caused by the Multi-AZ feature. The user may need to investigate further to identify the root cause of the latency and take appropriate actions to address it.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Schedule the automated backup in non-working hours.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use a large or higher size instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Provisioned IOPS storage.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Take a snapshot from standby Replica.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 399,
  "query" : "A user has created a public subnet with VPC and launched an EC2 instance within it.\nThe user is trying to delete the subnet.\nWhat will happen in this scenario?",
  "answer" : "Answer - B.\nIn AWS, when you try to delete a subnet with instances, it will not be allowed.\nThe below error message will be shown when u try to delete a subnet with instances.\nHence, option B is the CORRECT answer.\nFor more information on VPC and subnets, please visit the link.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\nWhen a user creates a public subnet within a VPC and launches an EC2 instance within it, the subnet becomes associated with that instance.\nIf the user tries to delete the subnet, the action will depend on whether the EC2 instance is running or terminated.\nIf the instance is running, AWS will not allow the user to delete the subnet. This is because a running instance needs a subnet to function, and deleting the subnet would render the instance unusable.\nTo delete the subnet, the user needs to terminate the EC2 instance first. Once the instance is terminated, the user can delete the subnet.\nIf the instance is already terminated, the user can delete the subnet without any issues. Deleting the subnet will not impact any other subnets or VPCs, as each subnet and VPC is independent of each other.\nOption A is incorrect because deleting the subnet will not automatically make the EC2 instance a part of the default subnet. The user will need to launch the instance in a different subnet.\nOption C is incorrect because deleting the subnet will not terminate the instances. As discussed, instances need to be terminated first before deleting the subnet.\nOption D is incorrect because a subnet can be deleted independently of the VPC, as long as there are no instances running within it.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It will delete the subnet and make the EC2 instance as a part of the default subnet.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It will not allow the user to delete the subnet until the instances are terminated.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "It will delete the subnet as well as terminate the instances.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The subnet can never be deleted independently, but the user has to delete the VPC first.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 400,
  "query" : "A user has created a VPC with a private and a public subnet using the VPC wizard and has also launched a NAT instance.\nThe user is trying to delete the VPC.\nWhat will happen in this scenario?",
  "answer" : "Answer - D.\nSince the user has launched a NAT instance, you will get the below error message when you try to delete the VPC.\nHence Option D is correct.\nFor more information on VPC and subnets, please visit the link.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\nThe correct answer is D. It will not allow to delete the VPC since it has a running NAT instance.\nExplanation: When a user creates a VPC using the VPC wizard in Amazon Web Services (AWS), it creates a VPC with one public subnet and one private subnet. The public subnet has an internet gateway attached to it, while the private subnet does not. The user can then launch instances in either the public or private subnet as per their requirements.\nWhen the user launches an instance in the private subnet, they need a way to connect to the internet. For this purpose, the user can launch a Network Address Translation (NAT) instance in the public subnet, which acts as a gateway for instances in the private subnet to connect to the internet.\nNow, if the user wants to delete the VPC, AWS will not allow it if there is a running NAT instance. This is because the NAT instance is a critical component of the VPC infrastructure and is required for instances in the private subnet to connect to the internet. If the user deletes the VPC without terminating the NAT instance, all instances in the private subnet lose their internet connectivity. Hence, AWS does not allow the deletion of a VPC with a running NAT instance.\nOption A, \"It will not allow to delete the VPC as it has subnets with route tables\", is incorrect because subnets with route tables are not a barrier to deleting a VPC. Route tables can be deleted before deleting a VPC.\nOption B, \"It will not allow to delete the VPC since it has a running route instance,\" is incorrect because there is no such thing as a \"route instance\" in AWS. It is likely a typo or a misinterpretation of the question.\nOption C, \"It will terminate the VPC along with all the instances launched by the wizard,\" is incorrect because it is not possible to delete a VPC with a running NAT instance, as mentioned earlier.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "It will not allow to delete the VPC as it has subnets with route tables.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It will not allow to delete the VPC since it has a running route instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It will terminate the VPC along with all the instances launched by the wizard.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "It will not allow to delete the VPC since it has a running NAT instance.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 401,
  "query" : "A user is planning to set up the Multi-AZ feature of RDS.\nWhich of the below mentioned conditions won't take advantage of the Multi-AZ feature?",
  "answer" : "Answer - C.\nAmazon RDS handles failovers automatically so you can resume database operations as quickly as possible without administrative intervention.\nThe primary DB instance switches over automatically to the standby replica if any of the following conditions occur.\nAn Availability Zone outage.\nThe primary DB instance fails.\nThe DB instance's server type is changed.\nThe operating system of the DB instance is undergoing software patching.\nA manual failover of the DB instance was initiated using Reboot with failover.\nHence, options A, B, and D are incorrect.\nOption C is CORRECT because if there is a region-wide failure, the Multi-AZ feature may not work.\nFor more information on multiAZ RDS, please visit the link:\nhttps://aws.amazon.com/rds/details/multi-az/\nThe Multi-AZ feature of Amazon RDS (Relational Database Service) is designed to provide high availability and reliability for database instances. It uses synchronous replication to maintain a standby copy of the primary database instance in a different Availability Zone (AZ) within the same region. This standby instance is updated in real-time with the changes made to the primary instance. In case of an outage or failure of the primary instance, the standby instance can be automatically promoted to become the new primary instance.\nNow, coming to the given options:\nA. Availability zone outage - This scenario is exactly what the Multi-AZ feature is designed for. In case of an AZ outage, the standby instance in a different AZ can take over and become the new primary instance, thus providing high availability and minimal downtime.\nB. A manual failover of the DB instance using Reboot with failover option - This option initiates a manual failover, which means that the user is explicitly asking for the standby instance to be promoted as the new primary instance. This also takes advantage of the Multi-AZ feature and is a recommended method to perform maintenance activities with minimal downtime.\nC. Region outage - The Multi-AZ feature cannot protect against a region-level outage as it only replicates data within the same region. In case of a region outage, the user must have a disaster recovery plan in place, such as using Amazon RDS Read Replicas in a different region or using database backup and restore.\nD. When the user changes the DB instance's server type - Changing the server type of the DB instance does not impact the Multi-AZ feature. The standby instance will continue to be updated in real-time with the changes made to the primary instance, and can take over in case of a primary instance failure.\nTherefore, the correct answer is option C - Region outage, which cannot be protected by the Multi-AZ feature of RDS.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Availability zone outage",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A manual failover of the DB instance using Reboot with failover option",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Region outage",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "When the user changes the DB instance’s server type.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 402,
  "query" : "A user has created a VPC with public and private subnets using the VPC wizard.\nIn this scenario, which component is NOT created by the VPC wizard?",
  "answer" : "Answer - A.\nOption A is correct because only one main route table will be created.\nIt is associated with the private subnet.\nA custom route table is associated with the public subnet.\nBelow is the general diagram of what is created when you have a private and public subnet used when using the VPC wizard.\n2 subnets - one private and one public.\nOne NAT Gateway to route traffic from the public to private subnet.\nOne internet gateway is attached to the VPC.Options B, C and D are incorrect because the VPC Wizard creates them by default.\nFor more information on VPC and subnets, please visit the URL:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\nSure, I'd be happy to help explain the answer to this question!\nWhen a user creates a VPC using the VPC wizard, several components are automatically created by default. These include the VPC itself, one or more subnets (depending on the options chosen), a default security group, and a default network ACL. However, not all components are created by the VPC wizard.\nLet's go through each answer choice and see which component is not created by the VPC wizard:\nA. VPC will create two main route tables associated with the public subnet and private subnet.\nThis is true. When a user creates a VPC with public and private subnets using the VPC wizard, two main route tables are created by default - one associated with the public subnet and one associated with the private subnet.\nB. VPC will create a private subnet with a size x.x.x.x/24 IPv4 CIDR block.\nThis is true. When a user creates a VPC with public and private subnets using the VPC wizard, a private subnet is created by default with a size of x.x.x.x/24 IPv4 CIDR block.\nC. VPC will create an Internet gateway.\nThis is true. When a user creates a VPC with public and private subnets using the VPC wizard, an Internet Gateway (IGW) is created by default. The IGW allows resources within the VPC to communicate with the internet.\nD. VPC will create a NAT Gateway in the public subnet.\nThis is false. When a user creates a VPC with public and private subnets using the VPC wizard, a NAT Gateway is not created by default. A NAT Gateway is used to enable instances in a private subnet to connect to the internet or other AWS services, but this must be explicitly configured by the user.\nTherefore, the correct answer to the question is D. A NAT Gateway is not created by the VPC wizard when a user creates a VPC with public and private subnets.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "VPC will create two main route tables associated with the public subnet and private subnet.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "VPC will create a private subnet with a size x.x.x.x/24 IPv4 CIDR block.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "VPC will create an Internet gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "VPC will create a NAT Gateway in the public subnet.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 403,
  "query" : "Which of the following types of servers would this CloudFormation template be most appropriate for? Choose a correct answer from the below options.",
  "answer" : "Answer - D.\nThe bastion host needs a minimum configuration and a public IP address.\nThe above CloudFormation template best fits this.\nFor more information on CloudFormation, please visit the below link-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html\nTo answer this question, we would need to see the CloudFormation template in question. However, given the options provided, we can make an educated guess based on the requirements of each server type.\nA. Domain Controller: A domain controller is typically used in a Microsoft Windows environment to manage user access and permissions. A CloudFormation template for this server would likely include the installation of Active Directory, DNS, and other related services. As such, it would require a Windows Server instance and associated licensing.\nB. Log collection server: A log collection server is used to collect and store log data from various sources for analysis and troubleshooting purposes. A CloudFormation template for this server would likely include the installation of a log collection tool such as Logstash or Fluentd, and the necessary configuration to collect and forward logs to a central repository such as Elasticsearch or Splunk. As such, it could be run on either Windows or Linux instances, depending on the log collection tool being used.\nC. Database server: A database server is used to store and manage data for an application or system. A CloudFormation template for this server would likely include the installation of a database management system such as Microsoft SQL Server, Oracle, or MySQL, and the necessary configuration to create and manage databases and users. As such, it would require either a Windows or Linux instance, depending on the database management system being used.\nD. Bastion host: A bastion host is used as a secure entry point into a private network, typically for administrative purposes. A CloudFormation template for this server would likely include the installation of an SSH or RDP server, and the necessary security group rules to restrict access to authorized users. As such, it could be run on either Windows or Linux instances, depending on the protocol being used.\nBased on the above descriptions, it is difficult to determine which type of server the CloudFormation template is most appropriate for without seeing the template itself. However, we can make some assumptions based on the typical requirements for each server type.\nIf the CloudFormation template includes the installation of Active Directory and related services, it is likely most appropriate for a Domain Controller.\nIf the CloudFormation template includes the installation of a log collection tool, it could be appropriate for a Log collection server.\nIf the CloudFormation template includes the installation of a database management system, it is likely most appropriate for a Database server.\nIf the CloudFormation template includes the installation of an SSH or RDP server, it could be appropriate for a Bastion host.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Domain Controller",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Log collection server",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Database server",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Bastion host.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 404,
  "query" : "A recent increase of users of an application hosted on an EC2 instance has caused the instance's OS to run out of CPU resources and crash.\nThe crash caused several users' unsaved data to be lost, and your supervisor wants to know how this problem can be avoided in the future.\nWhich of the following would you NOT recommend?",
  "answer" : "Answer - B.\nOption A is incorrect because this option would ensure that the user's unsaved data gets preserved just in case the instance crashes.\nOption B is CORRECT because taking frequent snapshots of the EBS volume during business hours may cause data loss (losing the data that is not yet written to the volume backed up via snapshot)\nAWS is strongly recommended to either detach the volume or freeze all writes before taking the snapshot to prevent data loss.\nHence, this option is certainly not recommended.\nOption C is incorrect because using a larger instance type can mitigate the problem of the instance running out of CPU.\nOption D is incorrect because Auto Scaling will ensure that that at least one (or minimum number of) instance(s) would be running to ensure that the application is always up and running.\nFor more information on EBS snapshots, please refer to the below URLs-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html https://forums.aws.amazon.com/thread.jspa?threadID=92160\nOption A: Redesign the application so that users' unsaved data is periodically written to disk.\nThis option is a valid solution to avoid loss of unsaved data during an EC2 instance crash. By periodically saving the users' data to disk, you ensure that even if the instance crashes, the data will not be lost. However, this option alone does not address the underlying cause of the crash, which is the insufficient CPU resources. Therefore, you should recommend this option along with other solutions to ensure that the instance can handle the increased load.\nOption B: Take frequent snapshots of the EBS volume during business hours to ensure users' data is backed up.\nThis option is not a solution to avoid the EC2 instance crash caused by insufficient CPU resources. Taking frequent snapshots of the EBS volume is a good practice to ensure that the data is backed up, but it does not prevent the instance from crashing due to lack of resources. Moreover, taking frequent snapshots during business hours may cause a significant impact on the application's performance, as it could cause I/O wait time on the EBS volume.\nOption C: Snapshot the EBS volume and re-deploy the application server as a larger instance type.\nThis option addresses the cause of the EC2 instance crash caused by insufficient CPU resources. By deploying a larger instance type, you increase the instance's CPU resources, which can handle the increased load. However, snapshotting the EBS volume and re-deploying the application server is a time-consuming process, and it involves downtime, which could cause inconvenience to the users. Moreover, this solution may not be cost-effective, as larger instance types are more expensive.\nOption D: Use Auto Scaling to deploy additional application server instances when the load is high.\nThis option is a recommended solution to avoid EC2 instance crashes caused by insufficient CPU resources. By using Auto Scaling, you can deploy additional application server instances to handle the increased load. This solution ensures that the application can handle the increased load without causing downtime or inconvenience to the users. Moreover, Auto Scaling allows you to save costs by automatically scaling down when the load decreases.\nConclusion:\nIn conclusion, the recommended solutions to avoid EC2 instance crashes caused by insufficient CPU resources are Option A, Option C, and Option D. Option B is not a recommended solution as it does not address the underlying cause of the EC2 instance crash.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Redesign the application so that users’ unsaved data is periodically written to disk.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Take frequent snapshots of the EBS volume during business hours to ensure users’ data is backed up.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Snapshot the EBS volume and re-deploy the application server as a larger instance type.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Auto Scaling to deploy additional application server instances when the load is high.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 405,
  "query" : "Why will the following CloudFormation template fail to deploy a stack?",
  "answer" : "Answer - C.\nOption C is CORRECT because the Resources section is mandatory for the CloudFormation template to work, and it is missing in this template.\nFor more information on CloudFormation templates, please refer to the below URL-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\nThe question states that a CloudFormation template will fail to deploy a stack and asks for the reason why. Let's look at each of the options provided as answers:\nA. CloudFormation templates do not use a \"Parameters\" section.\nThis answer is incorrect. The \"Parameters\" section is a standard part of a CloudFormation template and is used to pass input values into the template. It is not the reason why the template would fail to deploy a stack.\nB. A \"Conditions\" section is mandatory but is not included.\nThis answer is also incorrect. The \"Conditions\" section is optional and is used to specify conditional statements that determine whether certain resources are created or not. It is not a mandatory section and is not the reason why the template would fail to deploy a stack.\nC. A \"Resources\" section is mandatory but is not included.\nThis answer is correct. The \"Resources\" section is a mandatory part of a CloudFormation template and is used to define the AWS resources that should be created or modified when the stack is deployed. Without this section, there are no resources for CloudFormation to create or modify, and the template will fail to deploy.\nD. A template description is mandatory but is not included.\nThis answer is incorrect. While it is a best practice to include a template description, it is not a mandatory part of a CloudFormation template and is not the reason why the template would fail to deploy a stack.\nTherefore, the correct answer is C. A \"Resources\" section is mandatory but is not included.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "CloudFormation templates do not use a “Parameters” section.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A “Conditions” section is mandatory but is not included.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "A “Resources” section is mandatory but is not included.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "A template description is mandatory but is not included.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 406,
  "query" : "Your company is running a business analytics service that uses RDS with MySQL as the main database.\nThe database is configured with Multi-AZ.\nMost recently, the load on the database has increased rapidly with the launch of new features.\nBy looking at the logs, most of the load is generated by the read-only queries.\nBecause of the heavy read loads, the operations team has decided to put a set of read-replicas in place.\nThe new application features are launched via a cluster of containers into the virtual machines, where all the containers are having access to the same set of configurations.\nHow can you pair the read replicas together to make sure the application running in the containers can access them properly?",
  "answer" : "Correct Answer: A.\nOption A is CORRECT because, in Route 53, you can create individual record sets for each DNS endpoint associated with your read replicas and give them the same weight.\nThen the read requests are distributed across multiple read replicas.\nOption B is INCORRECT because the ELB cannot point to multiple replicas.\nOption C is INCORRECT because this is not a workable solution.\nElastic IP is an external resource, and routing the Elastic IP to read replica set is impossible.\nOption D is INCORRECT because it is possible to route internal Route53 multivalve answer records to a set of IPs.\nIn this scenario, the operations team has decided to use read replicas to handle the heavy read loads generated by the new application features. Read replicas in Amazon RDS provide read scaling for database workloads. Read replicas allow you to create one or more copies of your database that are in sync with the primary instance. Read replicas can be promoted to become the primary instance in the event of a primary failure, and they can also be used to offload read traffic from the primary instance.\nThe question asks how to pair the read replicas together so that the application running in the containers can access them properly. There are a few options for achieving this:\nA. Create Amazon Route 53 weighted record sets to distribute requests across the read replicas. This option involves using Amazon Route 53, a DNS service that can be used to route traffic to different resources based on various rules. Weighted record sets allow you to distribute traffic across multiple resources in proportion to their weights. In this case, you could create a weighted record set that points to each read replica, with a weight that reflects its capacity to handle read traffic. The application running in the containers could then use the DNS name associated with the weighted record set to access the read replicas.\nB. Create an ELB and point to all the read replicas. Use the URL of the ELB to access the read replicas from the application. This option involves using an Elastic Load Balancer (ELB) to distribute traffic across the read replicas. An ELB can be configured with a set of targets (in this case, the read replicas), and it will distribute incoming traffic across them using a round-robin algorithm. The application running in the containers could then use the URL associated with the ELB to access the read replicas.\nC. Create an Elastic IP and create a route table entry to point the IP to the read replicas. Use the IP to access the read replicas from the application. This option involves using an Elastic IP (EIP), which is a static IP address that can be associated with an AWS resource, such as an EC2 instance or an RDS instance. By creating a route table entry that points to the EIP, traffic can be routed to the read replicas. The application running in the containers could then use the EIP to access the read replicas.\nD. Read replica cluster is not supported in AWS. Use an external utility to create a DNS record and use the record to access the read replicas from the application. This option is incorrect because read replica clusters are supported in AWS. Also, it is not necessary to use an external utility to create a DNS record, as AWS provides DNS services such as Route 53 that can be used for this purpose.\nIn conclusion, options A, B, and C are all viable solutions for pairing the read replicas together so that the application running in the containers can access them properly. The best option will depend on the specific requirements of the application and the resources available.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create Amazon Route 53 weighted record sets to distribute requests across the read replicas.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create an ELB and point to all the read replicas. Use the URL of the ELB to access the read replicas from the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an Elastic IP and create a route table entry to point the IP to the read replicas. Use the IP to access the read replicas from the application.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Read replica cluster is not supported in AWS. Use an external utility to create a DNS record and use the record to access the read replicas from the application.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 407,
  "query" : "Which of the following is an example of a good Amazon DynamoDB hash key schema for provisioned throughput efficiency?",
  "answer" : "Answer - A.\nOption A is CORRECT because DynamoDB stores and retrieves each item based on the primary key (hash key) value which must be unique.\nEvery student would surely have a Student ID.\nHence, the data would be partitioned for each ID, which will make the data retrieval efficient.\nOption B is incorrect because the data should spread evenly across all partitions for the best throughput.\nWith only two colleges, there would be only two partitions.\nThis will not be as efficient as making Student ID the hash key.\nOption C is incorrect because partitioning on the Class ID will not be as efficient as doing so on the Student ID.Option D is incorrect because there are only two possible options: in-state and out-of-state.\nThis will not be as efficient as making Student ID the hash key.\nFor more information on DynamoDB tables, please visit the URL:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithTables.html\nTo answer this question, we need to understand how Amazon DynamoDB works and how it handles throughput. Amazon DynamoDB is a NoSQL database that provides consistent, single-digit millisecond latency at any scale. When designing a DynamoDB table, the choice of the partition key or hash key can significantly impact the performance and cost efficiency of the system.\nIn DynamoDB, data is partitioned across multiple physical storage partitions based on the partition key. Each partition has a maximum throughput limit, which is measured in read and write capacity units (RCUs and WCUs). To achieve optimal performance, it is essential to choose a hash key schema that evenly distributes the data across the partitions and minimizes hot partitions (partitions that receive significantly more requests than others).\nNow let's examine each of the options given in the question:\nA. Student ID where every student has a unique I. This is a good choice for a hash key as it is unique and evenly distributes the data across the partitions, minimizing the risk of hot partitions.\nB. College ID where there are two colleges in the university. This is not a good choice for a hash key, as it will result in only two partitions, and one partition will be much more active than the other, leading to a hot partition.\nC. Class ID where every student is in one of the four classes. This is not a good choice for a hash key, as it will result in only four partitions, and one partition will be much more active than the others, leading to a hot partition.\nD. Tuition Plan where the vast majority of students are in-state and the rest are out of state. This is not a good choice for a hash key, as it will result in only two partitions, and one partition will be much more active than the other, leading to a hot partition.\nTherefore, the correct answer is A. Student ID where every student has a unique ID, as it is unique and evenly distributes the data across the partitions, minimizing the risk of hot partitions, and therefore ensuring provisioned throughput efficiency.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Student ID where every student has a unique I.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "College ID where there are two colleges in the university.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Class ID where every student is in one of the four classes.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Tuition Plan where the vast majority of students are in state and the rest are out of state.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 408,
  "query" : "Which of the below mentioned ways can be used to provide additional layers of protection to all your EC2 resources?",
  "answer" : "Answer - D.\nTagging allows you to understand which resources belong to the test, development, and production environment if done properly.\nTags enable you to categorize your AWS resources differently, such as by purpose, owner, or environment.\nThis is useful when you have many resources of the same type - you can quickly identify a specific resource based on the tags you've assigned to it.\nEach tag consists of a key and an optional value, both of which you define.\nIf you have tagging, you can then also allow permissions based on the tags.\nYou can also use IP Address conditions in IAM policies for denying access to AWS resources.\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": {\n\"Effect\": \"Deny\",\n\"Action\": \"*\",\n\"Resource\": \"*\",\n\"Condition\": {\"NotIpAddress\": {\"aws:SourceIp\": [\n\"192.0.2.0/24\",\n\"203.0.113.0/24\"\n]}}\n}\n}\nOptions A, B, and C all provide an additional layer of protection to the EC2 instances.\nHence, D is the best answer.\nFor more information on tagging, please see the below link.\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\nAll of the options provided can be used to provide additional layers of protection to your EC2 resources, but let's examine each option in more detail:\nA. To control AWS API calls to EC2 instances, add policies that have a deny and/or allow permissions on tagged resources. This option suggests controlling API calls to EC2 instances by adding policies that specify deny or allow permissions on tagged resources. This can help you ensure that only authorized users or resources can access and manage your EC2 instances, adding an extra layer of security to your infrastructure.\nB. Ensure that the proper tagging strategies have been implemented to identify all of your EC2 resources. Proper tagging strategies can help you identify and organize your EC2 resources, and can also help you apply consistent security policies to your resources. By tagging your resources appropriately, you can easily group and manage your resources based on their function, environment, or any other criteria that make sense for your organization.\nC. Add an IP address condition to policies that specify that the requests to EC2 instances should come from a specific IP address or CIDR block range. By adding an IP address condition to policies, you can specify that requests to your EC2 instances should come from a specific IP address or CIDR block range. This can help you limit access to your instances and reduce the risk of unauthorized access or attacks.\nD. All actions listed here would provide additional layers of protection. This option suggests that all of the actions listed (A, B, and C) can be used to provide additional layers of protection to your EC2 resources.\nIn summary, to provide additional layers of protection to your EC2 resources, you can control API calls to your instances by adding policies that specify deny or allow permissions on tagged resources, implement proper tagging strategies to identify and organize your resources, and add IP address conditions to policies to limit access to your instances.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "To control AWS API calls to EC2 instances, add policies that have a deny and/or allow permissions on tagged resources.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Ensure that the proper tagging strategies have been implemented to identify all of your EC2 resources.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Add an IP address condition to policies that specify that the requests to EC2 instances should come from a specific IP address or CIDR block range.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All actions listed here would provide additional layers of protection.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 409,
  "query" : "Which of the following are correct statements with policy evaluation logic in AWS Identity and Access Management?",
  "answer" : "E.\nAnswer - C and E.\nOption A is incorrect because explicit deny always overrides an explicit allow.\nOption B is incorrect because all requests are denied by default.\nOption C is CORRECT because an explicit allow overrides the default deny.\nOption D is incorrect because explicit deny cannot be overridden by an explicit allow.\nOption E is CORRECT because all requests are denied by default.\nThe below diagram shows the evaluation logic of IAM policies.\nAnd as per the evaluation logic, it is clear that the above scenario leads to a default deny.\nFor more information on the IAM policy evaluation logic, please refer to the link-\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\nThe policy evaluation logic in AWS Identity and Access Management (IAM) determines whether a request made to AWS resources is allowed or denied based on the permissions granted or denied by policies attached to the principal (user, group, or role) making the request. The following are the correct statements with regards to policy evaluation logic in AWS IAM:\nA. An explicit deny does not override an explicit allow: This statement is true. If an explicit allow and an explicit deny conflict with each other, the explicit allow takes precedence over the explicit deny.\nB. By default, all requests are allowed: This statement is false. By default, all requests are denied, and access to resources must be explicitly granted through policies attached to the principal making the request.\nC. An explicit allow overrides default deny: This statement is also true. If there is no policy attached to the principal making the request, the default deny takes effect. However, if there is an explicit allow policy attached to the principal making the request, it overrides the default deny policy.\nD. An explicit allow overrides an explicit deny: This statement is true. If there is an explicit deny and an explicit allow that conflict with each other, the explicit allow takes precedence over the explicit deny.\nE. By default, all requests are denied: This statement is true. By default, all requests are denied, and access to resources must be explicitly granted through policies attached to the principal making the request.\nIn summary, statements A, C, D, and E are correct, while statement B is false.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "An explicit deny does not override an explicit allow.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "By default, all requests are allowed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An explicit allow overrides default deny.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "An explicit allow overrides an explicit deny.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "By default, all requests are denied.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 410,
  "query" : "A company has developed a sensor intended to be placed inside people's watches, monitoring the number of steps taken every day.\nThere is an expectation of thousands of sensors reporting every minute and hopes to scale to millions by the end of the year.\nA requirement for the project is that it needs to accept the data, run it through ETL to store in the warehouse, and archive it on Amazon Glacier, with room for a real-time dashboard for the sensor data to be added at a later date.\nWhat is the best method for architecting this application given the requirements?",
  "answer" : "Answer - D.\nOption A is incorrect because S3 is not ideal for handling huge amounts of real-time requests.\nOption B is incorrect because Amazon Cognito is not suitable for handling real-time data.\nOption C is incorrect because DynamoDB is not suitable for data ingestion and handling.\nOption D is CORRECT because the requirement is real-time data ingestion and analytics.\nThe best option is to use Kinesis for storing real-time incoming data.\nThe data can then be moved to S3 and then analyzed using EMR and Redshift.\nData can then be moved to Glacier for archival.\nMore information about the use of Amazon Kinesis:\nAmazon Kinesis is a platform for streaming data on AWS, making it easy to load and analyze streaming data, and also providing the ability for you to build custom streaming data applications for specialized needs.\nUse Amazon Kinesis Streams to collect and process large streams of data records in real-time.\nUse Amazon Kinesis Firehose to deliver real-time streaming data to destinations such as Amazon S3 and Amazon Redshift.\nUse Amazon Kinesis Analytics to process and analyze streaming data with standard SQL.\nMore information about the use of Amazon Cognito:\nAmazon Cognito lets you easily add user sign-up and sign-in and manage permissions for your mobile and web apps.\nYou can create your own user directory within Amazon Cognito, or you can authenticate users through social identity providers such as Facebook, Twitter, or Amazon; with SAML identity solutions; or by using your own identity system.\nIn addition, Amazon Cognito enables you to save data locally on users' devices, allowing your applications to work even when the devices are offline.\nYou can then synchronize data across users' devices so that their app experience remains consistent regardless of the device they use.\nThe best method for architecting this application given the requirements would be option D, which involves writing the sensor data directly to Amazon Kinesis, outputting the data into Amazon S3, and creating a lifecycle policy for Glacier archiving. Additionally, a parallel processing application is used to run the data through EMR and send it to a Redshift data warehouse. This architecture provides a scalable and efficient solution that can handle the expected high volume of sensor data.\nAmazon Kinesis is a real-time data streaming service that can handle large volumes of streaming data. In this architecture, the sensor data is directly written to Amazon Kinesis, which allows for real-time processing of the data as it is received. Kinesis provides high throughput and low latency, which is essential for processing large volumes of data in real-time.\nThe next step is to output the data to Amazon S3, where it can be stored and processed further. Amazon S3 provides a highly scalable, durable, and secure storage solution. A lifecycle policy can be applied to the S3 bucket, which moves older data to Amazon Glacier for long-term archival storage.\nTo process the data, a parallel processing application is used to run the data through EMR. Amazon EMR is a managed Hadoop framework that provides a scalable and cost-effective way to process large amounts of data. The data is extracted, transformed, and loaded (ETL) in EMR, and then sent to a Redshift data warehouse for further analysis.\nOverall, this architecture provides a scalable and efficient solution for handling the expected high volume of sensor data. It uses several AWS services that are designed for handling large volumes of data and processing it in real-time, while also providing long-term archival storage and the ability to perform complex analytics on the data.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Write the sensor data to Amazon S3 with a lifecycle policy for Glacier, create an EMR cluster that uses the bucket data, and run it through ETL. It then outputs that data into the Redshift data warehouse.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use Amazon Cognito to accept the data when the user pairs the sensor to the phone. Then have Cognito send the data to Dynamodb. Use Data Pipeline to create a job that takes the DynamoDB table and sends it to an EMR cluster for ETL, then outputs to Redshift and S3 while using S3 lifecycle policies to archive on Glacier.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Write the sensor data directly to a scalable DynamoDB; create a data pipeline that starts an EMR cluster using data from DynamoDB and sends the data to S3 and Redshift.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Write the sensor data directly to Amazon Kinesis, output the data into Amazon S3, and create a lifecycle policy for Glacier archiving. Also, have a parallel processing application that runs the data through EMR and sends it to a Redshift data warehouse.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 411,
  "query" : "You are working in a big organization as an AWS Solutions Architect.\nAt the moment, the company is managing AWS AMIs in different ways without a common pattern.\nDifferent teams follow their own processes to create AMIs based on various pipelines.\nYou plan to standardize the creation and management of AMIs through EC2 Image Builder.\nWhich of the following benefits can you achieve with EC2 Image Builder? (Select TWO)",
  "answer" : "E.\nCorrect Answers: B and D.\nOption A is incorrect because the operating system versions can be customized.\nUsers can choose a particular OS version based on their needs.\nOption B is CORRECT because users can customize components to build their desired output AMIs.\nTake the following as an example:\nOption C is incorrect because AMIs cannot be automatically deployed on EC2 instances.\nThe image pipeline provides an automation framework for building secure AMIs and container images on AWS.\nOption D is CORRECT because EC2 Image Builder supports this.\nThe generated AMIs can be distributed through an AMI pipeline.\nOption E is incorrect because EC2 Image Builder utilizes image recipes, and Packer templates cannot be used by EC2 Image Builder.\nReferences:\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/what-is-image-builder.html https://docs.aws.amazon.com/imagebuilder/latest/userguide/how-image-builder-works.html https://www.packer.io/docs/templates\nEC2 Image Builder is a fully managed service that enables you to easily build and maintain secure and up-to-date custom Amazon Machine Images (AMIs) for your applications. EC2 Image Builder provides a graphical interface, REST API, and command-line interface (CLI) to build, version, validate, test, and distribute your AMIs. With EC2 Image Builder, you can standardize the creation and management of AMIs, which simplifies the deployment of your applications, reduces security risks, and ensures compliance with your organization's policies.\nHere are the benefits you can achieve with EC2 Image Builder:\nA. EC2 Image Builder pipelines always use the latest operating system in which the latest security patches are installed.\nEC2 Image Builder provides a set of preconfigured Amazon Machine Images (AMIs) that have the latest security patches and updates installed. You can use these preconfigured AMIs as a base for your custom AMIs, which ensures that your AMIs have the latest security patches installed. EC2 Image Builder also provides a customizable schedule to ensure that your base AMI is regularly updated with the latest security patches and updates.\nB. In an EC2 Image Builder pipeline, users can choose to install Amazon managed components such as the CloudWatch agent and the CodeDeploy agent.\nEC2 Image Builder enables you to customize your AMIs with Amazon-managed components such as the CloudWatch agent, the Systems Manager agent, and the CodeDeploy agent. These components help you monitor and manage your applications and infrastructure, which improves your operational efficiency and reduces the risk of security incidents.\nC. With an EC2 Image Builder pipeline, users can automatically deploy the AMIs in EC2 instances or Auto Scaling groups in different AWS Regions.\nEC2 Image Builder enables you to deploy your custom AMIs to EC2 instances or Auto Scaling groups in different AWS Regions. This makes it easy to distribute your AMIs across multiple regions and ensures that your applications are available to users in different geographic locations.\nD. In an EC2 Image Builder pipeline, the generated AMIs can be automatically distributed to multiple AWS Regions or shared with other AWS accounts.\nEC2 Image Builder enables you to automatically distribute your custom AMIs to multiple AWS Regions or share them with other AWS accounts. This makes it easy to share your AMIs with other teams or customers and ensures that your applications are available in different environments.\nE. EC2 Image Builder pipelines are based on Packer so that Packer templates can be reused.\nEC2 Image Builder pipelines are based on Packer, an open-source tool for creating identical machine images for multiple platforms from a single source configuration. This means that you can reuse your Packer templates in EC2 Image Builder, which simplifies the migration of your existing workflows to EC2 Image Builder and makes it easy to create custom AMIs for multiple platforms.\nIn summary, EC2 Image Builder provides several benefits, including the ability to standardize the creation and management of AMIs, use preconfigured AMIs with the latest security patches, customize AMIs with Amazon-managed components, deploy AMIs to multiple regions and accounts, and reuse Packer templates in EC2 Image Builder pipelines.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "EC2 Image Builder pipelines always use the latest operating system in which the latest security patches are installed.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In an EC2 Image Builder pipeline, users can choose to install Amazon managed components such as the CloudWatch agent and the CodeDeploy agent.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "With an EC2 Image Builder pipeline, users can automatically deploy the AMIs in EC2 instances or Auto Scaling groups in different AWS Regions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In an EC2 Image Builder pipeline, the generated AMIs can be automatically distributed to multiple AWS Regions or shared with other AWS accounts.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "EC2 Image Builder pipelines are based on Packer so that Packer templates can be reused.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 412,
  "query" : "You help a team to create various AMIs and Docker images through EC2 Image Builder pipelines.\nOther teams want to use the same EC2 Image Builder resources, including components, recipes and images in their image pipelines.\nYou need to find a proper approach to share resources with other organizational units inside the AWS Organization or other specific AWS accounts.\nWhich of the following methods is suitable?",
  "answer" : "Correct Answer: D.\nOption A is incorrect because you need to configure resource shares instead of shared pipelines in AWS RAM.\nOption B is incorrect because you cannot directly share components, recipes or images through EC2 Image Builder.\nInstead, RAM should be used.\nOption C is incorrect because image pipelines cannot be used to share EC2 Image Builder resources such as components or recipes.\nOption D is CORRECT because RAM is appropriate for users to share EC2 Image Builder resources.\nTake the following snapshot as an example:\nReference:\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html#manage-shared-resources-share\nThe suitable approach to share EC2 Image Builder resources with other organizational units or AWS accounts is to use AWS Resource Access Manager (RAM) and create resource shares.\nAnswer D is the correct option. In AWS Resource Access Manager (RAM), you can create a resource share and add the shared components, images, or recipes that are used in the EC2 Image Builder pipelines. Then, you can configure the principals who are allowed to access the shared resources by adding the target organizational units or AWS accounts as principals. This approach allows the other teams to use the same shared resources in their own image pipelines.\nOption A is incorrect because it mentions creating shared pipelines, which is not a feature available in EC2 Image Builder. Instead, you create EC2 Image Builder pipelines that use shared resources.\nOption B is also incorrect because it suggests distributing resources to other organizational units or AWS accounts directly from the EC2 Image Builder. This approach does not provide the necessary control over the shared resources.\nOption C is also incorrect because sharing resources after the pipelines are finished successfully does not allow other teams to use the shared resources in their own pipelines. It also does not provide the necessary control over the shared resources.\nIn summary, the suitable approach to share EC2 Image Builder resources with other organizational units or AWS accounts is to use AWS Resource Access Manager (RAM) and create resource shares, and then add the shared components, images, or recipes in the resource shares and configure the principals which are allowed to access the shared resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In AWS Resource Access Manager (RAM), create shared pipelines by selecting the shared components, images, or recipes in EC2 Image Builder and select the target organizational units or AWS accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the EC2 Image Builder, select the shared resources such as components, recipes or images and distribute the resources to other organizational units or AWS accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the EC2 Image Builder image pipelines, share the resources to other organizational units or AWS accounts after the pipelines are finished successfully.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In AWS Resource Access Manager (RAM), add the shared components, images, or recipes in resource shares and configure the principals which are allowed to access the shared resources.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 413,
  "query" : "Your company uses a hybrid environment to host its infrastructure and services.\nApplications are mainly deployed in AWS.\nAs a part of the company strategy, certain Amazon EC2 instances need to be exported through the VM Import/Export tool as OVA files and deployed in the on-premises VMware vSphere environment.\nHow would you use the VM Import/Export tool in the right way?",
  "answer" : "Correct Answer: C.\nOption A is incorrect because Server Migration Connector is a component of AWS Server Migration Service, and it is not used by the VM Import/Export tool.\nOption B is incorrect because the VM Import/Export tool cannot be used to deploy the AWS discovery tools.\nDiscovery Tools are used by AWS Migration Hub to plan the migration from data centers to AWS.\nOption C is CORRECT because VM Import/Export uses the AWS CLI command “aws ec2 create-instance-export-task” to export an EC2 instance.\ni.e.\n“aws ec2 create-instance-export-task --instance-id my-instance-id --target-environment vmware --export-to-s3-task file://C:\\file.json”\nThis CLI command creates an OVA file in an S3 bucket.\nThe OVA file will be used to launch VMs in VMware vSphere.\nOption D is incorrect because “aws ec2 export-image” is used to export a VM directly from an Amazon Machine Image (AMI)\nPlease refer to the below.\nReferences:\nhttps://docs.aws.amazon.com/vm-import/latest/userguide/vmexport.html https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html\nThe correct answer for this question is C. Use the “aws ec2 create-instance-export-task” command to export EC2 instances and store the exported ova files in an S3 bucket.\nThe VM Import/Export tool is a feature provided by Amazon Web Services (AWS) that enables you to import virtual machine (VM) images from your virtualization environment to AWS and export them back to your on-premises environment. This tool supports multiple VM formats, including VMware vSphere.\nIn this scenario, the requirement is to export EC2 instances as OVA files and deploy them in the on-premises VMware vSphere environment. To achieve this, you can use the \"aws ec2 create-instance-export-task\" command, which allows you to export EC2 instances to an OVA file and store it in an Amazon S3 bucket.\nHere are the steps to follow:\n1. Create an S3 bucket to store the exported OVA files.\n2. Install the AWS Command Line Interface (CLI) on the machine from which you want to initiate the export task.\n3. Use the \"aws ec2 create-instance-export-task\" command to initiate the export task. This command requires several parameters, including the EC2 instance ID, the S3 bucket name, and the OVA format. For example, the following command exports an EC2 instance with the ID \"i-0123456789abcdef\" to an OVA file and stores it in the \"my-export-bucket\" bucket:\ncssCopy codeaws ec2 create-instance-export-task --instance-id i-0123456789abcdef --target-environment vmware --export-to-s3-task DiskImageFormat=OVA,ContainerFormat=ova,S3Bucket=my-export-bucket \n1. Wait for the export task to complete. You can use the \"aws ec2 describe-export-tasks\" command to check the status of the task. For example:\njavascriptCopy codeaws ec2 describe-export-tasks --export-task-ids export-ami-0123456789abcdef \n1. Download the exported OVA file from the S3 bucket to your on-premises VMware vSphere environment.\n2. Deploy the OVA file in VMware vSphere.\nIn conclusion, using the \"aws ec2 create-instance-export-task\" command to export EC2 instances and store the exported OVA files in an S3 bucket is the right way to use the VM Import/Export tool in this scenario.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Use the VM Import/Export tool to install the Server Migration Connector, generate ova files and migrate the EC2 instances to VMware vSphere.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the VM Import/Export tool to deploy the AWS discovery tools in EC2 instances and trigger migration jobs to VMware vSphere.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the “aws ec2 create-instance-export-task” command to export EC2 instances and store the exported ova files in an S3 bucket.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use the “aws ec2 export-image” command to export EC2 instances and download the exported ova files in a local disk.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 414,
  "query" : "AWS Lambda functions are widely used in your company by different teams.\nThere is a requirement to extend the Lambda functions to integrate with other tools for monitoring, observability and security such as AppDynamics, HashiCorp, Splunk, etc.\nWith the integration, some additional code needs to be run during function initialization, invocation or shut down.\nWhich of the following options can achieve the requirement by extending Lambda's execution environment?",
  "answer" : "Correct Answer: A.\nOption A is CORRECT because Lambda Extensions provides an easy way to integrate Lambda with other tools.\nThe Lambda Extensions can be deployed through Lambda Layers that are generated by the .zip archive files.\nLambda service extracts the extension files from the Lambda layers and manages the lifecycle of extensions.\nOption B is incorrect because there is no need to create other Lambda functions.\nThe existing Lambda functions should be extended by Lambda Extensions.\nOption C is incorrect because CloudWatch event rules cannot trigger additional code during function initialization or shut down.\nCreating extra Lambda functions is not necessary.\nOption D is incorrect because Lambda extensions cannot be used to process SQS messages from Lambda functions.\nSQS queues are not required for building extensions for AWS Lambda.\nReferences:\nhttps://aws.amazon.com/blogs/compute/building-extensions-for-aws-lambda-in-preview/, https://docs.aws.amazon.com/lambda/latest/dg/using-extensions.html\nLambda extensions are a way to integrate Lambda functions with tools for monitoring, observability, and security. They allow for additional code to be run during function initialization, invocation, or shut down. There are different options to extend Lambda's execution environment, and the question asks which one can achieve the requirement of integrating Lambda with other tools.\nA. Deploy Lambda extensions through Lambda layers by adding the extensions in the Lambda .zip archives.\nLambda layers are a way to manage common code and libraries across multiple Lambda functions. They allow you to package and deploy external dependencies that can be used by your function code. While it is possible to add Lambda extensions to a Lambda layer and deploy it with the function code, this option does not allow for a specific initialization or shut down order, as the layer is loaded alongside the function code. This option is not ideal if the additional code needs to run during a specific phase of the function execution.\nB. Deploy the required tools in other dedicated Lambda functions and integrate existing Lambda functions with these dedicated Lambda functions.\nThis option suggests deploying other Lambda functions that handle the integration with the required tools. While it is possible to do this, it adds complexity to the system and creates additional overhead, as it requires invoking multiple functions to complete the same task. This option is not optimal for performance and maintenance reasons.\nC. Extend Lambda function with CloudWatch event rules by configuring third-party Lambda functions to process events.\nCloudWatch event rules allow you to define events that trigger the execution of a Lambda function. This option suggests configuring third-party Lambda functions to process events and trigger the extension code. While this option may work for some scenarios, it may not be suitable for situations where the function needs to run the additional code during initialization or shut down.\nD. Integrate the Lambda function with Amazon SQS and process the queue messages by Lambda extensions.\nThis option suggests using Amazon SQS as a buffer to process messages containing the additional code to be executed during function initialization, invocation, or shut down. SQS can act as a trigger for Lambda, which can then process the messages and execute the additional code. This option allows for precise control over when the code is executed and provides a scalable and reliable way to handle the integration with the required tools.\nIn conclusion, option D is the best approach to achieve the requirement of extending Lambda's execution environment by integrating it with other tools for monitoring, observability, and security.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Deploy Lambda extensions through Lambda layers by adding the extensions in the Lambda .zip archives.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Deploy the required tools in other dedicated Lambda functions and integrate existing Lambda functions with these dedicated Lambda functions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Extend Lambda function with CloudWatch event rules by configuring third-party Lambda functions to process events.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Integrate the Lambda function with Amazon SQS and process the queue messages by Lambda extensions.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 415,
  "query" : "You are developing a Lambda function by packaging the Python code and dependencies in a container image which is stored in an AWS ECR repository.\nThere is a requirement to add extensions to the Lambda function for the integration with governance tools.\nYou have already prepared the extension code as my-extension.zip.\nWhat is the correct way to add the extensions to the Lambda function?",
  "answer" : "Correct Answer: B.\nOption A is incorrect because the Lambda function is managed with a Docker container image.\nUsers should not directly edit the Lambda function code through the Lambda console.\nOption B is CORRECT because the extensions files should be packaged into the /opt directory in the container image to use extensions in container images of Lambda functions.\nAfter the my-extension.zip file is put in the /opt folder, Lambda functions will use the Extensions API to manage the extension lifecycle.\nOption C is incorrect because update-function-code updates a Lambda function's code, and update-function-configuration modifies Lambda settings.\nEither of them cannot add extensions to Docker images.\nOption D is incorrect because extensions can be added to the container image of the Lambda function.\nReferences:\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html https://docs.aws.amazon.com/lambda/latest/dg/using-extensions.html https://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-configuration.html\nThe correct way to add extensions to a Lambda function that is packaged as a container image stored in an AWS ECR repository is D: Lambda extensions cannot be added to a container image. Instead, the Lambda function should use Lambda layers and the extension zip file should be added as a layer.\nLambda extensions are a feature of AWS Lambda that enables developers to integrate Lambda functions with third-party monitoring, observability, and security tools. Extensions are separate processes that can run in parallel with the Lambda function code and provide additional functionality, such as logging, tracing, and security.\nWhen building a Lambda function as a container image, it is not possible to add Lambda extensions directly to the container image. Instead, the recommended approach is to use Lambda layers to add the extension code to the Lambda function.\nLambda layers are a distribution mechanism for libraries, custom runtimes, and other function dependencies. Layers are essentially ZIP archives that contain libraries or other dependencies that are needed by the function code. Layers can be created and managed using the AWS Management Console, AWS CLI, or SDKs.\nTo add the extension code as a layer, you can create a new layer using the AWS CLI or Management Console and include the my-extension.zip file as the layer content. You can then attach the layer to the Lambda function by adding a reference to the layer in the function configuration.\nBy using layers to add Lambda extensions to the function, you can keep the function code and the extension code separate and manage them independently. This approach also allows you to reuse the same extension across multiple Lambda functions.\nIn summary, the correct way to add extensions to a Lambda function that is packaged as a container image stored in an AWS ECR repository is to use Lambda layers and add the extension code as a layer.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the Lambda console, edit the Lambda function code and add the my-extension.zip file to the /opt/extensions/ folder to install the governance tools.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the Dockerfile, add the my-extension.zip file to the /opt folder. Rebuild the Docker image and deploy the Docker container image to the Lambda function.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS Lambda CLI update-function-code or update-function-configuration command to upload the my-extension.zip file, unzip it and install the extensions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Lambda extensions cannot be added to a container image. Instead, the Lambda function should use Lambda layers and the extension zip file should be added as a layer.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 416,
  "query" : "Your team is developing a new Lambda function for a microservice component.\nYou need to package and deploy the Lambda function as a container image.\nThe container image should be built based on the python:buster image with other dependencies and libraries installed.\nIn order to use the container image properly for the Lambda function, which of the following actions is required?",
  "answer" : "Correct Answer: D.\nOption A is incorrect because there is no need to assume the IAM role in the Dockerfile.\nUsers should configure the IAM role when creating the Lambda function.\nOption B is incorrect because Lambda automatically forwards logs to CloudWatch Logs.\nUsers do not need to install the CloudWatch agent in Dockerfile.\nOption C is incorrect because after the Docker image has been built, users should push the image to ECR.\nThere is no ECR agent that needs to be installed in the Docker image.\nOption D is CORRECT because the container image for Lambda must implement the Lambda Runtime API that is added by the open-source runtime interface client.\nReferences:\nhttps://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/ https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\nTo package and deploy a Lambda function as a container image, you will need to follow the steps below:\n1.\nCreate a Dockerfile: A Dockerfile is a script that contains a set of instructions for building a Docker image. In this case, you will need to start with the python:buster image and add any dependencies or libraries that your Lambda function requires. You can use any package manager such as pip, conda or apt-get to install the dependencies.\n2.\nBuild the Docker image: Once you have created the Dockerfile, you can use it to build a Docker image using the \"docker build\" command. This will create a Docker image that contains your Lambda function code and all its dependencies.\n3.\nPush the Docker image to a container registry: To use the Docker image for your Lambda function, you will need to push it to a container registry such as Amazon Elastic Container Registry (ECR). This will allow Lambda to fetch the image and create a container from it.\nNow, let's review the answer options to determine which is required:\nA. In the Dockerfile, assume the IAM role through the “aws sts assume-role” CLI for the Lambda function during runtime. This is not required to package and deploy the Lambda function as a container image. Assuming the IAM role through the “aws sts assume-role” CLI is required if the Lambda function needs to access AWS resources, but it does not apply to building and deploying the container image.\nB. Install the CloudWatch Log agent in the container image for the Lambda function to forward its logs to a CloudWatch Log group. This is not required to package and deploy the Lambda function as a container image. The CloudWatch Log agent is used to forward logs from the container to CloudWatch Logs, but it is not necessary for building and deploying the container image.\nC. Install the Amazon Elastic Container Registry (Amazon ECR) agent for the Lambda function to interact with ECR to fetch the Docker image. This is not required because Amazon ECR is a container registry service that manages Docker images and makes them available to services like Lambda. Lambda can fetch the Docker image directly from Amazon ECR without any additional agent.\nD. Install the runtime interface client in the container image to make it compatible with Lambda. This is the correct answer. The runtime interface client is a piece of software that provides the interface between the Lambda service and the container. When Lambda creates a container from the Docker image, it will use the runtime interface client to communicate with the container and invoke the Lambda function code.\nTherefore, the action required to use the container image properly for the Lambda function is to install the runtime interface client in the container image to make it compatible with Lambda.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the Dockerfile, assume the IAM role through the “aws sts assume-role” CLI for the Lambda function during runtime.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Install the CloudWatch Log agent in the container image for the Lambda function to forward its logs to a CloudWatch Log group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Install the Amazon Elastic Container Registry (Amazon ECR) agent for the Lambda function to interact with ECR to fetch the Docker image.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Install the runtime interface client in the container image to make it compatible with Lambda.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 417,
  "query" : "Your team uses Elastic Beanstalk to manage a legacy JAVA application for a financial system.\nThe Elastic Beanstalk environment is based on Amazon Linux.\nNow you need to update the operating system to Amazon Linux 2 in order to take advantage of the latest Elastic Beanstalk functionality.\nDuring the update, there should be minimal service impact to the application.\nWhich of the following options describes the correct order to perform the operating system update?",
  "answer" : "Correct Answer: C.\nOption A is incorrect because terminating the old environment is not the correct procedure to update the operating system.\nSo, step 5 should not be in the first position.\nOption B is incorrect because step 2 is the unwanted step that does not describe any updates.\nOption C is CORRECT because this sequence describes the blue/green update strategy by creating a new environment, testing it and then swapping the DNS for the production environment.\nOption D is incorrect because issues cannot be followed before creating and deploying the code.\nThe sequence does not describe any of the update strategies.\nReferences:\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-platform-update-managed.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html\nThe correct order to update the operating system to Amazon Linux 2 in an Elastic Beanstalk environment with minimal service impact to the application is as follows:\n1.\nPerform a snapshot of the existing environment: Before making any changes to the environment, it is recommended to take a snapshot of the existing environment to ensure that you have a backup in case anything goes wrong during the update process. This will also allow you to roll back to the previous environment if needed.\n2.\nCreate a new environment: Create a new Elastic Beanstalk environment with Amazon Linux 2 as the operating system. This new environment will have the latest Elastic Beanstalk functionality and will allow you to test the application on the new operating system before switching to it.\n3.\nDeploy the application to the new environment: Deploy the application to the new environment and ensure that it is working correctly. This will allow you to test the application on Amazon Linux 2 without impacting the production environment.\n4.\nSwitch the DNS to the new environment: Once you have tested the application on the new environment, you can switch the DNS to point to the new environment. This will direct traffic to the new environment, and the old environment will no longer receive traffic.\n5.\nTerminate the old environment: After you have switched the DNS to the new environment and ensured that everything is working correctly, you can terminate the old environment. This will free up resources and prevent any unnecessary charges.\nTherefore, the correct order to perform the operating system update is B. 2 > 3 > 1 > 5, which involves creating a new environment with Amazon Linux 2, deploying the application to the new environment, switching the DNS to the new environment, and terminating the old environment.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "5 > 2 > 1 > 4",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "2 > 3 > 1 > 5",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "3 > 4 > 1 > 5",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "4 > 2 > 3 > 1",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 418,
  "query" : "You are working in a company as an AWS engineer.\nYour company uses a lot of Elastic Beanstalk applications on different platforms.\nMost of the Elastic Beanstalk environments do not enable platform updates.\nSo, your team has to update the platforms during scheduled maintenance windows manually.\nYou would like to enable managed platform updates through the Elastic Beanstalk console.\nWhich of the following options do you need to configure for the managed platform updates? (Select TWO.)",
  "answer" : "E.\nCorrect Answers: A and C.\nOption A is CORRECT because, for the managed platform updates in Elastic Beanstalk, you can choose the update level to be “Minor versions and patches” or “Patch only”.\nOption B is incorrect because Managed platform updates don't support updates across platform branches such as operating system, runtime, or Elastic Beanstalk components.\nOption C is CORRECT because you can select the weekly update time period for Elastic Beanstalk to perform the updates shown below:\nOption D is incorrect because users cannot configure the Instance reboot option.\nOption E is incorrect because the patch baseline from Systems Manager cannot be configured for the Elastic Beanstalk managed platform updates.\nReference:\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-platform-update-managed.html\nTo enable managed platform updates in Elastic Beanstalk, you need to configure the following two options:\nA. The update level: The update level determines the amount of control you have over the updates. There are two update levels to choose from:\nMinor updates only: This option will automatically apply only minor updates, such as patch releases and security updates, without requiring any additional configuration.\nAll updates: This option allows both minor and major updates, which include new feature releases that may require additional configuration.\nB. The weekly update period: The weekly update period specifies the day and time when Elastic Beanstalk should perform the updates. You can choose a specific day of the week and a specific time of day in the time zone of the environment. You should schedule the update period during a low-traffic time for your application to minimize downtime.\nOther options mentioned in the question are not related to managed platform updates in Elastic Beanstalk:\nC. The operating system: The operating system is chosen when creating the environment and cannot be changed later. It does not affect managed platform updates.\nD. Instance reboot: Instance reboot is not related to managed platform updates. However, during a platform update, Elastic Beanstalk will replace the running instances with new instances running the updated platform version.\nE. Patch baseline from Systems Manager: Patch baseline from Systems Manager is used for managing patch updates for EC2 instances running in an Amazon EC2 Auto Scaling group or an Amazon Elastic Container Service (ECS) cluster. It is not related to managed platform updates in Elastic Beanstalk.\nIn summary, to enable managed platform updates in Elastic Beanstalk, you need to configure the update level and the weekly update period.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The update level.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The operating system.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The weekly update period.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Instance reboot.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Patch baseline from Systems Manager.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 419,
  "query" : "Your company deploys an internal application in an Elastic Beanstalk environment which is created in a private VPC and has no access to the internet.\nThe application is used for monitoring and logging, and other VPC applications need to send requests to the internal application.\nFor security purposes, the traffic to the Elastic Beanstalk service should stay inside the Amazon network without exposure to the internet.\nHow would you achieve this requirement?",
  "answer" : "Correct Answer: B.\nOption A is incorrect because NAT Gateway is used to provide internet connectivity to private subnets.\nSo, there is no need for NAT Gateway for the above-mentioned requirement.\nOption B is CORRECT because the traffic to Elastic Beanstalk needs to be private without being exposed to the internet.\nThis requirement can be achieved by the VPC endpoint service powered by AWS PrivateLink.\nWith PrivateLink, users do not need to configure other connectivity services such as VPN connection or AWS Direct Connect.\nCheck the following snapshot for how to create the VPC endpoint:\nOption C is incorrect because disabling the DNS name is not a feasible option in an Elastic Beanstalk environment.\nOption D is incorrect because, in order to send other applications' traffic to the Elastic Beanstalk service, something must be used, i.e., VPC endpoint.\nReferences:\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/vpc.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/vpc-vpce.html\nThe requirement is to allow other VPC applications to send requests to an Elastic Beanstalk environment, which is created in a private VPC and has no access to the internet, without exposing the traffic to the internet for security purposes.\nOption A suggests creating a NAT Gateway in the public subnet and modifying the route table to connect other applications and the Elastic Beanstalk service through the NAT Gateway. This option is correct as NAT Gateway allows instances in a private subnet to connect to the internet or other AWS services, such as Elastic Beanstalk, through a public IP address without exposing the traffic to the internet. In this case, the NAT Gateway is placed in the public subnet, and the route table for the private subnet is updated to route traffic to the Elastic Beanstalk service via the NAT Gateway.\nOption B suggests configuring an interface VPC endpoint for the Elastic Beanstalk service. Requests are sent to Elastic Beanstalk through AWS PrivateLink. This option is also correct as AWS PrivateLink enables private connectivity between VPCs and AWS services, such as Elastic Beanstalk, without using public IPs or requiring traffic to traverse the internet. By configuring an interface VPC endpoint for Elastic Beanstalk, traffic is routed through a private IP address and remains within the Amazon network.\nOption C suggests disabling the DNS name in the Elastic Beanstalk environment to disallow connections through the public endpoint of Elastic Beanstalk. This option is incorrect as disabling the DNS name in Elastic Beanstalk environment will not prevent connections from other VPC applications. Furthermore, the Elastic Beanstalk environment requires DNS to resolve the endpoints for the services.\nOption D suggests that nothing needs to be done as Elastic Beanstalk provides the private DNS “com.amazonaws.region.elasticbeanstalk” by default. This option is incorrect as the private DNS name does not provide connectivity between VPCs and Elastic Beanstalk.\nTherefore, the correct options to achieve the requirement are A and B. Option A can be used when a NAT Gateway is preferred, and Option B can be used when using PrivateLink is preferred.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create a NAT Gateway in the public subnet. Modify the route table to connect other applications and the Elastic Beanstalk service through the NAT Gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an interface VPC endpoint for the Elastic Beanstalk service. Requests are sent to Elastic Beanstalk through AWS PrivateLink.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Disable DNS name in the Elastic Beanstalk environment to disallow the connections through the public endpoint of Elastic Beanstalk.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Nothing needs to be done as Elastic Beanstalk provides the private DNS “com.amazonaws.region.elasticbeanstalk” by default.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 420,
  "query" : "You are an AWS Solutions Architect.\nA company owns a large number of batch processing workloads in a local data center and plans to migrate these jobs in AWS Batch.\nThe company needs your help to decide which provisioning model should be chosen in AWS Batch jobs.\nWhich of the following situations would you use for AWS Fargate in AWS Batch? (Select TWO)",
  "answer" : "E.\nCorrect Answers: B and D.\nOption A is incorrect.\nBecause, for this option, the EC2 instance is more suitable as users can choose the instance configurations.\nOption B is CORRECT.\nBecause with Fargate, jobs can start faster at the initial scale-out of work as users do not need to wait for EC2 instances to launch.\nOption C is incorrect.\nBecause the EC2 instance is more suitable as users need to customize the AMI and instance type.\nUsers cannot do this with Fargate.\nOption D is CORRECT.\nBecause Fargate manages the servers and clusters of EC2 instances for AWS Batch jobs.\nUsers do not need to worry about the details of compute resources.\nOption E is incorrect.\nBecause with Fargate, users cannot select instance types or the minimum number of vCPUs.\nReferences:\nhttps://docs.aws.amazon.com/batch/latest/userguide/fargate.html https://aws.amazon.com/batch/faqs/\nAWS Batch is a fully managed service that enables you to run batch computing workloads on the AWS Cloud. AWS Batch provisions and optimizes the infrastructure required to execute batch computing workloads. It also enables you to use compute resources efficiently and automatically scale resources as your workload demands change.\nWhen deciding which provisioning model to choose for AWS Batch jobs, you need to consider various factors such as job requirements, workload size, and resource utilization. The following are the two situations where you would use AWS Fargate in AWS Batch:\nA. When a batch job needs access to particular instance configurations, including processors and GPUs: In this scenario, AWS Fargate would be a good option because it enables you to specify the exact amount of CPU and memory resources required for the job. Additionally, Fargate provides the flexibility to add GPU resources if required, which makes it an ideal choice for workloads that need to leverage specialized hardware.\nE. When a batch job needs the instance type to be C5 with multiple vCPUs: AWS Fargate offers a wide range of instance types with varying amounts of CPU, memory, and network resources. If the batch job requires a specific instance type, such as C5 with multiple vCPUs, Fargate can provision that instance type automatically. This ensures that the batch job has the necessary resources to run efficiently.\nThe other options in the question are not suitable for AWS Fargate provisioning in AWS Batch. For example:\nB. When a batch job needs to be started fast at the initial scale-out of work: While AWS Fargate does provide quick start times for containers, it may not be the ideal choice for batch jobs that need to scale quickly. Instead, AWS Batch can be used to quickly provision resources to meet the workload demand.\nC. When a batch job needs to use a compute environment based on a custom AMI and the instance type must be A1: AWS Fargate does not support custom AMIs or A1 instance types. Therefore, this scenario would not be a good fit for Fargate provisioning.\nD. When users do not want to provision or scale clusters of virtual machines to run containers for AWS Batch jobs: AWS Fargate provides an easy way to run containers without having to manage the underlying infrastructure. However, if users require more control over the infrastructure or need to run jobs that require specialized hardware or custom AMIs, AWS Batch may be a better choice.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "When a batch job needs access to particular instance configurations, including processors and GPUs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "When a batch job needs to be started fast at the initial scale-out of work.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "When a batch job needs to use a compute environment based on a custom AMI and the instance type must be A1.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "When users do not want to provision or scale clusters of virtual machines to run containers for AWS Batch jobs.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "When a batch job needs the instance type to be C5 with multiple vCPUs.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 421,
  "query" : "You are providing AWS consulting services to an IT company.\nThis company owns dozens of AWS accounts and prefers to set up an AWS Organization so that all of these accounts can be managed together under a root account.\nThe AWS administrator planned to create invitations for other accounts and asked for your advice.\nAbout inviting other accounts to join an AWS Organization, which statements are correct? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, E.\nInvitations are used to add accounts for an AWS Organization as below:\nDetails can be found in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html.\nOption A is incorrect because AWS CLI or AWS API also works, such as aws organizations invite-account-to-organization.\nOption B is CORRECT because one account can only join one AWS Organization.\nOption C is incorrect because it can be an IAM user as long as it has proper IAM permissions.\nOption D is incorrect because there is a limitation of creating invitations.\nUsers can send up to 20 invitations per day per organization.\nOption E is CORRECT because invitations must be responded to within 15 days.\nOtherwise, they will expire.\nSure, I'd be happy to explain the correct statements regarding inviting other accounts to join an AWS Organization.\nA. Organization invitations can only be created through the AWS Organization console. This statement is correct. Invitations to join an AWS Organization can only be created by the root user of the organization through the AWS Organization console. The root user needs to log in to the AWS Management Console, go to the AWS Organizations service, and then select the \"Invite account\" option to send an invitation to another account.\nB. One AWS account can join only one Organization even if it receives multiple invitations. This statement is also correct. Once an AWS account accepts an invitation to join an AWS Organization, it can only be a member of that one organization. It cannot join any other organization or be invited to join another organization.\nC. Only the root user of an AWS account can create invitations. This statement is not entirely correct. While the root user of an AWS account can create invitations, other IAM users with the \"organizations:InviteAccountToOrganization\" permission can also create invitations on behalf of the root user. However, the root user must first grant the permission to the IAM user.\nD. Users can create unlimited invitations per day per organization. This statement is not correct. There is a limit to the number of invitations that can be sent per day per organization. This limit is 20 invitations per day per organization, although it can be increased by contacting AWS support.\nE. If an invitation is not accepted or rejected for over 15 days, the invitation will expire. This statement is correct. When an AWS account receives an invitation to join an AWS Organization, it has 15 days to accept or reject the invitation. If the invitation is not accepted or rejected within 15 days, it will expire and can no longer be used to join the organization.\nI hope this detailed explanation helps you understand the correct statements regarding inviting other accounts to join an AWS Organization.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Organization invitations can only be created through the AWS Organization console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One AWS account can join only one Organization even if it receives multiple invitations.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Only the root user of an AWS account can create invitations.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users can create unlimited invitations per day per organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "If an invitation is not accepted or rejected for over 15 days, the invitation will expire.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 422,
  "query" : "As an AWS Solutions Architect, you are in charge of configuring a new AWS Organization among several AWS accounts.\nYou already created an Organization and sent invitations for other accounts to join.\nMost AWS accounts can join the Organization successfully.\nHowever, for one AWS account, it did not receive the invitation email so that it did not know how to join.\nHow should you fix the problem?",
  "answer" : "Correct Answer - B.\nRefer to the below screenshot on how to manage invitations:\nOne thing to note is that for open invitations, users can only perform the Cancel operation.\nOption A is incorrect because the user cannot resend the same invitation if it is still in the Open state.\nOption B is CORRECT because only after the first invitation is canceled, the user can create a new one to the same email id.\nIf there is already one open invitation, the user cannot create another one to the same account.\nThe error can be found below:\nOption C is incorrect because this is not required.\nOption B is more appropriate.\nOption D is incorrect because it has to wait for 15 days until it expires, which is unnecessary.\nAs an AWS Solutions Architect, you are responsible for configuring and managing AWS Organizations. When creating a new AWS Organization and inviting other accounts to join, it is possible that some accounts may not receive the invitation email due to various reasons such as spam filtering, incorrect email address, or network issues.\nTo fix the problem of an AWS account not receiving the invitation email to join the AWS Organization, you have several options:\nA. In the root AWS account, select the pending invitation and choose “resend email”: This option allows you to resend the invitation email to the AWS account that did not receive it. To do so, log in to the root AWS account of the Organization and navigate to the \"AWS Organizations\" console. In the \"Pending Invitations\" section, find the invitation for the account that did not receive the email and select the \"Resend email\" option. This will send a new invitation email to the AWS account.\nB. In the root AWS account of the Organization Master Account, cancel the invitation and then create a new invitation to this AWS account: This option involves canceling the previous invitation and creating a new one for the AWS account. To do so, log in to the root AWS account of the Organization Master Account and navigate to the \"AWS Organizations\" console. In the \"Pending Invitations\" section, find the invitation for the account that did not receive the email and select the \"Cancel Invitation\" option. Then, create a new invitation for the AWS account.\nC. Contact AWS enterprise support to help you resend the invitation email to this AWS account: If the above options do not work, you can contact AWS enterprise support to help you resend the invitation email to the AWS account. AWS enterprise support provides technical support, account management, and other services to enterprise customers who use AWS.\nD. In the root AWS account of the Organization, wait until the invitation expires and then create a new invitation to the AWS account: When you invite an AWS account to join an Organization, the invitation is valid for 30 days. If the invitation email is not received by the AWS account, you can wait until the invitation expires and then create a new invitation for the account.\nIn summary, the best approach to fixing the problem of an AWS account not receiving the invitation email to join an AWS Organization is to try option A or B first. If those options don't work, contact AWS enterprise support or wait until the invitation expires and create a new one.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the root AWS account, select the pending invitation and choose “resend email”.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the root AWS account of the Organization Master Account, cancel the invitation and then create a new invitation to this AWS account.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Contact AWS enterprise support to help you resend the invitation email to this AWS account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the root AWS account of the Organization, wait until the invitation expires and then create a new invitation to the AWS account.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 423,
  "query" : "You are an AWS Solutions Architect in a financial company.\nThe company recently started working on migrating legacy applications to AWS.\nYou planned to use a new AWS Organization to manage all AWS accounts so that you can easily configure accounts, assign organizational units, configure security policies, etc.\nWhich methods are valid for you to add accounts to the Organization? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A, D.\nThere are two methods to add accounts to the AWS Organization either by creating new accounts within an Organization or creating invitations.\nPlease refer to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\nand.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html.\nOption A is CORRECT because the user can create a new account that is part of the Organization.\nOption B is incorrect because other accounts can not create requests to join the Organization.\nThere is no CLI request-join-to-organization as well.\nOption C is incorrect because, in the AWS console, users cannot create requests to join an Organization.\nHowever, they can accept invitations.\nOption D is CORRECT because this can be done through the AWS console, CLI, or API.\nOption E is incorrect because the cross-account IAM role is not required in this scenario.\nAlso, there is no API call to add to an organization for other accounts.\nAs an AWS Solutions Architect in a financial company, you are planning to use a new AWS Organization to manage all AWS accounts for easy configuration, security policy assignment, and other management purposes. The following are valid methods for adding accounts to the Organization:\nA. In the AWS Organization console, create accounts within your organization.\nThis method involves creating new accounts directly within the AWS Organization console. You can create accounts with a specific name and email domain, and configure account settings like IAM roles, permissions, and billing information. This method is useful for new accounts that do not exist yet, or if you need to create a large number of accounts quickly.\nB. Use AWS CLI request-join-to-organization for other AWS accounts to join the Organization. After the Organization owner accepts the requests, the accounts will join successfully.\nThis method involves requesting other AWS accounts to join the AWS Organization using the AWS CLI. After the request is sent, the Organization owner must accept the request before the account can join the Organization. This method is useful if you need to add existing AWS accounts that are owned by other entities, like subsidiaries or partners, to your Organization.\nC. For other accounts, use root accounts to login to the AWS Organization console, create requests to the Organization owner to join the organization.\nThis method involves creating requests to the Organization owner from root accounts of other AWS accounts to join the AWS Organization. After the request is sent, the Organization owner must accept the request before the account can join the Organization. This method is useful if you need to add existing AWS accounts that are owned by other entities, like subsidiaries or partners, to your Organization, but you do not have access to their AWS accounts.\nD. In the root account of the Organization, create invitations to other accounts and wait for them to accept the invitations.\nThis method involves creating invitations to other AWS accounts to join the AWS Organization from the root account of the Organization. After the invitation is sent, the invited account must accept the invitation before it can join the Organization. This method is useful if you need to add existing AWS accounts that are owned by other entities, like subsidiaries or partners, to your Organization, but you do not have access to their AWS accounts.\nE. For other accounts, create a cross-account IAM role that allows the operation of add-account-to-organization for the resource of the AWS Organization ARN. Use an IAM user to assume the IAM role and send an API call to add the account to the Organization.\nThis method involves creating a cross-account IAM role that allows an IAM user to assume the role and send an API call to add an AWS account to the AWS Organization. This method is useful if you need to automate the process of adding multiple AWS accounts to your Organization, or if you need to add accounts programmatically using scripts or tools.\nIn summary, valid methods for adding accounts to an AWS Organization include creating accounts within the AWS Organization console, using AWS CLI request-join-to-organization, creating requests to the Organization owner from root accounts of other AWS accounts, creating invitations from the root account of the Organization, and using a cross-account IAM role to automate the process.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS Organization console, create accounts within your organization.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS CLI request-join-to-organization for other AWS accounts to join the Organization. After the Organization owner accepts the requests, the accounts will join successfully.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For other accounts, use root accounts to login to the AWS Organization console, create requests to the Organization owner to join the organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the root account of the Organization, create invitations to other accounts and wait for them to accept the invitations.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "For other accounts, create a cross-account IAM role that allows the operation of add-account-to-organization for the resource of the AWS Organization ARN. Use an IAM user to assume the IAM role and send an API call to add the account to the Organization.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 424,
  "query" : "Your team's AWS account is a root account of an AWS Organization, and you are in charge of configuring Organizational Units within the Organization.\nAt the moment, each Organizational Unit is supposed to be connected with a team.\nHowever, sometimes because of project changes or team restructuring, Organizational Units need to be adjusted as well.\nWhich operation is valid for Organizational Units?",
  "answer" : "Correct Answer - D.\nAn Organizational Unit can have AWS accounts and other Organizational Units as members.\nThis makes the whole structure similar to a tree.\nThe accounts are organized in a hierarchical, tree-like structure.\nCheck.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\non how to manage OUs.\nOption A is incorrect: Because an OU can have only one parent.\nThis is explained in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html.\nOption B is incorrect: Similar to.\nOption A, an AWS account can be a member of only one OU.\nOption C is incorrect: Before deleting an OU, you must first move all accounts out of the OU and any child OUs, and the child OUs need to be deleted as well.\nOption D is CORRECT: This is the right answer as none of the others are valid.\nSure, I can help with that!\nAn AWS Organization is a feature that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. The organization is a hierarchy of AWS accounts that can be grouped into Organizational Units (OUs). OUs help to organize accounts within an organization to reflect how teams, departments, or business units operate.\nNow, coming to the question, the correct answer is option D, None of the above is valid.\nLet's look at each option one by one:\nA. Configure an OU to be the child of two other OUs which are parents.\nThis option is not valid because OUs cannot have multiple parent OUs. An OU can only have one parent OU, except for the root OU which has no parent.\nB. As two departments use an AWS account, move the account to be a member of two OUs.\nThis option is not valid because an AWS account can only belong to one OU at a time. Moving an account to another OU removes it from the previous OU and adds it to the new OU.\nC. An OU and its members are no longer needed due to business needs, you can delete the OU directly, and the members will be automatically removed from the AWS Organization.\nThis option is not valid because deleting an OU directly removes all accounts, OUs, and policies attached to it. Deleting an OU also removes all of the accounts that belong to it, so it's important to be careful when deleting an OU.\nIn summary, none of the options provided in the question are valid operations for Organizational Units.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an OU to be the child of two other OUs which are parents.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "As two departments use an AWS account, move the account to be a member of two OUs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An OU and its members are no longer needed due to business needs, you can delete the OU directly, and the members will be automatically removed from the AWS Organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "None of the above is valid.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 425,
  "query" : "You have signed in to an AWS Organization's master account using an admin IAM user.\nYou need to move accounts to this Organization from one OU (Organizational Unit) to another or back to the root from an OU.\nHowever, the operation was disallowed due to a lack of permissions.\nSo you started looking at the IAM policies attached to this user.\nWhat are the minimum permissions you need to move accounts among OUs? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, C.\nUsers can move accounts between the Root and OUs in AWS Organization according to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html.\nHowever, permissions are needed to do that.\nOption A is incorrect because organizations:DescribeAccount is used to retrieve Organization details about the account, which is not required.\nOption B is CORRECT because organizations:DescribeOrganization is required if the move operation is done from console.\nOption C is CORRECT because organizations:MoveAccount permits users to move accounts in an Organization.\nOption D is incorrect because attaching SCP is not needed in this scenario.\nOption E is incorrect: because this will allow all actions for Organizations, which will not meet the least privilege.\nTo move accounts between OUs in an AWS Organization, you need to have the appropriate permissions. The necessary permissions can be obtained by attaching the required policies to an IAM user or role. Here are the two minimum permissions that you need to move accounts among OUs:\n1.\norganizations:DescribeAccount: This permission allows you to get information about the account that you want to move. It is necessary to determine the current location of the account and to ensure that you have the necessary permissions to move it.\n2.\norganizations:MoveAccount: This permission allows you to move an account from one OU to another or back to the root. It is necessary to perform the actual move operation.\nIn addition to the two minimum permissions, there are other permissions that may be required depending on the scenario. For example, if you need to move an account to a different AWS Region, you will also need the organizations:EnableAWSServiceAccess permission.\nThe other answer options are as follows:\norganizations:DescribeOrganization: This permission allows you to get information about the AWS Organization that you are working with. It is not necessary to move accounts among OUs.\norganizations:AttachPolicy: This permission allows you to attach a policy to an AWS account or an OU. It is not necessary to move accounts among OUs.\norganizations:*: This is a wildcard permission that grants all permissions for the AWS Organizations service. It is not necessary to move accounts among OUs, and it is generally not recommended to grant such broad permissions.\nIn summary, to move accounts among OUs in an AWS Organization, you need the organizations:DescribeAccount and organizations:MoveAccount permissions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "organizations:DescribeAccount",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "organizations:DescribeOrganization",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "organizations:MoveAccount",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "organizations:AttachPolicy",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "organizations:*",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 426,
  "query" : "You have maintained an AWS Organization and the Organization has below OUs (Organizational Units) configured:",
  "answer" : "Correct Answer - C.\nFor OUs, there are limited operations that users can do.\nPlease check.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html.\nOne thing to note is that the user cannot move an OU to another place from the console or CLI command.\nSo the user has to create a new OU and move accounts to it.\nOption A is incorrect: because the user cannot drag and drop an OU to another place.\nOption B is incorrect because it is unnecessary to move accounts out of the Organization.\nOtherwise, you have to re-invite these accounts.\nOption C is CORRECT because users can move accounts from an OU to another.\nAfter that, empty OU can be deleted.\nOption D is incorrect because there is no such CLI command to move an Organizational Unit.\nThe question describes a scenario where an AWS Organization has several Organizational Units (OUs) including Dev_Department and QA_Department. The task is to reorganize the structure of the OUs so that QA_Department becomes a child of Dev_Department and accounts 3 and 4 are moved to QA_Department.\nOption A: In the tree view of AWS Organization console, drag and drop QA_Department and its members to be a child of Dev_Department. This option is the simplest and easiest to implement. In the AWS Organization console, the user can simply click and drag the QA_Department and its member accounts to be a child of the Dev_Department. This option does not require any account moves or CLI commands.\nOption B: Move accounts 3 and 4 out of the AWS Organization, move QA_Department to be a child of Dev_Department. Add accounts 3 and 4 back to QA_Department. This option requires a bit more work. The user would first move accounts 3 and 4 out of the AWS Organization, then move QA_Department to be a child of Dev_Department. Finally, accounts 3 and 4 would be added back to QA_Department. This option may cause some disruption to the accounts that are moved out and added back in.\nOption C: Create a new OU under Dev_Department named QualityAssurance_Department. Move accounts 3 & 4 to the new OU. Delete the original empty OU QA_Department. This option is also straightforward. The user would create a new OU named QualityAssurance_Department under Dev_Department, move accounts 3 and 4 to the new OU, and then delete the original QA_Department. This option would not cause any disruption to the accounts being moved.\nOption D: Move accounts 3 and 4 under the Root of the AWS Organization. Use CLI move-organizational-unit to move QA_Department to be a child of Dev_Department. Then add accounts 3 and 4 to QA_Department. This option requires the use of the AWS CLI. The user would move accounts 3 and 4 to the Root of the AWS Organization, use the move-organizational-unit command to move QA_Department to be a child of Dev_Department, and then add accounts 3 and 4 back to QA_Department. This option may be more complex and time-consuming than the other options.\nIn conclusion, the best option for reorganizing the OUs in this scenario would be Option A: In the tree view of AWS Organization console, drag and drop QA_Department and its members to be a child of Dev_Department. It is the simplest and easiest to implement, and it does not require any account moves or CLI commands.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the tree view of AWS Organization console, drag and drop QA_Department and its members to be a child of Dev_Department.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Move accounts 3 and 4 out of the AWS Organization, move QA_Department to be a child of Dev_Department. Add accounts 3 and 4 back to QA_Department.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new OU under Dev_Department named QualityAssurance_Department. Move accounts 3 & 4 to the new OU. Delete the original empty OU QA_Department.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Move accounts 3 and 4 under the Root of the AWS Organization. Use CLI move-organizational-unit to move QA_Department to be a child of Dev_Department. Then add accounts 3 and 4 to QA_Department.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 427,
  "query" : "In an AWS Organization, the Root is attached with a default SCP that allows all actions on all resources.\nAnd other OUs or AWS accounts are attached with SCPs that contain Deny lists.\nFor example, an SCP that denies cloudtrail:StopLogging is attached to an OU.\nHowever, you think that the Deny lists can be improved to contain more services such as those that are not used.\nHow would you find out the AWS services that are allowed by the SCP but are never used?",
  "answer" : "Correct Answer - D.\nRefer to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\non how to improve SCPs by checking access data.\nOption A is incorrect because the AWS Organization console does not contain the last access data.\nOption B is incorrect because the IAM credential report provides IAM user information.\nThere is no access data for AWS Organization.\nOption C is incorrect because AWS Config Resources do not show resource information related to the Organization.\nThe user cannot identify which services are not used in AWS Config.\nOption D is CORRECT because the service report in Organization Activity can help identify the AWS services to be included in the Deny lists.\nTake below screenshot as an example:\nThe correct answer to the question is B. In the IAM credential report of AWS accounts, examine those services that are not required to be allowed by SCPs.\nService Control Policies (SCPs) are used to enforce organization-wide policies and prevent non-compliant actions. The default SCP attached to the root of an AWS organization allows all actions on all resources, but it can be overridden by attaching more restrictive policies to individual OUs or accounts.\nTo improve the SCPs and contain more services that are not used, it is essential to identify the AWS services that are allowed but never used. The best way to achieve this is by examining the IAM credential report of AWS accounts.\nThe IAM credential report is a detailed report that provides information about IAM users, groups, and roles. It includes information such as the user's access key age, access key last used date, and the permissions associated with the user. By examining the credential report, you can identify which services are allowed by the SCP but are never used.\nTo generate the IAM credential report, follow these steps:\n1. Sign in to the AWS Management Console as an IAM user with sufficient permissions.\n2. Open the IAM console.\n3. In the navigation pane, choose \"Credential Report.\"\n4. Click the \"Generate Report\" button.\nAfter the report is generated, you can download it as a CSV file and open it in a spreadsheet program like Microsoft Excel. Then, you can sort the data by service name and identify which services are allowed but never used.\nBy examining the IAM credential report of AWS accounts, you can identify the services that are not required to be allowed by SCPs and update the SCPs accordingly. This helps to improve the security of the organization and prevent non-compliant actions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS Organization console, identify allowed services that are never used by AWS accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the IAM credential report of AWS accounts, examine those services that are not required to be allowed by SCPs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In AWS Config Resources, list the AWS services that are not used by IAM users.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the IAM console, click the Service Control Policies and check the last accessed data to identify services that are never used.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 428,
  "query" : "Your AWS Organization has below hierarchy.",
  "answer" : "Correct Answer - A.\nAbout how SCPs work, please refer to the documentation in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html.\nOne rule is that any action that has an explicit Deny always takes priority.\nOption A is CORRECT because DEV2_OU inherits the SCP in Admin_OU which contains a Deny policy.\nThe policy overrides any Allow that other SCPs might grant.\nOption B is incorrect: because other SCPs that the OU has inherited should also be considered.\nOption C is incorrect because the SCP in DEV1_OU does not need to be considered as it is not a parent node for user Bob.\nOption D is incorrect: because the Allow policy does not override a Deny policy if it exists.\nTo answer this question, we need to understand the AWS Organizations hierarchy and how Service Control Policies (SCP) work.\nAWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. The organization consists of one or more accounts, and you can organize those accounts into groups called Organizational Units (OU).\nSCP is a type of policy that you can use to manage permissions across accounts in your organization. SCPs allow you to specify which AWS services and actions are allowed or denied for accounts and OUs in your organization. When you attach an SCP to an OU or an account, the policy affects all the entities in that container.\nNow, let's look at the AWS Organizations hierarchy given in the question:\nmarkdownCopy code- Root     - Admin_OU     - DEV1_OU         - DEV1_Account     - DEV2_OU         - DEV2_Account \nBob is trying to perform an S3 operation in the DEV2_Account. Let's look at each answer option one by one:\nA. The action will be denied as the SCP in Admin_OU denies the operation.\nThis answer is incorrect because the SCP attached to the Admin_OU has not been mentioned in the question. Therefore, we cannot assume that it denies the S3 operation.\nB. The action will be allowed as the SCP in the root has full AWS access, and Bob is attached with full S3 permissions.\nThis answer is incorrect because SCPs are evaluated from the closest container to the farthest container. In this case, the SCP in DEV2_OU, where Bob's account is, will take priority over the SCP in the Root.\nC. The action will be denied as SCP in DEV1_OU has an S3 Deny policy, which takes priority.\nThis answer is incorrect because the SCP in DEV1_OU is not relevant to the question since Bob is trying to perform the operation in the DEV2_Account, which is not part of the DEV1_OU.\nD. The action will be allowed as DEV2_OU is attached with an S3 Allow SCP policy, which takes priority.\nThis answer is correct because the SCP in DEV2_OU will take priority over any SCP in higher-level OUs. Since the SCP in DEV2_OU allows S3 operations, Bob will be able to perform the S3 operation in the DEV2_Account.\nIn conclusion, the correct answer is D.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The action will be denied as the SCP in Admin_OU denies the operation.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The action will be allowed as the SCP in the root has full AWS access, and Bob is attached with full S3 permissions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The action will be denied as SCP in DEV1_OU has an S3 Deny policy, which takes priority.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The action will be allowed as DEV2_OU is attached with an S3 Allow SCP policy, which takes priority.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 429,
  "query" : "You are in charge of configuring an AWS Organization as below hierarchy.",
  "answer" : "Correct Answer - B.\nOUs in AWS Organization inherit the SCPs from the parent OU.\nReference can be found in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html.\nOption A is incorrect because it is improper to attach the Deny SCP to Root as it affects all other nodes including Security_OU.\nOption B is CORRECT: because the Deny SCP only affects all OUs under Project_OU.\nOther OUs such as Security_OU are not affected.\nOption C is incorrect because Project_OU and Security_OU do not need to attach full access SCP since they can inherit from the Root.\nOption D is incorrect: because DEV_OU and QA_OU are not required to attach the Deny SCP separately since Project_OU as a parent can effectively do that.\nAWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organizations helps you to manage your accounts and apply policies across them. As an AWS Solutions Architect, you may be responsible for configuring AWS Organizations hierarchy and applying the appropriate policies to manage the accounts.\nAnswer A: This approach involves creating two Service Control Policies (SCPs) and attaching them to different Organizational Units (OUs) in the hierarchy. An SCP is a policy that you can use to specify the maximum permissions for AWS accounts in an organization or an OU. SCPs are attached to OUs to restrict access to specific AWS services and actions. In this approach, the following steps are followed:\n1.\nCreate an SCP that denies required actions and attach it to the Root OU. This will apply the policy to all accounts in the organization, including the accounts in the Project_OU, DEV1_OU, DEV2_OU, and QA1_OU. This policy can be used to restrict access to specific AWS services and actions.\n2.\nAttach another SCP that contains an Allow list in the Project_OU. This will override the SCP attached to the Root OU and allow the necessary actions for the accounts in the Project_OU.\nAnswer B: This approach involves attaching an SCP to the Project_OU to deny the deletion of IAM roles. This can be useful if you want to prevent users from accidentally or intentionally deleting IAM roles, which could cause serious security issues. The steps for this approach are:\n1. In the Project_OU, attach an SCP that contains a Deny list to deny the deletion of IAM roles. This will apply the policy to all accounts in the Project_OU.\nAnswer C: This approach involves attaching two SCPs to different OUs in the hierarchy. One SCP will be used to allow full access to specific AWS services and actions, while the other SCP will be used to deny access to certain services and actions. The following steps are followed:\n1.\nMake sure that Root, Project_OU, and Security_OU are attached with a full access SCP. This will allow the necessary actions for all accounts in these OUs.\n2.\nAttach another SCP that contains the Deny list to DEV1_OU, DEV2_OU, and QA1_OU. This will restrict access to specific AWS services and actions in these OUs.\nAnswer D: This approach involves creating an SCP that denies the required actions and attaching it to multiple OUs in the hierarchy. The following steps are followed:\n1.\nCreate an SCP that denies the required actions.\n2.\nAttach it to Project_OU, DEV_OU, and QA_OU. This will restrict access to specific AWS services and actions in these OUs.\nOverall, the approach taken depends on the specific requirements and constraints of the organization. The best approach will be the one that balances security and usability while meeting the organization's needs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an SCP that denies required actions and attach it to Root. Attach another SCP that contains an Allow list in Project_OU.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In Project_OU, attach an SCP that contains a Deny list to deny the deletion of IAM roles.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Make sure that Root, Project_OU, and Security_OU are attached with a full access SCP. Attach another SCP that contains the Deny list to DEV1_OU, DEV2_OU, and QA1_OU.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an SCP that denies the required actions. Attach it to Project_OU, DEV_OU, and QA_OU.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 430,
  "query" : "You are an AWS architect in an IT startup company.\nLast month you configured an AWS Organization.\nAlthough the default feature set of AWS Organization is “All Features”, you only enabled the “Consolidated Billing” feature at that time.\nLater on, you found it was necessary to use service control policies (SCPs) to provide a central control to enable “All Features” for the Organization.\nRecently, the company is short on budget and has to make a cost reduction.\nYour manager asked you whether you can modify “All Features” to “Consolidated Billing” to save some cost.\nHow would you answer this question?",
  "answer" : "Correct Answer - D.\nAbout AWS Organization pricing, please refer to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html.\nAWS Organization itself is a free service.\nUsers are only charged for the AWS resources in their accounts.\nOption A is incorrect: All Features cannot be switched back to Consolidated Billing.\nThis is described in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html.\nOption B is incorrect: Similar to.\nOption A.Option C is incorrect: AWS Organization is a free service regardless of how many OUs in the Organization.\nOption D is CORRECT: After All Features is enabled, it cannot be changed back.\nBesides, since it is a free service, you have to consider other AWS services to save some costs.\nThe correct answer is A.\nAWS Organization is a management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. This service offers two types of features, Consolidated Billing and All Features. Consolidated Billing enables you to consolidate payment for multiple AWS accounts. All Features includes Consolidated Billing and also provides you with additional management capabilities like Service Control Policies (SCPs) to control access to AWS services across your organization.\nWhen you create an AWS Organization, the default feature set is All Features, which means that all AWS services are enabled across all accounts in the organization. However, you can disable certain services using SCPs.\nIn the scenario described in the question, the organization was created with only the Consolidated Billing feature enabled. Later, the need arose to use SCPs to provide central control, which requires the All Features option to be enabled.\nIf the organization is short on budget and needs to reduce costs, it is possible to switch back to Consolidated Billing. However, before doing so, all attached SCPs need to be detached first. This is because SCPs only apply to accounts that have All Features enabled. If you switch back to Consolidated Billing, the SCPs will no longer be applied, and the organization will lose its central control. Additionally, it is worth noting that Consolidated Billing is a free feature and will not save any costs compared to All Features.\nIt is also important to note that AWS Organization itself is a free service, so modifying it will not have any impact on the budget. However, enabling All Features may result in additional costs if more AWS services are used across the organization.\nTherefore, the correct answer is A. All attached SCPs need to be detached first in the Organization before All Features is changed to Consolidated Billing. This can also save some cost as Consolidated Billing is a free feature.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "All attached SCPs need to be detached first in the Organization before All Features is changed to Consolidated Billing. This can also save some cost as Consolidated Billing is a free feature.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All Features can be switched to Consolidated Billing as long as the user has proper IAM permission. However, it will not save any cost as Consolidated Billing and All Features charge the same.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users can switch between All Features and Consolidated Billing anytime. However, this will not save any cost since AWS Organization is offered without charge as long as there are more than 5 OUs in the Organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All Features cannot be switched back to Consolidated Billing. Besides, AWS Organization is a free service, so it is not required to modify it.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 431,
  "query" : "You are providing AWS consulting services to an IT company.\nThis company owns dozens of AWS accounts and prefers to set up an AWS Organization so that all of these accounts can be managed together under a root account.\nThe AWS administrator planned to create invitations for other accounts and asked for your advice.\nAbout inviting other accounts to join an AWS Organization, which statements are correct? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, E.\nInvitations are used to add accounts for an AWS Organization as below:\nDetails can be found in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html.\nOption A is incorrect because AWS CLI or AWS API also works, such as aws organizations invite-account-to-organization.\nOption B is CORRECT because one account can only join one AWS Organization.\nOption C is incorrect because it can be an IAM user as long as it has proper IAM permissions.\nOption D is incorrect because there is a limitation of creating invitations.\nUsers can send up to 20 invitations per day per organization.\nOption E is CORRECT because invitations must be responded to within 15 days.\nOtherwise, they will expire.\nSure, I'd be happy to help!\nFirst, let's start with a brief overview of AWS Organizations. AWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. You can create an organization by using the AWS Management Console or the AWS Organizations API. Once you have an organization, you can invite other AWS accounts to join it as members. As members of an organization, the AWS accounts can share resources, policies, and services.\nNow, let's go through each statement and see which ones are correct:\nA. Organization invitations can only be created through the AWS Organization console.\nThis statement is not entirely correct. While you can create invitations through the AWS Management Console, you can also use the AWS Organizations API to create invitations programmatically. So, the correct statement would be \"Organization invitations can be created through the AWS Management Console or the AWS Organizations API.\"\nB. One AWS account can join only one Organization even if it receives multiple invitations.\nThis statement is correct. An AWS account can only be a member of one AWS Organization at a time.\nC. Only the root user of an AWS account can create invitations.\nThis statement is not entirely correct. While the root user can create invitations, you can also delegate invitation management to other IAM users in the account by granting them the necessary permissions. So, the correct statement would be \"Only the root user or delegated IAM users of an AWS account can create invitations.\"\nD. Users can create unlimited invitations per day per organization.\nThis statement is not correct. There is a limit to the number of invitations you can send per day per organization. The limit is 50 invitations per day.\nE. If an invitation is not accepted or rejected for over 15 days, the invitation will expire.\nThis statement is correct. If an invitation is not accepted or rejected within 15 days, it will expire and cannot be used to join the AWS Organization.\nSo, the correct statements are B and E.\nI hope this helps! Let me know if you have any other questions.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Organization invitations can only be created through the AWS Organization console.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "One AWS account can join only one Organization even if it receives multiple invitations.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Only the root user of an AWS account can create invitations.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users can create unlimited invitations per day per organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "If an invitation is not accepted or rejected for over 15 days, the invitation will expire.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 432,
  "query" : "As an AWS Solutions Architect, you are in charge of configuring a new AWS Organization among several AWS accounts.\nYou already created an Organization and sent invitations for other accounts to join.\nMost AWS accounts can join the Organization successfully.\nHowever, for one AWS account, it did not receive the invitation email so that it did not know how to join.\nHow should you fix the problem?",
  "answer" : "Correct Answer - B.\nRefer to the below screenshot on how to manage invitations:\nOne thing to note is that for open invitations, users can only perform the Cancel operation.\nOption A is incorrect because the user cannot resend the same invitation if it is still in the Open state.\nOption B is CORRECT because only after the first invitation is canceled, the user can create a new one to the same email id.\nIf there is already one open invitation, the user cannot create another one to the same account.\nThe error can be found below:\nOption C is incorrect because this is not required.\nOption B is more appropriate.\nOption D is incorrect because it has to wait for 15 days until it expires, which is unnecessary.\nAs an AWS Solutions Architect, if an AWS account does not receive an invitation email to join an AWS organization, the following steps should be taken to fix the issue:\nOption A: Resend the invitation email from the root AWS account of the organization. This option is recommended as it is a simple and quick fix. The root AWS account can select the pending invitation and choose “resend email.” This option ensures that the invitation is resent to the email address associated with the AWS account. If the email address was incorrect, it would not fix the problem.\nOption B: Cancel the invitation and then create a new invitation to the AWS account. This option should be taken if the first option did not work. Cancelling the invitation will remove it from the AWS account's invitation queue, and creating a new one will ensure that the invitation is sent to the correct email address.\nOption C: Contact AWS enterprise support to help you resend the invitation email to the AWS account. This option should be taken if the first two options fail. AWS enterprise support can assist in locating and resolving the issue with the invitation.\nOption D: Wait until the invitation expires and then create a new invitation to the AWS account. This option should only be taken if the first three options fail. Invitations expire after a set period, and creating a new one will ensure that the AWS account can join the organization. However, waiting for the invitation to expire will delay the AWS account's access to the organization's resources.\nIn summary, the best option to fix an AWS account not receiving an invitation email to join an AWS organization is to resend the invitation email from the root AWS account of the organization. If this fails, cancel the invitation and create a new one. If this still fails, contact AWS enterprise support for assistance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the root AWS account, select the pending invitation and choose “resend email”.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the root AWS account of the Organization Master Account, cancel the invitation and then create a new invitation to this AWS account.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Contact AWS enterprise support to help you resend the invitation email to this AWS account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the root AWS account of the Organization, wait until the invitation expires and then create a new invitation to the AWS account.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 433,
  "query" : "You are an AWS Solutions Architect in a financial company.\nThe company recently started working on migrating legacy applications to AWS.\nYou planned to use a new AWS Organization to manage all AWS accounts so that you can easily configure accounts, assign organizational units, configure security policies, etc.\nWhich methods are valid for you to add accounts to the Organization? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A, D.\nThere are two methods to add accounts to the AWS Organization either by creating new accounts within an Organization or creating invitations.\nPlease refer to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\nand.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html.\nOption A is CORRECT because the user can create a new account that is part of the Organization.\nOption B is incorrect because other accounts can not create requests to join the Organization.\nThere is no CLI request-join-to-organization as well.\nOption C is incorrect because, in the AWS console, users cannot create requests to join an Organization.\nHowever, they can accept invitations.\nOption D is CORRECT because this can be done through the AWS console, CLI, or API.\nOption E is incorrect because the cross-account IAM role is not required in this scenario.\nAlso, there is no API call to add to an organization for other accounts.\nAWS Organizations is a service that allows you to consolidate multiple AWS accounts into an organization. You can create an AWS Organization to manage and govern your AWS accounts easily, apply policies across your accounts, and automate account creation and management. As an AWS Solutions Architect in a financial company, you need to add accounts to the organization to manage them easily.\nThere are several methods to add accounts to an AWS Organization, but you need to choose the valid ones. The valid methods to add accounts to an AWS Organization are:\nA. In the AWS Organization console, create accounts within your organization. This is a valid method to add accounts to an AWS Organization. You can create accounts within the organization using the AWS Organization console. You need to provide the account name, email address, and choose the payment method. After creating the account, it will automatically join the organization.\nB. Use AWS CLI request-join-to-organization for other AWS accounts to join the Organization. After the Organization owner accepts the requests, the accounts will join successfully. This is another valid method to add accounts to an AWS Organization. You can use the AWS CLI to send a join request to the organization owner from the account that you want to add to the organization. The organization owner needs to accept the request for the account to join the organization.\nC. For other accounts, use root accounts to login to the AWS Organization console, create requests to the Organization owner to join the organization. This is not a valid method to add accounts to an AWS Organization. You should avoid using root accounts to log in to the AWS Organization console. Root accounts have unrestricted access to all resources in the AWS account, and you should not use them unless absolutely necessary.\nD. In the root account of the Organization, create invitations to other accounts and wait for them to accept the invitations. This is not a valid method to add accounts to an AWS Organization. Although you can invite accounts to join an organization, you should avoid using the root account to do so. Instead, you should use the methods described in A and B.\nE. For other accounts, create a cross-account IAM role that allows the operation of add-account-to-organization for the resource of the AWS Organization ARN. Use an IAM user to assume the IAM role and send an API call to add the account to the Organization. This is not a valid method to add accounts to an AWS Organization. There is no such operation called add-account-to-organization, and you should not create a cross-account IAM role to perform an operation that does not exist.\nIn summary, the valid methods to add accounts to an AWS Organization are A and B. You can create accounts within the organization using the AWS Organization console or use the AWS CLI to send a join request to the organization owner from the account that you want to add to the organization.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS Organization console, create accounts within your organization.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Use AWS CLI request-join-to-organization for other AWS accounts to join the Organization. After the Organization owner accepts the requests, the accounts will join successfully.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For other accounts, use root accounts to login to the AWS Organization console, create requests to the Organization owner to join the organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the root account of the Organization, create invitations to other accounts and wait for them to accept the invitations.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "For other accounts, create a cross-account IAM role that allows the operation of add-account-to-organization for the resource of the AWS Organization ARN. Use an IAM user to assume the IAM role and send an API call to add the account to the Organization.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 434,
  "query" : "Your team's AWS account is a root account of an AWS Organization, and you are in charge of configuring Organizational Units within the Organization.\nAt the moment, each Organizational Unit is supposed to be connected with a team.\nHowever, sometimes because of project changes or team restructuring, Organizational Units need to be adjusted as well.\nWhich operation is valid for Organizational Units?",
  "answer" : "Correct Answer - D.\nAn Organizational Unit can have AWS accounts and other Organizational Units as members.\nThis makes the whole structure similar to a tree.\nThe accounts are organized in a hierarchical, tree-like structure.\nCheck.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\non how to manage OUs.\nOption A is incorrect: Because an OU can have only one parent.\nThis is explained in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html.\nOption B is incorrect: Similar to.\nOption A, an AWS account can be a member of only one OU.\nOption C is incorrect: Before deleting an OU, you must first move all accounts out of the OU and any child OUs, and the child OUs need to be deleted as well.\nOption D is CORRECT: This is the right answer as none of the others are valid.\nSure, I can provide a detailed explanation of each answer option and why it is either valid or invalid for Organizational Units in an AWS Organization.\nA. Configure an OU to be the child of two other OUs which are parents. This answer option is not valid because an Organizational Unit can only have one parent OU. An OU can have multiple child OUs, but it cannot have more than one parent OU. Therefore, an OU cannot be connected to two parent OUs.\nB. As two departments use an AWS account, move the account to be a member of two OUs. This answer option is valid because you can move an AWS account to be a member of two or more Organizational Units. This allows you to grant access and apply policies to the account based on the requirements of both departments. Moving an account to multiple OUs does not affect the account's resources or billing.\nC. An OU and its members are no longer needed due to business needs, you can delete the OU directly, and the members will be automatically removed from the AWS Organization. This answer option is valid because you can delete an Organizational Unit that is no longer needed. When an OU is deleted, all members of the OU are automatically removed from the AWS Organization. However, it is important to note that any policies or permissions applied to the OU will also be removed.\nD. None of the above is valid. This answer option is not correct because option B and option C are both valid operations for Organizational Units in an AWS Organization. Therefore, the correct answer is not \"none of the above\".\nIn summary, option B and option C are valid operations for Organizational Units in an AWS Organization. Option A is not valid because an OU can only have one parent OU, and option D is incorrect because it implies that no valid operations exist.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an OU to be the child of two other OUs which are parents.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "As two departments use an AWS account, move the account to be a member of two OUs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "An OU and its members are no longer needed due to business needs, you can delete the OU directly, and the members will be automatically removed from the AWS Organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "None of the above is valid.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 435,
  "query" : "You have signed in to an AWS Organization's master account using an admin IAM user.\nYou need to move accounts to this Organization from one OU (Organizational Unit) to another or back to the root from an OU.\nHowever, the operation was disallowed due to a lack of permissions.\nSo you started looking at the IAM policies attached to this user.\nWhat are the minimum permissions you need to move accounts among OUs? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, C.\nUsers can move accounts between the Root and OUs in AWS Organization according to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html.\nHowever, permissions are needed to do that.\nOption A is incorrect because organizations:DescribeAccount is used to retrieve Organization details about the account, which is not required.\nOption B is CORRECT because organizations:DescribeOrganization is required if the move operation is done from console.\nOption C is CORRECT because organizations:MoveAccount permits users to move accounts in an Organization.\nOption D is incorrect because attaching SCP is not needed in this scenario.\nOption E is incorrect: because this will allow all actions for Organizations, which will not meet the least privilege.\nTo move accounts between OUs within an AWS Organization, you need to have the following minimum permissions:\n1.\norganizations:DescribeAccount: This permission allows the IAM user to view the details of the account being moved. This is a read-only permission and does not allow any modifications to be made.\n2.\norganizations:MoveAccount: This permission allows the IAM user to move the account from one OU to another within the same organization. This permission should be added to the IAM policy attached to the admin user to enable the movement of accounts.\nOption A (organizations:DescribeAccount) and Option C (organizations:MoveAccount) are the correct answers.\nOption B (organizations:DescribeOrganization) is not required to move accounts between OUs, as it only allows viewing the details of the organization itself.\nOption D (organizations:AttachPolicy) is also not required to move accounts between OUs, as it only allows attaching policies to entities within the organization.\nOption E (organizations:*) grants all permissions for AWS Organizations, which is not the best practice as it grants excessive privileges and increases the risk of accidental or unauthorized actions. Therefore, it should be avoided whenever possible.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "organizations:DescribeAccount",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "organizations:DescribeOrganization",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "organizations:MoveAccount",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "organizations:AttachPolicy",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "organizations:*",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 436,
  "query" : "You have maintained an AWS Organization and the Organization has below OUs (Organizational Units) configured:",
  "answer" : "Correct Answer - C.\nFor OUs, there are limited operations that users can do.\nPlease check.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html.\nOne thing to note is that the user cannot move an OU to another place from the console or CLI command.\nSo the user has to create a new OU and move accounts to it.\nOption A is incorrect: because the user cannot drag and drop an OU to another place.\nOption B is incorrect because it is unnecessary to move accounts out of the Organization.\nOtherwise, you have to re-invite these accounts.\nOption C is CORRECT because users can move accounts from an OU to another.\nAfter that, empty OU can be deleted.\nOption D is incorrect because there is no such CLI command to move an Organizational Unit.\nAWS Organizations is a service that allows you to consolidate multiple AWS accounts into an organization that you can centrally manage and control. An organizational unit (OU) is a container for accounts and other OUs, and can be used to group accounts based on business unit, application, or environment.\nThe question asks you to move accounts and OUs within an AWS Organization. The current state of the organization is not provided in the question, but there are several options for reconfiguring the organization based on the OUs and accounts mentioned.\nOption A: In the tree view of AWS Organization console, drag and drop QA_Department and its members to be a child of Dev_Department.\nThis option involves simply moving the QA_Department OU and its accounts under the Dev_Department OU. This can be done in the AWS Organizations console by clicking and dragging the QA_Department OU and dropping it onto the Dev_Department OU. This option is a straightforward way to reorganize the structure of the organization without moving accounts out of the organization.\nOption B: Move accounts 3 and 4 out of the AWS Organization, move QA_Department to be a child of Dev_Department. Add accounts 3 and 4 back to QA_Department.\nThis option involves temporarily removing accounts 3 and 4 from the organization before reconfiguring the OUs. Once the QA_Department OU has been moved under the Dev_Department OU, accounts 3 and 4 can be added back to the QA_Department OU. This option requires more steps than option A, but it is still relatively straightforward.\nOption C: Create a new OU under Dev_Department named QualityAssurance_Department. Move accounts 3 & 4 to the new OU. Delete the original empty OU QA_Department.\nThis option involves creating a new OU called QualityAssurance_Department under the Dev_Department OU and moving accounts 3 and 4 to the new OU. Once the accounts have been moved, the original QA_Department OU can be deleted. This option requires a few more steps than option A, but it may be preferred if you want to have a more granular OU structure.\nOption D: Move accounts 3 and 4 under the Root of the AWS Organization. Use CLI move-organizational-unit to move QA_Department to be a child of Dev_Department. Then add accounts 3 and 4 to QA_Department.\nThis option involves moving accounts 3 and 4 to the root of the organization before moving the QA_Department OU under the Dev_Department OU using the AWS CLI move-organizational-unit command. Once the QA_Department OU has been moved, accounts 3 and 4 can be added back to the QA_Department OU. This option requires the use of the AWS CLI and may be more complex than the other options.\nIn summary, the best option for reconfiguring the organization depends on the specific needs of the organization. Option A is the simplest and most straightforward option, while option C may be preferred if a more granular OU structure is desired. Option B and D are more complex and involve temporarily removing accounts from the organization, but they may be necessary in certain situations.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the tree view of AWS Organization console, drag and drop QA_Department and its members to be a child of Dev_Department.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Move accounts 3 and 4 out of the AWS Organization, move QA_Department to be a child of Dev_Department. Add accounts 3 and 4 back to QA_Department.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a new OU under Dev_Department named QualityAssurance_Department. Move accounts 3 & 4 to the new OU. Delete the original empty OU QA_Department.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Move accounts 3 and 4 under the Root of the AWS Organization. Use CLI move-organizational-unit to move QA_Department to be a child of Dev_Department. Then add accounts 3 and 4 to QA_Department.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 437,
  "query" : "In an AWS Organization, the Root is attached with a default SCP that allows all actions on all resources.\nAnd other OUs or AWS accounts are attached with SCPs that contain Deny lists.\nFor example, an SCP that denies cloudtrail:StopLogging is attached to an OU.\nHowever, you think that the Deny lists can be improved to contain more services such as those that are not used.\nHow would you find out the AWS services that are allowed by the SCP but are never used?",
  "answer" : "Correct Answer - D.\nRefer to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\non how to improve SCPs by checking access data.\nOption A is incorrect because the AWS Organization console does not contain the last access data.\nOption B is incorrect because the IAM credential report provides IAM user information.\nThere is no access data for AWS Organization.\nOption C is incorrect because AWS Config Resources do not show resource information related to the Organization.\nThe user cannot identify which services are not used in AWS Config.\nOption D is CORRECT because the service report in Organization Activity can help identify the AWS services to be included in the Deny lists.\nTake below screenshot as an example:\nOption B is the most appropriate answer to the question.\nExplanation:\nAWS Organizations is a service that allows managing multiple AWS accounts as a single entity to centralize security and compliance, simplify billing, and manage resource sharing across accounts. It helps to create a hierarchy of AWS accounts that can be grouped into organizational units (OUs) and controlled with Service Control Policies (SCPs).\nAn SCP is a policy that can be attached to an AWS Organizations entity (such as the Root, OUs, or accounts) to restrict permissions on AWS services and actions. SCPs are used to set the maximum permissions available to accounts or OUs within the organization.\nThe Root of an AWS Organization is created with a default SCP that allows all actions on all resources. This means that any new account created under the Root has unrestricted access to all AWS services and actions.\nTo increase security, SCPs can be created and attached to OUs or accounts to restrict permissions. These SCPs can contain \"Deny\" rules that block access to specific AWS services or actions.\nOption B proposes examining the IAM credential report of AWS accounts to identify those services that are not required to be allowed by SCPs.\nThe IAM credential report provides a comprehensive view of all IAM users and their access rights. It contains information about IAM users, their groups, roles, access keys, and permissions. This report also lists the services and actions that are allowed to each user.\nBy examining the IAM credential report, you can identify those services that are not required by users, and therefore, not required to be allowed by SCPs. These services can be added to the Deny list of SCPs to increase security.\nOption A proposes identifying allowed services that are never used by AWS accounts in the AWS Organization console. This option may not provide accurate information about the usage of AWS services because it only lists the allowed services, not the services that are actually used.\nOption C proposes listing the AWS services that are not used by IAM users in AWS Config Resources. AWS Config is a service that provides a detailed inventory of the resources in an AWS account. While it can help to identify unused resources, it may not provide accurate information about the usage of AWS services by IAM users.\nOption D proposes checking the last accessed data of services in the IAM console to identify services that are never used. This option can provide some information about the usage of AWS services, but it requires checking each service individually, which can be time-consuming and may not provide an accurate picture of overall usage.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS Organization console, identify allowed services that are never used by AWS accounts.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the IAM credential report of AWS accounts, examine those services that are not required to be allowed by SCPs.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In AWS Config Resources, list the AWS services that are not used by IAM users.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the IAM console, click the Service Control Policies and check the last accessed data to identify services that are never used.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 438,
  "query" : "Your AWS Organization has below hierarchy.",
  "answer" : "Correct Answer - A.\nAbout how SCPs work, please refer to the documentation in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html.\nOne rule is that any action that has an explicit Deny always takes priority.\nOption A is CORRECT because DEV2_OU inherits the SCP in Admin_OU which contains a Deny policy.\nThe policy overrides any Allow that other SCPs might grant.\nOption B is incorrect: because other SCPs that the OU has inherited should also be considered.\nOption C is incorrect because the SCP in DEV1_OU does not need to be considered as it is not a parent node for user Bob.\nOption D is incorrect: because the Allow policy does not override a Deny policy if it exists.\nTo answer this question, we need to understand the AWS Organizations hierarchy and Service Control Policies (SCPs) in AWS.\nAWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. You can create a hierarchy of organizational units (OUs) to group accounts and apply policies to them.\nSCPs are a type of organization policy that allow you to centrally manage and enforce policies across multiple AWS accounts. SCPs allow you to specify which AWS services and actions are allowed or denied for accounts and OUs in your organization. SCPs are evaluated in a top-down manner, with SCPs at the root level taking priority over those at lower levels.\nNow let's look at the hierarchy and SCPs in the given scenario:\nAWS Organization Hierarchy:\nRoot\nAdmin_OU\nBob's_Account\nDEV1_OU\nDEV2_OU\nSCP Policy:\nRoot\nAllow Full AWS Access\nAdmin_OU\nDeny S3 Access\nDEV1_OU\nDeny S3 Access\nDEV2_OU\nAllow S3 Access\nBased on the above information, let's evaluate the given answers:\nA. The action will be denied as the SCP in Admin_OU denies the operation. This answer is incorrect. While it is true that the Admin_OU has an SCP that denies S3 access, this SCP is not directly applied to Bob's_Account. Instead, Bob's_Account inherits the SCP from the Admin_OU. However, since the SCP at the root level allows full AWS access, Bob's_Account still has access to S3.\nB. The action will be allowed as the SCP in the root has full AWS access, and Bob is attached with full S3 permissions. This answer is incorrect. While the SCP at the root level allows full AWS access, this does not necessarily mean that Bob's_Account has full S3 permissions. Bob's_Account inherits the SCPs from the OUs it belongs to, which may limit its access to S3.\nC. The action will be denied as SCP in DEV1_OU has an S3 Deny policy, which takes priority. This answer is correct. Since the SCP in DEV1_OU denies S3 access, this takes priority over the SCP at the root level that allows full AWS access. As Bob's_Account belongs to DEV1_OU, it is subject to this deny policy and therefore cannot access S3.\nD. The action will be allowed as DEV2_OU is attached with an S3 Allow SCP policy, which takes priority. This answer is incorrect. While DEV2_OU has an SCP that allows S3 access, this SCP only applies to accounts that directly belong to the DEV2_OU. Bob's_Account does not belong to the DEV2_OU, so it does not inherit this SCP. Instead, it inherits the SCP from the Admin_OU, which denies S3 access.\nTherefore, the correct answer is C. The action will be denied as SCP in DEV1_OU has an S3 Deny policy, which takes priority.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The action will be denied as the SCP in Admin_OU denies the operation.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "The action will be allowed as the SCP in the root has full AWS access, and Bob is attached with full S3 permissions.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The action will be denied as SCP in DEV1_OU has an S3 Deny policy, which takes priority.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The action will be allowed as DEV2_OU is attached with an S3 Allow SCP policy, which takes priority.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 439,
  "query" : "You are in charge of configuring an AWS Organization as below hierarchy.",
  "answer" : "Correct Answer - B.\nOUs in AWS Organization inherit the SCPs from the parent OU.\nReference can be found in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html.\nOption A is incorrect because it is improper to attach the Deny SCP to Root as it affects all other nodes including Security_OU.\nOption B is CORRECT: because the Deny SCP only affects all OUs under Project_OU.\nOther OUs such as Security_OU are not affected.\nOption C is incorrect because Project_OU and Security_OU do not need to attach full access SCP since they can inherit from the Root.\nOption D is incorrect: because DEV_OU and QA_OU are not required to attach the Deny SCP separately since Project_OU as a parent can effectively do that.\nAWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. You can create a hierarchy of organizational units (OUs) to group AWS accounts and apply policies to them.\nThe question asks you to configure an AWS organization with a specific hierarchy and policies. Let's go through each answer option and see which one is correct.\nOption A: Create an SCP that denies required actions and attach it to Root. Attach another SCP that contains an Allow list in Project_OU.\nThis option suggests creating a Service Control Policy (SCP) that denies certain actions and attaching it to the root of the organization. Additionally, another SCP that contains an Allow list should be attached to the Project_OU.\nThis option is partially correct. Attaching an SCP that contains an Allow list to the Project_OU will allow only specific actions in that OU. However, attaching a Deny list SCP to the root of the organization may not be a good idea since it will apply to all OUs and accounts under the root, potentially causing unintended consequences.\nOption B: In Project_OU, attach an SCP that contains a Deny list to deny the deletion of IAM roles.\nThis option suggests attaching an SCP to the Project_OU that denies the deletion of IAM roles.\nThis option is too narrow in scope. It only addresses the deletion of IAM roles in the Project_OU, but there may be other policies and restrictions that need to be applied to other OUs and accounts in the organization.\nOption C: Make sure that Root, Project_OU, and Security_OU are attached with a full access SCP. Attach another SCP that contains the Deny list to DEV1_OU, DEV2_OU, and QA1_OU.\nThis option suggests attaching a full access SCP to the Root, Project_OU, and Security_OU. Additionally, an SCP that contains the Deny list should be attached to DEV1_OU, DEV2_OU, and QA1_OU.\nThis option is a better approach since it applies a full access SCP to the top-level OUs, allowing for maximum flexibility and control, and a Deny list SCP to the lower-level OUs, ensuring that specific policies and restrictions are enforced. However, this option does not cover all the OUs mentioned in the question.\nOption D: Create an SCP that denies the required actions. Attach it to Project_OU, DEV_OU, and QA_OU.\nThis option suggests creating an SCP that denies required actions and attaching it to Project_OU, DEV_OU, and QA_OU.\nThis option is too broad since it applies the SCP to multiple OUs without specifying which actions should be denied. Additionally, it does not cover the other OUs mentioned in the question.\nBased on the above analysis, Option C is the correct answer as it provides a balanced approach to policy enforcement across the organization. However, it is important to note that the specific policies and restrictions should be tailored to the specific requirements of the organization.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Create an SCP that denies required actions and attach it to Root. Attach another SCP that contains an Allow list in Project_OU.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In Project_OU, attach an SCP that contains a Deny list to deny the deletion of IAM roles.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Make sure that Root, Project_OU, and Security_OU are attached with a full access SCP. Attach another SCP that contains the Deny list to DEV1_OU, DEV2_OU, and QA1_OU.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an SCP that denies the required actions. Attach it to Project_OU, DEV_OU, and QA_OU.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 440,
  "query" : "You are an AWS architect in an IT startup company.\nLast month you configured an AWS Organization.\nAlthough the default feature set of AWS Organization is “All Features”, you only enabled the “Consolidated Billing” feature at that time.\nLater on, you found it was necessary to use service control policies (SCPs) to provide a central control to enable “All Features” for the Organization.\nRecently, the company is short on budget and has to make a cost reduction.\nYour manager asked you whether you can modify “All Features” to “Consolidated Billing” to save some cost.\nHow would you answer this question?",
  "answer" : "Correct Answer - D.\nAbout AWS Organization pricing, please refer to.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html.\nAWS Organization itself is a free service.\nUsers are only charged for the AWS resources in their accounts.\nOption A is incorrect: All Features cannot be switched back to Consolidated Billing.\nThis is described in.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html.\nOption B is incorrect: Similar to.\nOption A.Option C is incorrect: AWS Organization is a free service regardless of how many OUs in the Organization.\nOption D is CORRECT: After All Features is enabled, it cannot be changed back.\nBesides, since it is a free service, you have to consider other AWS services to save some costs.\nThe correct answer to this question is A. All attached SCPs need to be detached first in the Organization before All Features is changed to Consolidated Billing. This can also save some cost as Consolidated Billing is a free feature.\nWhen an AWS Organization is created, the default feature set is \"All Features,\" which includes Consolidated Billing and Service Control Policies (SCPs). However, it is possible to disable some of these features to customize the Organization according to specific needs.\nIn this case, the Organization was initially configured with only Consolidated Billing enabled. Later on, the need arose to use SCPs to provide central control over the Organization's features. SCPs allow organizations to set boundaries and rules for what AWS resources and services can be used within the Organization.\nIf an Organization with SCPs enabled wants to switch from \"All Features\" to \"Consolidated Billing\" only, all attached SCPs need to be detached first. SCPs can limit or allow specific AWS services and resources, and switching to Consolidated Billing alone without detaching SCPs can cause unwanted charges.\nConsolidated Billing is a free feature of AWS, and it allows organizations to consolidate billing and payment for multiple AWS accounts. On the other hand, \"All Features\" includes not only Consolidated Billing but also SCPs, which can help enforce policies across the Organization's accounts and resources.\nIn conclusion, if the company wants to switch from \"All Features\" to \"Consolidated Billing\" only to save costs, the recommended approach is to detach all SCPs first. This will not only ensure a smooth transition but also save some costs as Consolidated Billing is a free feature. However, it is important to note that AWS Organization itself is a free service, as long as there are more than 5 Organizational Units (OUs) in the Organization.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "All attached SCPs need to be detached first in the Organization before All Features is changed to Consolidated Billing. This can also save some cost as Consolidated Billing is a free feature.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All Features can be switched to Consolidated Billing as long as the user has proper IAM permission. However, it will not save any cost as Consolidated Billing and All Features charge the same.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users can switch between All Features and Consolidated Billing anytime. However, this will not save any cost since AWS Organization is offered without charge as long as there are more than 5 OUs in the Organization.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "All Features cannot be switched back to Consolidated Billing. Besides, AWS Organization is a free service, so it is not required to modify it.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 441,
  "query" : "You are in charge of maintaining an Oracle database in RDS.\nThe development team tells you that the database instance needs to communicate with a new S3 bucket.\nFor example, it should be able to save the backup files to S3, and at the same time, it can fetch Oracle Data Pump files from the same bucket when required.\nTo connect the database instance with the particular S3 bucket successfully, which options are the prerequisites? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - A, E.\nAbout how to transfer files between RDS Oracle and S3, please refer to the reference in https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/oracle-s3-integration.html.\nOption A is CORRECT: Because the IAM role is required to allow the DB instance to interact with the S3 bucket.\nOption B is incorrect: Because parameter group is not a necessary condition for this scenario.\nOption C is incorrect: Because a new subnet group is not required.\nThe default subnet group also works.\nOption D is incorrect: Because the S3 bucket does not need public access.\nThe RDS and S3 can communicate with each other under the same VPC.Option E is CORRECT: Because an option group that includes the S3_INTEGRATION option should be attached with the DB instance.\nCheck the below screenshot on how to add the S3_INTEGRATION option to the option group:\nTo connect an Oracle database instance in Amazon RDS to an S3 bucket, you need to configure several prerequisites. These are:\nA. Configure an IAM role with a policy that allows to read and write the S3 bucket objects. Associate the role with the RDS instance.\nYou need to create an IAM role that allows the RDS instance to read and write objects in the S3 bucket. The policy should grant permissions to access the bucket, list objects in the bucket, and perform read and write operations on the objects. Once the role and policy are created, you need to associate the role with the RDS instance.\nB. Create a parameter group that includes the parameter of the IAM S3 service role name. Attach the parameter group with the DB instance.\nYou also need to create a parameter group that includes the parameter of the IAM S3 service role name. This parameter specifies the IAM role that allows RDS to access the S3 bucket. After creating the parameter group, you need to attach it to the DB instance.\nC. Configure a new DB subnet group. Link the DB instance with the subnet group.\nIf the RDS instance is not already linked to a DB subnet group, you need to create a new one and associate it with the RDS instance. The DB subnet group defines the subnets in which the RDS instance is located. The subnets must be in the same VPC as the S3 bucket.\nD. For the new S3 bucket, ensure the Block Public Access settings are turned off. Enable the public access for the S3 bucket or add the S3 endpoint for the bucket.\nYou also need to ensure that the Block Public Access settings are turned off for the S3 bucket. This allows the RDS instance to access the bucket. If you don't want to enable public access, you can add the S3 endpoint for the bucket to the VPC route table.\nE. Create an option group that includes the S3_INTEGRATION option. Associate the DB instance with the option group.\nFinally, you need to create an option group that includes the S3_INTEGRATION option. This option enables the RDS instance to access the S3 bucket. After creating the option group, you need to associate it with the DB instance.\nIn summary, to connect an Oracle database instance in RDS to an S3 bucket, you need to create an IAM role with a policy that allows access to the bucket, create a parameter group that includes the IAM S3 service role name, link the DB instance with a DB subnet group, ensure that Block Public Access is turned off for the S3 bucket, and create an option group that includes the S3_INTEGRATION option and associate it with the DB instance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Configure an IAM role with a policy that allows to read and write the S3 bucket objects. Associate the role with the RDS instance.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a parameter group that includes the parameter of the IAM S3 service role name. Attach the parameter group with the DB instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a new DB subnet group. Link the DB instance with the subnet group.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "For the new S3 bucket, ensure the Block Public Access settings are turned off. Enable the public access for the S3 bucket or add the S3 endpoint for the bucket.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an option group that includes the S3_INTEGRATION option. Associate the DB instance with the option group.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 442,
  "query" : "In your organization, your DevOps team is in charge of provisioning resources in an AWS account.\nTim was a team member and created a Customer Managed Key in KMS several months ago.\nThe default key policy is removed, and the key policy is as below.",
  "answer" : "Correct Answer - A.\nAbout the default and recommended key policies in KMS, check the AWS documentation in https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html#key-policy-default.\nThe default key policy is as below:\n{\n\"Sid\": \"Enable IAM User Permissions\",\n\"Effect\": \"Allow\",\n\"Principal\": {\"AWS\": \"arn:aws:iam::111122223333:root\"},\n\"Action\": \"kms:*\",\n\"Resource\": \"*\"\n}\nThis allows the permissions of the key to be managed by IAM policies.\nOption A is CORRECT: Because even the root user cannot manage it.\nYou have to contact AWS Support to restore it.\nOption B is incorrect: Because the root user cannot manage the key policy either as the user is not allowed to do that.\nOption C is incorrect: Because the key policy cannot be modified by any IAM user anymore.\nOption D is incorrect: Because the key policy still denies the action even if the IAM user has an IAM policy to allow it.\nIn this scenario, Tim, a team member of the DevOps team, created a Customer Managed Key (CMK) in KMS several months ago. However, the default key policy has been removed, and the key policy has been modified. As a result, the DevOps team has lost access to the CMK, and they need to regain access to it.\nOption A, \"Contact AWS Support to regain access to the CMK,\" is not the best option in this scenario. Although AWS Support can help, it may take time to get a response, and the team may need to take immediate action.\nOption B, \"Log in as the root user of the AWS account and add another user as the key administrator,\" is not the best option either. In general, logging in as the root user is not recommended because it provides too much power and access. Additionally, this option does not explain how to recover the key policy that was removed.\nOption C, \"Use the IAM admin user to edit the key policy to allow all actions for the principal of arn:aws:iam::111122223333:root. Add other IAM users as key administrators or users if required,\" is the best option in this scenario. This option involves editing the key policy to add back the necessary permissions. It specifies adding back the principal of arn:aws:iam::111122223333:root, which allows the root user to have access to the CMK. The option also mentions adding other IAM users as key administrators or users if required, which is a good practice for team management.\nOption D, \"Create an IAM policy that allows the action of kms:PutKeyPolicy and attach the policy to an IAM user. Login into AWS console with the user and modify the key policy to the default one,\" is not the best option because it involves creating an IAM policy to modify the key policy. This option does not provide a clear solution for recovering the key policy that was removed.\nTherefore, Option C is the best answer to this question.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Contact AWS Support to regain access to the CMK.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Log in as the root user of the AWS account and add another user as the key administrator.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use the IAM admin user to edit the key policy to allow all actions for the principal of arn:aws:iam::111122223333:root. Add other IAM users as key administrators or users if required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an IAM policy that allows the action of kms:PutKeyPolicy and attach the policy to an IAM user. Login into AWS console with the user and modify the key policy to the default one.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 443,
  "query" : "A company has a new S3 bucket that stores very sensitive files.\nThese objects are supposed to be used only by IAM admin user.\nOther IAM users or roles should not have access.\nUsers in other AWS accounts cannot assume any role in reading the S3 objects either.\nYou plan to use the S3 bucket policy to apply the security rules.\nWhich option is the most secure one?",
  "answer" : "Correct Answer - B.\nExplicit deny should be considered as it takes the highest priority even if the action is explicitly allowed somewhere else.\nOptions A and C are eliminated.\nFor option D, it is not realistic to list all users and roles to deny the action.\nThe only option left is option.\nB.\nAbout how to use NotPrincipal, refer to https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_notprincipal.html.\nOption A is incorrect because other users can access the bucket as well if they have an Allow in their IAM policies.\nExplicit deny should be used.\nOption B is CORRECT With the policy, only the user Admin and root can access the bucket objects.\nOther IAM entities are denied.\nOption C is incorrect because it is unsuitable to use Allow with NotPrincipal.\nAny IAM users or roles which are not in the NotPrincipal list can access the objects.\nOption D is incorrect because you have to list all IAM users and roles in the Principal list, which is not appropriate.\nThe most secure option for the S3 bucket policy in this scenario is option A.\nExplanation: Option A specifies an \"Allow\" statement that grants access to only the IAM admin user and the root account, which are specified in the \"Principal\" element. The \"Action\" element is set to \"s3:*\" which allows access to all actions on the S3 bucket. The \"Resource\" element specifies the ARN of the S3 bucket and all objects within it.\nOption B, on the other hand, uses a \"Deny\" statement that denies access to all IAM users and roles except for the IAM admin user and the root account. While this seems secure, it is generally recommended to use \"Allow\" statements instead of \"Deny\" statements as they can be difficult to manage and troubleshoot in larger environments.\nOption C specifies an \"Allow\" statement that grants access to all IAM users and roles except for User1, User2, and so on, specified in the \"NotPrincipal\" element. While this would prevent users outside of the specified list from accessing the S3 bucket, it is still less secure than option A, which explicitly specifies the allowed IAM users and roles.\nOption D uses a \"Deny\" statement that denies access to User1, User2, and so on, specified in the \"Principal\" element. This would prevent the specified IAM users from accessing the S3 bucket, but it does not explicitly allow the IAM admin user and the root account, so it is less secure than option A. Additionally, as with option B, it is generally recommended to use \"Allow\" statements instead of \"Deny\" statements.\nIn conclusion, option A is the most secure option as it explicitly grants access only to the IAM admin user and the root account using an \"Allow\" statement.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": [ \"arn:aws:iam::444455556666:user/Admin\", \"arn:aws:iam::444455556666:root\" ]}, \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::BUCKETNAME\", \"arn:aws:s3:::BUCKETNAME/*\" ] }] }",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Deny\", \"NotPrincipal\": {\"AWS\": [ \"arn:aws:iam::444455556666:user/Admin\", \"arn:aws:iam::444455556666:root\" ]}, \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::BUCKETNAME\", \"arn:aws:s3:::BUCKETNAME/*\" ] }] }",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"NotPrincipal\": {\"AWS\": [ \"arn:aws:iam::444455556666:user/User1\", \"arn:aws:iam::444455556666:user/User2\", … \"arn:aws:iam::444455556666:user/UserX\" ]}, \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::BUCKETNAME\", \"arn:aws:s3:::BUCKETNAME/*\" ] }] }",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Deny\", \"Principal\": {\"AWS\": [ \"arn:aws:iam::444455556666:user/User1\", \"arn:aws:iam::444455556666:user/User2\", … \"arn:aws:iam::444455556666:user/UserX\" ]}, \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::BUCKETNAME\", \"arn:aws:s3:::BUCKETNAME/*\" ] }] }",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 444,
  "query" : "As an AWS Solutions Architect, you need to configure an identity service in AWS based on SAML.\nSince you already have a SAML identity provider outside of AWS, you plan to use the same IdP to manage user identities.\nTo create the SAML identity provider in IAM, the below steps may be required.",
  "answer" : "Correct Answer - D.\nPlease check https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_saml.html on how to create IAM SAML identity providers.\nOption A is incorrect: Because assume-role-with-web-identity is not used for SAML identity provider.\nStep 2 should not be included.\nOption B is incorrect: Same reason as.\nOption A.Option C is incorrect: Before creating the identity provider, you need to get the metadata document from IdP:\nOption D is CORRECT: After creating the identity provider in step 4, you need to configure the IdP side as steps 1 and 5 which add relying party trust between the IdP and AWS.\nTo configure an identity service in AWS based on SAML and use an existing SAML identity provider outside of AWS, you need to create a SAML identity provider in IAM. The steps required to create the SAML identity provider in IAM are:\n1.\nConfigure the SAML identity provider: In this step, you need to configure the SAML identity provider in IAM. To do this, you need to provide the metadata document of the SAML identity provider, which contains information about the identity provider, such as the entity ID, the endpoints for various SAML profiles, and the certificates used to sign the SAML messages.\n2.\nCreate a SAML provider entity in IAM: Once you have configured the SAML identity provider, you need to create a SAML provider entity in IAM. This entity represents the SAML identity provider in IAM and allows you to map SAML attributes to IAM roles and policies.\n3.\nCreate an IAM role for SAML authentication: In this step, you need to create an IAM role that allows users to assume the role after they have authenticated through the SAML identity provider. You can specify the SAML provider entity that you created in the previous step as the trusted entity for the role.\n4.\nMap SAML attributes to IAM roles and policies: In this step, you need to map the SAML attributes that are sent by the SAML identity provider to IAM roles and policies. This allows you to control access to AWS resources based on the attributes of the authenticated user.\n5.\nTest the SAML-based authentication: Once you have completed the above steps, you should test the SAML-based authentication to ensure that it is working as expected. You can do this by logging in to the AWS Management Console using the SAML identity provider.\nBased on the above steps, the correct sequence of steps required to create a SAML identity provider in IAM is option C: 1 -> 4 -> 3 -> 5. Option A is incorrect because it incorrectly places step 1 after step 4. Option B is incorrect because it incorrectly places step 3 before step 1. Option D is incorrect because it incorrectly places step 3 before step 4.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "4 -> 1 -> 5 -> 2",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "3 -> 1 -> 4 -> 2",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "1 -> 4 -> 3 -> 5",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "3 -> 4 -> 1 -> 5",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 445,
  "query" : "Your team is developing an Android app.\nYou need to use an Amazon Cognito Identity Pool to create unique identities for the app users and federate them with the identity provider from Google.\nYou also want to allow unauthenticated guest access for the application.\nGuests can get temporary tokens for limited access.\nHow would you implement the guest access using Amazon Cognito?",
  "answer" : "Correct Answer - A.\nAbout how to use Identity Pool for federated identities in AWS Cognito, please refer to.\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\nThe authentication flow can be found in https://docs.aws.amazon.com/en_pv/cognito/latest/developerguide/authentication-flow.html.\nOption A is CORRECT: Because the unauthenticated access feature can be enabled in Cognito Identity Pool as below:\nOption B is incorrect: Because the feature of unauthenticated identities is managed in Cognito Identity Pool instead of User Pool.\nOption C is incorrect: Because the GetCredentialsForIdentity API does not have a request type of Guest.\nOption D is incorrect: Because the GetOpenIdToken API does not provide the temporary tokens.\nUsers can request a new temporary identity ID via the GetId API.\nSure, I can provide you with a detailed explanation of how to implement guest access using Amazon Cognito for your Android app.\nAmazon Cognito is a service that provides authentication, authorization, and user management for your web and mobile apps. Cognito supports user sign-up, sign-in, and access control. Cognito Identity Pool is a Cognito component that enables you to create unique identities for your app users and federate them with identity providers such as Google, Facebook, or Amazon.\nTo implement guest access using Amazon Cognito, you can follow these steps:\n1.\nCreate an Amazon Cognito Identity Pool: You can create an Amazon Cognito Identity Pool by going to the Amazon Cognito console and clicking on \"Manage Federated Identities.\" From there, you can create a new identity pool and specify the identity providers you want to use, including Google.\n2.\nEnable unauthenticated access: Once you have created the identity pool, you need to enable unauthenticated access. Enabling unauthenticated access allows guest users to access your app without signing in. To enable unauthenticated access, go to the \"Authentication providers\" tab of your identity pool and select \"Enable access to unauthenticated identities.\"\n3.\nImplement guest access: To implement guest access, you need to use the Amazon Cognito Identity Pool API to request temporary credentials for your guest users. You can use the GetOpenIdToken API to get temporary tokens for your guest users. These temporary tokens allow your guest users to access your app with limited privileges.\nIn summary, the correct answer to the question is D. Guest users can request temporary tokens by using the GetOpenIdToken API. By using this API, you can provide temporary access to your app for guest users, while still maintaining secure authentication and authorization for your authenticated users.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Enable the unauthenticated access in Cognito Identity Pool. Guest users can request an identity ID via the GetId API.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Activate the unauthenticated access feature in Cognito User Pool. Link the User Pool in Cognito Identity Pool.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Users can get guest credentials via the GetCredentialsForIdentity API with a request type as Guest.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Guest users can request temporary tokens by using the GetOpenIdToken API.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 446,
  "query" : "You create an EBS snapshot for an application in non-production AWS account A.\nThe snapshot is encrypted by a customer-managed key (CMK-A)\nTo deploy the same application in the production AWS account B, you need to create an AMI using the snapshot and launch an EC2 instance.\nThe IAM admin user in account B is allowed to use CMK-A.\nHowever, the production EC2 instance has to use its own customer-managed key (CMK-B) to encrypt the EBS volume.\nWhich solution is the best?",
  "answer" : "E.\nF.\nCorrect Answer - B.\nWhen sharing an Amazon EBS snapshot between accounts, there are cases that a new CMK has to be used.\nReferences can be found in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html and https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html.\nOption A is incorrect: Because encrypted snapshots cannot be copied to non-encrypted ones.\nOption B is CORRECT: Because when creating snapshots, the new snapshot can be encrypted with a new CMK.\nAWS CLI command copy-snapshot uses the option of --KMS-key-id to specify the CMK.\nThe steps to create a proper encrypted EC2/AMI in account B are:\n1\nShare the EBS snapshot with account B.\n2\nThe IAM admin user in account B, copy the EBS snapshot from A to B (since it is shared and we have access to CMK A that works just fine) using CMK.\nB.3\nThe IAM admin user in account B create EC2 AMI out from the copied EBS Snapshot.\nOption C is incorrect: Because you need to create a copy of the snapshot, and you cannot just share the snapshot between Account \"A\" and Account \"B\".\nOption D is incorrect: Because you need to share the snapshot between Account \"A\" and Account \"B\"\nWithout sharing, you cannot create the image in Account \"B\".\nThe best solution for this scenario would be option B - Create an encrypted version of the snapshot (w/ CMK-A) and then create an AMI using the encrypted snapshot. Launch an EC2 instance using the AMI and encrypt the EBS volume with CMK-B.\nHere's a detailed explanation for each option:\nA. Copy the snapshot to another one and do not encrypt it. Share the new snapshot to account. This option is not recommended because the snapshot may contain sensitive data that needs to be encrypted. Moreover, it is not a secure practice to share unencrypted snapshots with other accounts.\nB. Create an encrypted version of the snapshot (w/ CMK-A) and then create an AMI using the encrypted snapshot. Launch an EC2 instance using the AMI and encrypt the EBS volume with CMK-B. This option is the best solution because it allows the user to create an AMI using the encrypted snapshot while still being able to encrypt the EBS volume with CMK-B. This ensures that the data is secure while being transferred from non-production account A to production account B. The IAM admin user in account B is allowed to use CMK-A, which means that the user can encrypt the snapshot with CMK-A before creating the AMI.\nC. Share the snapshot with account B and encrypt it with CMK-B This option is not recommended because it does not allow the user to create an AMI using the snapshot. Moreover, the snapshot may contain sensitive data that needs to be encrypted, and sharing unencrypted snapshots is not a secure practice.\nD. Create an AMI using the new snapshot and launch an EC2 instance. This option is not recommended because it does not allow the user to encrypt the EBS volume with CMK-B. The data in the EBS volume may contain sensitive information that needs to be protected, and using an unencrypted EBS volume is not a secure practice.\nE. Create an image in account B and change the encryption key to CMK-B. This option is not recommended because it does not allow the user to use the snapshot created in non-production account A. Creating a new image would require additional time and resources, and it may not contain the same data as the original snapshot.\nF. Launch an EC2 instance using the image. This option is not recommended because it does not allow the user to encrypt the EBS volume with CMK-B. The data in the EBS volume may contain sensitive information that needs to be protected, and using an unencrypted EBS volume is not a secure practice.\nIn conclusion, the best solution for this scenario is to create an encrypted version of the snapshot with CMK-A and then create an AMI using the encrypted snapshot. Launch an EC2 instance using the AMI and encrypt the EBS volume with CMK-B to ensure that the data is secure during transfer from non-production account A to production account B.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Copy the snapshot to another one and do not encrypt it. Share the new snapshot to account.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Created an encrypted version of the snapshot (w/ CMK-A) and then create an AMI using the encrypted snapshot. Launch an EC2 instance using the AMI and encrypt the EBS volume with CMK-",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Share the snapshot with account B and encrypt it with CMK-",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an AMI using the new snapshot and launch an EC2 instance.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create an image in account B and change the encryption key to CMK-",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Launch an EC2 instance using the image.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 447,
  "query" : "You use AWS Cognito User Pool to configure a user directory for an application.\nYou want to separate different users as readers, contributors, and editors of the app.\nFor example, the readers can only read contents from AWS S3 buckets.\nContributors can put contents into Amazon S3 buckets, and editors have the permissions to publish contents through an API in Amazon API Gateway.\nWhich method is the best to achieve this requirement in AWS Cognito?",
  "answer" : "Correct Answer - C.\nIn Amazon Cognito User Pool, you can configure users in groups to manage the permissions better.\nEach group can be linked with an IAM role ARN.\nThe reference can be found in https://docs.aws.amazon.com/en_pv/cognito/latest/developerguide/cognito-user-pools-user-groups.html.\nOption A is incorrect: Because users should be added into groups in the Cognito User Pool instead of IAM.\nOption B is incorrect: Because users in Cognito User Pool cannot be configured directly with an IAM role.\nOption C is CORRECT: Check the below example:\nGroup1 is linked with an IAM role, and User1 is added into the group.\nOption D is incorrect: Similar to Option B, it is inappropriate to attach an IAM policy to each user directly.\nThe best method to achieve this requirement in AWS Cognito is option C: In Amazon Cognito User Pool, create groups and assign IAM roles to them. Add users to the groups to assign the required permissions.\nAmazon Cognito User Pool provides user authentication and authorization, and can integrate with other AWS services such as Amazon S3, Amazon API Gateway, and IAM to control access to resources.\nOption A suggests using IAM groups to assign suitable IAM policies, and then assigning users to the IAM groups in Amazon Cognito User Pool. However, IAM policies are not directly assignable to Amazon Cognito User Pool users. It is more appropriate to use IAM policies to define permissions for specific AWS services, such as Amazon S3 or Amazon API Gateway.\nOption B suggests configuring different IAM roles for readers, contributors, and editors in IAM and then assigning each user with an IAM role in Amazon Cognito User Pool. However, IAM roles are intended to grant permissions to AWS resources, and may not be the best fit for this use case.\nOption D suggests attaching an IAM policy directly to each user in Amazon Cognito User Pool. While this approach is possible, it can be cumbersome to manage if there are many users with different roles and permissions.\nOption C is the best method to achieve the requirements. In Amazon Cognito User Pool, create groups for readers, contributors, and editors. For each group, assign an IAM role that has the necessary permissions for the corresponding AWS service. Then, add users to the appropriate groups to assign the required permissions. This approach is scalable and easy to manage, and provides fine-grained access control for users in different roles.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In IAM, add different groups and assign suitable IAM policies. In Amazon Cognito User Pool, assign users to the IAM groups.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure different IAM roles in IAM for readers, contributors and editors. In Amazon Cognito User Pool, configure each user with an IAM role.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In Amazon Cognito User Pool, create groups and assign IAM roles to them. Add users to the groups to assign the required permissions.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Directly attach an IAM policy to each user in Amazon Cognito User Pool. Make sure each user has an appropriate IAM policy according to the user role.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 448,
  "query" : "Your team creates a Customer Managed Key (CMK) in KMS in an AWS account (111122223333)\nThe key is supposed to be used by another account (444455556666) for encryption and decryption operations.\nAt the moment, it is known that only IAM user Bob and IAM role Admin in the account (444455556666) need access.\nWhich configurations are required together to achieve this requirement? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, D.\nCheck https://docs.aws.amazon.com/en_pv/kms/latest/developerguide/key-policy-modifying-external-accounts.html on how to allow users or roles in other accounts to use a CMK.\nOption A is incorrect: Because this key policy gives the external account (or users and roles in the external account) permission and violates the 'principle of least privilege.'\nOption B is CORRECT: Because this follows the 'principle of least privilege' by giving permission only to \"Admin\" and \"Bob\" as per the requirements stated.\nOption C is incorrect: Because only \"arn:aws:iam::444455556666:root\" is not required.\nOption D is CORRECT: In account 444455556666, IAM policies can be used to configure the permissions to use the key in another account.\nThis option will work together with.\nOption A to assign permissions.\nOption E is incorrect: Because the field should be \"Resource\" instead of \"Principal\" in IAM policies as it controls which resources to be allowed for IAM entities.\nThe correct answers for this question are A and B.\nExplanation: To allow IAM user Bob and IAM role Admin in the account (444455556666) to use the Customer Managed Key (CMK) created in KMS in another AWS account (111122223333) for encryption and decryption operations, two configurations are required together:\nA. Edit the Principal of the key policy as below: \"Principal\": { \"AWS\": [ \"arn:aws:iam::444455556666:root\" ] }\nThis configuration allows the AWS account 444455556666 root user to use the CMK. However, it's not recommended to grant access to the root user unless it's necessary.\nB. Edit the Principal of the key policy as below: \"Principal\": { \"AWS\": [ \"arn:aws:iam::444455556666:role/Admin\", \"arn:aws:iam::444455556666:user/Bob\" ] }\nThis configuration allows IAM user Bob and IAM role Admin in the account (444455556666) to use the CMK. This is a better approach as it follows the least privilege principle.\nC. Edit the Principal of the key policy as below: \"Principal\": { \"AWS\": [ \"arn:aws:iam::444455556666:root\", \"arn:aws:iam::444455556666:role/Admin\", \"arn:aws:iam::444455556666:user/Bob\" ] }\nOption C is not necessary because option B already includes the required IAM user and IAM role.\nD. In account 444455556666, allow the KMS encryption and decryption actions as below: \"Resource\": \"arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab\"\nOption D is not necessary because the question states that the key is supposed to be used by another account (111122223333), and the question specifically asks for the required configurations to achieve the given requirement.\nE. In account 444455556666, allow the KMS encryption and decryption actions as below: \"Principal\": \"arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab\"\nOption E is incorrect because it specifies the principal as the CMK itself, which doesn't make sense. The key policy should specify the AWS account or IAM user/role that's allowed to use the CMK.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Edit the Principal of the key policy as below: \"Principal\": { \"AWS\": [ \"arn:aws:iam::444455556666:root\" ] }",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Edit the Principal of the key policy as below: \"Principal\": { \"AWS\": [ \"arn:aws:iam::444455556666:role/Admin\", \"arn:aws:iam::444455556666:user/Bob\" ] }",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Edit the Principal of the key policy as below: \"Principal\": { \"AWS\": [ \"arn:aws:iam::444455556666:root\", \"arn:aws:iam::444455556666:role/Admin\", \"arn:aws:iam::444455556666:user/Bob\" ] }",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In account 444455556666, allow the KMS encryption and decryption actions as below: \"Resource\": \"arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab\"",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In account 444455556666, allow the KMS encryption and decryption actions as below: \"Principal\": \"arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab\"",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 449,
  "query" : "You work in a DevOps team, and your team maintains several applications deployed in AWS.\nAt the moment, there are dozens of server certificates stored in IAM.\nThese certificates are used for different purposes and have different expiry date.\nYou have to renew the certificates before they expire.\nOtherwise, the services will be impacted.\nYou want to use another approach to renew and manage these certificates.\nWhich method is the best?",
  "answer" : "Correct Answer - B.\nCheck https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html on how to manage server certificates in IAM and ACM.\nOption A is incorrect: Because for the imported server certificates in IAM, there is no IAM console to manage them.\nThis is one major disadvantage of managing certificates in IAM.\nOption B is CORRECT: Because ACM is a preferred solution.\nCertificates requested by ACM are free and automatically renew.\nOption C is incorrect: Because you cannot migrate the certificates from IAM to ACM directly.\nThere is no such console to do that.\nFor ACM, you can import third-party certificates to the service.\nOption D is incorrect: Because ACM cannot automatically renew imported third-party certificates.\nYou are responsible for monitoring the expiration date.Please check the reference in.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html\nOption B is the best method to renew and manage server certificates in AWS. Provisioning and managing server certificates in AWS Certificate Manager (ACM) is a recommended approach to secure your application's communication with HTTPS. AWS Certificate Manager takes care of the renewal process of the certificates for you automatically.\nACM is a service that lets you provision, manage, and deploy Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. ACM is integrated with several AWS services such as Elastic Load Balancing, CloudFront, Elastic Beanstalk, and API Gateway.\nWith ACM, you don't need to worry about renewing your SSL/TLS certificates manually or tracking the expiration dates of your certificates. ACM will automatically renew the certificates before they expire, and it will replace the expired certificates with new ones.\nOption A is not recommended since IAM is not designed to manage SSL/TLS certificates. It is a best practice to use a service that is specifically designed to manage certificates like AWS Certificate Manager.\nOption C is a possible solution, but it requires manual effort to migrate the certificates from IAM to ACM. Also, you would still need to create a renewal strategy manually in ACM to ensure that the certificates are renewed before they expire.\nOption D is not the best method since importing all third-party certificates into ACM is not always feasible. Additionally, it is not recommended to manage third-party certificates using a service that is specifically designed for managing certificates that are issued by AWS.\nIn summary, the best method to renew and manage server certificates in AWS is to provision and manage them using AWS Certificate Manager.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the IAM console, add a new strategy for server certificates to renew one month before the expiry date automatically.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Provision and manage the server certificates in AWS Certificate Manager (ACM). The certificates requested from ACM are automatically renewed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In IAM console, migrate the certificates from IAM to ACM then ACM can automatically renew the certificates one month before the expiry date.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Import all third-party certificates into ACM. ACM is responsible for the automatic renew for both third-party certificates and ACM provided certificates.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 450,
  "query" : "You start to use AWS Certificate Manager to manage certificates.\nAnd some existing certificates in IAM will not be used or will be replaced by the new ones in ACM.\nYou just create a new public server certificate for a domain name called www.example.com.\nHowever, the status of the certificate is “Pending validation”\nWhich option can be used by ACM as a validation approach? (Select TWO.)",
  "answer" : "E.\nCorrect Answer - B, D.\nRefer to the below approaches on how to validate pending certificates in ACM:\nhttps://docs.aws.amazon.com/en_pv/acm/latest/userguide/gs-acm-validate-dns.html https://docs.aws.amazon.com/en_pv/acm/latest/userguide/gs-acm-validate-email.html\nOption A is incorrect: Because ACM does not send SMS to validate if a user owns the domain name or not.\nOption B is CORRECT: User can choose either DNS validation or Email validation as below:\nOption C is incorrect: Because domain names registered through Route53 still need to be validated.\nOption D is CORRECT: The below email will be received as an email validation:\nOption E is incorrect: Because there is no Pending Validation icon.\nYou have to use either DNS validation or Email validation.\nAWS Certificate Manager (ACM) is a service provided by Amazon Web Services that enables the users to manage SSL/TLS certificates to secure their websites and applications that run on AWS services. When a user requests a new certificate, the certificate status becomes \"Pending validation\" until the ownership of the domain name can be verified. ACM provides several options for domain name validation.\nThe options for domain name validation are as follows:\nA. ACM sends an SMS to the registered phone number of the domain name. A user can validate it by clicking the validation URL. This option is useful when the domain owner wants to validate the ownership of the domain via SMS. When the user requests the validation of the domain, ACM sends an SMS with a validation URL to the phone number associated with the domain name. The user can click on the URL and validate the domain ownership.\nB. Use DNS to validate the domain ownership. You can insert ACM generated CNAME records into your DNS database. This option is useful when the domain owner wants to validate the ownership of the domain via DNS. When the user requests the validation of the domain, ACM generates a CNAME record that needs to be inserted into the DNS database of the domain. Once the CNAME record is verified, the domain ownership is considered valid.\nC. If the domain name is registered by Route53, no validation action is required. Just wait for a while, and it will be automatically validated. This option is useful when the domain owner has registered the domain name via Route53. ACM automatically validates the domain ownership if the domain name is registered via Route53. The user does not need to take any action and wait for a while, and the domain ownership will be considered valid.\nD. ACM sends emails to the contact addresses of the domain name. You can validate the domain owner in the email. This option is useful when the domain owner wants to validate the ownership of the domain via email. When the user requests the validation of the domain, ACM sends an email to the contact address associated with the domain name. The user can follow the instructions in the email and validate the domain ownership.\nE. If the domain name is registered by Route53, a Pending Validation icon is generated in the Registered Domains. Just click the Pending Validation icon, and ACM will modify the certificate status to Issued. This option is useful when the domain owner has registered the domain name via Route53. If the domain ownership is not automatically validated, the user can click on the Pending Validation icon in the Registered Domains section of Route53. Clicking the icon validates the domain ownership, and ACM modifies the certificate status to Issued.\nIn summary, ACM provides several options for domain name validation to ensure the security of the SSL/TLS certificates. The user can choose the option that is most suitable for their needs.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "ACM sends an SMS to the registered phone number of the domain name. A user can validate it by clicking the validation URL.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Use DNS to validate the domain ownership. You can insert ACM generated CNAME records into your DNS database.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "If the domain name is registered by Route53, no validation action is required. Just wait for a while, and it will be automatically validated.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "ACM sends emails to the contact addresses of the domain name. You can validate the domain owner in the email.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "If the domain name is registered by Route53, a Pending Validation icon is generated in the Registered Domains. Just click the Pending Validation icon, and ACM will modify the certificate status to Issued.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 451,
  "query" : "You implement a REST API using a Lambda Function.\nThe API is exposed through the AWS API gateway.\nThe Lambda Function calls a third-party service to retrieve the data.\nBut this third-party service may not respond in time.\nYou already increased the timeout of the Lambda Function to be 15 minutes.\nHowever, sometimes users still get an HTTP 504 error after about 30 seconds.\nWhich of the following options is the most possible reason?",
  "answer" : "Correct Answer - C.\nPlease check https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html for the Amazon API Gateway limits.\nThe integration timeout is 50 milliseconds - 29 seconds for all integration types.\nAnd the limit cannot be increased.\nOption A is incorrect: Because HTTP 504 is Gateway Timeout instead of Internal Server Error.\nOption B is incorrect: When the third-party service does not respond in time, the API Gateway reaches its limit after 29 seconds.\nIt is not the third-party service that returns the HTTP 504 error code in this scenario.\nOption C is CORRECT: Because the API call through API Gateway cannot exceed 29 seconds due to the limits.\nThis is the most possible reason to cause the issue.\nOption D is incorrect: Because the limit of Lambda Function can be 15 minutes according to https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\nThe limit of API Gateway has caused the issue.\nThe most possible reason for users still getting an HTTP 504 error after about 30 seconds, despite having increased the timeout of the Lambda Function to 15 minutes, is that the maximum integration timeout for AWS API Gateway is 29 seconds.\nOption A is incorrect because an HTTP 504 error indicates a Gateway Timeout error, not an Internal Server Error. Checking the CloudWatch Logs of the Lambda Function would not provide any additional information in this case.\nOption B is partially correct, as the third-party service may indeed be returning an HTTP 504 error after 30 seconds, but the Lambda Function is not simply forwarding the error to the API Gateway. The Lambda Function is timing out after 15 minutes, but the API Gateway is hitting its maximum integration timeout of 29 seconds, resulting in the HTTP 504 error being returned to the user.\nOption C is the most likely reason for the HTTP 504 error, as the maximum integration timeout for AWS API Gateway is 29 seconds. This means that even though the timer for the Lambda Function has not yet expired, the API Gateway has already hit its limitation and returned the HTTP 504 error to the user.\nOption D is incorrect because when a Lambda Function integrates with API Gateway, the effective Lambda Function execution time limit is equal to the timeout set for the API Gateway integration. In this case, the maximum integration timeout for the API Gateway is the limiting factor, not the Lambda Function execution time limit.\nTo resolve the issue, the integration timeout for the AWS API Gateway needs to be increased to a value higher than the expected response time of the third-party service. Alternatively, caching or other performance optimization techniques may be used to reduce the response time of the third-party service.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The error code indicates that there is an Internal Server Error. Check the CloudWatch Logs of the Lambda Function.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The third-party service has returned an HTTP 504 error after 30 seconds to the Lambda Function. Then the Lambda Function forwards the error to the API Gateway.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The maximum of integration timeout for AWS API Gateway is 29 seconds. Although the timer for Lambda Function does not expire yet, the API Gateway already hits its limitation.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "When Lambda Function integrates with API Gateway, the effective Lambda Function execution time limit is 30 seconds although the limit is set to 15 minutes.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 452,
  "query" : "You have deployed a REST API in Amazon API Gateway.\nAnd the API integrates with a DynamoDB table.\nThe table has a large amount of data.\nFor certain queries, it may take over 10 seconds to finish.\nYou want to set a custom timeout of 5 seconds for the API so that users do not have to wait a long time for the response.\nHow should you implement this?",
  "answer" : "Correct Answer - D.\nUsers can set up the API Integration Request using the API Gateway console.\nDetails can be found in .https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html.\nThe below screenshot is an example of configuring the integration timeout:\nOption A is incorrect: Because this cannot guarantee that the query time is less than 5 seconds.\nThe custom timeout needs to be modified through API Gateway.\nOption B is incorrect: Because the integration timeout of API gateway can be configured between 50 milliseconds and 29 seconds.\nOption C is incorrect: Because this option does not configure the integration timeout for the REST API.\nOption D is CORRECT: This option creates an integration timeout for the DynamoDB table.\nIf the processing time in DynamoDB is longer than 5 seconds, the incoming request is disconnected by the API Gateway.\nThe correct answer for this question is D. In the integration request of the API gateway, uncheck the “Use Default Timeout” box and configure the custom timeout to be 5 seconds.\nExplanation:\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It can integrate with various AWS services, including DynamoDB.\nIn this scenario, the REST API is integrated with a DynamoDB table that contains a large amount of data. Some queries may take over 10 seconds to finish, which can cause a long wait time for users. To address this, the API should have a custom timeout of 5 seconds.\nOption A is incorrect because configuring DynamoDB Accelerator (DAX) in the DynamoDB table can improve the performance of the DynamoDB queries. However, it does not affect the timeout of the API Gateway.\nOption B is incorrect because the timeout of API Gateway can be customized. By default, the timeout is 30 seconds, but it can be adjusted up to 29 seconds. However, the recommended approach is to set a lower timeout for a specific API resource if necessary.\nOption C is incorrect because increasing the DynamoDB read capacity can improve the performance of DynamoDB queries, but it does not affect the timeout of the API Gateway.\nOption D is the correct answer because it allows you to configure a custom timeout for the API Gateway. To set a custom timeout, you need to uncheck the “Use Default Timeout” box in the integration request and enter the desired timeout value (in seconds). In this case, the value should be set to 5 seconds to meet the requirement.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "You can configure the DynamoDB Accelerator (DAX) in the DynamoDB table so that the table can process the queries from the API Gateway faster.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The timeout of API gateway is 30 seconds and cannot be customized. Define a custom timeout of 5 seconds at the application or function level.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Enlarge the DynamoDB read capacity to a higher value to shorten the waiting time for the users of the REST API.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the integration request of the API gateway, uncheck the “Use Default Timeout” box and configure the custom timeout to be 5 seconds.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 453,
  "query" : "You are developing a REST API in Amazon API Gateway.\nThe API integrates with a Lambda Function.\nYou found that when clients hit the API using an unsupported method, the API Gateway has returned a Missing Authentication Token (403) response.\nAs the response does not provide much information, you want to customize the response.\nFor example, you want to change the error code to 404 and add some custom HTTP headers.\nHow would you implement this?",
  "answer" : "Correct Answer - A.\nUsers can customize the gateway response using the API Gateway console according to\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-gateway-response-using-the-console.html.\nOption A is CORRECT: Users can select the particular Missing Authentication Token 403 gateway response under the API and customize it as below screenshot:\nOption B is incorrect: In this scenario, it needs to modify the default gateway response.\nThe integration response of the API method should not be changed.\nOption C is incorrect: Because in the AWS API Gateway console, users cannot forward a gateway response to another.\nInstead, the status code, headers and bodies should be customized.\nOption D is incorrect: Similar to option.\nB.\nThe method configuration should not be modified.\nIt does not change the API Gateway behavior when clients hit an unsupported method.\nThe correct answer is A. In the AWS API Gateway console, choose Gateway Responses under the API and select the 403 response. Add custom headers and modify the response status code to 404.\nWhen a client hits an API using an unsupported method, the API Gateway returns a Missing Authentication Token (403) response. This response does not provide much information, so you may want to customize it. Specifically, you want to change the error code to 404 and add some custom HTTP headers.\nTo implement this, you need to customize the Gateway Response. Gateway Responses are a set of HTTP responses that can be returned by API Gateway when there is an error or when a request does not match any of the defined API methods.\nHere are the steps to customize the Gateway Response:\n1. Go to the AWS API Gateway console.\n2. Select your API and choose Gateway Responses from the left-hand menu.\n3. Select the 403 response from the list of responses.\n4. Click on the pencil icon to edit the response.\n5. Modify the response status code to 404.\n6. Add the custom HTTP headers that you want to include.\n7. Click Save to save your changes.\nAfter you have customized the Gateway Response, API Gateway will return the modified response whenever a client hits the API using an unsupported method.\nOption B is incorrect because modifying the integration response would only modify the response returned from the Lambda function. It would not modify the Gateway Response returned by API Gateway.\nOption C is incorrect because forwarding the Missing Authentication Token 403 response to an HTTP 404 Not Found response would not be a valid solution. The client is not requesting a resource that is not found, but rather using an unsupported HTTP method.\nOption D is also incorrect because forwarding the 403 response to a 404 response is not a valid solution. It does not accurately represent the error that occurred, and it could cause confusion for clients consuming the API. Additionally, the custom headers should be added to the Gateway Response, not the forwarded response.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the AWS API Gateway console, choose Gateway Responses under the API and select the 403 response. Add custom headers and modify the response status code to 404.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "In the API method`s integration response, add the custom headers and modify the status code to 404.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "In the AWS API Gateway console, select the Missing Authentication Token 403 response. Forward it to an HTTP 404 Not Found response. Select the 404 response and add custom headers accordingly.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Select the API method and click the integration response. Forward the 403 response to a 404 Not Found response. In the meantime, add the custom headers in the forwarded response.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 454,
  "query" : "You implement an API in AWS API Gateway.\nThe API integrates with a Lambda Function which returns the query results from an RDS database.\nFor security purposes, you want the API to allow traffic only from a VPC endpoint since it should be used internally.\nThe VPC endpoint ID is vpc-11bb22cc.\nWhat is the best method to implement this?",
  "answer" : "Correct Answer - B.\nAmazon API Gateway resource policies are used to control whether a specified principal can invoke the API.\nOption A is incorrect: Because vpce-11bb22cc can not be used as a Principal.\nThe field of Principal should be “*”.\nOption B is CORRECT: Because it allows all Principals to use the API and then denies the actions if the SourceVpce is not vpce-11bb22cc.\nThis policy guarantees that only the traffic from vpce-11bb22cc can access the API.\nOption C is incorrect: Because explicit deny is better than explicit allow as it has a clear boundary.\nOther entities may still be able to access this API if there are explicit allows in their IAM policies.\nOption D is incorrect: Because the explicit deny takes priorities.\nSo the first part of the policy denies all the traffic, including the requests from the endpoint of vpce-11bb22cc.\nReferences:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html.\nThe correct answer is option C.\nExplanation:\nThe scenario describes an API implemented in AWS API Gateway that integrates with a Lambda function that returns query results from an RDS database. The API should only allow traffic from a specific VPC endpoint for internal use.\nAPI Gateway provides resource policies that can be used to allow or deny access to API resources based on specific conditions. In this case, the condition is that the traffic should only come from the VPC endpoint with ID vpc-11bb22cc.\nOption A specifies that the principal is the VPC endpoint ID. However, this is incorrect because the principal should be the source of the traffic. The VPC endpoint ID should be used in the condition instead.\nOption B includes a Deny statement that blocks access to the API from all sources except the VPC endpoint. However, this can cause unintended consequences in the future if additional sources need access to the API. This option also includes unnecessary permissions by allowing access from all principals. Therefore, this option is incorrect.\nOption D includes a Deny statement before the Allow statement, which can also cause unintended consequences in the future. This option is not the best practice for implementing a resource policy.\nOption C is the correct answer because it allows access only from the specific VPC endpoint with ID vpc-11bb22cc. The policy statement in option C specifies the following:\nThe Effect is set to Allow, which allows access to the resource.\nThe Principal is set to *, which means any principal is allowed access to the resource.\nThe Action is set to execute-api:Invoke, which allows invocation of the API Gateway method.\nThe Resource is set to the ARN of the API Gateway resource.\nThe Condition is set to aws:SourceVpce, which specifies that access is allowed only if the request is from the specified VPC endpoint with ID vpc-11bb22cc.\nTherefore, option C is the best method to implement the required security measures for the API Gateway resource to allow traffic only from the specified VPC endpoint for internal use.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Implement the below policy in the API Gateway Resource Policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": \"vpce-11bb22cc\", \"Action\": \"execute-api:Invoke\", \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/*\" ] } ] }",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Implement the below policy in the API Gateway Resource Policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"execute-api:Invoke\", \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/*\" ] }, { \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"execute-api:Invoke\", \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/*\" ], \"Condition\" : { \"StringNotEquals\": { \"aws:SourceVpce\": \"vpce-11bb22cc\" } } } ] }",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Implement the below policy in the API Gateway Resource Policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"execute-api:Invoke\", \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/*\" ], \"Condition\" : { \"StringEquals\": { \"aws:SourceVpce\": \"vpce-11bb22cc\" } } } ] }",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Implement the below policy in the API Gateway Resource Policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"execute-api:Invoke\", \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/*\" ] }, { \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"execute-api:Invoke\", \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/*\" ], \"Condition\" : { \"StringEquals\": { \"aws:SourceVpce\": \"vpce-11bb22cc\" } } } ] }",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 455,
  "query" : "You are developing a mobile application.\nAs it is a proof of concept, you want to deliver the project as soon as possible and you prefer using a serverless pattern.\nThe whole application contains the presentation tier, the logical tier and the database tier.\nThe component in the presentation tier is the mobile application running on a user device.\nFor the logical tier and database tier, which services should you use as the serverless mobile backend?",
  "answer" : "Correct Answer - D.\nAs the serverless pattern is required, we should choose the serverless services that AWS fully manages.\nOption A is incorrect: Because Amazon RDS MySQL is not a pure serverless service.\nUsers still need to configure the database instance type, storage and provisioned IOPS.\nDetails can be found in https://d1.awsstatic.com/whitepapers/AWS_Serverless_Multi-Tier_Architectures.pdf.\nOption B is incorrect: Because AWS ECS with EC2 instance is not a serverless service, ECS with Fargate can be a serverless service.\nIn this scenario, Amazon API Gateway with AWS Lambda is the most appropriate for the logic tier.\nOption C is incorrect: Because AWS Auto Scaling Group and Amazon DocumentDB are not serverless services.\nOption D is CORRECT: Because all the mentioned services are serverless.\nUsers do not need to create or maintain servers for the whole architecture.\nCheck https://d1.awsstatic.com/whitepapers/AWS_Serverless_Multi-Tier_Architectures.pdf on how to create AWS serverless multi-tier architectures via Amazon API Gateway and AWS Lambda.\nOption D is the correct answer.\nThe logical tier contains Amazon API Gateway and AWS Lambda. API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. Lambda functions can be triggered by API Gateway endpoints.\nThe API Gateway endpoints are secured via Amazon Cognito user pools, which provides a secure user directory that scales to hundreds of millions of users. Cognito user pools provide user authentication and authorization services, allowing users to sign up, sign in, and manage their profile information.\nAmazon DynamoDB is used for the data tier. DynamoDB is a fully managed NoSQL database service that can handle any amount of traffic and scale up or down automatically. DynamoDB is a great fit for mobile applications as it provides low latency and high throughput for read and write operations.\nOverall, this architecture is a good fit for a proof of concept mobile application that needs to be delivered quickly. The serverless pattern allows for easy scaling and cost savings, and the use of fully managed services means that developers can focus on writing code rather than managing infrastructure.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "The logical tier contains Amazon API Gateway and AWS Lambda. The API Gateway endpoints are secured via Amazon Certificate Manager. Amazon RDS MySQL is used for the data tier.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The logic tier contains Amazon API Gateway and AWS ECS with EC2 instances. The API Gateway endpoints are secured via third-party identity providers. The data tier includes the resource of Amazon DynamoD.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The logic tier contains Amazon CloudFront and AWS Auto Scaling Group. Amazon Cognito identity pools provide identity services. Amazon DocumentDB is used for the data tier.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "The logic tier contains Amazon API Gateway and AWS Lambda. The API Gateway endpoints are secured via Amazon Cognito user pools. Amazon DynamoDB is used for the data tier.",
    "correct" : true,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 456,
  "query" : "You have built a web API in AWS using API Gateway, AWS Lambda, and DynamoDB.\nFor the DynamoDB table, the read and write capacities have been provisioned as 100\nRecently, you found ProvisionedThroughputExceededException for some client requests, which mainly happens when the traffic is high during the daytime.\nWhich of the following actions can help to reduce the number of exceptions? (Select TWO)",
  "answer" : "E.\nAnswer - A, C.\nIn this scenario, to reduce the number of ProvisionedThroughputExceededException, there are two approaches.\nOne is to enlarge the capacities of the DynamoDB table.\nThe other is to reduce the traffic.\nOption A is CORRECT: Because DynamoDB supports auto-scaling, can be used to enlarge capacity.\nTake the below snapshot as an example:\nOption B is incorrect: Because adding retires can potentially increase the traffic, and it does not help reduce the number of exceptions.\nOption C is CORRECT: Because the SQS queue can control the requests made to the DynamoDB table through asynchronous processing and it acts as a cache that controls the rate of requests to the table.\nOption D is incorrect: Because CloudFront does not work with DynamoDB.\nInstead, Elasticache can be used as a caching layer for DynamoDB.Option E is incorrect: Because users can directly enable DynamoDB auto scaling rather than configure it manually.\nThe ProvisionedThroughputExceededException occurs when the provisioned capacity of a DynamoDB table is exceeded. In this case, the table is provisioned with 100 read and write capacities. However, during high traffic periods, this capacity may not be sufficient to handle the requests, which causes the exception.\nTo reduce the number of ProvisionedThroughputExceededExceptions, the following actions can be taken:\nA. Enable auto-scaling for the read and write capacities in the DynamoDB table: Auto-scaling allows the read and write capacities to increase or decrease automatically based on the traffic to the table. This ensures that the table can handle the traffic during peak periods and reduces the likelihood of the ProvisionedThroughputExceededException.\nB. Add retries to the AWS Lambda function: When a ProvisionedThroughputExceededException occurs, retries can be added to the AWS Lambda function. This will ensure that the request is retried after a certain amount of time. If the capacity issue is resolved during this time, the request will be successful.\nC. Configure an SQS queue to control the traffic to the DynamoDB table asynchronously: This involves putting extra messages in an SQS queue before they reach the DynamoDB table. The messages can be processed by the table at a pace that it can handle. This approach can help reduce the likelihood of ProvisionedThroughputExceededExceptions.\nD. Configure AWS CloudFront as a caching layer in front of DynamoDB: CloudFront can cache frequently accessed data from the DynamoDB table, reducing the number of requests to the table. This can help reduce the likelihood of ProvisionedThroughputExceededExceptions.\nE. Create a CloudWatch alarm for ProvisionedThroughputExceededException and enlarge the provisioned capacities if the alarm is triggered: A CloudWatch alarm can be set up to monitor the number of ProvisionedThroughputExceededExceptions. If the alarm is triggered, the provisioned capacities can be increased to handle the traffic.\nIn summary, to reduce the likelihood of ProvisionedThroughputExceededExceptions for a DynamoDB table in AWS, it is recommended to enable auto-scaling, add retries to the AWS Lambda function, use an SQS queue to control the traffic, configure AWS CloudFront as a caching layer, and create a CloudWatch alarm for monitoring the issue.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the DynamoDB table, enable auto-scaling for the read and write capacities.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Adding retries to the AWS Lambda function if ProvisionedThroughputExceededException happens.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure an SQS queue to control the traffic to the DynamoDB table asynchronously. The extra messages are put in the queue first before they reach the table.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Configure AWS CloudFront as a caching layer in front of DynamoDB to reduce traffic to the DynamoDB table.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Create a CloudWatch alarm for ProvisionedThroughputExceededException. Enlarge the provisioned capacities if the alarm is triggered.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 457,
  "query" : "You create a Lambda Function to process clients' queries from a web application.\nThe workload is time sensitive so that you want the Lambda function to finish processing and respond as quickly as possible.\nYou prefer allocating more resources such as memories or CPUs to the Lambda Function.\nHowever, you are unsure how many resources are being used at the moment.\nWhich action can help you to get the current memory usage and allocate more resources if needed?",
  "answer" : "Correct Answer - B.\nCheck https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html on how to configure Lambda Functions.\nUsers can allocate memories for a Lambda Function as below:\nOption A is incorrect: We cannot use the default CloudWatch metrics to get the allocated and used memory.\nOption B is CORRECT: We can get the max memory used from the CloudWatch logs.\nIf the max memory used is close to the memory allocated, it is better to allocate more memories to speed up the processing.\nOption C is incorrect: Because the reserved concurrency does not influence how many memories or CPUs the Lambda Function has to process a request.\nOption D is incorrect: Although Lambda Function is serverless, users are still responsible for monitoring the running status and allocating more resources if needed.\nThe correct answer is B. Check CloudWatch Logs to get the max memory used and allocate more memory if needed.\nExplanation:\nWhen you create a Lambda Function, you specify the amount of memory and CPU allocated to it. The more memory and CPU you allocate, the faster the function will execute. However, if you allocate too much memory, you will waste resources and pay more than necessary. On the other hand, if you allocate too little memory, the function may run slowly or not at all.\nTo optimize the performance of your Lambda Function, you need to monitor its resource usage and adjust the allocation as needed. One way to do this is by checking the CloudWatch Logs for your function.\nThe CloudWatch Logs for a Lambda Function include a log stream for each invocation of the function. Each log stream includes a log message that shows the amount of memory and CPU allocated to the function and the amount of memory used during the invocation.\nBy analyzing the CloudWatch Logs, you can determine the maximum amount of memory used by your function and adjust the memory allocation accordingly. If you find that your function frequently runs out of memory, you can increase the memory allocation to improve its performance.\nChecking the default CloudWatch metrics (Answer A) can provide some information about the allocated and used memories, but it won't give you the detailed information needed to optimize the performance of your function. Similarly, checking the max number of invocations (Answer C) won't tell you anything about the resource usage of your function.\nAnswer D is incorrect because while Lambda Function resources can auto-scale, it is still up to the user to determine the initial amount of resources to allocate and monitor their usage to optimize performance.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "Check the default CloudWatch metrics to get the allocated and used memories. Pre-allocate more resources to the Lambda Function if required.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Check CloudWatch Logs to get the max memory used and allocate more memory if needed.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Check CloudWatch metrics to get the max number of invocations. Increase the reserved concurrency to speed up the processing time.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Memories and CPUs in Lambda Function are managed by AWS and can auto-scale if required. You are not supposed to allocate more resources by yourself.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 458,
  "query" : "You are developing a new Lambda Function.\nSometimes the function invocation may fail, such as when retries are exhausted or the event age has been exceeded.\nFor these failed events, you want to route the asynchronous function failed results to a destination resource automatically without writing additional code.\nWhich of the following destination is supported?",
  "answer" : "Correct Answer - A.\nLambda asynchronous invocations can put an event or message on Amazon Simple Notification Service (SNS), Amazon Simple Queue Service (SQS), or Amazon EventBridge for further processing.\nCheck the explanations in.\nhttps://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\nOption A is CORRECT because the EventBridge event bus can be configured as a destination of asynchronous function results.\nOptions B, C and D are incorrect because Lambda functions cannot route execution results to these services.\nWhen developing a new Lambda Function, there may be scenarios where the function invocation fails due to various reasons, such as when retries are exhausted or the event age has been exceeded. In such cases, it is essential to automatically route these failed events to a destination resource without writing additional code.\nTo achieve this, AWS provides several destination resources to route asynchronous function failed results. Among the given options, the destination resource that is supported for this purpose is the EventBridge event bus (Option A).\nEventBridge is a serverless event bus service offered by AWS that makes it easy to connect different AWS services with application data. It allows you to route events between various AWS services and SaaS applications in real-time. EventBridge enables you to build event-driven architectures that are scalable and flexible.\nBy using EventBridge with Lambda, you can create event rules that trigger when a Lambda function throws an exception or returns an error. You can configure the event rules to send the failed events to various target resources, such as Amazon SNS, AWS Step Functions, AWS Lambda functions, and other supported services.\nWith EventBridge, you can create a rule that triggers whenever a Lambda function invocation fails. When the rule is triggered, the failed event is forwarded to the destination resource, such as an SNS topic or a Lambda function, where you can process and analyze the event data further.\nIn summary, EventBridge event bus is the supported destination resource to automatically route asynchronous function failed results from Lambda Functions without writing additional code.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "EventBridge event bus",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "AWS Step Functions",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Amazon MQ",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "AWS SWF.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 459,
  "query" : "You have deployed a new Lambda Function and an RDS MySQL database.\nThe Lambda is not configured in any VPC and has internet access.\nAs the Lambda Function needs to communicate with the RDS database to fetch data, you configure the Lambda Function in the private subnet where the database resides.\nHowever, after that, the Lambda Function has lost the internet connection.\nHow would you resolve this problem?",
  "answer" : "E.\nCorrect Answer - A.\nRefer to https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html for how to configure a Lambda Function to access resources in a VPC.\nYou can configure the Lambda Function to access to a VPC as below:\nOption A is CORRECT: Because when a function is attached in the private subnet, it does not have access to the internet unless the VPC provides access.\nThe NAT Gateway can provide the internet access for the resources in the private gateway.\nOption B is incorrect: In this question, the Lambda function needs the internet access.\nIt does not mention that it needs to access other AWS services.\nVPC endpoints cannot fulfill the requirements.\nOption C is incorrect: Because if the Lambda function is not attached to the VPC, it cannot communicate with the RDS instance.\nHowever, the Lambda function needs to fetch data from the RDS database.\nOption D is incorrect: Because the NAT gateway should be used for the resources in the private subnet to connect to the internet.\nAttaching an internet gateway in the public subnet does not help.\nThe scenario in the question describes a Lambda function that needs to access an RDS MySQL database residing in a private subnet. After configuring the Lambda function in the private subnet, it has lost internet access. To resolve this issue, we need to consider the following:\n1.\nPrivate Subnet and Internet Access: By default, resources deployed in a private subnet cannot access the internet directly. To access the internet, we need to use a NAT (Network Address Translation) gateway or a proxy server.\n2.\nNAT Gateway: A NAT gateway is a managed service by AWS that allows resources within a private subnet to access the internet. It acts as an intermediary between the private subnet and the internet. When a resource in the private subnet needs to access the internet, it sends its traffic to the NAT gateway, which then forwards the traffic to the internet. The NAT gateway replaces the private IP address with a public IP address and vice versa. To use a NAT gateway, we need to deploy it in a public subnet and route the outbound traffic from the private subnet to the NAT gateway.\n3.\nVPC Endpoints: A VPC endpoint is another option for enabling internet access to a Lambda function in a private subnet. It allows resources within a VPC to communicate with AWS services such as S3 and DynamoDB over a private connection rather than going through the public internet. VPC endpoints are secure and provide high-bandwidth, low-latency connections to AWS services.\n4.\nInternet Gateway: An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in a VPC and the internet. An internet gateway enables instances in a private subnet to communicate with the internet using public IP addresses. To use an internet gateway, we need to attach it to a public subnet and modify the route table to use it for outbound traffic.\nNow, let's look at each of the answer options in more detail:\nA. In the VPC private subnet, route the outbound traffic to a NAT gateway in a public subnet. This is the correct answer. By routing the outbound traffic from the Lambda function to a NAT gateway in a public subnet, the Lambda function can access the internet. The NAT gateway will handle the translation of private IP addresses to public IP addresses.\nB. Create VPC endpoints for all the services that the Lambda Function needs to access. This is also a valid option but is not the best solution in this case. Creating VPC endpoints only allows the Lambda function to access specific AWS services privately. It does not enable the Lambda function to access the internet.\nC. Do not attach the VPC access to the Lambda Function as, by default, the Lambda Function has the internet access. This option is not valid as it contradicts the scenario given in the question. The question states that the Lambda function is deployed without VPC access and, therefore, does not have internet access.\nD. Attach an Internet gateway to the public subnet of the VPC. This option is not valid as attaching an internet gateway to the public subnet of the VPC does not enable the Lambda function to access the internet directly. It only allows resources in the public subnet to access the internet.\nE. Modify the route table to use the internet gateway for all inbound and outbound traffic. This option is not valid as modifying the route table to use the internet gateway for all inbound and outbound traffic enables internet access to all resources in the VPC, including the public subnet. It is not a secure solution and could potentially expose the resources to security threats.\nIn conclusion, the correct answer to the question is A. In the VPC private subnet, route the outbound traffic to a NAT gateway in a public subnet. This allows the Lambda function to access the internet through the NAT gateway while maintaining the security of the resources in the private",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "In the VPC private subnet, route the outbound traffic to a NAT gateway in a public subnet.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create VPC endpoints for all the services that the Lambda Function needs to access.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Do not attach the VPC access to the Lambda Function as, by default, the Lambda Function has the internet access.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Attach an Internet gateway to the public subnet of the VP.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Modify the route table to use the internet gateway for all inbound and outbound traffic.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
}, {
  "id" : 460,
  "query" : "You are developing an application using a microservices architecture.\nSome serverless AWS services such as Lambda, SQS and DynamoDB are used.\nOne Lambda Function is deployed to reset users' passwords, but it does not run frequently.\nWhen the function is inactive and called, it may take a longer time to handle the requests.\nYou want to minimize the processing time for the function.\nWhich of the following Lambda \"features\" you can use to get greater control over the performance of your serverless applications at any scale?",
  "answer" : "Correct Answer - C.\nIf the Lambda Function is not used for a long time, AWS may recycle the container.\nAnd if there are new requests to the function, AWS needs to deploy the container again for the function.\nIn order to pre-warm the Lambda function, the best way is to use the \"Provisioned Concurrency\" which is a \"feature\" of the Lambda function.\nhttps://aws.amazon.com/about-aws/whats-new/2019/12/aws-lambda-announces-provisioned-concurrency/\nOptions A, B and D are incorrect: Because these are NOT a \"feature\" of the Lambda function.\nThe correct answer is option C: Configure the Lambda Function to use \"Provisioned Concurrency\" always to stay active so that the Lambda container does not get reused by AWS.\nExplanation:\nIn a microservices architecture, it's essential to ensure that each service is optimized for maximum performance. In this case, we want to minimize the processing time for the Lambda Function that resets users' passwords, especially when the function is inactive.\nProvisioned Concurrency is a feature that allows you to pre-warm a set number of Lambda containers to handle incoming requests. It's different from On-Demand Concurrency, where AWS manages the number of containers needed to handle requests. With Provisioned Concurrency, you have control over the number of containers that are running, and they remain warm and ready to handle requests at all times.\nUsing Provisioned Concurrency, we can keep the Lambda Function warm and active even when it's not in use. This ensures that the function is always ready to handle incoming requests, minimizing the processing time. Additionally, because the containers remain warm, there is no additional cold start time when the function is invoked, which further improves performance.\nOption A is incorrect because the language used by the function does not affect its performance. However, some languages may have longer cold start times than others, which could affect the function's performance when it's not in use.\nOption B is incorrect because while warming up a function with CloudWatch events can help reduce the cold start time, it does not keep the function active when it's not in use.\nOption D is incorrect because triggering a CloudWatch alarm to call the Lambda Function defeats the purpose of using serverless architecture, where resources are only provisioned when needed. Additionally, it could result in unnecessary charges for unused resources.",
  "marked" : 0,
  "timespent" : 0,
  "options" : [ {
    "text" : "For the Lambda Function, avoid using languages such as Java that need a long compilation time. Use Node.js or Python instead.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure a CloudWatch Event rule and select the Lambda Function as its target. The event is executed every minute to warm up the function so that the function stays active.",
    "correct" : false,
    "selected" : false
  }, {
    "text" : "Configure the Lambda Function to use \"Provisioned Concurrency\" always to stay active so that the Lambda container does not get reused by AWS.",
    "correct" : true,
    "selected" : false
  }, {
    "text" : "Create a CloudWatch alarm to monitor the invocations of the function. If there are no invocations for 1 hour, trigger the alarm to call the Lambda Function so that the function stays warm.",
    "correct" : false,
    "selected" : false
  } ],
  "objectives" : [ ]
} ]